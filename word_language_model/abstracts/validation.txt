

To choose a multi-winner rule, i.e., a voting rule that selects a subset of
$k$ alternatives based on preferences of a certain population, is a hard and
ambiguous task. Depending on the context, it varies widely what constitutes an
"optimal" committee. In this paper, we offer a new perspective to measure the
quality of committees and---consequently---multi-winner rules. We provide a
quantitative analysis using methods from the theory of approximation algorithms
and estimate how well multi-winner rules approximate two extreme objectives:
diversity as captured by the (Approval) Chamberlin--Courant rule (CC) and
individual excellence as captured by Approval Voting (AV). With both
theoretical and experimental methods we establish a classification of
multi-winner rules in terms of their quantitative alignment with these two
opposing objectives.



Literature involving preferences of artificial agents or human beings often
assume their preferences can be represented using a complete transitive binary
relation. Much has been written however on different models of preferences. We
review some of the reasons that have been put forward to justify more complex
modeling, and review some of the techniques that have been proposed to obtain
models of such preferences.



Artificial intelligence (AI) is an extensive scientific discipline which
enables computer systems to solve problems by emulating complex biological
processes such as learning, reasoning and self-correction. This paper presents
a comprehensive review of the application of AI techniques for improving
performance of optical communication systems and networks. The use of AI-based
techniques is first studied in applications related to optical transmission,
ranging from the characterization and operation of network components to
performance monitoring, mitigation of nonlinearities, and quality of
transmission estimation. Then, applications related to optical network control
and management are also reviewed, including topics like optical network
planning and operation in both transport and access networks. Finally, the
paper also presents a summary of opportunities and challenges in optical
networking where AI is expected to play a key role in the near future.



In supervised approaches for keyphrase extraction, a candidate phrase is
encoded with a set of hand-crafted features and machine learning algorithms are
trained to discriminate keyphrases from non-keyphrases. Although the
manually-designed features have shown to work well in practice, feature
engineering is a difficult process that requires expert knowledge and normally
does not generalize well. In this paper, we present SurfKE, a feature learning
framework that exploits the text itself to automatically discover patterns that
keyphrases exhibit. Our model represents the document as a graph and
automatically learns feature representation of phrases. The proposed model
obtains remarkable improvements in performance over strong baselines.



The Graph Brain Project is an experiment in how the use of automated
mathematical discovery software, databases, large collaboration, and systematic
investigation provide a model for how mathematical research might proceed in
the future.
  Our Project began with the development of a program that can be used to
generate invariant-relation and property-relation conjectures in many areas of
mathematics. This program can produce conjectures which are not implied by
existing (published) theorems. Here we propose a new approach to push forward
existing mathematical research goals---using automated mathematical discovery
software. We suggest how to initiate and harness large-scale collaborative
mathematics. We envision mathematical research labs similar to what exist in
other sciences, new avenues for funding, new opportunities for training
students, and a more efficient and effective use of published mathematical
research.
  And our experiment in graph theory can be imitated in many other areas of
mathematical research. Big Mathematics is the idea of large, systematic,
collaborative research on problems of existing mathematical interest. What is
possible when we put our skills, tools, and results together systematically?



The Semantic Web is becoming a large scale framework that enables data to be
published, shared, and reused in the form of ontologies. The ontology which is
considered as basic building block of semantic web consists of two layers
including data and schema layer. With the current exponential development of
ontologies in both data size and complexity of schemas, ontology understanding
which is playing an important role in different tasks such as ontology
engineering, ontology learning, etc., is becoming more difficult. Ontology
summarization as a way to distill knowledge from an ontology and generate an
abridge version to facilitate a better understanding is getting more attention
recently. There are various approaches available for ontology summarization
which are focusing on different measures in order to produce a proper summary
for a given ontology. In this paper, we mainly focus on the common metrics
which are using for ontology summarization and meet the state-of-the-art in
ontology summarization.



Conversational systems have come a long way after decades of research and
development, from Eliza and Parry in the 60's and 70's, to task-completion
systems as in the ATIS project, to intelligent personal assistants such as
Siri, and to today's social chatbots like XiaoIce. Social chatbots' appeal lies
in not only their ability to respond to users' diverse requests, but also in
being able to establish an emotional connection with users. The latter is done
by satisfying the users' essential needs for communication, affection, and
social belonging. The design of social chatbots must focus on user engagement
and take both intellectual quotient (IQ) and emotional quotient (EQ) into
account. Users should want to engage with the social chatbot; as such, we
define the success metric for social chatbots as conversation-turns per session
(CPS). Using XiaoIce as an illustrative example, we discuss key technologies in
building social chatbots from core chat to visual sense to skills. We also show
how XiaoIce can dynamically recognize emotion and engage the user throughout
long conversations with appropriate interpersonal responses. As we become the
first generation of humans ever living with AI, social chatbots that are
well-designed to be both useful and empathic will soon be ubiquitous.



The research on deep reinforcement learning which estimates Q-value by deep
learning has been attracted the interest of researchers recently. In deep
reinforcement learning, it is important to efficiently learn the experiences
that an agent has collected by exploring environment. In this research, we
propose NEC2DQN that improves learning speed of a poor sample efficiency
algorithm such as DQN by using good one such as NEC at the beginning of
learning. We show it is able to learn faster than Double DQN or N-step DQN in
the experiments of Pong.



Many works in collaborative robotics and human-robot interaction focuses on
identifying and predicting human behaviour while considering the information
about the robot itself as given. This can be the case when sensors and the
robot are calibrated in relation to each other and often the reconfiguration of
the system is not possible, or extra manual work is required. We present a deep
learning based approach to remove the constraint of having the need for the
robot and the vision sensor to be fixed and calibrated in relation to each
other. The system learns the visual cues of the robot body and is able to
localise it, as well as estimate the position of robot joints in 3D space by
just using a 2D color image. The method uses a cascaded convolutional neural
network, and we present the structure of the network, describe our own
collected dataset, explain the network training and achieved results. A fully
trained system shows promising results in providing an accurate mask of where
the robot is located and a good estimate of its joints positions in 3D. The
accuracy is not good enough for visual servoing applications yet, however, it
can be sufficient for general safety and some collaborative tasks not requiring
very high precision. The main benefit of our method is the possibility of the
vision sensor to move freely. This allows it to be mounted on moving objects,
for example, a body of the person or a mobile robot working in the same
environment as the robots are operating in.



Indian regional movie dataset is the first database of regional Indian
movies, users and their ratings. It consists of movies belonging to 18
different Indian regional languages and metadata of users with varying
demographics. Through this dataset, the diversity of Indian regional cinema and
its huge viewership is captured. We analyze the dataset that contains roughly
10K ratings of 919 users and 2,851 movies using some supervised and
unsupervised collaborative filtering techniques like Probabilistic Matrix
Factorization, Matrix Completion, Blind Compressed Sensing etc. The dataset
consists of metadata information of users like age, occupation, home state and
known languages. It also consists of metadata of movies like genre, language,
release year and cast. India has a wide base of viewers which is evident by the
large number of movies released every year and the huge box-office revenue.
This dataset can be used for designing recommendation systems for Indian users
and regional movies, which do not, yet, exist. The dataset can be downloaded
from \href{https://goo.gl/EmTPv6}{https://goo.gl/EmTPv6}.



Towards bridging the gap between machine and human intelligence, it is of
utmost importance to introduce environments that are visually realistic and
rich in content. In such environments, one can evaluate and improve a crucial
property of practical intelligent systems, namely \emph{generalization}. In
this work, we build \emph{House3D}, a rich, extensible and efficient
environment that contains 45,622 human-designed 3D scenes of houses, ranging
from single-room studios to multi-storeyed houses, equipped with a diverse set
of fully labeled 3D objects, textures and scene layouts, based on the SUNCG
dataset (Song et al., 2017). With an emphasis on semantic-level generalization,
we study the task of concept-driven navigation, \emph{RoomNav}, using a subset
of houses in House3D. In RoomNav, an agent navigates towards a target specified
by a semantic concept. To succeed, the agent learns to comprehend the scene it
lives in by developing perception, understand the concept by mapping it to the
correct semantics, and navigate to the target by obeying the underlying
physical rules. We train RL agents with both continuous and discrete action
spaces and show their ability to generalize in new unseen environments. In
particular, we observe that (1) training is substantially harder on large house
sets but results in better generalization, (2) using semantic signals (e.g.,
segmentation mask) boosts the generalization performance, and (3) gated
networks on semantic input signal lead to improved training performance and
generalization. We hope House3D, including the analysis of the RoomNav task,
serves as a building block towards designing practical intelligent systems and
we wish it to be broadly adopted by the community.



Recent work in deep reinforcement learning has allowed algorithms to learn
complex tasks such as Atari 2600 games just from the reward provided by the
game, but these algorithms presently require millions of training steps in
order to learn, making them approximately five orders of magnitude slower than
humans. One reason for this is that humans build robust shared representations
that are applicable to collections of problems, making it much easier to
assimilate new variants. This paper first introduces the idea of
automatically-generated game sets to aid in transfer learning research, and
then demonstrates the utility of shared representations by showing that models
can substantially benefit from the incorporation of relevant architectural
priors. This technique affords a remarkable 50x positive transfer on a toy
problem-set.



Cognition does not only depend on bottom-up sensor feature abstraction, but
also relies on contextual information being passed top-down. Context is higher
level information that helps to predict belief states at lower levels. The main
contribution of this paper is to provide a formalisation of perceptual context
and its integration into a new process model for cognitive hierarchies. Several
simple instantiations of a cognitive hierarchy are used to illustrate the role
of context. Notably, we demonstrate the use context in a novel approach to
visually track the pose of rigid objects with just a 2D camera.



In this thesis we propose new methods for crossover operator namely: cut on
worst gene (COWGC), cut on worst L+R gene (COWLRGC) and Collision Crossovers.
And also we propose several types of mutation operator such as: worst gene with
random gene mutation (WGWRGM) , worst LR gene with random gene mutation
(WLRGWRGM), worst gene with worst gene mutation (WGWWGM), worst gene with
nearest neighbour mutation (WGWNNM), worst gene with the worst around the
nearest neighbour mutation (WGWWNNM), worst gene inserted beside nearest
neighbour mutation (WGIBNNM), random gene inserted beside nearest neighbour
mutation (RGIBNNM), Swap worst gene locally mutation (SWGLM), Insert best
random gene before worst gene mutation (IBRGBWGM) and Insert best random gene
before random gene mutation (IBRGBRGM). In addition to proposing four selection
strategies, namely: select any crossover (SAC), select any mutation (SAM),
select best crossover (SBC) and select best mutation (SBM). The first two are
based on selection of the best crossover and mutation operator respectively,
and the other two strategies randomly select any operator. So we investigate
the use of more than one crossover/mutation operator (based on the proposed
strategies) to enhance the performance of genetic algorithms. Our experiments,
conducted on several Travelling Salesman Problems (TSP), show the superiority
of some of the proposed methods in crossover and mutation over some of the
well-known crossover and mutation operators described in the literature. In
addition, using any of the four strategies (SAC, SAM, SBC and SBM), found to be
better than using one crossover/mutation operator in general, because those
allow the GA to avoid local optima, or the so-called premature convergence.
Keywords: GAs, Collision crossover, Multi crossovers, Multi mutations, TSP.



This paper demonstrates the development of ontology for satellite databases.
First, I create a computational ontology for the Union of Concerned Scientists
(UCS) Satellite Database (UCSSD for short), called the UCS Satellite Ontology
(or UCSSO). Second, in developing UCSSO I show that The Space Situational
Awareness Ontology (SSAO) (Rovetto and Kelso 2016)--an existing space domain
reference ontology--and related ontology work by the author (Rovetto 2015,
2016) can be used either (i) with a database-specific local ontology such as
UCSSO, or (ii) in its stead. In case (i), local ontologies such as UCSSO can
reuse SSAO terms, perform term mappings, or extend it. In case (ii), the
author's orbital space ontology work, such as the SSAO, is usable by the UCSSD
and organizations with other space object catalogs, as a reference ontology
suite providing a common semantically-rich domain model. The SSAO, UCSSO, and
the broader Orbital Space Environment Domain Ontology project is online at
http://purl.org/space-ontology and GitHub. This ontology effort aims, in part,
to provide accurate formal representations of the domain for various
applications. Ontology engineering has the potential to facilitate the sharing
and integration of satellite data from federated databases and sensors for
safer spaceflight.



We present the Moments in Time Dataset, a large-scale human-annotated
collection of one million short videos corresponding to dynamic events
unfolding within three seconds. Modeling the spatial-audio-temporal dynamics
even for actions occurring in 3 second videos poses many challenges: meaningful
events do not include only people, but also objects, animals, and natural
phenomena; visual and auditory events can be symmetrical or not in time
("opening" means "closing" in reverse order), and transient or sustained. We
describe the annotation process of our dataset (each video is tagged with one
action or activity label among 339 different classes), analyze its scale and
diversity in comparison to other large-scale video datasets for action
recognition, and report results of several baseline models addressing
separately and jointly three modalities: spatial, temporal and auditory. The
Moments in Time dataset designed to have a large coverage and diversity of
events in both visual and auditory modalities, can serve as a new challenge to
develop models that scale to the level of complexity and abstract reasoning
that a human processes on a daily basis.



Current crowdsourcing platforms provide little support for worker feedback.
Workers are sometimes invited to post free text describing their experience and
preferences in completing tasks. They can also use forums such as Turker
Nation1 to exchange preferences on tasks and requesters. In fact, crowdsourcing
platforms rely heavily on observing workers and inferring their preferences
implicitly. In this work, we believe that asking workers to indicate their
preferences explicitly improve their experience in task completion and hence,
the quality of their contributions. Explicit elicitation can indeed help to
build more accurate worker models for task completion that captures the
evolving nature of worker preferences. We design a worker model whose accuracy
is improved iteratively by requesting preferences for task factors such as
required skills, task payment, and task relevance. We propose a generic
framework, develop efficient solutions in realistic scenarios, and run
extensive experiments that show the benefit of explicit preference elicitation
over implicit ones with statistical significance.



We present Chameleon, a novel hybrid (mixed-protocol) framework for secure
function evaluation (SFE) which enables two parties to jointly compute a
function without disclosing their private inputs. Chameleon combines the best
aspects of generic SFE protocols with the ones that are based upon additive
secret sharing. In particular, the framework performs linear operations in the
ring $\mathbb{Z}_{2^l}$ using additively secret shared values and nonlinear
operations using Yao's Garbled Circuits or the Goldreich-Micali-Wigderson
protocol. Chameleon departs from the common assumption of additive or linear
secret sharing models where three or more parties need to communicate in the
online phase: the framework allows two parties with private inputs to
communicate in the online phase under the assumption of a third node generating
correlated randomness in an offline phase. Almost all of the heavy
cryptographic operations are precomputed in an offline phase which
substantially reduces the communication overhead. Chameleon is both scalable
and significantly more efficient than the ABY framework (NDSS'15) it is based
on. Our framework supports signed fixed-point numbers. In particular,
Chameleon's vector dot product of signed fixed-point numbers improves the
efficiency of mining and classification of encrypted data for algorithms based
upon heavy matrix multiplications. Our evaluation of Chameleon on a 5 layer
convolutional deep neural network shows 133x and 4.2x faster executions than
Microsoft CryptoNets (ICML'16) and MiniONN (CCS'17), respectively.



We propose expected policy gradients (EPG), which unify stochastic policy
gradients (SPG) and deterministic policy gradients (DPG) for reinforcement
learning. Inspired by expected sarsa, EPG integrates (or sums) across actions
when estimating the gradient, instead of relying only on the action in the
sampled trajectory. For continuous action spaces, we first derive a practical
result for Gaussian policies and quadric critics and then extend it to an
analytical method for the universal case, covering a broad class of actors and
critics, including Gaussian, exponential families, and reparameterised policies
with bounded support. For Gaussian policies, we show that it is optimal to
explore using covariance proportional to the matrix exponential of the scaled
Hessian of the critic with respect to the actions. EPG also provides a general
framework for reasoning about policy gradient methods, which we use to
establish a new general policy gradient theorem, of which the stochastic and
deterministic policy gradient theorems are special cases. Furthermore, we prove
that EPG reduces the variance of the gradient estimates without requiring
deterministic policies and with little computational overhead. Finally, we show
that EPG outperforms existing approaches on six challenging domains involving
the simulated control of physical systems.



Monte Carlo inference has asymptotic guarantees, but can be slow when using
generic proposals. Handcrafted proposals that rely on user knowledge about the
posterior distribution can be efficient, but are difficult to derive and
implement. This paper proposes to let users express their posterior knowledge
in the form of proposal programs, which are samplers written in probabilistic
programming languages. One strategy for writing good proposal programs is to
combine domain-specific heuristic algorithms with neural network models. The
heuristics identify high probability regions, and the neural networks model the
posterior uncertainty around the outputs of the algorithm. Proposal programs
can be used as proposal distributions in importance sampling and
Metropolis-Hastings samplers without sacrificing asymptotic consistency, and
can be optimized offline using inference compilation. Support for optimizing
and using proposal programs is easily implemented in a sampling-based
probabilistic programming runtime. The paper illustrates the proposed technique
with a proposal program that combines RANSAC and neural networks to accelerate
inference in a Bayesian linear regression with outliers model.



In order to answer natural language questions over knowledge graphs, most
processing pipelines involve entity and relation linking. Traditionally, entity
linking and relation linking has been performed either as dependent sequential
tasks or independent parallel tasks. In this paper, we propose a framework
called "EARL", which performs entity linking and relation linking as a joint
single task. EARL uses a graph connection based solution to the problem. We
model the linking task as an instance of the Generalised Travelling Salesman
Problem (GTSP) and use GTSP approximate algorithm solutions. We later develop
EARL which uses a pair-wise graph-distance based solution to the problem.The
system determines the best semantic connection between all keywords of the
question by referring to a knowledge graph. This is achieved by exploiting the
"connection density" between entity candidates and relation candidates. The
"connection density" based solution performs at par with the approximate GTSP
solution.We have empirically evaluated the framework on a dataset with 5000
questions. Our system surpasses state-of-the-art scores for entity linking task
by reporting an accuracy of 0.65 to 0.40 from the next best entity linker.



Learning of user preferences, as represented by, for example, Conditional
Preference Networks (CP-nets), has become a core issue in AI research. Recent
studies investigate learning of CP-nets from randomly chosen examples or from
membership and equivalence queries. To assess the optimality of learning
algorithms as well as to better understand the combinatorial structure of
classes of CP-nets, it is helpful to calculate certain learning-theoretic
information complexity parameters. This paper determines bounds on or exact
values of some of the most central information complexity parameters, namely
the VC dimension, the (recursive) teaching dimension, the self-directed
learning complexity, and the optimal mistake bound, for classes of acyclic
CP-nets. We further provide an algorithm that learns tree-structured CP-nets
from membership queries. Using our results on complexity parameters, we assess
the optimality of our algorithm as well as that of another query learning
algorithm for acyclic CP-nets presented in the literature. Our algorithm is
near-optimal, and can, under certain assumptions be adapted to the case when
the membership oracle is faulty.



Trust is essential for human-robot collaboration and user adoption of
autonomous systems, such as robot assistants. This paper introduces a
computational model which integrates trust into robot decision-making.
Specifically, we learn from data a partially observable Markov decision process
(POMDP) with human trust as a latent variable. The trust-POMDP model provides a
principled approach for the robot to (i) infer the trust of a human teammate
through interaction, (ii) reason about the effect of its own actions on human
behaviors, and (iii) choose actions that maximize team performance over the
long term. We validated the model through human subject experiments on a
table-clearing task in simulation (201 participants) and with a real robot (20
participants). The results show that the trust-POMDP improves human-robot team
performance in this task. They further suggest that maximizing trust in itself
may not improve team performance.



In recent years, there have been tremendous advancements in the field of
machine learning. These advancements have been made through both academic as
well as industrial research. Lately, a fair amount of research has been
dedicated to the usage of generative models in the field of computer vision and
image classification. These generative models have been popularized through a
new framework called Generative Adversarial Networks. Moreover, many modified
versions of this framework have been proposed in the last two years. We study
the original model proposed by Goodfellow et al. as well as modifications over
the original model and provide a comparative analysis of these models.



Apart from few exceptions, the mathematical runtime analysis of evolutionary
algorithms is mostly concerned with expected runtimes. In this work, we argue
that stochastic domination is a notion that should be used more frequently in
this area. Stochastic domination allows to formulate much more informative
performance guarantees than the expectation alone, it allows to decouple the
algorithm analysis into the true algorithmic part of detecting a domination
statement and probability theoretic part of deriving the desired probabilistic
guarantees from this statement, and it allows simpler and more natural proofs.
  As particular results, we prove a fitness level theorem which shows that the
runtime is dominated by a sum of independent geometric random variables, we
prove tail bounds for several classic problems, and we give a short and natural
proof for Witt's result that the runtime of any $(\mu,p)$ mutation-based
algorithm on any function with unique optimum is subdominated by the runtime of
a variant of the (1+1) evolutionary algorithm on the OneMax function.



Traditional radio systems are strictly co-designed on the lower levels of the
OSI stack for compatibility and efficiency. Although this has enabled the
success of radio communications, it has also introduced lengthy standardization
processes and imposed static allocation of the radio spectrum. Various
initiatives have been undertaken by the research community to tackle the
problem of artificial spectrum scarcity by both making frequency allocation
more dynamic and building flexible radios to replace the static ones. There is
reason to believe that just as computer vision and control have been overhauled
by the introduction of machine learning, wireless communication can also be
improved by utilizing similar techniques to increase the flexibility of
wireless networks. In this work, we pose the problem of discovering low-level
wireless communication schemes ex-nihilo between two agents in a fully
decentralized fashion as a reinforcement learning problem. Our proposed
approach uses policy gradients to learn an optimal bi-directional communication
scheme and shows surprisingly sophisticated and intelligent learning behavior.
We present the results of extensive experiments and an analysis of the fidelity
of our approach.



Fuzzing is the process of finding security vulnerabilities in
input-processing code by repeatedly testing the code with modified inputs. In
this paper, we formalize fuzzing as a reinforcement learning problem using the
concept of Markov decision processes. This in turn allows us to apply
state-of-the-art deep Q-learning algorithms that optimize rewards, which we
define from runtime properties of the program under test. By observing the
rewards caused by mutating with a specific set of actions performed on an
initial program input, the fuzzing agent learns a policy that can next generate
new higher-reward inputs. We have implemented this new approach, and
preliminary empirical evidence shows that reinforcement fuzzing can outperform
baseline random fuzzing.



We propose Machines Talking To Machines (M2M), a framework combining
automation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents
for goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with
just a task schema and an API client from the dialogue system developer, but it
is also customizable to cater to task-specific interactions. Compared to the
Wizard-of-Oz approach for data collection, M2M achieves greater diversity and
coverage of salient dialogue flows while maintaining the naturalness of
individual utterances. In the first phase, a simulated user bot and a
domain-agnostic system bot converse to exhaustively generate dialogue
"outlines", i.e. sequences of template utterances and their semantic parses. In
the second phase, crowd workers provide contextual rewrites of the dialogues to
make the utterances more natural while preserving their meaning. The entire
process can finish within a few hours. We propose a new corpus of 3,000
dialogues spanning 2 domains collected with M2M, and present comparisons with
popular dialogue datasets on the quality and diversity of the surface forms and
dialogue flows.



We present AliMe Assist, an intelligent assistant designed for creating an
innovative online shopping experience in E-commerce. Based on question
answering (QA), AliMe Assist offers assistance service, customer service, and
chatting service. It is able to take voice and text input, incorporate context
to QA, and support multi-round interaction. Currently, it serves millions of
customer questions per day and is able to address 85% of them. In this paper,
we demonstrate the system, present the underlying techniques, and share our
experience in dealing with real-world QA in the E-commerce field.



In order for people to be able to trust and take advantage of the results of
advanced machine learning and artificial intelligence solutions for real
decision making, people need to be able to understand the machine rationale for
given output. Research in explain artificial intelligence (XAI) addresses the
aim, but there is a need for evaluation of human relevance and
understandability of explanations. Our work contributes a novel methodology for
evaluating the quality or human interpretability of explanations for machine
learning models. We present an evaluation benchmark for instance explanations
from text and image classifiers. The explanation meta-data in this benchmark is
generated from user annotations of image and text samples. We describe the
benchmark and demonstrate its utility by a quantitative evaluation on
explanations generated from a recent machine learning algorithm. This research
demonstrates how human-grounded evaluation could be used as a measure to
qualify local machine-learning explanations.



The dramatic success of deep neural networks across multiple application
areas often relies on experts painstakingly designing a network architecture
specific to each task. To simplify this process and make it more accessible, an
emerging research effort seeks to automate the design of neural network
architectures, using e.g. evolutionary algorithms or reinforcement learning or
simple search in a constrained space of neural modules.
  Considering the typical size of the search space (e.g. $10^{10}$ candidates
for a $10$-layer network) and the cost of evaluating a single candidate,
current architecture search methods are very restricted. They either rely on
static pre-built modules to be recombined for the task at hand, or they define
a static hand-crafted framework within which they can generate new
architectures from the simplest possible operations.
  In this paper, we relax these restrictions, by capitalizing on the collective
wisdom contained in the plethora of neural networks published in online code
repositories. Concretely, we (a) extract and publish GitGraph, a corpus of
neural architectures and their descriptions; (b) we create problem-specific
neural architecture search spaces, implemented as a textual search mechanism
over GitGraph; (c) we propose a method of identifying unique common subgraphs
within the architectures solving each problem (e.g., image processing,
reinforcement learning), that can then serve as modules in the newly created
problem specific neural search space.



Feature engineering is one of the most important but tedious tasks in data
science projects. This work studies automation of feature learning for
relational data. We first theoretically proved that learning relevant features
from relational data for a given predictive analytics problem is NP-hard.
However, it is possible to empirically show that an efficient rule based
approach predefining transformations as a priori based on heuristics can
extract very useful features from relational data. Indeed, the proposed
approach outperformed the state of the art solutions with a significant margin.
We further introduce a deep neural network which automatically learns
appropriate transformations of relational data into a representation that
predicts the target variable well instead of being predefined as a priori by
users. In an extensive experiment with Kaggle competitions, the proposed
methods could win late medals. To the best of our knowledge, this is the first
time an automation system could win medals in Kaggle competitions with complex
relational data.



Despite the several successes of deep learning systems, there are concerns
about their limitations, discussed most recently by Gary Marcus. This paper
discusses Marcus's concerns and some others, together with solutions to several
of these problems provided by the "P theory of intelligence" and its
realisation in the "SP computer model". The main advantages of the SP system
are: relatively small requirements for data and the ability to learn from a
single experience; the ability to model both hierarchical and non-hierarchical
structures; strengths in several kinds of reasoning, including `commonsense'
reasoning; transparency in the representation of knowledge, and the provision
of an audit trail for all processing; the likelihood that the SP system could
not be fooled into bizarre or eccentric recognition of stimuli, as deep
learning systems can be; the SP system provides a robust solution to the
problem of `catastrophic forgetting' in deep learning systems; the SP system
provides a theoretically-coherent solution to the problems of correcting over-
and under-generalisations in learning, and learning correct structures despite
errors in data; unlike most research on deep learning, the SP programme of
research draws extensively on research on human learning, perception, and
cognition; and the SP programme of research has an overarching theory,
supported by evidence, something that is largely missing from research on deep
learning. In general, the SP system provides a much firmer foundation than deep
learning for the development of artificial general intelligence.



A recommender system aims to recommend items that a user is interested in
among many items. The need for the recommender system has been expanded by the
information explosion. Various approaches have been suggested for providing
meaningful recommendations to users. One of the proposed approaches is to
consider a recommender system as a Markov decision process (MDP) problem and
try to solve it using reinforcement learning (RL). However, existing RL-based
methods have an obvious drawback. To solve an MDP in a recommender system, they
encountered a problem with the large number of discrete actions that bring RL
to a larger class of problems. In this paper, we propose a novel RL-based
recommender system. We formulate a recommender system as a gridworld game by
using a biclustering technique that can reduce the state and action space
significantly. Using biclustering not only reduces space but also improves the
recommendation quality effectively handling the cold-start problem. In
addition, our approach can provide users with some explanation why the system
recommends certain items. Lastly, we examine the proposed algorithm on a
real-world dataset and achieve a better performance than the widely used
recommendation algorithm.



This paper concerns open-world classification, where the classifier not only
needs to classify test examples into seen classes that have appeared in
training but also reject examples from unseen or novel classes that have not
appeared in training. Specifically, this paper focuses on discovering the
hidden unseen classes of the rejected examples. Clearly, without prior
knowledge this is difficult. However, we do have the data from the seen
training classes, which can tell us what kind of similarity/difference is
expected for examples from the same class or from different classes. It is
reasonable to assume that this knowledge can be transferred to the rejected
examples and used to discover the hidden unseen classes in them. This paper
aims to solve this problem. It first proposes a joint open classification model
with a sub-model for classifying whether a pair of examples belongs to the same
or different classes. This sub-model can serve as a distance function for
clustering to discover the hidden classes of the rejected examples.
Experimental results show that the proposed model is highly promising.



Like any large software system, a full-fledged DBMS offers an overwhelming
amount of configuration knobs. These range from static initialisation
parameters like buffer sizes, degree of concurrency, or level of replication to
complex runtime decisions like creating a secondary index on a particular
column or reorganising the physical layout of the store. To simplify the
configuration, industry grade DBMSs are usually shipped with various advisory
tools, that provide recommendations for given workloads and machines. However,
reality shows that the actual configuration, tuning, and maintenance is usually
still done by a human administrator, relying on intuition and experience.
Recent work on deep reinforcement learning has shown very promising results in
solving problems, that require such a sense of intuition. For instance, it has
been applied very successfully in learning how to play complicated games with
enormous search spaces. Motivated by these achievements, in this work we
explore how deep reinforcement learning can be used to administer a DBMS.
First, we will describe how deep reinforcement learning can be used to
automatically tune an arbitrary software system like a DBMS by defining a
problem environment. Second, we showcase our concept of NoDBA at the concrete
example of index selection and evaluate how well it recommends indexes for
given workloads.



The increasing use of deep neural networks for safety-critical applications,
such as autonomous driving and flight control, raises concerns about their
safety and reliability. Formal verification can address these concerns by
guaranteeing that a deep learning system operates as intended, but the state of
the art is limited to small systems. In this work-in-progress report we give an
overview of our work on mitigating this difficulty, by pursuing two
complementary directions: devising scalable verification techniques, and
identifying design choices that result in deep learning systems that are more
amenable to verification.



With the demand for machine learning increasing, so does the demand for tools
which make it easier to use. Automated machine learning (AutoML) tools have
been developed to address this need, such as the Tree-Based Pipeline
Optimization Tool (TPOT) which uses genetic programming to build optimal
pipelines. We introduce Layered TPOT, a modification to TPOT which aims to
create pipelines equally good as the original, but in significantly less time.
This approach evaluates candidate pipelines on increasingly large subsets of
the data according to their fitness, using a modified evolutionary algorithm to
allow for separate competition between pipelines trained on different sample
sizes. Empirical evaluation shows that, on sufficiently large datasets, Layered
TPOT indeed finds better models faster.



A problem faced by many instructors is that of designing exams that
accurately assess the abilities of the students. Typically these exams are
prepared several days in advance, and generic question scores are used based on
rough approximation of the question difficulty and length. For example, for a
recent class taught by the author, there were 30 multiple choice questions
worth 3 points, 15 true/false with explanation questions worth 4 points, and 5
analytical exercises worth 10 points. We describe a novel framework where
algorithms from machine learning are used to modify the exam question weights
in order to optimize the exam scores, using the overall class grade as a proxy
for a student's true ability. We show that significant error reduction can be
obtained by our approach over standard weighting schemes, and we make several
new observations regarding the properties of the "good" and "bad" exam
questions that can have impact on the design of improved future evaluation
methods.



Topological data analysis offers a robust way to extract useful information
from noisy, unstructured data by identifying its underlying structure.
Recently, an efficient quantum algorithm was proposed [Lloyd, Garnerone,
Zanardi, Nat. Commun. 7, 10138 (2016)] for calculating Betti numbers of data
points -- topological features that count the number of topological holes of
various dimensions in a scatterplot. Here, we implement a proof-of-principle
demonstration of this quantum algorithm by employing a six-photon quantum
processor to successfully analyze the topological features of Betti numbers of
a network including three data points, providing new insights into data
analysis in the era of quantum computing.



Displaying the large number of bands in a hyper spectral image on a
trichromatic monitor has been an active research topic. The visualized image
shall convey as much information as possible form the original data and
facilitate image interpretation. Most existing methods display HSIs in false
colors which contradict with human's experience and expectation. In this paper,
we propose a nonlinear approach to visualize an input HSI with natural colors
by taking advantage of a corresponding RGB image. Our approach is based on
Moving Least Squares, an interpolation scheme for reconstructing a surface from
a set of control points, which in our case is a set of matching pixels between
the HSI and the corresponding RGB image. Based on MLS, the proposed method
solves for each spectral signature a unique transformation so that the non
linear structure of the HSI can be preserved. The matching pixels between a
pair of HSI and RGB image can be reused to display other HSIs captured b the
same imaging sensor with natural colors. Experiments show that the output image
of the proposed method no only have natural colors but also maintain the visual
information necessary for human analysis.



Most of the knowledge Representation formalisms developed for representing
prescriptive norms can be categorized as either suitable for representing
either low level or high level norms.We argue that low level norm
representations do not advance the cause of autonomy in agents in the sense
that it is not the agent itself that determines the normative position it
should be at a particular time, on the account of a more general rule. In other
words an agent on some external system for a nitty gritty prescriptions of its
obligations and prohibitions. On the other hand, high level norms which have an
explicit description of a norm's precondition and have some form of
implication, do not as they exist in the literature do not support generalized
inferences about violation like low level norm representations do. This paper
presents a logical formalism for the representation of high level norms in open
societies that enable violation inferences that detail the situation in which
the norm violation took place and the identity of the norm violation. Norms are
formalized as logic programs whose heads specify what an agent is obliged or
permitted to do when a situation arises and within what time constraint of the
situation.Each norm is also assigned an identity using some reification scheme.
The body of each logic program describes the nature of the situation in which
the agent is expected to act or desist from acting. This kind of violation is
novel in the literature.



The seminal work of Chow and Liu (1968) shows that approximation of a finite
probabilistic system by Markov trees can achieve the minimum information loss
with the topology of a maximum spanning tree. Our current paper generalizes the
result to Markov networks of tree width $\leq k$, for every fixed $k\geq 2$. In
particular, we prove that approximation of a finite probabilistic system with
such Markov networks has the minimum information loss when the network topology
is achieved with a maximum spanning $k$-tree. While constructing a maximum
spanning $k$-tree is intractable for even $k=2$, we show that polynomial
algorithms can be ensured by a sufficient condition accommodated by many
meaningful applications. In particular, we prove an efficient algorithm for
learning the optimal topology of higher order correlations among random
variables that belong to an underlying linear structure.



The paper describes combinatorial framework for planning of geological
exploration for oil-gas fields. The suggested scheme of the geological
exploration involves the following stages: (1) building of special 4-layer
tree-like model (layer of geological exploration): productive layer, group of
productive layers, oil-gas field, oil-gas region (or group of the fields); (2)
generations of local design (exploration) alternatives for each low-layer
geological objects: conservation, additional search, independent utilization,
joint utilization; (3) multicriteria (i.e., multi-attribute) assessment of the
design (exploration) alternatives and their interrelation (compatibility) and
mapping if the obtained vector estimates into integrated ordinal scale; (4)
hierarchical design ('bottom-up') of composite exploration plans for each
oil-gas field; (5) integration of the plans into region plans and (6)
aggregation of the region plans into a general exploration plan. Stages 2, 3,
4, and 5 are based on hierarchical multicriteria morphological design (HMMD)
method (assessment of ranking of alternatives, selection and composition of
alternatives into composite alternatives). The composition problem is based on
morphological clique model. Aggregation of the obtained modular alternatives
(stage 6) is based on detection of a alternatives 'kernel' and its extension by
addition of elements (multiple choice model). In addition, the usage of
multiset estimates for alternatives is described as well. The alternative
estimates are based on expert judgment. The suggested combinatorial planning
methodology is illustrated by numerical examples for geological exploration of
Yamal peninsula.



Chit-chat models are known to have several problems: they lack specificity,
do not display a consistent personality and are often not very captivating. In
this work we present the task of making chit-chat more engaging by conditioning
on profile information. We collect data and train models to (i) condition on
their given profile information; and (ii) information about the person they are
talking to, resulting in improved dialogues, as measured by next utterance
prediction. Since (ii) is initially unknown our model is trained to engage its
partner with personal topics, and we show the resulting dialogue can be used to
predict profile information about the interlocutors.



We analyze the language learned by an agent trained with reinforcement
learning as a component of the ActiveQA system [Buck et al., 2017]. In
ActiveQA, question answering is framed as a reinforcement learning task in
which an agent sits between the user and a black box question-answering system.
The agent learns to reformulate the user's questions to elicit the optimal
answers. It probes the system with many versions of a question that are
generated via a sequence-to-sequence question reformulation model, then
aggregates the returned evidence to find the best answer. This process is an
instance of \emph{machine-machine} communication. The question reformulation
model must adapt its language to increase the quality of the answers returned,
matching the language of the question answering system. We find that the agent
does not learn transformations that align with semantic intuitions but
discovers through learning classical information retrieval techniques such as
tf-idf re-weighting and stemming.



Here we present CaosDB, a Research Data Management System (RDMS) designed to
ensure seamless integration of inhomogeneous data sources and repositories of
legacy data. Its primary purpose is the management of data from biomedical
sciences, both from simulations and experiments during the complete research
data lifecycle. An RDMS for this domain faces particular challenges: Research
data arise in huge amounts, from a wide variety of sources, and traverse a
highly branched path of further processing. To be accepted by its users, an
RDMS must be built around workflows of the scientists and practices and thus
support changes in workflow and data structure. Nevertheless it should
encourage and support the development and observation of standards and
furthermore facilitate the automation of data acquisition and processing with
specialized software. The storage data model of an RDMS must reflect these
complexities with appropriate semantics and ontologies while offering simple
methods for finding, retrieving, and understanding relevant data. We show how
CaosDB responds to these challenges and give an overview of the CaosDB Server,
its data model and its easy-to-learn CaosDB Query Language. We briefly discuss
the status of the implementation, how we currently use CaosDB, and how we plan
to use and extend it.



How does the machine classify styles in art? And how does it relate to art
historians's methods for analyzing style? Several studies have shown the
ability of the machine to learn and predict style categories, such as
Renaissance, Baroque, Impressionism, etc., from images of paintings. This
implies that the machine can learn an internal representation encoding
discriminative features through its visual analysis. However, such a
representation is not necessarily interpretable. We conducted a comprehensive
study of several of the state-of-the-art convolutional neural networks applied
to the task of style classification on 77K images of paintings, and analyzed
the learned representation through correlation analysis with concepts derived
from art history. Surprisingly, the networks could place the works of art in a
smooth temporal arrangement mainly based on learning style labels, without any
a priori knowledge of time of creation, the historical time and context of
styles, or relations between styles. The learned representations showed that
there are few underlying factors that explain the visual variations of style in
art. Some of these factors were found to correlate with style patterns
suggested by Heinrich W\"olfflin (1846-1945). The learned representations also
consistently highlighted certain artists as the extreme distinctive
representative of their styles, which quantitatively confirms art historian
observations.



Online Reputation Monitoring (ORM) is concerned with the use of computational
tools to measure the reputation of entities online, such as politicians or
companies. In practice, current ORM methods are constrained to the generation
of data analytics reports, which aggregate statistics of popularity and
sentiment on social media. We argue that this format is too restrictive as end
users often like to have the flexibility to search for entity-centric
information that is not available in predefined charts. As such, we propose the
inclusion of entity retrieval capabilities as a first step towards the
extension of current ORM capabilities. However, an entity's reputation is also
influenced by the entity's relationships with other entities. Therefore, we
address the problem of Entity-Relationship (E-R) retrieval in which the goal is
to search for multiple connected entities. This is a challenging problem which
traditional entity search systems cannot cope with. Besides E-R retrieval we
also believe ORM would benefit of text-based entity-centric prediction
capabilities, such as predicting entity popularity on social media based on
news events or the outcome of political surveys. However, none of these tasks
can provide useful results if there is no effective entity disambiguation and
sentiment analysis tailored to the context of ORM. Consequently, this thesis
address two computational problems in Online Reputation Monitoring: Entity
Retrieval and Text Mining. We researched and developed methods to extract,
retrieve and predict entity-centric information spread across the Web.



This paper builds on an existing notion of group responsibility and proposes
two ways to define the degree of group responsibility: structural and
functional degrees of responsibility. These notions measure the potential
responsibilities of (agent) groups for avoiding a state of affairs. According
to these notions, a degree of responsibility for a state of affairs can be
assigned to a group of agents if, and to the extent that, the group has the
potential to preclude the state of affairs.



The evaluation of interactive machine learning systems remains a difficult
task. These systems learn from and adapt to the human, but at the same time,
the human receives feedback and adapts to the system. Getting a clear
understanding of these subtle mechanisms of co-operation and co-adaptation is
challenging. In this chapter, we report on our experience in designing and
evaluating various interactive machine learning applications from different
domains. We argue for coupling two types of validation: algorithm-centered
analysis, to study the computational behaviour of the system; and
human-centered evaluation, to observe the utility and effectiveness of the
application for end-users. We use a visual analytics application for guided
search, built using an interactive evolutionary approach, as an exemplar of our
work. Our observation is that human-centered design and evaluation complement
algorithmic analysis, and can play an important role in addressing the
"black-box" effect of machine learning. Finally, we discuss research
opportunities that require human-computer interaction methodologies, in order
to support both the visible and hidden roles that humans play in interactive
machine learning.



Geometric analysis is a very capable theory to understand the influence of
the high dimensionality of the input data in machine learning (ML) and
knowledge discovery (KD). With our approach we can assess how far the
application of a specific KD/ML-algorithm to a concrete data set is prone to
the curse of dimensionality. To this end we extend V.~Pestov's axiomatic
approach to the instrinsic dimension of data sets, based on the seminal work by
M.~Gromov on concentration phenomena, and provide an adaptable and
computationally feasible model for studying observable geometric invariants
associated to features that are natural to both the data and the learning
procedure. In detail, we investigate data represented by formal contexts and
give first theoretical as well as experimental insights into the intrinsic
dimension of a concept lattice. Because of the correspondence between formal
concepts and maximal cliques in graphs, applications to social network analysis
are at hand.



The cognitive theory of true conditions (CTTC) is a proposal to describe the
model-theoretic semantics of symbolic cognitive architectures and design the
implementation of cognitive abilities. The CTTC is formulated mathematically
using the multi-optional many-sorted past present future(MMPPF) structures.
This article defines mathematically the MMPPF structures and the formal
languages proposed to describe them by the CTTC.



Symbol emergence through a robot's own interactive exploration of the world
without human intervention has been investigated now for several decades.
However, methods that enable a machine to form symbol systems in a robust
bottom-up manner are still missing. Clearly, this shows that we still do not
have an appropriate computational understanding that explains symbol emergence
in biological and artificial systems. Over the years it became more and more
clear that symbol emergence has to be posed as a multi-faceted problem.
Therefore, we will first review the history of the symbol emergence problem in
different fields showing their mutual relations. Then we will describe recent
work and approaches to solve this problem with the aim of providing an
integrative and comprehensive overview of symbol emergence for future research.



Reinforcement Learning (RL) is a research area that has blossomed
tremendously in recent years and has shown remarkable potential in among others
successfully playing computer games. However, there only exists a few game
platforms that provide diversity in tasks and state-space needed to advance RL
algorithms. The existing platforms offer RL access to Atari- and a few
web-based games, but no platform fully expose access to Flash games. This is
unfortunate because applying RL to Flash games have potential to push the
research of RL algorithms.
  This paper introduces the Flash Reinforcement Learning platform (FlashRL)
which attempts to fill this gap by providing an environment for thousands of
Flash games on a novel platform for Flash automation. It opens up easy
experimentation with RL algorithms for Flash games, which has previously been
challenging. The platform shows excellent performance with as little as 5% CPU
utilization on consumer hardware. It shows promising results for novel
reinforcement learning algorithms.



This paper presents the first deep reinforcement learning (DRL) framework to
estimate the optimal Dynamic Treatment Regimes from observational medical data.
This framework is more flexible and adaptive for high dimensional action and
state spaces than existing reinforcement learning methods to model real-life
complexity in heterogeneous disease progression and treatment choices, with the
goal of providing doctor and patients the data-driven personalized decision
recommendations. The proposed DRL framework comprises (i) a supervised learning
step to predict the most possible expert actions, and (ii) a deep reinforcement
learning step to estimate the long-term value function of Dynamic Treatment
Regimes. Both steps depend on deep neural networks.
  As a key motivational example, we have implemented the proposed framework on
a data set from the Center for International Bone Marrow Transplant Research
(CIBMTR) registry database, focusing on the sequence of prevention and
treatments for acute and chronic graft versus host disease after
transplantation. In the experimental results, we have demonstrated promising
accuracy in predicting human experts' decisions, as well as the high expected
reward function in the DRL-based dynamic treatment regimes.



Proportional representation (PR) is a fundamental principle of many
democracies world-wide which employ PR-based voting rules to elect their
representatives. The normative properties of these voting rules however, are
often only understood in the context of sincere voting.
  In this paper we consider PR in the presence of strategic voters. We
construct a voting rule such that for every preference profile there exists at
least one costly voting equilibrium satisfying PR with respect to voters'
private and unrevealed preferences - such a voting rule is said to be
strategically robust. In contrast, a commonly applied voting rule is shown not
be strategically robust. Furthermore, we prove a limit on `how strategically
robust' a PR-based voting rule can be; we show that there is no PR-based voting
rule which ensures that every equilibrium satisfies PR. Collectively, our
results highlight the possibility and limit of achieving PR in the presence of
strategic voters and a positive role for mechanisms, such as pre-election
polls, which coordinate voter behaviour towards equilibria which satisfy PR.



We propose two general and falsifiable hypotheses about expectations on
generalization error when learning in the context of concept drift. One posits
that as drift rate increases, the forgetting rate that minimizes generalization
error will also increase and vice versa. The other posits that as a learner's
forgetting rate increases, the bias/variance profile that minimizes
generalization error will have lower variance and vice versa. These hypotheses
lead to the concept of the sweet path, a path through the 3-d space of
alternative drift rates, forgetting rates and bias/variance profiles on which
generalization error will be minimized, such that slow drift is coupled with
low forgetting and low bias, while rapid drift is coupled with fast forgetting
and low variance. We present experiments that support the existence of such a
sweet path. We also demonstrate that simple learners that select appropriate
forgetting rates and bias/variance profiles are highly competitive with the
state-of-the-art in incremental learners for concept drift on real-world drift
problems.



The ability of intelligent agents to play games in human-like fashion is
popularly considered a benchmark of progress in Artificial Intelligence.
Similarly, performance on multi-disciplinary tasks such as Visual Question
Answering (VQA) is considered a marker for gauging progress in Computer Vision.
In our work, we bring games and VQA together. Specifically, we introduce the
first computational model aimed at Pictionary, the popular word-guessing social
game. We first introduce Sketch-QA, an elementary version of Visual Question
Answering task. Styled after Pictionary, Sketch-QA uses incrementally
accumulated sketch stroke sequences as visual data. Notably, Sketch-QA involves
asking a fixed question ("What object is being drawn?") and gathering
open-ended guess-words from human guessers. We analyze the resulting dataset
and present many interesting findings therein. To mimic Pictionary-style
guessing, we subsequently propose a deep neural model which generates
guess-words in response to temporally evolving human-drawn sketches. Our model
even makes human-like mistakes while guessing, thus amplifying the human
mimicry factor. We evaluate our model on the large-scale guess-word dataset
generated via Sketch-QA task and compare with various baselines. We also
conduct a Visual Turing Test to obtain human impressions of the guess-words
generated by humans and our model. Experimental results demonstrate the promise
of our approach for Pictionary and similarly themed games.



Designing tax policies that are effective in curbing tax evasion and maximize
state revenues requires a rigorous understanding of taxpayer behavior. This
work explores the problem of determining the strategy a self-interested,
risk-averse tax entity is expected to follow, as it "navigates" - in the
context of a Markov Decision Process - a government-controlled tax environment
that includes random audits, penalties and occasional tax amnesties. Although
simplified versions of this problem have been previously explored, the mere
assumption of risk-aversion (as opposed to risk-neutrality) raises the
complexity of finding the optimal policy well beyond the reach of analytical
techniques. Here, we obtain approximate solutions via a combination of
Q-learning and recent advances in Deep Reinforcement Learning. By doing so, we
i) determine the tax evasion behavior expected of the taxpayer entity, ii)
calculate the degree of risk aversion of the "average" entity given empirical
estimates of tax evasion, and iii) evaluate sample tax policies, in terms of
expected revenues. Our model can be useful as a testbed for "in-vitro" testing
of tax policies, while our results lead to various policy recommendations.



Multi-vehicle routing has become increasingly important with the rapid
development of autonomous vehicle technology. Dial-a-ride problem (DARP), a
variant of vehicle routing problem (VRP), deals with the allocation of customer
requests to vehicles, scheduling the pick-up and drop-off times and the
sequence of serving those requests by ensuring high customer satisfaction with
minimized travel cost. In this paper, we propose an improved tabu search (ITS)
heuristic for static DARP with the objective of obtaining high-quality
solutions in short time. Two new techniques, initialization heuristic, and time
window adjustment are proposed to achieve faster convergence to the global
optimum. Various numerical experiments are conducted for the proposed solution
methodology using DARP test instances from the literature and the convergence
speed up is validated.



We address the problem to tackle the very similar objects like Chihuahua or
muffin problem to recognize at least in human vision level. Our regular deep
structured machine learning still does not solve it. We saw many times for
about year in our community the problem. Today we proposed the state-of-the-art
solution for it. Our approach is quite tricky to get the very high accuracy. We
propose the deep transfer learning method which could be tackled all this type
of problems not limited to just Chihuahua or muffin problem. It is the best
method to train with small data set not like require huge amount data.



Reinforcement Learning (RL) is a research area that has blossomed
tremendously in recent years and has shown remarkable potential for artificial
intelligence based opponents in computer games. This success is primarily due
to vast capabilities of Convolutional Neural Networks (ConvNet), enabling
algorithms to extract useful information from noisy environments. Capsule
Network (CapsNet) is a recent introduction to the Deep Learning algorithm group
and has only barely begun to be explored. The network is an architecture for
image classification, with superior performance for classification of the MNIST
dataset. CapsNets have not been explored beyond image classification.
  This thesis introduces the use of CapsNet for Q-Learning based game
algorithms. To successfully apply CapsNet in advanced game play, three main
contributions follow. First, the introduction of four new game environments as
frameworks for RL research with increasing complexity, namely Flash RL, Deep
Line Wars, Deep RTS, and Deep Maze. These environments fill the gap between
relatively simple and more complex game environments available for RL research
and are in the thesis used to test and explore the CapsNet behavior.
  Second, the thesis introduces a generative modeling approach to produce
artificial training data for use in Deep Learning models including CapsNets. We
empirically show that conditional generative modeling can successfully generate
game data of sufficient quality to train a Deep Q-Network well.
  Third, we show that CapsNet is a reliable architecture for Deep Q-Learning
based algorithms for game AI. A capsule is a group of neurons that determine
the presence of objects in the data and is in the literature shown to increase
the robustness of training and predictions while lowering the amount training
data needed. It should, therefore, be ideally suited for game plays.



Planning robust executions under uncertainty is a fundamental challenge for
building autonomous robots. Partially Observable Markov Decision Processes
(POMDPs) provide a standard framework for modeling uncertainty in many robot
applications. A key algorithmic problem for POMDPs is policy synthesis. While
this problem has traditionally been posed w.r.t. optimality objectives, many
robot applications are better modeled by POMDPs where the objective is a
boolean requirement. In this paper, we study the latter problem in a setting
where the requirement is a safe-reachability property, which states that with a
probability above a certain threshold, it is possible to eventually reach a
goal state while satisfying a safety requirement. The central challenge in our
problem is that it requires reasoning over a vast space of probability
distributions. What's more, it has been shown that policy synthesis of POMDPs
with reachability objectives is undecidable in general. To address these
challenges, we introduce the notion of a goal-constrained belief space, which
only contains beliefs (probability distributions over states) reachable from
the initial belief under desired executions. This constrained space is
generally much smaller than the original belief space. Our approach compactly
represents this space over a bounded horizon using symbolic constraints, and
employs an incremental Satisfiability Modulo Theories (SMT) solver to
efficiently search for a valid policy over it. We evaluate our method using a
case study involving a partially observable robotics domain with uncertain
obstacles. Our results suggest that it is possible to synthesize policies over
large belief spaces with a small number of SMT solver calls by focusing on
goal-constrained belief space, and our method o ers a stronger guarantee of
both safety and reachability than alternative unconstrained/constrained POMDP
formulations.



Linear approximations to the decision boundary of a complex model have become
one of the most popular tools for interpreting predictions. In this paper, we
study such linear explanations produced either post-hoc by a few recent methods
or generated along with predictions with contextual explanation networks
(CENs). We focus on two questions: (i) whether linear explanations are always
consistent or can be misleading, and (ii) when integrated into the prediction
process, whether and how explanations affect the performance of the model. Our
analysis sheds more light on certain properties of explanations produced by
different methods and suggests that learning models that explain and predict
jointly is often advantageous.



Accurate and transparent prediction of cancer survival times on the level of
individual patients can inform and improve patient care and treatment
practices. In this paper, we design a model that concurrently learns to
accurately predict patient-specific survival distributions and to explain its
predictions in terms of patient attributes such as clinical tests or
assessments. Our model is flexible and based on a recurrent network, can handle
various modalities of data including temporal measurements, and yet constructs
and uses simple explanations in the form of patient- and time-specific linear
regression. For analysis, we use two publicly available datasets and show that
our networks outperform a number of baselines in prediction while providing a
way to inspect the reasons behind each prediction.



The cross entropy (CE) method is a model based search method to solve
optimization problems where the objective function has minimal structure. The
Monte-Carlo version of the CE method employs the naive sample averaging
technique which is inefficient, both computationally and space wise. We provide
a novel stochastic approximation version of the CE method, where the sample
averaging is replaced with incremental geometric averaging. This approach can
save considerable computational and storage costs. Our algorithm is incremental
in nature and possesses additional attractive features such as accuracy,
stability, robustness and convergence to the global optimum for a particular
class of objective functions. We evaluate the algorithm on a variety of global
optimization benchmark problems and the results obtained corroborate our
theoretical findings.



Negative affect is a proxy for mental health in adults. By being able to
predict participants' negative affect states unobtrusively, researchers and
clinicians will be better positioned to deliver targeted, just-in-time mental
health interventions via mobile applications. This work attempts to personalize
the passive recognition of negative affect states via group-based modeling of
user behavior patterns captured from mobility, communication, and activity
patterns. Results show that group models outperform generalized models in a
dataset based on two weeks of users' daily lives.



We describe the first automatic approach for merging coreference annotations
obtained from multiple annotators into a single gold standard. This merging is
subject to certain linguistic hard constraints and optimization criteria that
prefer solutions with minimal divergence from annotators. The representation
involves an equivalence relation over a large number of elements. We use Answer
Set Programming to describe two representations of the problem and four
objective functions suitable for different datasets. We provide two
structurally different real-world benchmark datasets based on the METU-Sabanci
Turkish Treebank and we report our experiences in using the Gringo, Clasp, and
Wasp tools for computing optimal adjudication results on these datasets.



Tactical driving decision making is crucial for autonomous driving systems
and has attracted considerable interest in recent years. In this paper, we
propose several practical components that can speed up deep reinforcement
learning algorithms towards tactical decision making tasks: 1) non-uniform
action skipping as a more stable alternative to action-repetition frame
skipping, 2) a counter-based penalty for lanes on which ego vehicle has less
right-of-road, and 3) heuristic inference-time action masking for apparently
undesirable actions. We evaluate the proposed components in a realistic driving
simulator and compare them with several baselines. Results show that the
proposed scheme provides superior performance in terms of safety, efficiency,
and comfort.



We present Adaptive Memory Networks (AMN) that processes input-question pairs
to dynamically construct a network architecture optimized for lower inference
times for Question Answering (QA) tasks. AMN processes the input story to
extract entities and stores them in memory banks. Starting from a single bank,
as the number of input entities increases, AMN learns to create new banks as
the entropy in a single bank becomes too high. Hence, after processing an
input-question(s) pair, the resulting network represents a hierarchical
structure where entities are stored in different banks, distanced by question
relevance. At inference, one or few banks are used, creating a tradeoff between
accuracy and performance. AMN is enabled by dynamic networks that allow input
dependent network creation and efficiency in dynamic mini-batching as well as
our novel bank controller that allows learning discrete decision making with
high accuracy. In our results, we demonstrate that AMN learns to create
variable depth networks depending on task complexity and reduces inference
times for QA tasks.



Recently, feature selection has become an increasingly important area of
research due to the surge in high-dimensional datasets in all areas of modern
life. A plethora of feature selection algorithms have been proposed, but it is
difficult to truly analyse the quality of a given algorithm. Ideally, an
algorithm would be evaluated by measuring how well it removes known bad
features. Acquiring datasets with such features is inherently difficult, and so
a common technique is to add synthetic bad features to an existing dataset.
While adding noisy features is an easy task, it is very difficult to
automatically add complex, redundant features. This work proposes one of the
first approaches to generating redundant features, using a novel genetic
programming approach. Initial experiments show that our proposed method can
automatically create difficult, redundant features which have the potential to
be used for creating high-quality feature selection benchmark datasets.



We propose the Onto2Vec method, an approach to learn feature vectors for
biological entities based on their annotations to biomedical ontologies. Our
method can be applied to a wide range of bioinformatics research problems such
as similarity-based prediction of interactions between proteins, classification
of interaction types using supervised learning, or clustering.



Multi-view sequential learning is a fundamental problem in machine learning
dealing with multi-view sequences. In a multi-view sequence, there exists two
forms of interactions between different views: view-specific interactions and
cross-view interactions. In this paper, we present a new neural architecture
for multi-view sequential learning called the Memory Fusion Network (MFN) that
explicitly accounts for both interactions in a neural architecture and
continuously models them through time. The first component of the MFN is called
the System of LSTMs, where view-specific interactions are learned in isolation
through assigning an LSTM function to each view. The cross-view interactions
are then identified using a special attention mechanism called the Delta-memory
Attention Network (DMAN) and summarized through time with a Multi-view Gated
Memory. Through extensive experimentation, MFN is compared to various proposed
approaches for multi-view sequential learning on multiple publicly available
benchmark datasets. MFN outperforms all the existing multi-view approaches.
Furthermore, MFN outperforms all current state-of-the-art models, setting new
state-of-the-art results for these multi-view datasets.



Knowledge graphs, on top of entities and their relationships, contain another
important element: literals. Literals encode interesting properties (e.g. the
height) of entities that are not captured by links between entities alone. Most
of the existing work on embedding (or latent feature) based knowledge graph
modeling focuses mainly on the relations between entities. In this work, we
study the effect of incorporating literal information into existing knowledge
graph models. Our approach, which we name LiteralE, is an extension that can be
plugged into existing latent feature methods. LiteralE merges entity embeddings
with their literal information using a learnable, parametrized function, such
as a simple linear or nonlinear transformation, or a multilayer neural network.
We extend several popular embedding models using LiteralE and evaluate the
performance on the task of link prediction. Despite its simplicity, LiteralE
proves to be an effective way to incorporate literal information into existing
embedding based models, improving their performance on different standard
datasets, which we augmented with their literals and provide as testbed for
further research.



Multi-person articulated pose tracking in complex unconstrained videos is an
important and challenging problem. In this paper, going along the road of
top-down approaches, we propose a decent and efficient pose tracker based on
pose flows. First, we design an online optimization framework to build
association of cross-frame poses and form pose flows. Second, a novel pose flow
non maximum suppression (NMS) is designed to robustly reduce redundant pose
flows and re-link temporal disjoint pose flows. Extensive experiments show our
method significantly outperforms best reported results on two standard Pose
Tracking datasets (PoseTrack dataset and PoseTrack Challenge dataset) by 13 mAP
25 MOTA and 6 mAP 3 MOTA respectively. Moreover, in the case of working on
detected poses in individual frames, the extra computation of proposed pose
tracker is very minor, requiring 0.01 second per frame only.



This paper presents a quantitative fine-grained manual evaluation approach to
comparing the performance of different machine translation (MT) systems. We
build upon the well-established Multidimensional Quality Metrics (MQM) error
taxonomy and implement a novel method that assesses whether the differences in
performance for MQM error types between different MT systems are statistically
significant. We conduct a case study for English-to-Croatian, a language
direction that involves translating into a morphologically rich language, for
which we compare three MT systems belonging to different paradigms: pure
phrase-based, factored phrase-based and neural. First, we design an
MQM-compliant error taxonomy tailored to the relevant linguistic phenomena of
Slavic languages, which made the annotation process feasible and accurate.
Errors in MT outputs were then annotated by two annotators following this
taxonomy. Subsequently, we carried out a statistical analysis which showed that
the best-performing system (neural) reduces the errors produced by the worst
system (pure phrase-based) by more than half (54\%). Moreover, we conducted an
additional analysis of agreement errors in which we distinguished between short
(phrase-level) and long distance (sentence-level) errors. We discovered that
phrase-based MT approaches are of limited use for long distance agreement
phenomena, for which neural MT was found to be especially effective.



In this work we aim to solve a large collection of tasks using a single
reinforcement learning agent with a single set of parameters. A key challenge
is to handle the increased amount of data and extended training time, which is
already a problem in single task learning. We have developed a new distributed
agent IMPALA (Importance-Weighted Actor Learner Architecture) that can scale to
thousands of machines and achieve a throughput rate of 250,000 frames per
second. We achieve stable learning at high throughput by combining decoupled
acting and learning with a novel off-policy correction method called V-trace,
which was critical for achieving learning stability. We demonstrate the
effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a
set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and
Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare
et al., 2013a)). Our results show that IMPALA is able to achieve better
performance than previous agents, use less data and crucially exhibits positive
transfer between tasks as a result of its multi-task approach.



Decomposition methods have been proposed in the past to approximate solutions
to large sequential decision making problems. In contexts where an agent
interacts with multiple entities, utility decomposition can be used where each
individual entity is considered independently. The individual utility functions
are then combined in real time to solve the global problem. Although these
techniques can perform well empirically, they sacrifice optimality. This paper
proposes an approach inspired from multi-fidelity optimization to learn a
correction term with a neural network representation. Learning this correction
can significantly improve performance. We demonstrate this approach on a
pedestrian avoidance problem for autonomous driving. By leveraging strategies
to avoid a single pedestrian, the decomposition method can scale to avoid
multiple pedestrians. We verify empirically that the proposed correction method
leads to a significant improvement over the decomposition method alone and
outperforms a policy trained on the full scale problem without utility
decomposition.



Motivated by recent developments impacting our view of Fermi's paradox
(absence of extraterrestrials and their manifestations from our past light
cone), we suggest a reassessment of the problem itself, as well as of
strategies employed by SETI projects so far. The need for such reevaluation is
fueled not only by the failure of searches thus far, but also by great advances
recently made in astrophysics, astrobiology, computer science and future
studies, which have remained largely ignored in SETI practice. As an example of
the new approach, we consider the effects of the observed metallicity and
temperature gradients in the Milky Way on the spatial distribution of
hypothetical advanced extraterrestrial intelligent communities. While,
obviously, properties of such communities and their sociological and
technological preferences are entirely unknown, we assume that (1) they operate
in agreement with the known laws of physics, and (2) that at some point they
typically become motivated by a meta-principle embodying the central role of
information-processing; a prototype of the latter is the recently suggested
Intelligence Principle of Steven J. Dick. There are specific conclusions of
practical interest to be drawn from coupling of these reasonable assumptions
with the astrophysical and astrochemical structure of the Galaxy. In
particular, we suggest that the outer regions of the Galactic disk are most
likely locations for advanced SETI targets, and that intelligent communities
will tend to migrate outward through the Galaxy as their capacities of
information-processing increase, for both thermodynamical and astrochemical
reasons. This can also be regarded as a possible generalization of the Galactic
Habitable Zone, concept currently much investigated in astrobiology.



We introduce a highly structured family of hard satisfiable 3-SAT formulas
corresponding to an ordered spin-glass model from statistical physics. This
model has provably "glassy" behavior; that is, it has many local optima with
large energy barriers between them, so that local search algorithms get stuck
and have difficulty finding the true ground state, i.e., the unique satisfying
assignment. We test the hardness of our formulas with two Davis-Putnam solvers,
Satz and zChaff, the recently introduced Survey Propagation (SP), and two local
search algorithms, Walksat and Record-to-Record Travel (RRT). We compare our
formulas to random 3-XOR-SAT formulas and to two other generators of hard
satisfiable instances, the minimum disagreement parity formulas of Crawford et
al., and Hirsch's hgen. For the complete solvers the running time of our
formulas grows exponentially in sqrt(n), and exceeds that of random 3-XOR-SAT
formulas for small problem sizes. SP is unable to solve our formulas with as
few as 25 variables. For Walksat, our formulas appear to be harder than any
other known generator of satisfiable instances. Finally, our formulas can be
solved efficiently by RRT but only if the parameter d is tuned to the height of
the barriers between local minima, and we use this parameter to measure the
barrier heights in random 3-XOR-SAT formulas as well.



In many applications of natural language processing (NLP) it is necessary to
determine the likelihood of a given word combination. For example, a speech
recognizer may need to determine which of the two word combinations ``eat a
peach'' and ``eat a beach'' is more likely. Statistical NLP methods determine
the likelihood of a word combination from its frequency in a training corpus.
However, the nature of language is such that many word combinations are
infrequent and do not occur in any given corpus. In this work we propose a
method for estimating the probability of such previously unseen word
combinations using available information on ``most similar'' words.
  We describe probabilistic word association models based on distributional
word similarity, and apply them to two tasks, language modeling and pseudo-word
disambiguation. In the language modeling task, a similarity-based model is used
to improve probability estimates for unseen bigrams in a back-off language
model. The similarity-based method yields a 20% perplexity improvement in the
prediction of unseen bigrams and statistically significant reductions in
speech-recognition error.
  We also compare four similarity-based estimation methods against back-off and
maximum-likelihood estimation methods on a pseudo-word sense disambiguation
task in which we controlled for both unigram and bigram frequency to avoid
giving too much weight to easy-to-disambiguate high-frequency configurations.
The similarity-based methods perform up to 40% better on this particular task.



Several recent SIGGRAPH papers on surface simplification are described.



It is argued that colour name strategy, object name strategy, and chunking
strategy in memory are all aspects of the same general phenomena, called
stereotyping. It is pointed out that the Berlin-Kay universal partial ordering
of colours and the frequency of traffic accidents classified by colour are
surprisingly similar. Some consequences of the existence of a name strategy for
the philosophy of language and mathematics are discussed. It is argued that
real valued quantities occur {\it ab initio}. The implication of real valued
truth quantities is that the {\bf Continuum Hypothesis} of pure mathematics is
side-stepped. The existence of name strategy shows that thought/sememes and
talk/phonemes can be separate, and this vindicates the assumption of thought
occurring before talk used in psycholinguistic speech production models.



Trilogy is a collaborative project whose key aim is the development of an
integrated virtual laboratory to support research training within each
institution and collaborative projects between the partners. In this paper, the
architecture and underpinning platform of the system is described with
particular emphasis being placed on the structure and the integration of the
distributed database. A key element is the ontology that provides the
multi-agent system with a conceptualisation specification of the domain; this
ontology is explained, accompanied by a discussion how such a system is
integrated and used within the virtual laboratory. Although in this paper,
Telecommunications and in particular Broadband networks are used as exemplars,
the underlying system principles are applicable to any domain where a
combination of experimental and literature-based resources are required.



The abstract mathematical theory of partial differential equations (PDEs) is
formulated in terms of manifolds, scalar fields, tensors, and the like, but
these algebraic structures are hardly recognizable in actual PDE solvers. The
general aim of the Sophus programming style is to bridge the gap between theory
and practice in the domain of PDE solvers. Its main ingredients are a library
of abstract datatypes corresponding to the algebraic structures used in the
mathematical theory and an algebraic expression style similar to the expression
style used in the mathematical theory. Because of its emphasis on abstract
datatypes, Sophus is most naturally combined with object-oriented languages or
other languages supporting abstract datatypes. The resulting source code
patterns are beyond the scope of current compiler optimizations, but are
sufficiently specific for a dedicated source-to-source optimizer. The limited,
domain-specific, character of Sophus is the key to success here. This kind of
optimization has been tested on computationally intensive Sophus style code
with promising results. The general approach may be useful for other styles and
in other application domains as well.



Given a set of numbers, the balanced partioning problem is to divide them
into two subsets, so that the sum of the numbers in each subset are as nearly
equal as possible, subject to the constraint that the cardinalities of the
subsets be within one of each other. We combine the balanced largest
differencing method (BLDM) and Korf's complete Karmarkar-Karp algorithm to get
a new algorithm that optimally solves the balanced partitioning problem. For
numbers with twelve significant digits or less, the algorithm can optimally
solve balanced partioning problems of arbitrary size in practice. For numbers
with greater precision, it first returns the BLDM solution, then continues to
find better solutions as time allows.



We present a technique for automatic induction of slot annotations for
subcategorization frames, based on induction of hidden classes in the EM
framework of statistical estimation. The models are empirically evalutated by a
general decision test. Induction of slot labeling for subcategorization frames
is accomplished by a further application of EM, and applied experimentally on
frame observations derived from parsing large corpora. We outline an
interpretation of the learned representations as theoretical-linguistic
decompositional lexical entries.



A pattern-based approach to the presentation, codification and reuse of
property specifications for finite-state verification was proposed by Dwyer and
his collegues. The patterns enable non-experts to read and write formal
specifications for realistic systems and facilitate easy conversion of
specifications between formalisms, such as LTL, CTL, QRE. In this paper, we
extend the pattern system with events - changes of values of variables in the
context of LTL.



This paper is concerned with tracking and interpreting scholarly documents in
distributed research communities. We argue that current approaches to document
description, and current technological infrastructures particularly over the
World Wide Web, provide poor support for these tasks. We describe the design of
a digital library server which will enable authors to submit a summary of the
contributions they claim their documents makes, and its relations to the
literature. We describe a knowledge-based Web environment to support the
emergence of such a community-constructed semantic hypertext, and the services
it could provide to assist the interpretation of an idea or document in the
context of its literature. The discussion considers in detail how the approach
addresses usability issues associated with knowledge structuring environments.



Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental prior probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown prior distribution. We combine both ideas and
get a parameterless theory of universal Artificial Intelligence. We give strong
arguments that the resulting AIXI model is the most intelligent unbiased agent
possible. We outline for a number of problem classes, including sequence
prediction, strategic games, function minimization, reinforcement and
supervised learning, how the AIXI model can formally solve them. The major
drawback of the AIXI model is that it is uncomputable. To overcome this
problem, we construct a modified algorithm AIXI-tl, which is still effectively
more intelligent than any other time t and space l bounded agent. The
computation time of AIXI-tl is of the order tx2^l. Other discussed topics are
formal definitions of intelligence order relations, the horizon problem and
relations of the AIXI theory to other AI approaches.



In (Apt et al, TOPLAS 1998) we introduced the imperative programming language
Alma-0 that supports declarative programming. In this paper we illustrate the
hybrid programming style of Alma-0 by means of various examples that complement
those presented in (Apt et al, TOPLAS 1998). The presented Alma-0 programs
illustrate the versatility of the language and show that ``don't know''
nondeterminism can be naturally combined with assignment.



In this paper we propose a new type of random CSP model, called Model RB,
which is a revision to the standard Model B. It is proved that phase
transitions from a region where almost all problems are satisfiable to a region
where almost all problems are unsatisfiable do exist for Model RB as the number
of variables approaches infinity. Moreover, the critical values at which the
phase transitions occur are also known exactly. By relating the hardness of
Model RB to Model B, it is shown that there exist a lot of hard instances in
Model RB.



We present a PSPACE algorithm that decides satisfiability of the graded modal
logic Gr(K_R)---a natural extension of propositional modal logic K_R by
counting expressions---which plays an important role in the area of knowledge
representation. The algorithm employs a tableaux approach and is the first
known algorithm which meets the lower bound for the complexity of the problem.
Thus, we exactly fix the complexity of the problem and refute an
ExpTime-hardness conjecture. We extend the results to the logic Gr(K_(R \cap
I)), which augments Gr(K_R) with inverse relations and intersection of
accessibility relations. This establishes a kind of ``theoretical benchmark''
that all algorithmic approaches can be measured against.



The implicit theory that a simulation represents is precisely not in the
individual choices but rather in the 'envelope' of possible trajectories - what
is important is the shape of the whole envelope. Typically a huge amount of
computation is required when experimenting with factors bearing on the dynamics
of a simulation to tease out what affects the shape of this envelope. In this
paper we present a methodology aimed at systematically exploring this envelope.
We propose a method for searching for tendencies and proving their necessity
relative to a range of parameterisations of the model and agents' choices, and
to the logic of the simulation language. The exploration consists of a forward
chaining generation of the trajectories associated to and constrained by such a
range of parameterisations and choices. Additionally, we propose a
computational procedure that helps implement this exploration by translating a
Multi Agent System simulation into a constraint-based search over possible
trajectories by 'compiling' the simulation rules into a more specific form,
namely by partitioning the simulation rules using appropriate modularity in the
simulation. An example of this procedure is exhibited.
  Keywords: Constraint Search, Constraint Logic Programming, Proof, Emergence,
Tendencies



We consider the concept of a local set of inference rules. A local rule set
can be automatically transformed into a rule set for which bottom-up evaluation
terminates in polynomial time. The local-rule-set transformation gives
polynomial-time evaluation strategies for a large variety of rule sets that
cannot be given terminating evaluation strategies by any other known automatic
technique. This paper discusses three new results. First, it is shown that
every polynomial-time predicate can be defined by an (unstratified) local rule
set. Second, a new machine-recognizable subclass of the local rule sets is
identified. Finally we show that locality, as a property of rule sets, is
undecidable in general.



We show that the e-commerce domain can provide all the right ingredients for
successful data mining and claim that it is a killer domain for data mining. We
describe an integrated architecture, based on our expe-rience at Blue Martini
Software, for supporting this integration. The architecture can dramatically
reduce the pre-processing, cleaning, and data understanding effort often
documented to take 80% of the time in knowledge discovery projects. We
emphasize the need for data collection at the application server layer (not the
web server) in order to support logging of data and metadata that is essential
to the discovery process. We describe the data transformation bridges required
from the transaction processing systems and customer event streams (e.g.,
clickstreams) to the data warehouse. We detail the mining workbench, which
needs to provide multiple views of the data through reporting, data mining
algorithms, visualization, and OLAP. We con-clude with a set of challenges.



To study the structure of solutions for random k-SAT and random CSPs, this
paper introduces the concept of average similarity degree to characterize how
solutions are similar to each other. It is proved that under certain
conditions, as r (i.e. the ratio of constraints to variables) increases, the
limit of average similarity degree when the number of variables approaches
infinity exhibits phase transitions at a threshold point, shifting from a
smaller value to a larger value abruptly. For random k-SAT this phenomenon will
occur when k>4 . It is further shown that this threshold point is also a
singular point with respect to r in the asymptotic estimate of the second
moment of the number of solutions. Finally, we discuss how this work is helpful
to understand the hardness of solving random instances and a possible
application of it to the design of search algorithms.



This paper investigates the formal pragmatics of ambiguous expressions by
modeling ambiguity in a multi-agent system. Such a framework allows us to give
a more refined notion of the kind of information that is conveyed by ambiguous
expressions. We analyze how ambiguity affects the knowledge of the dialog
participants and, especially, what they know about each other after an
ambiguous sentence has been uttered. The agents communicate with each other by
means of a TELL-function, whose application is constrained by an implementation
of some of Grice's maxims. The information states of the multi-agent system
itself are represented as a Kripke structures and TELL is an update function on
those structures. This framework enables us to distinguish between the
information conveyed by ambiguous sentences vs. the information conveyed by
disjunctions, and between semantic ambiguity vs. perceived ambiguity.



The eventual goal of a language model is to accurately predict the value of a
missing word given its context. We present an approach to word prediction that
is based on learning a representation for each word as a function of words and
linguistics predicates in its context. This approach raises a few new questions
that we address. First, in order to learn good word representations it is
necessary to use an expressive representation of the context. We present a way
that uses external knowledge to generate expressive context representations,
along with a learning method capable of handling the large number of features
generated this way that can, potentially, contribute to each prediction.
Second, since the number of words ``competing'' for each prediction is large,
there is a need to ``focus the attention'' on a smaller subset of these. We
exhibit the contribution of a ``focus of attention'' mechanism to the
performance of the word predictor. Finally, we describe a large scale
experimental study in which the approach presented is shown to yield
significant improvements in word prediction tasks.



We describe a slightly sub-exponential time algorithm for learning parity
functions in the presence of random classification noise. This results in a
polynomial-time algorithm for the case of parity functions that depend on only
the first O(log n log log n) bits of input. This is the first known instance of
an efficient noise-tolerant algorithm for a concept class that is provably not
learnable in the Statistical Query model of Kearns. Thus, we demonstrate that
the set of problems learnable in the statistical query model is a strict subset
of those problems learnable in the presence of noise in the PAC model.
  In coding-theory terms, what we give is a poly(n)-time algorithm for decoding
linear k by n codes in the presence of random noise for the case of k = c log n
loglog n for some c > 0. (The case of k = O(log n) is trivial since one can
just individually check each of the 2^k possible messages and choose the one
that yields the closest codeword.)
  A natural extension of the statistical query model is to allow queries about
statistical properties that involve t-tuples of examples (as opposed to single
examples). The second result of this paper is to show that any class of
functions learnable (strongly or weakly) with t-wise queries for t = O(log n)
is also weakly learnable with standard unary queries. Hence this natural
extension to the statistical query model does not increase the set of weakly
learnable functions.



Different mathematical models of recognition processes are known. In the
present paper we consider a pattern recognition algorithm as an oracle
computation on a Turing machine. Such point of view seems to be useful in
pattern recognition as well as in recursion theory. Use of recursion theory in
pattern recognition shows connection between a recognition algorithm comparison
problem and complexity problems of oracle computation. That is because in many
cases we can take into account only the number of sign computations or in other
words volume of oracle information needed. Therefore, the problem of
recognition algorithm preference can be formulated as a complexity optimization
problem of oracle computation. Furthermore, introducing a certain "natural"
preference relation on a set of recognizing algorithms, we discover it to be
nontransitive. This relates to the well known nontransitivity paradox in
probability theory.
  Keywords: Pattern Recognition, Recursion Theory, Nontransitivity, Preference
Relation



This paper explores the kinds of probabilistic relations that are important
in syntactic disambiguation. It proposes that two widely used kinds of
relations, lexical dependencies and structural relations, have complementary
disambiguation capabilities. It presents a new model based on structural
relations, the Tree-gram model, and reports experiments showing that structural
relations should benefit from enrichment by lexical dependencies.



A non-deterministic call-by-need lambda-calculus \calc with case,
constructors, letrec and a (non-deterministic) erratic choice, based on
rewriting rules is investigated. A standard reduction is defined as a variant
of left-most outermost reduction. The semantics is defined by contextual
equivalence of expressions instead of using $\alpha\beta(\eta)$-equivalence. It
is shown that several program transformations are correct, for example all
(deterministic) rules of the calculus, and in addition the rules for garbage
collection, removing indirections and unique copy.
  This shows that the combination of a context lemma and a meta-rewriting on
reductions using complete sets of commuting (forking, resp.) diagrams is a
useful and successful method for providing a semantics of a functional
programming language and proving correctness of program transformations.



The well-founded semantics is one of the most widely studied and used
semantics of logic programs with negation. In the case of finite propositional
programs, it can be computed in polynomial time, more specifically, in
O(|At(P)|size(P)) steps, where size(P) denotes the total number of occurrences
of atoms in a logic program P. This bound is achieved by an algorithm
introduced by Van Gelder and known as the alternating-fixpoint algorithm.
Improving on the alternating-fixpoint algorithm turned out to be difficult. In
this paper we study extensions and modifications of the alternating-fixpoint
approach. We then restrict our attention to the class of programs whose rules
have no more than one positive occurrence of an atom in their bodies. For
programs in that class we propose a new implementation of the
alternating-fixpoint method in which false atoms are computed in a top-down
fashion. We show that our algorithm is faster than other known algorithms and
that for a wide class of programs it is linear and so, asymptotically optimal.



The Bayesian framework is ideally suited for induction problems. The
probability of observing $x_t$ at time $t$, given past observations
$x_1...x_{t-1}$ can be computed with Bayes' rule if the true distribution $\mu$
of the sequences $x_1x_2x_3...$ is known. The problem, however, is that in many
cases one does not even have a reasonable estimate of the true distribution. In
order to overcome this problem a universal distribution $\xi$ is defined as a
weighted sum of distributions $\mu_i\inM$, where $M$ is any countable set of
distributions including $\mu$. This is a generalization of Solomonoff
induction, in which $M$ is the set of all enumerable semi-measures. Systems
which predict $y_t$, given $x_1...x_{t-1}$ and which receive loss $l_{x_t y_t}$
if $x_t$ is the true next symbol of the sequence are considered. It is proven
that using the universal $\xi$ as a prior is nearly as good as using the
unknown true distribution $\mu$. Furthermore, games of chance, defined as a
sequence of bets, observations, and rewards are studied. The time needed to
reach the winning zone is bounded in terms of the relative entropy of $\mu$ and
$\xi$. Extensions to arbitrary alphabets, partial and delayed prediction, and
more active systems are discussed.



It has been argued by Shepard that there is a robust psychological law that
relates the distance between a pair of items in psychological space and the
probability that they will be confused with each other. Specifically, the
probability of confusion is a negative exponential function of the distance
between the pair of items. In experimental contexts, distance is typically
defined in terms of a multidimensional Euclidean space-but this assumption
seems unlikely to hold for complex stimuli. We show that, nonetheless, the
Universal Law of Generalization can be derived in the more complex setting of
arbitrary stimuli, using a much more universal measure of distance. This
universal distance is defined as the length of the shortest program that
transforms the representations of the two items of interest into one another:
the algorithmic information distance. It is universal in the sense that it
minorizes every computable distance: it is the smallest computable distance. We
show that the universal law of generalization holds with probability going to
one-provided the confusion probabilities are computable. We also give a
mathematically more appealing form



The provably asymptotically fastest algorithm within a factor of 5 for
formally described problems will be constructed. The main idea is to enumerate
all programs provably equivalent to the original problem by enumerating all
proofs. The algorithm could be interpreted as a generalization and improvement
of Levin search, which is, within a multiplicative constant, the fastest
algorithm for inverting functions. Blum's speed-up theorem is avoided by taking
into account only programs for which a correctness proof exists. Furthermore,
it is shown that the fastest program that computes a certain function is also
one of the shortest programs provably computing this function. To quantify this
statement, the definition of Kolmogorov complexity is extended, and two new
natural measures for the complexity of a function are defined.



A new three-stage computer artificial neural network model of the
tip-of-the-tongue phenomenon is proposed. Each word's node is build from some
interconnected learned auto-associative two-layer neural networks each of which
represents separate word's semantic, lexical, or phonological components. The
model synthesizes memory, psycholinguistic, and metamemory approaches, bridges
speech errors and naming chronometry research traditions, and can explain
quantitatively many tip-of-the-tongue effects.



In evolutionary algorithms, the fitness of a population increases with time
by mutating and recombining individuals and by a biased selection of more fit
individuals. The right selection pressure is critical in ensuring sufficient
optimization progress on the one hand and in preserving genetic diversity to be
able to escape from local optima on the other. We propose a new selection
scheme, which is uniform in the fitness values. It generates selection pressure
towards sparsely populated fitness regions, not necessarily towards higher
fitness, as is the case for all other selection schemes. We show that the new
selection scheme can be much more effective than standard selection schemes.



This paper deals with a problem from discrete-time robust control which
requires the solution of constraints over the reals that contain both universal
and existential quantifiers. For solving this problem we formulate it as a
program in a (fictitious) constraint logic programming language with explicit
quantifier notation. This allows us to clarify the special structure of the
problem, and to extend an algorithm for computing approximate solution sets of
first-order constraints over the reals to exploit this structure. As a result
we can deal with inputs that are clearly out of reach for current symbolic
solvers.



Unlike traditional reinforcement learning (RL), market-based RL is in
principle applicable to worlds described by partially observable Markov
Decision Processes (POMDPs), where an agent needs to learn short-term memories
of relevant previous events in order to execute optimal actions. Most previous
work, however, has focused on reactive settings (MDPs) instead of POMDPs. Here
we reimplement a recent approach to market-based RL and for the first time
evaluate it in a toy POMDP setting.



Reinforcement learning means finding the optimal course of action in
Markovian environments without knowledge of the environment's dynamics.
Stochastic optimization algorithms used in the field rely on estimates of the
value of a policy. Typically, the value of a policy is estimated from results
of simulating that very policy in the environment. This approach requires a
large amount of simulation as different points in the policy space are
considered. In this paper, we develop value estimators that utilize data
gathered when using one policy to estimate the value of using another policy,
resulting in much more data-efficient algorithms. We consider the question of
accumulating a sufficient experience and give PAC-style bounds.



Lev T. Kuzin (1928--1997) is one of the founders of modern cybernetics and
information science in Russia. He was awarded and honored the USSR State Prize
for inspiring vision into the future of technical cybernetics and his invention
and innovation of key technologies.
  The last years he interested in the computational models of geometrical and
algebraic nature and their applications in various branches of computer science
and information technologies. In the recent years the interest in computation
models based on object notion has grown tremendously stimulating an interest to
Kuzin's ideas. This year of 50th Anniversary of Cybernetics and on the occasion
of his 70th birthday on September 12, 1998 seems especially appropriate for
discussing Kuzin's Research Program.



Solomonoff's uncomputable universal prediction scheme $\xi$ allows to predict
the next symbol $x_k$ of a sequence $x_1...x_{k-1}$ for any Turing computable,
but otherwise unknown, probabilistic environment $\mu$. This scheme will be
generalized to arbitrary environmental classes, which, among others, allows the
construction of computable universal prediction schemes $\xi$. Convergence of
$\xi$ to $\mu$ in a conditional mean squared sense and with $\mu$ probability 1
is proven. It is shown that the average number of prediction errors made by the
universal $\xi$ scheme rapidly converges to those made by the best possible
informed $\mu$ scheme. The schemes, theorems and proofs are given for general
finite alphabet, which results in additional complications as compared to the
binary case. Several extensions of the presented theory and results are
outlined. They include general loss functions and bounds, games of chance,
infinite alphabet, partial and delayed prediction, classification, and more
active systems.



Many classification problems require decisions among a large number of
competing classes. These tasks, however, are not handled well by general
purpose learning methods and are usually addressed in an ad-hoc fashion. We
suggest a general approach -- a sequential learning model that utilizes
classifiers to sequentially restrict the number of competing classes while
maintaining, with high probability, the presence of the true outcome in the
candidates set. Some theoretical and computational properties of the model are
discussed and we argue that these are important in NLP-like domains. The
advantages of the model are illustrated in an experiment in part-of-speech
tagging.



The paper discusses the basic principles and the architecture of the software
toolkit for constructing knowledge-based systems which can be used
cooperatively over computer networks and also embedded into larger software
systems in different ways. Presented architecture is based on frame knowledge
representation and production rules, which also allows to interface high-level
programming languages and relational databases by exposing corresponding
classes or database tables as frames. Frames located on the remote computers
can also be transparently accessed and used in inference, and the dynamic
knowledge for specific frames can also be transferred over the network. The
issues of implementation of such a system are addressed, which use Java
programming language, CORBA and XML for external knowledge representation.
Finally, some applications of the toolkit are considered, including e-business
approach to knowledge sharing, intelligent web behaviours, etc.



A new three-stage computer artificial neural network model of the
tip-of-the-tongue phenomenon is shortly described, and its stochastic nature
was demonstrated. A way to calculate strength and appearance probability of
tip-of-the-tongue states, neural network mechanism of feeling-of-knowing
phenomenon are proposed. The model synthesizes memory, psycholinguistic, and
metamemory approaches, bridges speech errors and naming chronometry research
traditions. A model analysis of a tip-of-the-tongue case from Anton Chekhov's
short story 'A Horsey Name' is performed. A new 'throw-up-one's-arms effect' is
defined.



We introduce a transformation system for concurrent constraint programming
(CCP). We define suitable applicability conditions for the transformations
which guarantee that the input/output CCP semantics is preserved also when
distinguishing deadlocked computations from successful ones and when
considering intermediate results of (possibly) non-terminating computations.
  The system allows us to optimize CCP programs while preserving their intended
meaning: In addition to the usual benefits that one has for sequential
declarative languages, the transformation of concurrent programs can also lead
to the elimination of communication channels and of synchronization points, to
the transformation of non-deterministic computations into deterministic ones,
and to the crucial saving of computational space. Furthermore, since the
transformation system preserves the deadlock behavior of programs, it can be
used for proving deadlock freeness of a given program wrt a class of queries.
To this aim it is sometimes sufficient to apply our transformations and to
specialize the resulting program wrt the given queries in such a way that the
obtained program is trivially deadlock free.



The unification problem in algebras capable of describing sets has been
tackled, directly or indirectly, by many researchers and it finds important
applications in various research areas--e.g., deductive databases, theorem
proving, static analysis, rapid software prototyping. The various solutions
proposed are spread across a large literature. In this paper we provide a
uniform presentation of unification of sets, formalizing it at the level of set
theory. We address the problem of deciding existence of solutions at an
abstract level. This provides also the ability to classify different types of
set unification problems. Unification algorithms are uniformly proposed to
solve the unification problem in each of such classes.
  The algorithms presented are partly drawn from the literature--and properly
revisited and analyzed--and partly novel proposals. In particular, we present a
new goal-driven algorithm for general ACI1 unification and a new simpler
algorithm for general (Ab)(Cl) unification.



The notion of arc consistency plays a central role in constraint
satisfaction. It is known that the notion of local consistency can be extended
to constraint optimisation problems defined by soft constraint frameworks based
on an idempotent cost combination operator. This excludes non idempotent
operators such as + which define problems which are very important in practical
applications such as Max-CSP, where the aim is to minimize the number of
violated constraints. In this paper, we show that using a weak additional axiom
satisfied by most existing soft constraints proposals, it is possible to define
a notion of soft arc consistency that extends the classical notion of arc
consistency and this even in the case of non idempotent cost combination
operators. A polynomial time algorithm for enforcing this soft arc consistency
exists and its space and time complexities are identical to that of enforcing
arc consistency in CSPs when the cost combination operator is strictly
monotonic (for example Max-CSP). A directional version of arc consistency is
potentially even stronger than the non-directional version, since it allows non
local propagation of penalties. We demonstrate the utility of directional arc
consistency by showing that it not only solves soft constraint problems on
trees, but that it also implies a form of local optimality, which we call arc
irreducibility.



Tarski gave a general semantics for deductive reasoning: a formula a may be
deduced from a set A of formulas iff a holds in all models in which each of the
elements of A holds. A more liberal semantics has been considered: a formula a
may be deduced from a set A of formulas iff a holds in all of the "preferred"
models in which all the elements of A hold. Shoham proposed that the notion of
"preferred" models be defined by a partial ordering on the models of the
underlying language. A more general semantics is described in this paper, based
on a set of natural properties of choice functions. This semantics is here
shown to be equivalent to a semantics based on comparing the relative
"importance" of sets of models, by what amounts to a qualitative probability
measure. The consequence operations defined by the equivalent semantics are
then characterized by a weakening of Tarski's properties in which the
monotonicity requirement is replaced by three weaker conditions. Classical
propositional connectives are characterized by natural introduction-elimination
rules in a nonmonotonic setting. Even in the nonmonotonic setting, one obtains
classical propositional logic, thus showing that monotonicity is not required
to justify classical propositional connectives.



We propose that a regulation mechanism based on Hebbian covariance plasticity
may cause the brain to operate near criticality. We analyze the effect of such
a regulation on the dynamics of a network with excitatory and inhibitory
neurons and uniform connectivity within and across the two populations. We show
that, under broad conditions, the system converges to a critical state lying at
the common boundary of three regions in parameter space; these correspond to
three modes of behavior: high activity, low activity, oscillation.



On the base of recently proposed three-stage quantitative neural network
model of the tip-of-the-tongue (TOT) phenomenon a possibility to occur of TOT
states coursed by neural network interneuron links' disruption has been
studied. Using a numerical example it was found that TOTs coursed by interneron
links' disruption are in (1.5 + - 0.3)x1000 times less probable then those
coursed by irrelevant (incomplete) neural network localization. It was shown
that delayed TOT states' etiology cannot be related to neural network
interneuron links' disruption.



The problem of making sequential decisions in unknown probabilistic
environments is studied. In cycle $t$ action $y_t$ results in perception $x_t$
and reward $r_t$, where all quantities in general may depend on the complete
history. The perception $x_t$ and reward $r_t$ are sampled from the (reactive)
environmental probability distribution $\mu$. This very general setting
includes, but is not limited to, (partial observable, k-th order) Markov
decision processes. Sequential decision theory tells us how to act in order to
maximize the total expected reward, called value, if $\mu$ is known.
Reinforcement learning is usually used if $\mu$ is unknown. In the Bayesian
approach one defines a mixture distribution $\xi$ as a weighted sum of
distributions $\nu\in\M$, where $\M$ is any class of distributions including
the true environment $\mu$. We show that the Bayes-optimal policy $p^\xi$ based
on the mixture $\xi$ is self-optimizing in the sense that the average value
converges asymptotically for all $\mu\in\M$ to the optimal value achieved by
the (infeasible) Bayes-optimal policy $p^\mu$ which knows $\mu$ in advance. We
show that the necessary condition that $\M$ admits self-optimizing policies at
all, is also sufficient. No other structural assumptions are made on $\M$. As
an example application, we discuss ergodic Markov decision processes, which
allow for self-optimizing policies. Furthermore, we show that $p^\xi$ is
Pareto-optimal in the sense that there is no other policy yielding higher or
equal value in {\em all} environments $\nu\in\M$ and a strictly higher value in
at least one.



Searching the space of policies directly for the optimal policy has been one
popular method for solving partially observable reinforcement learning
problems. Typically, with each change of the target policy, its value is
estimated from the results of following that very policy. This requires a large
number of interactions with the environment as different polices are
considered. We present a family of algorithms based on likelihood ratio
estimation that use data gathered when executing one policy (or collection of
policies) to estimate the value of a different policy. The algorithms combine
estimation and optimization stages. The former utilizes experience to build a
non-parametric representation of an optimized function. The latter performs
optimization on this estimate. We show positive empirical results and provide
the sample complexity bound.



This paper presents a new approach to solving N-queen problems, which
involves a model of distributed autonomous agents with artificial life (ALife)
and a method of representing N-queen constraints in an agent environment. The
distributed agents locally interact with their living environment, i.e., a
chessboard, and execute their reactive behaviors by applying their behavioral
rules for randomized motion, least-conflict position searching, and cooperating
with other agents etc. The agent-based N-queen problem solving system evolves
through selection and contest according to the rule of Survival of the Fittest,
in which some agents will die or be eaten if their moving strategies are less
efficient than others. The experimental results have shown that this system is
capable of solving large-scale N-queen problems. This paper also provides a
model of ALife agents for solving general CSPs.



The first quantitative neural network model of feelings and emotions is
proposed on the base of available data on their neuroscience and evolutionary
biology nature, and on a neural network human memory model which admits
distinct description of conscious and unconscious mental processes in a time
dependent manner. As an example, proposed model is applied to quantitative
description of the feeling of knowing.



In this paper we expose the theoretical background underlying our current
research. This consists in the development of behaviour-based knowledge
systems, for closing the gaps between behaviour-based and knowledge-based
systems, and also between the understandings of the phenomena they model. We
expose the requirements and stages for developing behaviour-based knowledge
systems and discuss their limits. We believe that these are necessary
conditions for the development of higher order cognitive capacities, in
artificial and natural cognitive systems.



Much work on argument systems has focussed on preferred extensions which
define the maximal collectively defensible subsets. Identification and
enumeration of these subsets is (under the usual assumptions) computationally
demanding. We consider approaches to deciding if a subset S is a preferred
extension which query a representations encoding all such extensions, so that
the computational effort is invested once only (for the initial enumeration)
rather than for each separate query.



The need for a circumscriptive formalism that allows for simple yet elegant
modular problem representation has led Lifschitz (AIJ, 1995) to introduce
nested abnormality theories (NATs) as a tool for modular knowledge
representation, tailored for applying circumscription to minimize exceptional
circumstances. Abstracting from this particular objective, we propose L_{CIRC},
which is an extension of generic propositional circumscription by allowing
propositional combinations and nesting of circumscriptive theories. As shown,
NATs are naturally embedded into this language, and are in fact of equal
expressive capability. We then analyze the complexity of L_{CIRC} and NATs, and
in particular the effect of nesting. The latter is found to be a source of
complexity, which climbs the Polynomial Hierarchy as the nesting depth
increases and reaches PSPACE-completeness in the general case. We also identify
meaningful syntactic fragments of NATs which have lower complexity. In
particular, we show that the generalization of Horn circumscription in the NAT
framework remains CONP-complete, and that Horn NATs without fixed letters can
be efficiently transformed into an equivalent Horn CNF, which implies
polynomial solvability of principal reasoning tasks. Finally, we also study
extensions of NATs and briefly address the complexity in the first-order case.
Our results give insight into the ``cost'' of using L_{CIRC} (resp. NATs) as a
host language for expressing other formalisms such as action theories,
narratives, or spatial theories.



We present a novel, general, optimally fast, incremental way of searching for
a universal algorithm that solves each task in a sequence of tasks. The Optimal
Ordered Problem Solver (OOPS) continually organizes and exploits previously
found solutions to earlier tasks, efficiently searching not only the space of
domain-specific algorithms, but also the space of search algorithms.
Essentially we extend the principles of optimal nonincremental universal search
to build an incremental universal learner that is able to improve itself
through experience. In illustrative experiments, our self-improver becomes the
first general system that learns to solve all n disk Towers of Hanoi tasks
(solution size 2^n-1) for n up to 30, profiting from previously solved, simpler
tasks involving samples of a simple context free language.



Groenendijk and Stokhof (1984, 1996; Groenendijk 1999) provide a logically
attractive theory of the semantics of natural language questions, commonly
referred to as the partition theory. Two central notions in this theory are
entailment between questions and answerhood. For example, the question "Who is
going to the party?" entails the question "Is John going to the party?", and
"John is going to the party" counts as an answer to both. Groenendijk and
Stokhof define these two notions in terms of partitions of a set of possible
worlds.
  We provide a syntactic characterization of entailment between questions and
answerhood . We show that answers are, in some sense, exactly those formulas
that are built up from instances of the question. This result lets us compare
the partition theory with other approaches to interrogation -- both linguistic
analyses, such as Hamblin's and Karttunen's semantics, and computational
systems, such as Prolog. Our comparison separates a notion of answerhood into
three aspects: equivalence (when two questions or answers are interchangeable),
atomic answers (what instances of a question count as answers), and compound
answers (how answers compose).



We implement Groenendijk and Stokhof's partition semantics of questions in a
simple question answering algorithm. The algorithm is sound, complete, and
based on tableau theorem proving. The algorithm relies on a syntactic
characterization of answerhood: Any answer to a question is equivalent to some
formula built up only from instances of the question. We prove this
characterization by translating the logic of interrogation to classical
predicate logic and applying Craig's interpolation theorem.



We show how coupling of local optimization processes can lead to better
solutions than multi-start local optimization consisting of independent runs.
This is achieved by minimizing the average energy cost of the ensemble, subject
to synchronization constraints between the state vectors of the individual
local minimizers. From an augmented Lagrangian which incorporates the
synchronization constraints both as soft and hard constraints, a network is
derived wherein the local minimizers interact and exchange information through
the synchronization constraints. From the viewpoint of neural networks, the
array can be considered as a Lagrange programming network for continuous
optimization and as a cellular neural network (CNN). The penalty weights
associated with the soft state synchronization constraints follow from the
solution to a linear program. This expresses that the energy cost of the
ensemble should maximally decrease. In this way successful local minimizers can
implicitly impose their state to the others through a mechanism of master-slave
dynamics resulting into a cooperative search mechanism. Improved information
spreading within the ensemble is obtained by applying the concept of
small-world networks. This work suggests, in an interdisciplinary context, the
importance of information exchange and state synchronization within ensembles,
towards issues as evolution, collective behaviour, optimality and intelligence.



This paper presents the DLV system, which is widely considered the
state-of-the-art implementation of disjunctive logic programming, and addresses
several aspects. As for problem solving, we provide a formal definition of its
kernel language, function-free disjunctive logic programs (also known as
disjunctive datalog), extended by weak constraints, which are a powerful tool
to express optimization problems. We then illustrate the usage of DLV as a tool
for knowledge representation and reasoning, describing a new declarative
programming methodology which allows one to encode complex problems (up to
$\Delta^P_3$-complete problems) in a declarative fashion. On the foundational
side, we provide a detailed analysis of the computational complexity of the
language of DLV, and by deriving new complexity results we chart a complete
picture of the complexity of this language and important fragments thereof.
  Furthermore, we illustrate the general architecture of the DLV system which
has been influenced by these results. As for applications, we overview
application front-ends which have been developed on top of DLV to solve
specific knowledge representation tasks, and we briefly describe the main
international projects investigating the potential of the system for industrial
exploitation. Finally, we report about thorough experimentation and
benchmarking, which has been carried out to assess the efficiency of the
system. The experimental results confirm the solidity of DLV and highlight its
potential for emerging application areas like knowledge management and
information integration.



The probability of observing $x_t$ at time $t$, given past observations
$x_1...x_{t-1}$ can be computed with Bayes' rule if the true generating
distribution $\mu$ of the sequences $x_1x_2x_3...$ is known. If $\mu$ is
unknown, but known to belong to a class $M$ one can base ones prediction on the
Bayes mix $\xi$ defined as a weighted sum of distributions $\nu\in M$. Various
convergence results of the mixture posterior $\xi_t$ to the true posterior
$\mu_t$ are presented. In particular a new (elementary) derivation of the
convergence $\xi_t/\mu_t\to 1$ is provided, which additionally gives the rate
of convergence. A general sequence predictor is allowed to choose an action
$y_t$ based on $x_1...x_{t-1}$ and receives loss $\ell_{x_t y_t}$ if $x_t$ is
the next symbol of the sequence. No assumptions are made on the structure of
$\ell$ (apart from being bounded) and $M$. The Bayes-optimal prediction scheme
$\Lambda_\xi$ based on mixture $\xi$ and the Bayes-optimal informed prediction
scheme $\Lambda_\mu$ are defined and the total loss $L_\xi$ of $\Lambda_\xi$ is
bounded in terms of the total loss $L_\mu$ of $\Lambda_\mu$. It is shown that
$L_\xi$ is bounded for bounded $L_\mu$ and $L_\xi/L_\mu\to 1$ for $L_\mu\to
\infty$. Convergence of the instantaneous losses are also proven.



This paper first analyzes the resolution complexity of two random CSP models
(i.e. Model RB/RD) for which we can establish the existence of phase
transitions and identify the threshold points exactly. By encoding CSPs into
CNF formulas, it is proved that almost all instances of Model RB/RD have no
tree-like resolution proofs of less than exponential size. Thus, we not only
introduce new families of CNF formulas hard for resolution, which is a central
task of Proof-Complexity theory, but also propose models with both many hard
instances and exact phase transitions. Then, the implications of such models
are addressed. It is shown both theoretically and experimentally that an
application of Model RB/RD might be in the generation of hard satisfiable
instances, which is not only of practical importance but also related to some
open problems in cryptography such as generating one-way functions.
Subsequently, a further theoretical support for the generation method is shown
by establishing exponential lower bounds on the complexity of solving random
satisfiable and forced satisfiable instances of RB/RD near the threshold.
Finally, conclusions are presented, as well as a detailed comparison of Model
RB/RD with the Hamiltonian cycle problem and random 3-SAT, which, respectively,
exhibit three different kinds of phase transition behavior in NP-complete
problems.



Most traditional artificial intelligence (AI) systems of the past 50 years
are either very limited, or based on heuristics, or both. The new millennium,
however, has brought substantial progress in the field of theoretically optimal
and practically feasible algorithms for prediction, search, inductive inference
based on Occam's razor, problem solving, decision making, and reinforcement
learning in environments of a very general type. Since inductive inference is
at the heart of all inductive sciences, some of the results are relevant not
only for AI and computer science but also for physics, provoking nontraditional
predictions based on Zuse's thesis of the computer-generated universe.



Cluster analysis often serves as the initial step in the process of data
classification. In this paper, the problem of clustering different length input
data is considered. The edit distance as the minimum number of elementary edit
operations needed to transform one vector into another is used. A heuristic for
clustering unequal length vectors, analogue to the well known k-means algorithm
is described and analyzed. This heuristic determines cluster centroids
expanding shorter vectors to the lengths of the longest ones in each cluster in
a specific way. It is shown that the time and space complexities of the
heuristic are linear in the number of input vectors. Experimental results on
real data originating from a system for classification of Web attacks are
given.



We note the importance of time-scales, meaning, and availability of
information for the emergence of novel information meta-structures at a global
scale. We discuss previous work in this area and develop future perspectives.
We focus on the transmission of scientific articles and the integration of
traditional conferences with their virtual extensions on the Internet, their
time-scales, and availability. We mention the Semantic Web as an effort for
integrating meaningful information.



We describe how specialized database technology and data analysis methods
were applied by the Swedish defense to help deal with the violation of Swedish
marine territory by foreign submarine intruders during the Eighties and early
Nineties. Among several approaches tried some yielded interesting information,
although most of the key questions remain unanswered. We conclude with a survey
of belief-function- and genetic-algorithm-based methods which were proposed to
support interpretation of intelligence reports and prediction of future
submarine positions, respectively.



We give a purely model-theoretic characterization of the semantics of logic
programs with negation-as-failure allowed in clause bodies. In our semantics
the meaning of a program is, as in the classical case, the unique minimum model
in a program-independent ordering. We use an expanded truth domain that has an
uncountable linearly ordered set of truth values between False (the minimum
element) and True (the maximum), with a Zero element in the middle. The truth
values below Zero are ordered like the countable ordinals. The values above
Zero have exactly the reverse order. Negation is interpreted as reflection
about Zero followed by a step towards Zero; the only truth value that remains
unaffected by negation is Zero. We show that every program has a unique minimum
model M_P, and that this model can be constructed with a T_P iteration which
proceeds through the countable ordinals. Furthermore, we demonstrate that M_P
can also be obtained through a model intersection construction which
generalizes the well-known model intersection theorem for classical logic
programming. Finally, we show that by collapsing the true and false values of
the infinite-valued model M_P to (the classical) True and False, we obtain a
three-valued model identical to the well-founded one.



Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various
aspects of language in one model. Many existing algorithms developed for
learning and inference in DBNs are applicable to probabilistic language
modeling. To demonstrate the potential of DBNs for natural language processing,
we employ a DBN in an information extraction task. We show how to assemble
wealth of emerging linguistic instruments for shallow parsing, syntactic and
semantic tagging, morphological decomposition, named entity recognition etc. in
order to incrementally build a robust information extraction system. Our method
outperforms previously published results on an established benchmark domain.



We give a brief introduction to the AIXI model, which unifies and overcomes
the limitations of sequential decision theory and universal Solomonoff
induction. While the former theory is suited for active agents in known
environments, the latter is suited for passive prediction of unknown
environments.



Given the joint chances of a pair of random variables one can compute
quantities of interest, like the mutual information. The Bayesian treatment of
unknown chances involves computing, from a second order prior distribution and
the data likelihood, a posterior distribution of the chances. A common
treatment of incomplete data is to assume ignorability and determine the
chances by the expectation maximization (EM) algorithm. The two different
methods above are well established but typically separated. This paper joins
the two approaches in the case of Dirichlet priors, and derives efficient
approximations for the mean, mode and the (co)variance of the chances and the
mutual information. Furthermore, we prove the unimodality of the posterior
distribution, whence the important property of convergence of EM to the global
maximum in the chosen framework. These results are applied to the problem of
selecting features for incremental learning and naive Bayes classification. A
fast filter based on the distribution of mutual information is shown to
outperform the traditional filter based on empirical mutual information on a
number of incomplete real data sets.



Self-Organising Maps (SOMs) are effective tools in classification problems,
and in recent years the even more powerful Dynamic Growing Neural Networks, a
variant of SOMs, have been developed. Automatic Classification (also called
clustering) is an important and difficult problem in many Astrophysical
experiments, for instance, Gamma Ray Burst classification, or gamma-hadron
separation. After a brief introduction to classification problem, we discuss
Self-Organising Maps in section 2. Section 3 discusses with various models of
growing neural networks and finally in section 4 we discuss the research
perspectives in growing neural networks for efficient classification in
astrophysical problems.



A model of sensory information processing is presented. The model assumes
that learning of internal (hidden) generative models, which can predict the
future and evaluate the precision of that prediction, is of central importance
for information extraction. Furthermore, the model makes a bridge to
goal-oriented systems and builds upon the structural similarity between the
architecture of a robust controller and that of the hippocampal entorhinal
loop. This generative control architecture is mapped to the neocortex and to
the hippocampal entorhinal loop. Implicit memory phenomena; priming and
prototype learning are emerging features of the model. Mathematical theorems
ensure stability and attractive learning properties of the architecture.
Connections to reinforcement learning are also established: both the control
network, and the network with a hidden model converge to (near) optimal policy
under suitable conditions. Falsifying predictions, including the role of the
feedback connections between neocortical areas are made.



On the basis of convolutional (Hamming) version of recent Neural Network
Assembly Memory Model (NNAMM) for intact two-layer autoassociative Hopfield
network optimal receiver operating characteristics (ROCs) have been derived
analytically. A method of taking into account explicitly a priori probabilities
of alternative hypotheses on the structure of information initiating memory
trace retrieval and modified ROCs (mROCs, a posteriori probabilities of correct
recall vs. false alarm probability) are introduced. The comparison of empirical
and calculated ROCs (or mROCs) demonstrates that they coincide quantitatively
and in this way intensities of cues used in appropriate experiments may be
estimated. It has been found that basic ROC properties which are one of
experimental findings underpinning dual-process models of recognition memory
can be explained within our one-factor NNAMM.



This paper presents our computational methodology using Genetic Algorithms
(GA) for exploring the nature of RNA editing. These models are constructed
using several genetic editing characteristics that are gleaned from the RNA
editing system as observed in several organisms. We have expanded the
traditional Genetic Algorithm with artificial editing mechanisms as proposed by
(Rocha, 1997). The incorporation of editing mechanisms provides a means for
artificial agents with genetic descriptions to gain greater phenotypic
plasticity, which may be environmentally regulated. Our first implementations
of these ideas have shed some light into the evolutionary implications of RNA
editing. Based on these understandings, we demonstrate how to select proper RNA
editors for designing more robust GAs, and the results will show promising
applications to real-world problems. We expect that the framework proposed will
both facilitate determining the evolutionary role of RNA editing in biology,
and advance the current state of research in Genetic Algorithms.



Relational representation of knowledge makes it possible to perform all the
computations and decision making in a uniform relational way by means of
special relational compositions called triangle and square products. In this
paper some applications in manufacturing related to cost analysis are
described. Testing fuzzy relational structures for various relational
properties allows us to discover dependencies, hierarchies, similarities, and
equivalences of the attributes characterizing technological processes and
manufactured artifacts in their relationship to costs and performance.
  A brief overview of mathematical aspects of BK-relational products is given
in Appendix 1 together with further references in the literature.



Various optimality properties of universal sequence predictors based on
Bayes-mixtures in general, and Solomonoff's prediction scheme in particular,
will be studied. The probability of observing $x_t$ at time $t$, given past
observations $x_1...x_{t-1}$ can be computed with the chain rule if the true
generating distribution $\mu$ of the sequences $x_1x_2x_3...$ is known. If
$\mu$ is unknown, but known to belong to a countable or continuous class $\M$
one can base ones prediction on the Bayes-mixture $\xi$ defined as a
$w_\nu$-weighted sum or integral of distributions $\nu\in\M$. The cumulative
expected loss of the Bayes-optimal universal prediction scheme based on $\xi$
is shown to be close to the loss of the Bayes-optimal, but infeasible
prediction scheme based on $\mu$. We show that the bounds are tight and that no
other predictor can lead to significantly smaller bounds. Furthermore, for
various performance measures, we show Pareto-optimality of $\xi$ and give an
Occam's razor argument that the choice $w_\nu\sim 2^{-K(\nu)}$ for the weights
is optimal, where $K(\nu)$ is the length of the shortest program describing
$\nu$. The results are applied to games of chance, defined as a sequence of
bets, observations, and rewards. The prediction schemes (and bounds) are
compared to the popular predictors based on expert advice. Extensions to
infinite alphabets, partial, delayed and probabilistic prediction,
classification, and more active systems are briefly discussed.



The public sector comprises government agencies, ministries, education
institutions, health providers and other types of government, commercial and
not-for-profit organisations. Unlike commercial enterprises, this environment
is highly heterogeneous in all aspects. This forms a complex network which is
not always optimised. A lack of optimisation and communication hinders
information sharing between the network nodes limiting the flow of information.
Another limiting aspect is privacy of personal information and security of
operations of some nodes or segments of the network. Attempts to reorganise the
network or improve communications to make more information available for
sharing and analysis may be hindered or completely halted by public concerns
over privacy, political agendas, social and technological barriers. This paper
discusses a technical solution for information sharing while addressing the
privacy concerns with no need for reorganisation of the existing public sector
infrastructure . The solution is based on imposing an additional layer of
Intelligent Software Agents and Knowledge Bases for data mining and analysis.



This paper studies the potential of identifying lexical paraphrases within a
single corpus, focusing on the extraction of verb paraphrases. Most previous
approaches detect individual paraphrase instances within a pair (or set) of
comparable corpora, each of them containing roughly the same information, and
rely on the substantial level of correspondence of such corpora. We present a
novel method that successfully detects isolated paraphrase instances within a
single corpus without relying on any a-priori structure and information. A
comparison suggests that an instance-based approach may be combined with a
vector based approach in order to assess better the paraphrase likelihood for
many verb pairs.



Employing the allegorical imagery from the film "The Matrix", we motivate and
discuss our `Cyborg Astrobiologist' research program. In this research program,
we are using a wearable computer and video camcorder in order to test and train
a computer-vision system to be a field-geologist and field-astrobiologist.



Physics analysis in astroparticle experiments requires the capability of
recognizing new phenomena; in order to establish what is new, it is important
to develop tools for automatic classification, able to compare the final result
with data from different detectors. A typical example is the problem of Gamma
Ray Burst detection, classification, and possible association to known sources:
for this task physicists will need in the next years tools to associate data
from optical databases, from satellite experiments (EGRET, GLAST), and from
Cherenkov telescopes (MAGIC, HESS, CANGAROO, VERITAS).



This paper describes how fitness inheritance can be used to estimate fitness
for a proportion of newly sampled candidate solutions in the Bayesian
optimization algorithm (BOA). The goal of estimating fitness for some candidate
solutions is to reduce the number of fitness evaluations for problems where
fitness evaluation is expensive. Bayesian networks used in BOA to model
promising solutions and generate the new ones are extended to allow not only
for modeling and sampling candidate solutions, but also for estimating their
fitness. The results indicate that fitness inheritance is a promising concept
in BOA, because population-sizing requirements for building appropriate models
of promising solutions lead to good fitness estimates even if only a small
proportion of candidate solutions is evaluated using the actual fitness
function. This can lead to a reduction of the number of actual fitness
evaluations by a factor of 30 or more.



Computability logic is a formal theory of (interactive) computability in the
same sense as classical logic is a formal theory of truth. This approach was
initiated very recently in "Introduction to computability logic" (Annals of
Pure and Applied Logic 123 (2003), pp.1-99). The present paper reintroduces
computability logic in a more compact and less technical way. It is written in
a semitutorial style with a general computer science, logic or mathematics
audience in mind. An Internet source on the subject is available at
http://www.cis.upenn.edu/~giorgi/cl.html, and additional material at
http://www.csc.villanova.edu/~japaridz/CL/gsoll.html .



For an intelligent agent to be truly autonomous, it must be able to adapt its
representation to the requirements of its task as it interacts with the world.
Most current approaches to on-line feature extraction are ad hoc; in contrast,
this paper presents an algorithm that bases judgments of state compatibility
and state-space abstraction on principled criteria derived from the
psychological principle of cognitive economy. The algorithm incorporates an
active form of Q-learning, and partitions continuous state-spaces by merging
and splitting Voronoi regions. The experiments illustrate a new methodology for
testing and comparing representations by means of learning curves. Results from
the puck-on-a-hill task demonstrate the algorithm's ability to learn effective
representations, superior to those produced by some other, well-known, methods.



We study the properties of the Minimum Description Length principle for
sequence prediction, considering a two-part MDL estimator which is chosen from
a countable class of models. This applies in particular to the important case
of universal sequence prediction, where the model class corresponds to all
algorithms for some fixed universal Turing machine (this correspondence is by
enumerable semimeasures, hence the resulting models are stochastic). We prove
convergence theorems similar to Solomonoff's theorem of universal induction,
which also holds for general Bayes mixtures. The bound characterizing the
convergence speed for MDL predictions is exponentially larger as compared to
Bayes mixtures. We observe that there are at least three different ways of
using MDL for prediction. One of these has worse prediction properties, for
which predictions only converge if the MDL estimator stabilizes. We establish
sufficient conditions for this to occur. Finally, some immediate consequences
for complexity relations and randomness criteria are proven.



Spam, also known as Unsolicited Commercial Email (UCE), is the bane of email
communication. Many data mining researchers have addressed the problem of
detecting spam, generally by treating it as a static text classification
problem. True in vivo spam filtering has characteristics that make it a rich
and challenging domain for data mining. Indeed, real-world datasets with these
characteristics are typically difficult to acquire and to share. This paper
demonstrates some of these characteristics and argues that researchers should
pursue in vivo spam filtering as an accessible domain for investigating them.



The recently initiated approach called computability logic is a formal theory
of interactive computation. See a comprehensive online source on the subject at
http://www.cis.upenn.edu/~giorgi/cl.html . The present paper contains a
soundness and completeness proof for the deductive system CL3 which axiomatizes
the most basic first-order fragment of computability logic called the
finite-depth, elementary-base fragment. Among the potential application areas
for this result are the theory of interactive computation, constructive applied
theories, knowledgebase systems, systems for resource-bound planning and
action. This paper is self-contained as it reintroduces all relevant
definitions as well as main motivations.



We investigate the possibility of modelling the syntax and semantics of
natural language by constraints, or rules, imposed by the multi-dimensional
type theory Nabla. The only multiplicity we explicitly consider is two, namely
one dimension for the syntax and one dimension for the semantics, but the
general perspective is important. For example, issues of pragmatics could be
handled as additional dimensions.
  One of the main problems addressed is the rather complicated repertoire of
operations that exists besides the notion of categories in traditional Montague
grammar. For the syntax we use a categorial grammar along the lines of Lambek.
For the semantics we use so-called lexical and logical combinators inspired by
work in natural logic. Nabla provides a concise interpretation and a sequent
calculus as the basis for implementations.



In this paper we summarized a framework for designing grammar-based procedure
for the automatic extraction of the semantic content from spoken queries.
Starting with a case study and following an approach which combines the notions
of fuzziness and robustness in sentence parsing, we showed we built practical
domain-dependent rules which can be applied whenever it is possible to
superimpose a sentence-level semantic structure to a text without relying on a
previous deep syntactical analysis. This kind of procedure can be also
profitably used as a pre-processing tool in order to cut out part of the
sentence which have been recognized to have no relevance in the understanding
process. In the case of particular dialogue applications where there is no need
to build a complex semantic structure (e.g. word spotting or excerpting) the
presented methodology may represent an efficient alternative solution to a
sequential composition of deep linguistic analysis modules. Even if the query
generation problem may not seem a critical application it should be held in
mind that the sentence processing must be done on-line. Having this kind of
constraints we cannot design our system without caring for efficiency and thus
provide an immediate response. Another critical issue is related to whole
robustness of the system. In our case study we tried to make experiences on how
it is possible to deal with an unreliable and noisy input without asking the
user for any repetition or clarification. This may correspond to a similar
problem one may have when processing text coming from informal writing such as
e-mails, news and in many cases Web pages where it is often the case to have
irrelevant surrounding information.



Intelligent systems based on first-order logic on the one hand, and on
artificial neural networks (also called connectionist systems) on the other,
differ substantially. It would be very desirable to combine the robust neural
networking machinery with symbolic knowledge representation and reasoning
paradigms like logic programming in such a way that the strengths of either
paradigm will be retained. Current state-of-the-art research, however, fails by
far to achieve this ultimate goal. As one of the main obstacles to be overcome
we perceive the question how symbolic knowledge can be encoded by means of
connectionist systems: Satisfactory answers to this will naturally lead the way
to knowledge extraction algorithms and to integrated neural-symbolic systems.



As computing technology becomes more pervasive, personal devices such as the
PDA, cell-phone, and notebook should use context to determine how to act.
Location is one form of context that can be used in many ways. We present a
multiple-device system that collects and clusters GPS data into significant
locations. These locations are then used to determine travel times and a
probabilistic model of the user's schedule, which is used to intelligently
alert the user. We evaluate our system and suggest how it should be integrated
with a variety of applications.



Recurrent neural networks are often used for learning time-series data. Based
on a few assumptions we model this learning task as a minimization problem of a
nonlinear least-squares cost function. The special structure of the cost
function allows us to build a connection to reinforcement learning. We exploit
this connection and derive a convergent, policy iteration-based algorithm.
Furthermore, we argue that RNN training can be fit naturally into the
reinforcement learning framework.



General natural dialogue processing requires large amounts of domain
knowledge as well as linguistic knowledge in order to ensure acceptable
coverage and understanding. There are several ways of integrating lexical
resources (e.g. dictionaries, thesauri) and knowledge bases or ontologies at
different levels of dialogue processing. We concentrate in this paper on how to
exploit domain knowledge for filtering interpretation hypotheses generated by a
robust semantic parser. We use domain knowledge to semantically constrain the
hypothesis space. Moreover, adding an inference mechanism allows us to complete
the interpretation when information is not explicitly available. Further, we
discuss briefly how this can be generalized towards a predictive natural
interactive system.



Computability logic (CL) is a systematic formal theory of computational tasks
and resources, which, in a sense, can be seen as a semantics-based alternative
to (the syntactically introduced) linear logic. With its expressive and
flexible language, where formulas represent computational problems and "truth"
is understood as algorithmic solvability, CL potentially offers a comprehensive
logical basis for constructive applied theories and computing systems
inherently requiring constructive and computationally meaningful underlying
logics.
  Among the best known constructivistic logics is Heyting's intuitionistic
calculus INT, whose language can be seen as a special fragment of that of CL.
The constructivistic philosophy of INT, however, has never really found an
intuitively convincing and mathematically strict semantical justification. CL
has good claims to provide such a justification and hence a materialization of
Kolmogorov's known thesis "INT = logic of problems". The present paper contains
a soundness proof for INT with respect to the CL semantics. A comprehensive
online source on CL is available at http://www.cis.upenn.edu/~giorgi/cl.html



An efficient and flexible engine for computing fixed points is critical for
many practical applications. In this paper, we firstly present a goal-directed
fixed point computation strategy in the logic programming paradigm. The
strategy adopts a tabled resolution (or memorized resolution) to mimic the
efficient semi-naive bottom-up computation. Its main idea is to dynamically
identify and record those clauses that will lead to recursive variant calls,
and then repetitively apply those alternatives incrementally until the fixed
point is reached. Secondly, there are many situations in which a fixed point
contains a large number or even infinite number of solutions. In these cases, a
fixed point computation engine may not be efficient enough or feasible at all.
We present a mode-declaration scheme which provides the capabilities to reduce
a fixed point from a big solution set to a preferred small one, or from an
infeasible infinite set to a finite one. The mode declaration scheme can be
characterized as a meta-level operation over the original fixed point. We show
the correctness of the mode declaration scheme. Thirdly, the mode-declaration
scheme provides a new declarative method for dynamic programming, which is
typically used for solving optimization problems. There is no need to define
the value of an optimal solution recursively, instead, defining a general
solution suffices. The optimal value as well as its corresponding concrete
solution can be derived implicitly and automatically using a mode-directed
fixed point computation engine. Finally, this fixed point computation engine
has been successfully implemented in a commercial Prolog system. Experimental
results are shown to indicate that the mode declaration improves both time and
space performances in solving dynamic programming problems.



Computability logic is a formal theory of computational tasks and resources.
Formulas in it represent interactive computational problems, and "truth" is
understood as algorithmic solvability. Interactive computational problems, in
turn, are defined as a certain sort games between a machine and its
environment, with logical operators standing for operations on such games.
Within the ambitious program of finding axiomatizations for incrementally rich
fragments of this semantically introduced logic, the earlier article "From
truth to computability I" proved soundness and completeness for system CL3,
whose language has the so called parallel connectives (including negation),
choice connectives, choice quantifiers, and blind quantifiers. The present
paper extends that result to the significantly more expressive system CL4 with
the same collection of logical operators. What makes CL4 expressive is the
presence of two sorts of atoms in its language: elementary atoms, representing
elementary computational problems (i.e. predicates, i.e. problems of zero
degree of interactivity), and general atoms, representing arbitrary
computational problems. CL4 conservatively extends CL3, with the latter being
nothing but the general-atom-free fragment of the former. Removing the blind
(classical) group of quantifiers from the language of CL4 is shown to yield a
decidable logic despite the fact that the latter is still first-order. A
comprehensive online source on computability logic can be found at
http://www.cis.upenn.edu/~giorgi/cl.html



We present methods to synthesize cooperative strategies for multi-vehicle
control problems using mixed integer linear programming. Complex multi-vehicle
control problems are expressed as mixed logical dynamical systems. Optimal
strategies for these systems are then solved for using mixed integer linear
programming. We motivate the methods on problems derived from an adversarial
game between two teams of robots called RoboFlag. We assume the strategy for
one team is fixed and governed by state machines. The strategy for the other
team is generated using our methods. Finally, we perform an average case
computational complexity study on our approach.



We study the problem of deciding whether some PSPACE-complete problems have
models of bounded size. Contrary to problems in NP, models of PSPACE-complete
problems may be exponentially large. However, such models may take polynomial
space in a succinct representation. For example, the models of a QBF are
explicitely represented by and-or trees (which are always of exponential size)
but can be succinctely represented by circuits (which can be polynomial or
exponential). We investigate the complexity of deciding the existence of such
succinct models when a bound on size is given.



To test incomplete search algorithms for constraint satisfaction problems
such as 3-SAT, we need a source of hard, but satisfiable, benchmark instances.
A simple way to do this is to choose a random truth assignment A, and then
choose clauses randomly from among those satisfied by A. However, this method
tends to produce easy problems, since the majority of literals point toward the
``hidden'' assignment A. Last year, Achlioptas, Jia and Moore proposed a
problem generator that cancels this effect by hiding both A and its complement.
While the resulting formulas appear to be just as hard for DPLL algorithms as
random 3-SAT formulas with no hidden assignment, they can be solved by WalkSAT
in only polynomial time. Here we propose a new method to cancel the attraction
to A, by choosing a clause with t > 0 literals satisfied by A with probability
proportional to q^t for some q < 1. By varying q, we can generate formulas
whose variables have no bias, i.e., which are equally likely to be true or
false; we can even cause the formula to ``deceptively'' point away from A. We
present theoretical and experimental results suggesting that these formulas are
exponentially hard both for DPLL algorithms and for incomplete algorithms such
as WalkSAT.



The evaluation of incomplete satisfiability solvers depends critically on the
availability of hard satisfiable instances. A plausible source of such
instances consists of random k-SAT formulas whose clauses are chosen uniformly
from among all clauses satisfying some randomly chosen truth assignment A.
Unfortunately, instances generated in this manner tend to be relatively easy
and can be solved efficiently by practical heuristics. Roughly speaking, as the
formula's density increases, for a number of different algorithms, A acts as a
stronger and stronger attractor. Motivated by recent results on the geometry of
the space of satisfying truth assignments of random k-SAT and NAE-k-SAT
formulas, we introduce a simple twist on this basic model, which appears to
dramatically increase its hardness. Namely, in addition to forbidding the
clauses violated by the hidden assignment A, we also forbid the clauses
violated by its complement, so that both A and complement of A are satisfying.
It appears that under this "symmetrization'' the effects of the two attractors
largely cancel out, making it much harder for algorithms to find any truth
assignment. We give theoretical and experimental evidence supporting this
assertion.



We study the connection between the order of phase transitions in
combinatorial problems and the complexity of decision algorithms for such
problems. We rigorously show that, for a class of random constraint
satisfaction problems, a limited connection between the two phenomena indeed
exists. Specifically, we extend the definition of the spine order parameter of
Bollobas et al. to random constraint satisfaction problems, rigorously showing
that for such problems a discontinuity of the spine is associated with a
$2^{\Omega(n)}$ resolution complexity (and thus a $2^{\Omega(n)}$ complexity of
DPLL algorithms) on random instances. The two phenomena have a common
underlying cause: the emergence of ``large'' (linear size) minimally
unsatisfiable subformulas of a random formula at the satisfiability phase
transition.
  We present several further results that add weight to the intuition that
random constraint satisfaction problems with a sharp threshold and a continuous
spine are ``qualitatively similar to random 2-SAT''. Finally, we argue that it
is the spine rather than the backbone parameter whose continuity has
implications for the decision complexity of combinatorial problems, and we
provide experimental evidence that the two parameters can behave in a different
manner.



We derive novel conditions that guarantee convergence of the Sum-Product
algorithm (also known as Loopy Belief Propagation or simply Belief Propagation)
to a unique fixed point, irrespective of the initial messages. The
computational complexity of the conditions is polynomial in the number of
variables. In contrast with previously existing conditions, our results are
directly applicable to arbitrary factor graphs (with discrete variables) and
are shown to be valid also in the case of factors containing zeros, under some
additional conditions. We compare our bounds with existing ones, numerically
and, if possible, analytically. For binary variables with pairwise
interactions, we derive sufficient conditions that take into account local
evidence (i.e., single variable factors) and the type of pair interactions
(attractive or repulsive). It is shown empirically that this bound outperforms
existing bounds.



A new technique is presented developed to learn multi-class concepts from
clinical electroencephalograms. A desired concept is represented as a neuronal
computational model consisting of the input, hidden, and output neurons. In
this model the hidden neurons learn independently to classify the
electroencephalogram segments presented by spectral and statistical features.
This technique has been applied to the electroencephalogram data recorded from
65 sleeping healthy newborns in order to learn a brain maturation concept of
newborns aged between 35 and 51 weeks. The 39399 and 19670 segments from these
data have been used for learning and testing the concept, respectively. As a
result, the concept has correctly classified 80.1% of the testing segments or
87.7% of the 65 records.



In this paper we describe a new method combining the polynomial neural
network and decision tree techniques in order to derive comprehensible
classification rules from clinical electroencephalograms (EEGs) recorded from
sleeping newborns. These EEGs are heavily corrupted by cardiac, eye movement,
muscle and noise artifacts and as a consequence some EEG features are
irrelevant to classification problems. Combining the polynomial network and
decision tree techniques, we discover comprehensible classification rules
whilst also attempting to keep their classification error down. This technique
is shown to outperform a number of commonly used machine learning technique
applied to automatically recognize artifacts in the sleep EEGs.



This paper describes and evaluates the Metalinguistic Operation Processor
(MOP) system for automatic compilation of metalinguistic information from
technical and scientific documents. This system is designed to extract
non-standard terminological resources that we have called Metalinguistic
Information Databases (or MIDs), in order to help update changing glossaries,
knowledge bases and ontologies, as well as to reflect the metastable dynamics
of special-domain knowledge.



We survey a new area of parameter-free similarity distance measures useful in
data-mining, pattern recognition, learning and automatic semantics extraction.
Given a family of distances on a set of objects, a distance is universal up to
a certain precision for that family if it minorizes every distance in the
family between every two objects in the set, up to the stated precision (we do
not require the universal distance to be an element of the family). We consider
similarity distances for two types of objects: literal objects that as such
contain all of their meaning, like genomes or books, and names for objects. The
latter may have literal embodyments like the first type, but may also be
abstract like ``red'' or ``christianity.'' For the first type we consider a
family of computable distance measures corresponding to parameters expressing
similarity according to particular features between pairs of literal objects.
For the second type we consider similarity distances generated by web users
corresponding to particular semantic relations between the (names for) the
designated objects. For both families we give universal similarity distance
measures, incorporating all particular distance measures in the family. In the
first case the universal distance is based on compression and in the second
case it is based on Google page counts related to search terms. In both cases
experiments on a massive scale give evidence of the viability of the
approaches.



We study a class of random 3-SAT instances having exactly one solution. The
properties of this ensemble considerably differ from those of a random 3-SAT
ensemble. It is numerically shown that the running time of several complete and
stochastic local search algorithms monotonically increases as the clause
density is decreased. Therefore, there is no easy-hard-easy pattern of hardness
as for standard random 3-SAT ensemble. Furthermore, the running time for short
single-solution formulas increases with the problem size much faster than for
random 3-SAT formulas from the phase transition region.



When acquiring an image of a paper document, the image printed on the back
page sometimes shows through. The mixture of the front- and back-page images
thus obtained is markedly nonlinear, and thus constitutes a good real-life test
case for nonlinear blind source separation.
  This paper addresses a difficult version of this problem, corresponding to
the use of "onion skin" paper, which results in a relatively strong
nonlinearity of the mixture, which becomes close to singular in the lighter
regions of the images. The separation is achieved through the MISEP technique,
which is an extension of the well known INFOMAX method. The separation results
are assessed with objective quality measures. They show an improvement over the
results obtained with linear separation, but have room for further improvement.



Discovering patterns from data is an important task in data mining. There
exist techniques to find large collections of many kinds of patterns from data
very efficiently. A collection of patterns can be regarded as a summary of the
data. A major difficulty with patterns is that pattern collections summarizing
the data well are often very large.
  In this dissertation we describe methods for summarizing pattern collections
in order to make them also more understandable. More specifically, we focus on
the following themes: 1) Quality value simplifications. 2) Pattern orderings.
3) Pattern chains and antichains. 4) Change profiles. 5) Inverse pattern
discovery.



Individual-intelligence research, from a neurological perspective, discusses
the hierarchical layers of the cortex as a structure that performs conceptual
abstraction and specification. This theory has been used to explain how
motor-cortex regions responsible for different behavioral modalities such as
writing and speaking can be utilized to express the same general concept
represented higher in the cortical hierarchy. For example, the concept of a
dog, represented across a region of high-level cortical-neurons, can either be
written or spoken about depending on the individual's context. The higher-layer
cortical areas project down the hierarchy, sending abstract information to
specific regions of the motor-cortex for contextual implementation. In this
paper, this idea is expanded to incorporate collective-intelligence within a
hyper-cortical construct. This hyper-cortex is a multi-layered network used to
represent abstract collective concepts. These ideas play an important role in
understanding how collective-intelligence systems can be engineered to handle
problem abstraction and solution specification. Finally, a collection of common
problems in the scientific community are solved using an artificial
hyper-cortex generated from digital-library metadata.



Recursive loops in a logic program present a challenging problem to the PLP
framework. On the one hand, they loop forever so that the PLP backward-chaining
inferences would never stop. On the other hand, they generate cyclic
influences, which are disallowed in Bayesian networks. Therefore, in existing
PLP approaches logic programs with recursive loops are considered to be
problematic and thus are excluded. In this paper, we propose an approach that
makes use of recursive loops to build a stationary dynamic Bayesian network.
Our work stems from an observation that recursive loops in a logic program
imply a time sequence and thus can be used to model a stationary dynamic
Bayesian network without using explicit time parameters. We introduce a
Bayesian knowledge base with logic clauses of the form $A \leftarrow
A_1,...,A_l, true, Context, Types$, which naturally represents the knowledge
that the $A_i$s have direct influences on $A$ in the context $Context$ under
the type constraints $Types$. We then use the well-founded model of a logic
program to define the direct influence relation and apply SLG-resolution to
compute the space of random variables together with their parental connections.
We introduce a novel notion of influence clauses, based on which a declarative
semantics for a Bayesian knowledge base is established and algorithms for
building a two-slice dynamic Bayesian network from a logic program are
developed.



In this work we consider the task of relaxing the i.i.d assumption in pattern
recognition (or classification), aiming to make existing learning algorithms
applicable to a wider range of tasks. Pattern recognition is guessing a
discrete label of some object based on a set of given examples (pairs of
objects and labels). We consider the case of deterministically defined labels.
Traditionally, this task is studied under the assumption that examples are
independent and identically distributed. However, it turns out that many
results of pattern recognition theory carry over a weaker assumption. Namely,
under the assumption of conditional independence and identical distribution of
objects, while the only assumption on the distribution of labels is that the
rate of occurrence of each label should be above some positive threshold.
  We find a broad class of learning algorithms for which estimations of the
probability of a classification error achieved under the classical i.i.d.
assumption can be generalised to the similar estimates for the case of
conditionally i.i.d. examples.



We bound the future loss when predicting any (computably) stochastic sequence
online. Solomonoff finitely bounded the total deviation of his universal
predictor M from the true distribution m by the algorithmic complexity of m.
Here we assume we are at a time t>1 and already observed x=x_1...x_t. We bound
the future prediction performance on x_{t+1}x_{t+2}... by a new variant of
algorithmic complexity of m given x, plus the complexity of the randomness
deficiency of x. The new complexity is monotone in its condition in the sense
that this complexity can only decrease if the condition is prolonged. We also
briefly discuss potential generalizations to Bayesian model classes and to
classification problems.



This article presents an overview of computability logic -- the
game-semantically constructed logic of interactive computational tasks and
resources. There is only one non-overview, technical section in it, devoted to
a proof of the soundness of affine logic with respect to the semantics of
computability logic. A comprehensive online source on the subject can be found
at http://www.cis.upenn.edu/~giorgi/cl.html



Results about the redundancy of circumscriptive and default theories are
presented. In particular, the complexity of establishing whether a given theory
is redundant is establihsed.



This paper explores the concept of engagement, the process by which
individuals in an interaction start, maintain and end their perceived
connection to one another. The paper reports on one aspect of engagement among
human interactors--the effect of tracking faces during an interaction. It also
describes the architecture of a robot that can participate in conversational,
collaborative interactions with engagement gestures. Finally, the paper reports
on findings of experiments with human participants who interacted with a robot
when it either performed or did not perform engagement gestures. Results of the
human-robot studies indicate that people become engaged with robots: they
direct their attention to the robot more often in interactions where engagement
gestures are present, and they find interactions more appropriate when
engagement gestures are present than when they are not.



An algorithm for answering conjunctive queries over SHIQ knowledge bases that
is coNP in data complexity is given. The algorithm is based on the tableau
algorithm for reasoning with individuals in SHIQ. The blocking conditions of
the tableau are weakened in such a way that the set of models the modified
algorithm yields suffices to check query entailment. The modified blocking
conditions are based on the ones proposed by Levy and Rousset for reasoning
with Horn Rules in the description logic ALCNR.



A person is given a numbered sequence of positions on a sheet of paper. The
person is asked, "Which will be the next (or the next after that) position?"
Everyone has an opinion as to how he or she would proceed. There are regular
sequences for which there is general agreement on how to continue. However,
there are less regular sequences for which this assessment is less certain.
There are sequences for which every continuation is perceived to be arbitrary.
I would like to present a mathematical model that reflects these opinions and
perceptions with the aid of a valuation function. It is necessary to apply a
rich set of invariant features of position sequences to ensure the quality of
this model. All other properties of the model are arbitrary.



We describe message-passing and decimation approaches for lossy source coding
using low-density generator matrix (LDGM) codes. In particular, this paper
addresses the problem of encoding a Bernoulli(0.5) source: for randomly
generated LDGM codes with suitably irregular degree distributions, our methods
yield performance very close to the rate distortion limit over a range of
rates. Our approach is inspired by the survey propagation (SP) algorithm,
originally developed by Mezard et al. for solving random satisfiability
problems. Previous work by Maneva et al. shows how SP can be understood as
belief propagation (BP) for an alternative representation of satisfiability
problems. In analogy to this connection, our approach is to define a family of
Markov random fields over generalized codewords, from which local
message-passing rules can be derived in the standard way. The overall source
encoding method is based on message-passing, setting a subset of bits to their
preferred values (decimation), and reducing the code.



We develop and analyze methods for computing provably optimal {\em maximum a
posteriori} (MAP) configurations for a subclass of Markov random fields defined
on graphs with cycles. By decomposing the original distribution into a convex
combination of tree-structured distributions, we obtain an upper bound on the
optimal value of the original problem (i.e., the log probability of the MAP
assignment) in terms of the combined optimal values of the tree problems. We
prove that this upper bound is tight if and only if all the tree distributions
share an optimal configuration in common. An important implication is that any
such shared configuration must also be a MAP configuration for the original
distribution. Next we develop two approaches to attempting to obtain tight
upper bounds: (a) a {\em tree-relaxed linear program} (LP), which is derived
from the Lagrangian dual of the upper bounds; and (b) a {\em tree-reweighted
max-product message-passing algorithm} that is related to but distinct from the
max-product algorithm. In this way, we establish a connection between a certain
LP relaxation of the mode-finding problem, and a reweighted form of the
max-product (min-sum) message-passing algorithm.



Max-product "belief propagation" is an iterative, local, message-passing
algorithm for finding the maximum a posteriori (MAP) assignment of a discrete
probability distribution specified by a graphical model. Despite the
spectacular success of the algorithm in many application areas such as
iterative decoding, computer vision and combinatorial optimization which
involve graphs with many cycles, theoretical results about both correctness and
convergence of the algorithm are known in few cases (Weiss-Freeman Wainwright,
Yeddidia-Weiss-Freeman, Richardson-Urbanke}.
  In this paper we consider the problem of finding the Maximum Weight Matching
(MWM) in a weighted complete bipartite graph. We define a probability
distribution on the bipartite graph whose MAP assignment corresponds to the
MWM. We use the max-product algorithm for finding the MAP of this distribution
or equivalently, the MWM on the bipartite graph. Even though the underlying
bipartite graph has many short cycles, we find that surprisingly, the
max-product algorithm always converges to the correct MAP assignment as long as
the MAP assignment is unique. We provide a bound on the number of iterations
required by the algorithm and evaluate the computational cost of the algorithm.
We find that for a graph of size $n$, the computational cost of the algorithm
scales as $O(n^3)$, which is the same as the computational cost of the best
known algorithm. Finally, we establish the precise relation between the
max-product algorithm and the celebrated {\em auction} algorithm proposed by
Bertsekas. This suggests possible connections between dual algorithm and
max-product algorithm for discrete optimization problems.



The concept of a temporal phylogenetic network is a mathematical model of
evolution of a family of natural languages. It takes into account the fact that
languages can trade their characteristics with each other when linguistic
communities are in contact, and also that a contact is only possible when the
languages are spoken at the same time. We show how computational methods of
answer set programming and constraint logic programming can be used to generate
plausible conjectures about contacts between prehistoric linguistic
communities, and illustrate our approach by applying it to the evolutionary
history of Indo-European languages.
  To appear in Theory and Practice of Logic Programming (TPLP).



The prime number theorem, established by Hadamard and de la Vall'ee Poussin
independently in 1896, asserts that the density of primes in the positive
integers is asymptotic to 1 / ln x. Whereas their proofs made serious use of
the methods of complex analysis, elementary proofs were provided by Selberg and
Erd"os in 1948. We describe a formally verified version of Selberg's proof,
obtained using the Isabelle proof assistant.



In this paper, we try to further demonstrate that the models of random CSP
instances proposed by [Xu and Li, 2000; 2003] are of theoretical and practical
interest. Indeed, these models, called RB and RD, present several nice
features. First, it is quite easy to generate random instances of any arity
since no particular structure has to be integrated, or property enforced, in
such instances. Then, the existence of an asymptotic phase transition can be
guaranteed while applying a limited restriction on domain size and on
constraint tightness. In that case, a threshold point can be precisely located
and all instances have the guarantee to be hard at the threshold, i.e., to have
an exponential tree-resolution complexity. Next, a formal analysis shows that
it is possible to generate forced satisfiable instances whose hardness is
similar to unforced satisfiable ones. This analysis is supported by some
representative results taken from an intensive experimentation that we have
carried out, using complete and incomplete search methods.



Preference queries are relational algebra or SQL queries that contain
occurrences of the winnow operator ("find the most preferred tuples in a given
relation"). Such queries are parameterized by specific preference relations.
Semantic optimization techniques make use of integrity constraints holding in
the database. In the context of semantic optimization of preference queries, we
identify two fundamental properties: containment of preference relations
relative to integrity constraints and satisfaction of order axioms relative to
integrity constraints. We show numerous applications of those notions to
preference query evaluation and optimization. As integrity constraints, we
consider constraint-generating dependencies, a class generalizing functional
dependencies. We demonstrate that the problems of containment and satisfaction
of order axioms can be captured as specific instances of constraint-generating
dependency entailment. This makes it possible to formulate necessary and
sufficient conditions for the applicability of our techniques as constraint
validity problems. We characterize the computational complexity of such
problems.



Research on integrated neural-symbolic systems has made significant progress
in the recent past. In particular the understanding of ways to deal with
symbolic knowledge within connectionist systems (also called artificial neural
networks) has reached a critical mass which enables the community to strive for
applicable implementations and use cases. Recent work has covered a great
variety of logics used in artificial intelligence and provides a multitude of
techniques for dealing with them within the context of artificial neural
networks. We present a comprehensive survey of the field of neural-symbolic
integration, including a new classification of system according to their
architectures and abilities.



We define a class of probabilistic models in terms of an operator algebra of
stochastic processes, and a representation for this class in terms of
stochastic parameterized grammars. A syntactic specification of a grammar is
mapped to semantics given in terms of a ring of operators, so that grammatical
composition corresponds to operator addition or multiplication. The operators
are generators for the time-evolution of stochastic processes. Within this
modeling framework one can express data clustering models, logic programs,
ordinary and stochastic differential equations, graph grammars, and stochastic
chemical reaction kinetics. This mathematical formulation connects these
apparently distant fields to one another and to mathematical methods from
quantum field theory and operator algebra.



This paper is concerned with the reliable inference of optimal
tree-approximations to the dependency structure of an unknown distribution
generating data. The traditional approach to the problem measures the
dependency strength between random variables by the index called mutual
information. In this paper reliability is achieved by Walley's imprecise
Dirichlet model, which generalizes Bayesian learning with Dirichlet priors.
Adopting the imprecise Dirichlet model results in posterior interval
expectation for mutual information, and in a set of plausible trees consistent
with the data. Reliable inference about the actual tree is achieved by focusing
on the substructure common to all the plausible trees. We develop an exact
algorithm that infers the substructure in time O(m^4), m being the number of
random variables. The new algorithm is applied to a set of data sampled from a
known distribution. The method is shown to reliably infer edges of the actual
tree even when the data are very scarce, unlike the traditional approach.
Finally, we provide lower and upper credibility limits for mutual information
under the imprecise Dirichlet model. These enable the previous developments to
be extended to a full inferential method for trees.



The paper gives a soundness and completeness proof for the implicative
fragment of intuitionistic calculus with respect to the semantics of
computability logic, which understands intuitionistic implication as
interactive algorithmic reduction. This concept -- more precisely, the
associated concept of reducibility -- is a generalization of Turing
reducibility from the traditional, input/output sorts of problems to
computational tasks of arbitrary degrees of interactivity. See
http://www.cis.upenn.edu/~giorgi/cl.html for a comprehensive online source on
computability logic.



This technical note describes a monotone and continuous fixpoint operator to
compute the answer sets of programs with aggregates. The fixpoint operator
relies on the notion of aggregate solution. Under certain conditions, this
operator behaves identically to the three-valued immediate consequence operator
$\Phi^{aggr}_P$ for aggregate programs, independently proposed Pelov et al.
This operator allows us to closely tie the computational complexity of the
answer set checking and answer sets existence problems to the cost of checking
a solution of the aggregates in the program. Finally, we relate the semantics
described by the operator to other proposals for logic programming with
aggregates.
  To appear in Theory and Practice of Logic Programming (TPLP).



Control flow compilation is a hybrid between classical WAM compilation and
meta-call, limited to the compilation of non-recursive clause bodies. This
approach is used successfully for the execution of dynamically generated
queries in an inductive logic programming setting (ILP). Control flow
compilation reduces compilation times up to an order of magnitude, without
slowing down execution. A lazy variant of control flow compilation is also
presented. By compiling code by need, it removes the overhead of compiling
unreached code (a frequent phenomenon in practical ILP settings), and thus
reduces the size of the compiled code. Both dynamic compilation approaches have
been implemented and were combined with query packs, an efficient ILP execution
mechanism. It turns out that locality of data and code is important for
performance. The experiments reported in the paper show that lazy control flow
compilation is superior in both artificial and real life settings.



Lexical constraints on the input of speech and on-line handwriting systems
improve the performance of such systems. A significant gain in speed can be
achieved by integrating in a digraph structure the different Hidden Markov
Models (HMM) corresponding to the words of the relevant lexicon. This
integration avoids redundant computations by sharing intermediate results
between HMM's corresponding to different words of the lexicon. In this paper,
we introduce a token passing method to perform simultaneously the computation
of the a posteriori probabilities of all the words of the lexicon. The coding
scheme that we introduce for the tokens is optimal in the information theory
sense. The tokens use the minimum possible number of bits. Overall, we optimize
simultaneously the execution speed and the memory requirement of the
recognition systems.



The general pupose of the scholarly communication process is to support the
creation and dissemination of ideas within the scientific community. At a finer
granularity, there exists multiple stages which, when confronted by a member of
the community, have different requirements and therefore different solutions.
In order to take a researcher's idea from an initial inspiration to a community
resource, the scholarly communication infrastructure may be required to 1)
provide a scientist initial seed ideas; 2) form a team of well suited
collaborators; 3) located the most appropriate venue to publish the formalized
idea; 4) determine the most appropriate peers to review the manuscript; and 5)
disseminate the end product to the most interested members of the community.
Through the various delinieations of this process, the requirements of each
stage are tied soley to the multi-functional resources of the community: its
researchers, its journals, and its manuscritps. It is within the collection of
these resources and their inherent relationships that the solutions to
scholarly communication are to be found. This paper describes an associative
network composed of multiple scholarly artifacts that can be used as a medium
for supporting the scholarly communication process.



In this paper, computational aspects of the panel aggregation problem are
addressed. Motivated primarily by applications of risk assessment, an algorithm
is developed for aggregating large corpora of internally incoherent probability
assessments. The algorithm is characterized by a provable performance
guarantee, and is demonstrated to be orders of magnitude faster than existing
tools when tested on several real-world data-sets. In addition, unexpected
connections between research in risk assessment and wireless sensor networks
are exposed, as several key ideas are illustrated to be useful in both fields.



We study the complexity and expressive power of conjunctive queries over
unranked labeled trees represented using a variety of structure relations such
as ``child'', ``descendant'', and ``following'' as well as unary relations for
node labels. We establish a framework for characterizing structures
representing trees for which conjunctive queries can be evaluated efficiently.
Then we completely chart the tractability frontier of the problem and establish
a dichotomy theorem for our axis relations, i.e., we find all subset-maximal
sets of axes for which query evaluation is in polynomial time and show that for
all other cases, query evaluation is NP-complete. All polynomial-time results
are obtained immediately using the proof techniques from our framework.
Finally, we study the expressiveness of conjunctive queries over trees and show
that for each conjunctive query, there is an equivalent acyclic positive query
(i.e., a set of acyclic conjunctive queries), but that in general this query is
not of polynomial size.



This paper presents a soundness and completeness proof for propositional
intuitionistic calculus with respect to the semantics of computability logic.
The latter interprets formulas as interactive computational problems,
formalized as games between a machine and its environment. Intuitionistic
implication is understood as algorithmic reduction in the weakest possible --
and hence most natural -- sense, disjunction and conjunction as
deterministic-choice combinations of problems (disjunction = machine's choice,
conjunction = environment's choice), and "absurd" as a computational problem of
universal strength. See http://www.cis.upenn.edu/~giorgi/cl.html for a
comprehensive online source on computability logic.



We establish the convergence of the min-sum message passing algorithm for
minimization of a broad class of quadratic objective functions: those that
admit a convex decomposition. Our results also apply to the equivalent problem
of the convergence of Gaussian belief propagation.



We propose consensus propagation, an asynchronous distributed protocol for
averaging numbers across a network. We establish convergence, characterize the
convergence rate for regular graphs, and demonstrate that the protocol exhibits
better scaling properties than pairwise averaging, an alternative that has
received much recent attention. Consensus propagation can be viewed as a
special case of belief propagation, and our results contribute to the belief
propagation literature. In particular, beyond singly-connected graphs, there
are very few classes of relevant problems for which belief propagation is known
to converge.



We investigate a prototypical agent-based model, the Naming Game, on random
geometric networks. The Naming Game is a minimal model, employing local
communications that captures the emergence of shared communication schemes
(languages) in a population of autonomous semiotic agents. Implementing the
Naming Games on random geometric graphs, local communications being local
broadcasts, serves as a model for agreement dynamics in large-scale,
autonomously operating wireless sensor networks. Further, it captures essential
features of the scaling properties of the agreement process for
spatially-embedded autonomous agents. We also present results for the case when
a small density of long-range communication links are added on top of the
random geometric graph, resulting in a "small-world"-like network and yielding
a significantly reduced time to reach global agreement.



The Unified Software Development Process (USDP) and UML have been now
generally accepted as the standard methodology and modeling language for
developing Object-Oriented Systems. Although Agent-based Systems introduces new
issues, we consider that USDP and UML can be used in an extended manner for
modeling Agent-based Systems. The paper presents a methodology for designing
agent-based systems and the specific models expressed in an UML-based notation
corresponding to each phase of the software development process. UML was
extended using the provided mechanism: stereotypes. Therefore, this approach
can be managed with any CASE tool supporting UML. A Case Study, the development
of a specific agent-based Student Evaluation System (SAS), is presented.



Mobile agents research is clearly aiming towards imposing agent based
development as the next generation of tools for writing software. This paper
comes with its own contribution to this global goal by introducing a novel
unifying framework meant to bring simplicity and interoperability to and among
agent platforms as we know them today. In addition to this, we also introduce a
set of agent behaviors which, although tailored for and from the area of
virtual learning environments, are none the less generic enough to be used for
rapid, simple, useful and reliable agent deployment. The paper also presents an
illustrative case study brought forward to prove the feasibility of our design.



E-learning is nowadays one of the most interesting of the "e- " domains
available through the Internet. The main problem to create a Web-based, virtual
environment is to model the traditional domain and to implement the model using
the most suitable technologies. We analyzed the distance learning domain and
investigated the possibility to implement some e-learning services using mobile
agent technologies. This paper presents a model of the Student Assessment
Service (SAS) and an agent-based framework developed to be used for
implementing specific applications. A specific Student Assessment application
that relies on the framework was developed.



Logical formalisms for reasoning about relations between spatial regions play
a fundamental role in geographical information systems, spatial and constraint
databases, and spatial reasoning in AI. In analogy with Halpern and Shoham's
modal logic of time intervals based on the Allen relations, we introduce a
family of modal logics equipped with eight modal operators that are interpreted
by the Egenhofer-Franzosa (or RCC8) relations between regions in topological
spaces such as the real plane. We investigate the expressive power and
computational complexity of logics obtained in this way. It turns out that our
modal logics have the same expressive power as the two-variable fragment of
first-order logic, but are exponentially less succinct. The complexity ranges
from (undecidable and) recursively enumerable to highly undecidable, where the
recursively enumerable logics are obtained by considering substructures of
structures induced by topological spaces. As our undecidability results also
capture logics based on the real line, they improve upon undecidability results
for interval temporal logics by Halpern and Shoham. We also analyze modal
logics based on the five RCC5 relations, with similar results regarding the
expressive power, but weaker results regarding the complexity.



The aim of this paper is to address the question: Can an artificial neural
network (ANN) model be used as a possible characterization of the power of the
human mind? We will discuss what might be the relationship between such a model
and its natural counterpart. A possible characterization of the different power
capabilities of the mind is suggested in terms of the information contained (in
its computational complexity) or achievable by it. Such characterization takes
advantage of recent results based on natural neural networks (NNN) and the
computational power of arbitrary artificial neural networks (ANN). The possible
acceptance of neural networks as the model of the human mind's operation makes
the aforementioned quite relevant.



The peer-review process is the most widely accepted certification mechanism
for officially accepting the written results of researchers within the
scientific community. An essential component of peer-review is the
identification of competent referees to review a submitted manuscript. This
article presents an algorithm to automatically determine the most appropriate
reviewers for a manuscript by way of a co-authorship network data structure and
a relative-rank particle-swarm algorithm. This approach is novel in that it is
not limited to a pre-selected set of referees, is computationally efficient,
requires no human-intervention, and, in some instances, can automatically
identify conflict of interest situations. A useful application of this
algorithm would be to open commentary peer-review systems because it provides a
weighting for each referee with respects to their expertise in the domain of a
manuscript. The algorithm is validated using referee bid data from the 2005
Joint Conference on Digital Libraries.



This paper presents results of an ongoing interdisciplinary study to develop
a computational theory of creativity for engineering design. Human design
activities are surveyed, and popular computer-aided design methodologies are
examined. It is argued that semiotics has the potential to merge and unite
various design approaches into one fundamental theory that is naturally
interpretable and so comprehensible in terms of computer use. Reviewing related
work in philosophy, psychology, and cognitive science provides a general and
encompassing vision of the creativity phenomenon. Basic notions of algebraic
semiotics are given and explained in terms of design. This is to define a model
of the design creative process, which is seen as a process of semiosis, where
concepts and their attributes represented as signs organized into systems are
evolved, blended, and analyzed, resulting in the development of new concepts.
The model allows us to formally describe and investigate essential properties
of the design process, namely its dynamics and non-determinism inherent in
creative thinking. A stable pattern of creative thought - analogical and
metaphorical reasoning - is specified to demonstrate the expressive power of
the modeling approach; illustrative examples are given. The developed theory is
applied to clarify the nature of emergence in design: it is shown that while
emergent properties of a product may influence its creative value, emergence
can simply be seen as a by-product of the creative process. Concluding remarks
summarize the research, point to some unresolved issues, and outline directions
for future work.



This paper investigates the concept of digital city. First, a functional
analysis of a digital city is made in the light of the modern study of
urbanism; similarities between the virtual and urban constructions are pointed
out. Next, a semiotic perspective on the subject matter is elaborated, and a
terminological basis is introduced to treat a digital city as a self-organizing
meaning-producing system intended to support social or spatial navigation. An
explicit definition of a digital city is formulated. Finally, the proposed
approach is discussed, conclusions are given, and future work is outlined.



Recently, extensive efforts have been made on the application of expert
system technique to solving the process planning task in the machining domain.
This paper introduces a new formal method to design CAPP expert systems. The
formal method is applied to provide a contour of the CAPP expert system
building technology. Theoretical aspects of the formalism are described and
illustrated by an example of know-how analysis. Flexible facilities to utilize
multiple knowledge types and multiple planning strategies within one system are
provided by the technology.



We present an unsupervised learning algorithm that mines large text corpora
for patterns that express implicit semantic relations. For a given input word
pair X:Y with some unspecified semantic relations, the corresponding output
list of patterns <P1,...,Pm> is ranked according to how well each pattern Pi
expresses the relations between X and Y. For example, given X=ostrich and
Y=bird, the two highest ranking output patterns are "X is the largest Y" and "Y
such as the X". The output patterns are intended to be useful for finding
further pairs with the same relations, to support the construction of lexicons,
ontologies, and semantic networks. The patterns are sorted by pertinence, where
the pertinence of a pattern Pi for a word pair X:Y is the expected relational
similarity between the given pair and typical pairs for Pi. The algorithm is
empirically evaluated on two tasks, solving multiple-choice SAT word analogy
questions and classifying semantic relations in noun-modifier pairs. On both
tasks, the algorithm achieves state-of-the-art results, performing
significantly better than several alternative pattern ranking algorithms, based
on tf-idf.



Event-driven reactive functionalities are an urgent need in nowadays
distributed service-oriented applications and (Semantic) Web-based
environments. An important problem to be addressed is how to correctly and
efficiently capture and process the event-based behavioral, reactive logic
represented as ECA rules in combination with other conditional decision logic
which is represented as derivation rules. In this paper we elaborate on a
homogeneous integration approach which combines derivation rules, reaction
rules (ECA rules) and other rule types such as integrity constraint into the
general framework of logic programming. The developed ECA-LP language provides
expressive features such as ID-based updates with support for external and
self-updates of the intensional and extensional knowledge, transac-tions
including integrity testing and an event algebra to define and process complex
events and actions based on a novel interval-based Event Calculus variant.



We consider sensor scheduling as the optimal observability problem for
partially observable Markov decision processes (POMDP). This model fits to the
cases where a Markov process is observed by a single sensor which needs to be
dynamically adjusted or by a set of sensors which are selected one at a time in
a way that maximizes the information acquisition from the process. Similar to
conventional POMDP problems, in this model the control action is based on all
past measurements; however here this action is not for the control of state
process, which is autonomous, but it is for influencing the measurement of that
process. This POMDP is a controlled version of the hidden Markov process, and
we show that its optimal observability problem can be formulated as an average
cost Markov decision process (MDP) scheduling problem. In this problem, a
policy is a rule for selecting sensors or adjusting the measuring device based
on the measurement history. Given a policy, we can evaluate the estimation
entropy for the joint state-measurement processes which inversely measures the
observability of state process for that policy. Considering estimation entropy
as the cost of a policy, we show that the problem of finding optimal policy is
equivalent to an average cost MDP scheduling problem where the cost function is
the entropy function over the belief space. This allows the application of the
policy iteration algorithm for finding the policy achieving minimum estimation
entropy, thus optimum observability.



We give some semantic results for an epistemic logic incorporating dynamic
operators to describe information changing events. Such events include
epistemic changes, where agents become more informed about the non-changing
state of the world, and ontic changes, wherein the world changes. The events
are executed in information states that are modeled as pointed Kripke models.
Our contribution consists of three semantic results. (i) Given two information
states, there is an event transforming one into the other. The linguistic
correspondent to this is that every consistent formula can be made true in
every information state by the execution of an event. (ii) A more technical
result is that: every event corresponds to an event in which the postconditions
formalizing ontic change are assignments to `true' and `false' only (instead of
assignments to arbitrary formulas in the logical language). `Corresponds' means
that execution of either event in a given information state results in
bisimilar information states. (iii) The third, also technical, result is that
every event corresponds to a sequence of events wherein all postconditions are
assignments of a single atom only (instead of simultaneous assignments of more
than one atom).



An important problem to be addressed within Event-Driven Architecture (EDA)
is how to correctly and efficiently capture and process the event/action-based
logic. This paper endeavors to bridge the gap between the Knowledge
Representation (KR) approaches based on durable events/actions and such
formalisms as event calculus, on one hand, and event-condition-action (ECA)
reaction rules extending the approach of active databases that view events as
instantaneous occurrences and/or sequences of events, on the other. We propose
formalism based on reaction rules (ECA rules) and a novel interval-based event
logic and present concrete RuleML-based syntax, semantics and implementation.
We further evaluate this approach theoretically, experimentally and on an
example derived from common industry use cases and illustrate its benefits.



Outsourcing of complex IT infrastructure to IT service providers has
increased substantially during the past years. IT service providers must be
able to fulfil their service-quality commitments based upon predefined Service
Level Agreements (SLAs) with the service customer. They need to manage, execute
and maintain thousands of SLAs for different customers and different types of
services, which needs new levels of flexibility and automation not available
with the current technology. The complexity of contractual logic in SLAs
requires new forms of knowledge representation to automatically draw inferences
and execute contractual agreements. A logic-based approach provides several
advantages including automated rule chaining allowing for compact knowledge
representation as well as flexibility to adapt to rapidly changing business
requirements. We suggest adequate logical formalisms for representation and
enforcement of SLA rules and describe a proof-of-concept implementation. The
article describes selected formalisms of the ContractLog KR and their adequacy
for automated SLA management and presents results of experiments to demonstrate
flexibility and scalability of the approach.



We develop a new collaborative filtering (CF) method that combines both
previously known users' preferences, i.e. standard CF, as well as product/user
attributes, i.e. classical function approximation, to predict a given user's
interest in a particular product. Our method is a generalized low rank matrix
completion problem, where we learn a function whose inputs are pairs of vectors
-- the standard low rank matrix completion problem being a special case where
the inputs to the function are the row and column indices of the matrix. We
solve this generalized matrix completion problem using tensor product kernels
for which we also formally generalize standard kernel properties. Benchmark
experiments on movie ratings show the advantages of our generalized matrix
completion method over the standard matrix completion one with no information
about movies or people, as well as over standard multi-task or single task
learning methods.



A useful method for representing Bayesian classifiers is through
\emph{discriminant functions}. Here, using copula functions, we propose a new
model for discriminants. This model provides a rich and generalized class of
decision boundaries. These decision boundaries significantly boost the
classification accuracy especially for high dimensional feature spaces. We
strengthen our analysis through simulation results.



We investigate systematically the impact of human intervention in the
training of computer players in a strategy board game. In that game, computer
players utilise reinforcement learning with neural networks for evolving their
playing strategies and demonstrate a slow learning speed. Human intervention
can significantly enhance learning performance, but carry-ing it out
systematically seems to be more of a problem of an integrated game development
environment as opposed to automatic evolutionary learning.



When genetic algorithms are used to evolve decision trees, key tree quality
parameters can be recursively computed and re-used across generations of
partially similar decision trees. Simply storing instance indices at leaves is
enough for fitness to be piecewise computed in a lossless fashion. We show the
derivation of the (substantial) expected speed-up on two bounding case problems
and trace the attractive property of lossless fitness inheritance to the
divide-and-conquer nature of decision trees. The theoretical results are
supported by experimental evidence.



We propose a method for improving approximate inference methods that corrects
for the influence of loops in the graphical model. The method is applicable to
arbitrary factor graphs, provided that the size of the Markov blankets is not
too large. It is an alternative implementation of an idea introduced recently
by Montanari and Rizzo (2005). In its simplest form, which amounts to the
assumption that no loops are present, the method reduces to the minimal Cluster
Variation Method approximation (which uses maximal factors as outer clusters).
On the other hand, using estimates of the effect of loops (obtained by some
approximate inference algorithm) and applying the Loop Correcting (LC) method
usually gives significantly better results than applying the approximate
inference algorithm directly without loop corrections. Indeed, we often observe
that the loop corrected error is approximately the square of the error of the
approximate inference method used to estimate the effect of loops. We compare
different variants of the Loop Correcting method with other approximate
inference methods on a variety of graphical models, including "real world"
networks, and conclude that the LC approach generally obtains the most accurate
results.



The new social media sites -- blogs, wikis, Flickr and Digg, among others --
underscore the transformation of the Web to a participatory medium in which
users are actively creating, evaluating and distributing information. Digg is a
social news aggregator which allows users to submit links to, vote on and
discuss news stories. Each day Digg selects a handful of stories to feature on
its front page. Rather than rely on the opinion of a few editors, Digg
aggregates opinions of thousands of its users to decide which stories to
promote to the front page.
  Digg users can designate other users as ``friends'' and easily track friends'
activities: what new stories they submitted, commented on or read. The friends
interface acts as a \emph{social filtering} system, recommending to user
stories his or her friends liked or found interesting. By tracking the votes
received by newly submitted stories over time, we showed that social filtering
is an effective information filtering approach. Specifically, we showed that
(a) users tend to like stories submitted by friends and (b) users tend to like
stories their friends read and liked. As a byproduct of social filtering,
social networks also play a role in promoting stories to Digg's front page,
potentially leading to ``tyranny of the minority'' situation where a
disproportionate number of front page stories comes from the same small group
of interconnected users. Despite this, social filtering is a promising new
technology that can be used to personalize and tailor information to individual
users: for example, through personal front pages.



Approximation of the optimal two-part MDL code for given data, through
successive monotonically length-decreasing two-part MDL codes, has the
following properties: (i) computation of each step may take arbitrarily long;
(ii) we may not know when we reach the optimum, or whether we will reach the
optimum at all; (iii) the sequence of models generated may not monotonically
improve the goodness of fit; but (iv) the model associated with the optimum has
(almost) the best goodness of fit. To express the practically interesting
goodness of fit of individual models for individual data sets we have to rely
on Kolmogorov complexity.



In this paper, we determine the complexity of the satisfiability problem for
various logics obtained by adding numerical quantifiers, and other
constructions, to the traditional syllogistic. In addition, we demonstrate the
incompleteness of some recently proposed proof-systems for these logics.



We bound the future loss when predicting any (computably) stochastic sequence
online. Solomonoff finitely bounded the total deviation of his universal
predictor $M$ from the true distribution $mu$ by the algorithmic complexity of
$mu$. Here we assume we are at a time $t>1$ and already observed $x=x_1...x_t$.
We bound the future prediction performance on $x_{t+1}x_{t+2}...$ by a new
variant of algorithmic complexity of $mu$ given $x$, plus the complexity of the
randomness deficiency of $x$. The new complexity is monotone in its condition
in the sense that this complexity can only decrease if the condition is
prolonged. We also briefly discuss potential generalizations to Bayesian model
classes and to classification problems.



Distance learning universities usually afford their students the flexibility
to advance their studies at their own pace. This can lead to a considerable
fluctuation of student populations within a program's courses, possibly
affecting the academic viability of a program as well as the related required
resources. Providing a method that estimates this population could be of
substantial help to university management and academic personnel. We describe
how to use course precedence constraints to calculate alternative tuition paths
and then use Markov models to estimate future populations. In doing so, we
identify key issues of a large scale potential deployment.



The rise of the social media sites, such as blogs, wikis, Digg and Flickr
among others, underscores the transformation of the Web to a participatory
medium in which users are collaboratively creating, evaluating and distributing
information. The innovations introduced by social media has lead to a new
paradigm for interacting with information, what we call 'social information
processing'. In this paper, we study how social news aggregator Digg exploits
social information processing to solve the problems of document recommendation
and rating. First, we show, by tracking stories over time, that social networks
play an important role in document recommendation. The second contribution of
this paper consists of two mathematical models. The first model describes how
collaborative rating and promotion of stories emerges from the independent
decisions made by many users. The second model describes how a user's
influence, the number of promoted stories and the user's social network,
changes in time. We find qualitative agreement between predictions of the model
and user data gathered from Digg.



Reinforcement learning means learning a policy--a mapping of observations
into actions--based on feedback from the environment. The learning can be
viewed as browsing a set of policies while evaluating them by trial through
interaction with the environment. We present an application of gradient ascent
algorithm for reinforcement learning to a complex domain of packet routing in
network communication and compare the performance of this algorithm to other
routing methods on a benchmark problem.



This article introduces the conjecture that "mathematics, logic and related
disciplines may usefully be understood as information compression (IC) by
'multiple alignment', 'unification' and 'search' (ICMAUS)".
  As a preparation for the two main sections of the article, concepts of
information and information compression are reviewed. Related areas of research
are also described including IC in brains and nervous systems, and IC in
relation to inductive inference, Minimum Length Encoding and probabilistic
reasoning. The ICMAUS concepts and a computer model in which they are embodied
are briefly described.
  The first of the two main sections describes how many of the commonly-used
forms and structures in mathematics, logic and related disciplines (such as
theoretical linguistics and computer programming) may be seen as devices for
IC. In some cases, these forms and structures may be interpreted in terms of
the ICMAUS framework.
  The second main section describes a selection of examples where processes of
calculation and inference in mathematics, logic and related disciplines may be
understood as IC. In many cases, these examples may be understood more
specifically in terms of the ICMAUS concepts.



In this paper, we are interested in optimal decisions in a partially
observable Markov universe. Our viewpoint departs from the dynamic programming
viewpoint: we are directly approximating an optimal strategic tree depending on
the observation. This approximation is made by means of a parameterized
probabilistic law. In this paper, a particular family of hidden Markov models,
with input and output, is considered as a learning framework. A method for
optimizing the parameters of these HMMs is proposed and applied. This
optimization method is based on the cross-entropic principle.



We show that the class of conditional distributions satisfying the coarsening
at Random (CAR) property for discrete data has a simple and robust algorithmic
description based on randomized uniform multicovers: combinatorial objects
generalizing the notion of partition of a set. However, the complexity of a
given CAR mechanism can be large: the maximal "height" of the needed
multicovers can be exponential in the number of points in the sample space. The
results stem from a geometric interpretation of the set of CAR distributions as
a convex polytope and a characterization of its extreme points. The hierarchy
of CAR models defined in this way could be useful in parsimonious statistical
modelling of CAR mechanisms, though the results also raise doubts in applied
work as to the meaningfulness of the CAR assumption in its full generality.



Artificial Neural Network (ANN) is used as numerical methode in solving
modified Nonlinear Schroedinger (NLS) equation with Dispersion Managed System
(DMS) for jitter analysis. We take the optical axis z and the time t as input,
and then some relevant values such as the change of position and the center
frequency of the pulse, and further the mean square time of incoming pulse
which are needed for jitter analysis. It shows that ANN yields numerical
solutions which are adaptive with respect to the numerical errors and also
verifies the previous numerical results using conventional numerical method.
Our result indicates that DMS can minimize the timing jitter induced by some
amplifiers.



Steering traffic in cities is a very complex task, since improving efficiency
involves the coordination of many actors. Traditional approaches attempt to
optimize traffic lights for a particular density and configuration of traffic.
The disadvantage of this lies in the fact that traffic densities and
configurations change constantly. Traffic seems to be an adaptation problem
rather than an optimization problem. We propose a simple and feasible
alternative, in which traffic lights self-organize to improve traffic flow. We
use a multi-agent simulation to study three self-organizing methods, which are
able to outperform traditional rigid and adaptive methods. Using simple rules
and no direct communication, traffic lights are able to self-organize and adapt
to changing traffic conditions, reducing waiting times, number of stopped cars,
and increasing average speeds.



We present a method for hierarchic categorization and taxonomy evolution
description. We focus on the structure of epistemic communities (ECs), or
groups of agents sharing common knowledge concerns. Introducing a formal
framework based on Galois lattices, we categorize ECs in an automated and
hierarchically structured way and propose criteria for selecting the most
relevant epistemic communities - for instance, ECs gathering a certain
proportion of agents and thus prototypical of major fields. This process
produces a manageable, insightful taxonomy of the community. Then, the
longitudinal study of these static pictures makes possible an historical
description. In particular, we capture stylized facts such as field progress,
decline, specialization, interaction (merging or splitting), and paradigm
emergence. The detection of such patterns in social networks could fruitfully
be applied to other contexts.



The Minimum Description Length (MDL) principle is solidly based on a provably
ideal method of inference using Kolmogorov complexity. We test how the theory
behaves in practice on a general problem in model selection: that of learning
the best model granularity. The performance of a model depends critically on
the granularity, for example the choice of precision of the parameters. Too
high precision generally involves modeling of accidental noise and too low
precision may lead to confusion of models that should be distinguished. This
precision is often determined ad hoc. In MDL the best model is the one that
most compresses a two-part code of the data set: this embodies ``Occam's
Razor.'' In two quite different experimental settings the theoretical value
determined using MDL coincides with the best value found experimentally. In the
first experiment the task is to recognize isolated handwritten characters in
one subject's handwriting, irrespective of size and orientation. Based on a new
modification of elastic matching, using multiple prototypes per character, the
optimal prediction rate is predicted for the learned parameter (length of
sampling interval) considered most likely by MDL, which is shown to coincide
with the best value found experimentally. In the second experiment the task is
to model a robot arm with two degrees of freedom using a three layer
feed-forward neural network where we need to determine the number of nodes in
the hidden layer giving best modeling performance. The optimal model (the one
that extrapolizes best on unseen examples) is predicted for the number of nodes
in the hidden layer considered most likely by MDL, which again is found to
coincide with the best value found experimentally.



An approach to induction is presented, based on the idea of analysing the
context of a given problem into `circumstances'. This approach, fully Bayesian
in form and meaning, provides a complement or in some cases an alternative to
that based on de Finetti's representation theorem and on the notion of infinite
exchangeability. In particular, it gives an alternative interpretation of those
formulae that apparently involve `unknown probabilities' or `propensities'.
Various advantages and applications of the presented approach are discussed,
especially in comparison to that based on exchangeability. Generalisations are
also discussed.



The theory of computational complexity is used to underpin a recent model of
neocortical sensory processing. We argue that encoding into reconstruction
networks is appealing for communicating agents using Hebbian learning and
working on hard combinatorial problems, which are easy to verify. Computational
definition of the concept of intelligence is provided. Simulations illustrate
the idea.



Discrete temporal transitions occur in a variety of domains, but this work is
mainly motivated by applications in molecular biology: explaining and analyzing
observed transcriptome and proteome time series by literature and database
knowledge. The starting point of a formal concept analysis model is presented.
The objects of a formal context are states of the interesting entities, and the
attributes are the variable properties defining the current state (e.g.
observed presence or absence of proteins). Temporal transitions assign a
relation to the objects, defined by deterministic or non-deterministic
transition rules between sets of pre- and postconditions. This relation can be
generalized to its transitive closure, i.e. states are related if one results
from the other by a transition sequence of arbitrary length. The focus of the
work is the adaptation of the attribute exploration algorithm to such a
relational context, so that questions concerning temporal dependencies can be
asked during the exploration process and be answered from the computed stem
base. Results are given for the abstract example of a game and a small gene
regulatory network relevant to a biomedical question.



We have constructed a simple semiclassical model of neural network where
neurons have quantum links with one another in a chosen way and affect one
another in a fashion analogous to action potentials. We have examined the role
of stochasticity introduced by the quantum potential and compare the system
with the classical system of an integrate-and-fire model by Hopfield. Average
periodicity and short term retentivity of input memory are noted.



We try to design a quantum neural network with qubits instead of classical
neurons with deterministic states, and also with quantum operators replacing
teh classical action potentials. With our choice of gates interconnecting teh
neural lattice, it appears that the state of the system behaves in ways
reflecting both the strengths of coupling between neurons as well as initial
conditions. We find that depending whether there is a threshold for emission
from excited to ground state, the system shows either aperiodic oscillations or
coherent ones with periodicity depending on the strength of coupling.



We present some results from simulation of a network of nodes connected by
c-NOT gates with nearest neighbors. Though initially we begin with pure states
of varying boundary conditions, the updating with time quickly involves a
complicated entanglement involving all or most nodes. As a normal c-NOT gate,
though unitary for a single pair of nodes, seems to be not so when used in a
network in a naive way, we use a manifestly unitary form of the transition
matrix with c?-NOT gates, which invert the phase as well as flipping the qubit.
This leads to complete entanglement of the net, but with variable coefficients
for the different components of the superposition. It is interesting to note
that by a simple logical back projection the original input state can be
recovered in most cases. We also prove that it is not possible for a sequence
of unitary operators working on a net to make it move from an aperiodic regime
to a periodic one, unlike some classical cases where phase-locking happens in
course of evolution. However, we show that it is possible to introduce by hand
periodic orbits to sets of initial states, which may be useful in forming
dynamic pattern recognition systems.



We outline the rationale and preliminary results of using the State Context
Property (SCOP) formalism, originally developed as a generalization of quantum
mechanics, to describe the contextual manner in which concepts are evoked,
used, and combined to generate meaning. The quantum formalism was developed to
cope with problems arising in the description of (1) the measurement process,
and (2) the generation of new states with new properties when particles become
entangled. Similar problems arising with concepts motivated the formal
treatment introduced here. Concepts are viewed not as fixed representations,
but entities existing in states of potentiality that require interaction with a
context--a stimulus or another concept--to 'collapse' to an instantiated form
(e.g. exemplar, prototype, or other possibly imaginary instance). The stimulus
situation plays the role of the measurement in physics, acting as context that
induces a change of the cognitive state from superposition state to collapsed
state. The collapsed state is more likely to consist of a conjunction of
concepts for associative than analytic thought because more stimulus or concept
properties take part in the collapse. We provide two contextual measures of
conceptual distance--one using collapse probabilities and the other weighted
properties--and show how they can be applied to conjunctions using the pet fish
problem



This paper discusses the benefits of describing the world as information,
especially in the study of the evolution of life and cognition. Traditional
studies encounter problems because it is difficult to describe life and
cognition in terms of matter and energy, since their laws are valid only at the
physical scale. However, if matter and energy, as well as life and cognition,
are described in terms of information, evolution can be described consistently
as information becoming more complex.
  The paper presents eight tentative laws of information, valid at multiple
scales, which are generalizations of Darwinian, cybernetic, thermodynamic,
psychological, philosophical, and complexity principles. These are further used
to discuss the notions of life, cognition and their evolution.



Ordinal regression is an important type of learning, which has properties of
both classification and regression. Here we describe a simple and effective
approach to adapt a traditional neural network to learn ordinal categories. Our
approach is a generalization of the perceptron method for ordinal regression.
On several benchmark datasets, our method (NNRank) outperforms a neural network
classification method. Compared with the ordinal regression methods using
Gaussian processes and support vector machines, NNRank achieves comparable
performance. Moreover, NNRank has the advantages of traditional neural
networks: learning in both online and batch modes, handling very large training
datasets, and making rapid predictions. These features make NNRank a useful and
complementary tool for large-scale data processing tasks such as information
retrieval, web page ranking, collaborative filtering, and protein ranking in
Bioinformatics.



Information integration applications, such as mediators or mashups, that
require access to information resources currently rely on users manually
discovering and integrating them in the application. Manual resource discovery
is a slow process, requiring the user to sift through results obtained via
keyword-based search. Although search methods have advanced to include evidence
from document contents, its metadata and the contents and link structure of the
referring pages, they still do not adequately cover information sources --
often called ``the hidden Web''-- that dynamically generate documents in
response to a query. The recently popular social bookmarking sites, which allow
users to annotate and share metadata about various information sources, provide
rich evidence for resource discovery. In this paper, we describe a
probabilistic model of the user annotation process in a social bookmarking
system del.icio.us. We then use the model to automatically find resources
relevant to a particular information domain. Our experimental results on data
obtained from \emph{del.icio.us} show this approach as a promising method for
helping automate the resource discovery task.



We present a formal model to represent and solve the unicast/multicast
routing problem in networks with Quality of Service (QoS) requirements. To
attain this, first we translate the network adapting it to a weighted graph
(unicast) or and-or graph (multicast), where the weight on a connector
corresponds to the multidimensional cost of sending a packet on the related
network link: each component of the weights vector represents a different QoS
metric value (e.g. bandwidth, cost, delay, packet loss). The second step
consists in writing this graph as a program in Soft Constraint Logic
Programming (SCLP): the engine of this framework is then able to find the best
paths/trees by optimizing their costs and solving the constraints imposed on
them (e.g. delay < 40msec), thus finding a solution to QoS routing problems.
Moreover, c-semiring structures are a convenient tool to model QoS metrics. At
last, we provide an implementation of the framework over scale-free networks
and we suggest how the performance can be improved.



The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network
algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a
learning rate and annealing schemes for learning rate and neighbourhood size.
We discuss the relative performance of the PLSOM and the SOM and demonstrate
some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally
we discuss some example applications of the PLSOM and present a proof of
ordering under certain limited conditions.



With the great success in simulating many intelligent behaviors using
computing devices, there has been an ongoing debate whether all conscious
activities are computational processes. In this paper, the answer to this
question is shown to be no. A certain phenomenon of consciousness is
demonstrated to be fully represented as a computational process using a quantum
computer. Based on the computability criterion discussed with Turing machines,
the model constructed is shown to necessarily involve a non-computable element.
The concept that this is solely a quantum effect and does not work for a
classical case is also discussed.



In this paper artificial neural networks and support vector machines are used
to reduce the amount of vibration data that is required to estimate the Time
Domain Average of a gear vibration signal. Two models for estimating the time
domain average of a gear vibration signal are proposed. The models are tested
on data from an accelerated gear life test rig. Experimental results indicate
that the required data for calculating the Time Domain Average of a gear
vibration signal can be reduced by up to 75% when the proposed models are
implemented.



In this paper, we present a method to optimise rough set partition sizes, to
which rule extraction is performed on HIV data. The genetic algorithm
optimisation technique is used to determine the partition sizes of a rough set
in order to maximise the rough sets prediction accuracy. The proposed method is
tested on a set of demographic properties of individuals obtained from the
South African antenatal survey. Six demographic variables were used in the
analysis, these variables are; race, age of mother, education, gravidity,
parity, and age of father, with the outcome or decision being either HIV
positive or negative. Rough set theory is chosen based on the fact that it is
easy to interpret the extracted rules. The prediction accuracy of equal width
bin partitioning is 57.7% while the accuracy achieved after optimising the
partitions is 72.8%. Several other methods have been used to analyse the HIV
data and their results are stated and compared to that of rough set theory
(RST).



A virtual plague is a process in which a behavior-affecting property spreads
among characters in a Massively Multiplayer Online Game (MMOG). The MMOG
individuals constitute a synthetic population, and the game can be seen as a
form of interactive executable model for studying disease spread, albeit of a
very special kind. To a game developer maintaining an MMOG, recognizing,
monitoring, and ultimately controlling a virtual plague is important,
regardless of how it was initiated. The prospect of using tools, methods and
theory from the field of epidemiology to do this seems natural and appealing.
We will address the feasibility of such a prospect, first by considering some
basic measures used in epidemiology, then by pointing out the differences
between real world epidemics and virtual plagues. We also suggest directions
for MMOG developer control through epidemiological modeling. Our aim is
understanding the properties of virtual plagues, rather than trying to
eliminate them or mitigate their effects, as would be in the case of real
infectious disease.



This paper I assume that in humans the creation of knowledge depends on a
discrete time, or stage, sequential decision-making process subjected to a
stochastic, information transmitting environment. For each time-stage, this
environment randomly transmits Shannon type information-packets to the
decision-maker, who examines each of them for relevancy and then determines his
optimal choices. Using this set of relevant information-packets, the
decision-maker adapts, over time, to the stochastic nature of his environment,
and optimizes the subjective expected rate-of-growth of knowledge. The
decision-maker's optimal actions, lead to a decision function that involves,
over time, his view of the subjective entropy of the environmental process and
other important parameters at each time-stage of the process. Using this model
of human behavior, one could create psychometric experiments using computer
simulation and real decision-makers, to play programmed games to measure the
resulting human performance.



In this paper, we study the application of sparse principal component
analysis (PCA) to clustering and feature selection problems. Sparse PCA seeks
sparse factors, or linear combinations of the data variables, explaining a
maximum amount of variance in the data while having only a limited number of
nonzero coefficients. PCA is often used as a simple clustering technique and
sparse factors allow us here to interpret the clusters in terms of a reduced
set of variables. We begin with a brief introduction and motivation on sparse
PCA and detail our implementation of the algorithm in d'Aspremont et al.
(2005). We then apply these results to some classic clustering and feature
selection problems arising in biology.



This work aims at optimizing injection networks, which consist in adding a
set of long-range links (called bypass links) in mobile multi-hop ad hoc
networks so as to improve connectivity and overcome network partitioning. To
this end, we rely on small-world network properties, that comprise a high
clustering coefficient and a low characteristic path length. We investigate the
use of two genetic algorithms (generational and steady-state) to optimize three
instances of this topology control problem and present results that show
initial evidence of their capacity to solve it.



A generalized information formula related to logical probability and fuzzy
set is deduced from the classical information formula. The new information
measure accords with to Popper's criterion for knowledge evolution very much.
In comparison with square error criterion, the information criterion does not
only reflect error of a proposition, but also reflects the particularity of the
event described by the proposition. It gives a proposition with less logical
probability higher evaluation. The paper introduces how to select a prediction
or sentence from many for forecasts and language translations according to the
generalized information criterion. It also introduces the rate fidelity theory,
which comes from the improvement of the rate distortion theory in the classical
information theory by replacing distortion (i.e. average error) criterion with
the generalized mutual information criterion, for data compression and
communication efficiency. Some interesting conclusions are obtained from the
rate-fidelity function in relation to image communication. It also discusses
how to improve Popper's theory.



This research hypothesized that a practical approach in the form of a
solution framework known as Natural Language Understanding and Reasoning for
Intelligence (NaLURI), which combines full-discourse natural language
understanding, powerful representation formalism capable of exploiting
ontological information and reasoning approach with advanced features, will
solve the following problems without compromising practicality factors: 1)
restriction on the nature of question and response, and 2) limitation to scale
across domains and to real-life natural language text.



Haplotype Inference is a challenging problem in bioinformatics that consists
in inferring the basic genetic constitution of diploid organisms on the basis
of their genotype. This information allows researchers to perform association
studies for the genetic variants involved in diseases and the individual
responses to therapeutic agents.
  A notable approach to the problem is to encode it as a combinatorial problem
(under certain hypotheses, such as the pure parsimony criterion) and to solve
it using off-the-shelf combinatorial optimization techniques. The main methods
applied to Haplotype Inference are either simple greedy heuristic or exact
methods (Integer Linear Programming, Semidefinite Programming, SAT encoding)
that, at present, are adequate only for moderate size instances.
  We believe that metaheuristic and hybrid approaches could provide a better
scalability. Moreover, metaheuristics can be very easily combined with problem
specific heuristics and they can also be integrated with tree-based search
techniques, thus providing a promising framework for hybrid systems in which a
good trade-off between effectiveness and efficiency can be reached.
  In this paper we illustrate a feasibility study of the approach and discuss
some relevant design issues, such as modeling and design of approximate solvers
that combine constructive heuristics, local search-based improvement strategies
and learning mechanisms. Besides the relevance of the Haplotype Inference
problem itself, this preliminary analysis is also an interesting case study
because the formulation of the problem poses some challenges in modeling and
hybrid metaheuristic solver design that can be generalized to other problems.



We propose a special computational device which uses light rays for solving
the subset-sum problem. The device has a graph-like representation and the
light is traversing it by following the routes given by the connections between
nodes. The nodes are connected by arcs in a special way which lets us to
generate all possible subsets of the given set. To each arc we assign either a
number from the given set or a predefined constant. When the light is passing
through an arc it is delayed by the amount of time indicated by the number
placed in that arc. At the destination node we will check if there is a ray
whose total delay is equal to the target value of the subset sum problem (plus
some constants).



This article presents a technique for proving problems hard for classes of
the polynomial hierarchy or for PSPACE. The rationale of this technique is that
some problem restrictions are able to simulate existential or universal
quantifiers. If this is the case, reductions from Quantified Boolean Formulae
(QBF) to these restrictions can be transformed into reductions from QBFs having
one more quantifier in the front. This means that a proof of hardness of a
problem at level n in the polynomial hierarchy can be split into n separate
proofs, which may be simpler than a proof directly showing a reduction from a
class of QBFs to the considered problem.



We consider the general problem of finding the minimum weight $\bm$-matching
on arbitrary graphs. We prove that, whenever the linear programming (LP)
relaxation of the problem has no fractional solutions, then the belief
propagation (BP) algorithm converges to the correct solution. We also show that
when the LP relaxation has a fractional solution then the BP algorithm can be
used to solve the LP relaxation. Our proof is based on the notion of graph
covers and extends the analysis of (Bayati-Shah-Sharma 2005 and Huang-Jebara
2007}.
  These results are notable in the following regards: (1) It is one of a very
small number of proofs showing correctness of BP without any constraint on the
graph structure. (2) Variants of the proof work for both synchronous and
asynchronous BP; it is the first proof of convergence and correctness of an
asynchronous BP algorithm for a combinatorial optimization problem.



Message passing algorithms have proved surprisingly successful in solving
hard constraint satisfaction problems on sparse random graphs. In such
applications, variables are fixed sequentially to satisfy the constraints.
Message passing is run after each step. Its outcome provides an heuristic to
make choices at next step. This approach has been referred to as `decimation,'
with reference to analogous procedures in statistical physics.
  The behavior of decimation procedures is poorly understood. Here we consider
a simple randomized decimation algorithm based on belief propagation (BP), and
analyze its behavior on random k-satisfiability formulae. In particular, we
propose a tree model for its analysis and we conjecture that it provides
asymptotically exact predictions in the limit of large instances. This
conjecture is confirmed by numerical simulations.



The ability of a classifier to take on new information and classes by
evolving the classifier without it having to be fully retrained is known as
incremental learning. Incremental learning has been successfully applied to
many classification problems, where the data is changing and is not all
available at once. In this paper there is a comparison between Learn++, which
is one of the most recent incremental learning algorithms, and the new proposed
method of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has
shown good incremental learning capabilities on benchmark datasets on which the
new ILUGA method has been tested. ILUGA has also shown good incremental
learning ability using only a few classifiers and does not suffer from
catastrophic forgetting. The results obtained for ILUGA on the Optical
Character Recognition (OCR) and Wine datasets are good, with an overall
accuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT
for the difficult multi-class OCR dataset.



Data mining allows the exploration of sequences of phenomena, whereas one
usually tends to focus on isolated phenomena or on the relation between two
phenomena. It offers invaluable tools for theoretical analyses and exploration
of the structure of sentences, texts, dialogues, and speech. We report here the
results of an attempt at using it for inspecting sequences of verbs from French
accounts of road accidents. This analysis comes from an original approach of
unsupervised training allowing the discovery of the structure of sequential
data. The entries of the analyzer were only made of the verbs appearing in the
sentences. It provided a classification of the links between two successive
verbs into four distinct clusters, allowing thus text segmentation. We give
here an interpretation of these clusters by applying a statistical analysis to
independent semantic annotations.



A method for the construction of approximate analytical expressions for the
stationary marginal densities of general stochastic search processes is
proposed. By the marginal densities, regions of the search space that with high
probability contain the global optima can be readily defined. The density
estimation procedure involves a controlled number of linear operations, with a
computational cost per iteration that grows linearly with problem size.



Near optimal decoding of good error control codes is generally a difficult
task. However, for a certain type of (sufficiently) good codes an efficient
decoding algorithm with near optimal performance exists. These codes are
defined via a combination of constituent codes with low complexity trellis
representations. Their decoding algorithm is an instance of (loopy) belief
propagation and is based on an iterative transfer of constituent beliefs. The
beliefs are thereby given by the symbol probabilities computed in the
constituent trellises. Even though weak constituent codes are employed close to
optimal performance is obtained, i.e., the encoder/decoder pair (almost)
achieves the information theoretic capacity. However, (loopy) belief
propagation only performs well for a rather specific set of codes, which limits
its applicability.
  In this paper a generalisation of iterative decoding is presented. It is
proposed to transfer more values than just the constituent beliefs. This is
achieved by the transfer of beliefs obtained by independently investigating
parts of the code space. This leads to the concept of discriminators, which are
used to improve the decoder resolution within certain areas and defines
discriminated symbol beliefs. It is shown that these beliefs approximate the
overall symbol probabilities. This leads to an iteration rule that (below
channel capacity) typically only admits the solution of the overall decoding
problem. Via a Gauss approximation a low complexity version of this algorithm
is derived. Moreover, the approach may then be applied to a wide range of
channel maps without significant complexity increase.



A computer model of "a sense of humour" suggested previously
[arXiv:0711.2058,0711.2061], relating the humorous effect with a specific
malfunction in information processing, is given in somewhat different
exposition. Psychological aspects of humour are elaborated more thoroughly. The
mechanism of laughter is formulated on the more general level. Detailed
discussion is presented for the higher levels of information processing, which
are responsible for a perception of complex samples of humour. Development of a
sense of humour in the process of evolution is discussed.



Support Vector Machines (SVMs) are a relatively new supervised classification
technique to the land cover mapping community. They have their roots in
Statistical Learning Theory and have gained prominence because they are robust,
accurate and are effective even when using a small training sample. By their
nature SVMs are essentially binary classifiers, however, they can be adopted to
handle the multiple classification tasks common in remote sensing studies. The
two approaches commonly used are the One-Against-One (1A1) and One-Against-All
(1AA) techniques. In this paper, these approaches are evaluated in as far as
their impact and implication for land cover mapping. The main finding from this
research is that whereas the 1AA technique is more predisposed to yielding
unclassified and mixed pixels, the resulting classification accuracy is not
significantly different from 1A1 approach. It is the authors conclusion
therefore that ultimately the choice of technique adopted boils down to
personal preference and the uniqueness of the dataset at hand.



Computer model of a "sense of humour" suggested previously [arXiv:0711.2058,
0711.2061, 0711.2270] is raised to the level of a realistic algorithm.



We consider how an agent should update her uncertainty when it is represented
by a set $\P$ of probability distributions and the agent observes that a random
variable $X$ takes on value $x$, given that the agent makes decisions using the
minimax criterion, perhaps the best-studied and most commonly-used criterion in
the literature. We adopt a game-theoretic framework, where the agent plays
against a bookie, who chooses some distribution from $\P$. We consider two
reasonable games that differ in what the bookie knows when he makes his choice.
Anomalies that have been observed before, like time inconsistency, can be
understood as arising important because different games are being played,
against bookies with different information. We characterize the important
special cases in which the optimal decision rules according to the minimax
criterion amount to either conditioning or simply ignoring the information.
Finally, we consider the relationship between conditioning and calibration when
uncertainty is described by sets of probabilities.



This book proposes to separate knowledge from software and to make it a
commodity that is called knowware. The architecture, representation and
function of Knowware are discussed. The principles of knowware engineering and
its three life cycle models: furnace model, crystallization model and spiral
model are proposed and analyzed. Techniques of software/knowware co-engineering
are introduced. A software component whose knowledge is replaced by knowware is
called mixware. An object and component oriented development schema of mixware
is introduced. In particular, the tower model and ladder model for mixware
development are proposed and discussed. Finally, knowledge service and knowware
based Web service are introduced and compared with Web service. In summary,
knowware, software and hardware should be considered as three equally important
underpinnings of IT industry.
  Ruqian Lu is a professor of computer science of the Institute of Mathematics,
Academy of Mathematics and System Sciences. He is a fellow of Chinese Academy
of Sciences. His research interests include artificial intelligence, knowledge
engineering and knowledge based software engineering. He has published more
than 100 papers and 10 books. He has won two first class awards from the
Academia Sinica and a National second class prize from the Ministry of Science
and Technology. He has also won the sixth Hua Loo-keng Mathematics Prize.



It is a common belief that in any environment where life is possible, life
will be generated. Here it is suggested that the cause for a spontaneous
generation of complex systems is probability driven processes. Based on
equilibrium thermodynamics, it is argued that in low occupation number
statistical systems, the second law of thermodynamics yields an increase of
thermal entropy and a canonic energy distribution. However, in high occupation
number statistical systems, the same law for the same reasons yields an
increase of information and a Benford's law/power-law energy distribution. It
is therefore, plausible, that eventually the heat death is not necessarily the
end of the universe.



We study the performance of stochastic local search algorithms for random
instances of the $K$-satisfiability ($K$-SAT) problem. We introduce a new
stochastic local search algorithm, ChainSAT, which moves in the energy
landscape of a problem instance by {\em never going upwards} in energy.
ChainSAT is a \emph{focused} algorithm in the sense that it considers only
variables occurring in unsatisfied clauses. We show by extensive numerical
investigations that ChainSAT and other focused algorithms solve large $K$-SAT
instances almost surely in linear time, up to high clause-to-variable ratios
$\alpha$; for example, for K=4 we observe linear-time performance well beyond
the recently postulated clustering and condensation transitions in the solution
space. The performance of ChainSAT is a surprise given that by design the
algorithm gets trapped into the first local energy minimum it encounters, yet
no such minima are encountered. We also study the geometry of the solution
space as accessed by stochastic local search algorithms.



Contributing to the rigorous understanding of BP, in this paper we relate the
convergence of BP to spectral properties of the graph. This encompasses a
result for random graphs with a ``planted'' solution; thus, we obtain the first
rigorous result on BP for graph coloring in the case of a complex graphical
structure (as opposed to trees). In particular, the analysis shows how Belief
Propagation breaks the symmetry between the $3!$ possible permutations of the
color classes.



The generation of meaningless "words" matching certain statistical and/or
linguistic criteria is frequently needed for experimental purposes in
Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in
the Cognitive Neuroscience literatue. The process for building nonwords
sometimes has to be based on linguistic units such as syllables or morphemes,
resulting in a numerical explosion of combinations when the size of the
nonwords is increased. In this paper, a reactive tabu search scheme is proposed
to generate nonwords of variables size. The approach builds pseudowords by
using a modified Metaheuristic algorithm based on a local search procedure
enhanced by a feedback-based scheme. Experimental results show that the new
algorithm is a practical and effective tool for nonword generation.



Chemotaxis can be defined as an innate behavioural response by an organism to
a directional stimulus, in which bacteria, and other single-cell or
multicellular organisms direct their movements according to certain chemicals
in their environment. This is important for bacteria to find food (e.g.,
glucose) by swimming towards the highest concentration of food molecules, or to
flee from poisons. Based on self-organized computational approaches and similar
stigmergic concepts we derive a novel swarm intelligent algorithm. What strikes
from these observations is that both eusocial insects as ant colonies and
bacteria have similar natural mechanisms based on stigmergy in order to emerge
coherent and sophisticated patterns of global collective behaviour. Keeping in
mind the above characteristics we will present a simple model to tackle the
collective adaptation of a social swarm based on real ant colony behaviors (SSA
algorithm) for tracking extrema in dynamic environments and highly multimodal
complex functions described in the well-know De Jong test suite. Later, for the
purpose of comparison, a recent model of artificial bacterial foraging (BFOA
algorithm) based on similar stigmergic features is described and analyzed.
Final results indicate that the SSA collective intelligence is able to cope and
quickly adapt to unforeseen situations even when over the same cooperative
foraging period, the community is requested to deal with two different and
contradictory purposes, while outperforming BFOA in adaptive speed. Results
indicate that the present approach deals well in severe Dynamic Optimization
problems.



In this paper, we present a Mirroring Neural Network architecture to perform
non-linear dimensionality reduction and Object Recognition using a reduced
lowdimensional characteristic vector. In addition to dimensionality reduction,
the network also reconstructs (mirrors) the original high-dimensional input
vector from the reduced low-dimensional data. The Mirroring Neural Network
architecture has more number of processing elements (adalines) in the outer
layers and the least number of elements in the central layer to form a
converging-diverging shape in its configuration. Since this network is able to
reconstruct the original image from the output of the innermost layer (which
contains all the information about the input pattern), these outputs can be
used as object signature to classify patterns. The network is trained to
minimize the discrepancy between actual output and the input by back
propagating the mean squared error from the output layer to the input layer.
After successfully training the network, it can reduce the dimension of input
vectors and mirror the patterns fed to it. The Mirroring Neural Network
architecture gave very good results on various test patterns.



This paper proposes an unsupervised learning technique by using Multi-layer
Mirroring Neural Network and Forgy's clustering algorithm. Multi-layer
Mirroring Neural Network is a neural network that can be trained with
generalized data inputs (different categories of image patterns) to perform
non-linear dimensionality reduction and the resultant low-dimensional code is
used for unsupervised pattern classification using Forgy's algorithm. By
adapting the non-linear activation function (modified sigmoidal function) and
initializing the weights and bias terms to small random values, mirroring of
the input pattern is initiated. In training, the weights and bias terms are
changed in such a way that the input presented is reproduced at the output by
back propagating the error. The mirroring neural network is capable of reducing
the input vector to a great degree (approximately 1/30th the original size) and
also able to reconstruct the input pattern at the output layer from this
reduced code units. The feature set (output of central hidden layer) extracted
from this network is fed to Forgy's algorithm, which classify input data
patterns into distinguishable classes. In the implementation of Forgy's
algorithm, initial seed points are selected in such a way that they are distant
enough to be perfectly grouped into different categories. Thus a new method of
unsupervised learning is formulated and demonstrated in this paper. This method
gave impressive results when applied to classification of different image
patterns.



Computability logic (CL) (see http://www.cis.upenn.edu/~giorgi/cl.html) is a
semantical platform and research program for redeveloping logic as a formal
theory of computability, as opposed to the formal theory of truth which it has
more traditionally been. Formulas in CL stand for (interactive) computational
problems, understood as games between a machine and its environment; logical
operators represent operations on such entities; and "truth" is understood as
existence of an effective solution, i.e., of an algorithmic winning strategy.
  The formalism of CL is open-ended, and may undergo series of extensions as
the study of the subject advances. The main groups of operators on which CL has
been focused so far are the parallel, choice, branching, and blind operators.
The present paper introduces a new important group of operators, called
sequential. The latter come in the form of sequential conjunction and
disjunction, sequential quantifiers, and sequential recurrences. As the name
may suggest, the algorithmic intuitions associated with this group are those of
sequential computations, as opposed to the intuitions of parallel computations
associated with the parallel group of operations: playing a sequential
combination of games means playing its components in a sequential fashion, one
after one.
  The main technical result of the present paper is a sound and complete
axiomatization of the propositional fragment of computability logic whose
vocabulary, together with negation, includes all three -- parallel, choice and
sequential -- sorts of conjunction and disjunction. An extension of this result
to the first-order level is also outlined.



Population stratification is a problem encountered in several areas of
biology and public health. We tackle this problem by mapping a population and
its elements attributes into a hypergraph, a natural extension of the concept
of graph or network to encode associations among any number of elements. On
this hypergraph, we construct a statistical model reflecting our intuition
about how the elements attributes can emerge from a postulated population
structure. Finally, we introduce the concept of stratification
representativeness as a mean to identify the simplest stratification already
containing most of the information about the population structure. We
demonstrate the power of this framework stratifying an animal and a human
population based on phenotypic and genotypic properties, respectively.



In this paper we shall review the common problems associated with Piecewise
Linear Separation incremental algorithms. This kind of neural models yield poor
performances when dealing with some classification problems, due to the
evolving schemes used to construct the resulting networks. So as to avoid this
undesirable behavior we shall propose a modification criterion. It is based
upon the definition of a function which will provide information about the
quality of the network growth process during the learning phase. This function
is evaluated periodically as the network structure evolves, and will permit, as
we shall show through exhaustive benchmarks, to considerably improve the
performance(measured in terms of network complexity and generalization
capabilities) offered by the networks generated by these incremental models.



The concept of a judgment as a logical action which introduces new
information into a deductive system is examined. This leads to a way of
mathematically representing implication which is distinct from the familiar
material implication, according to which "If A then B" is considered to be
equivalent to "B or not-A". This leads, in turn, to a resolution of the paradox
of the raven.



Constraint satisfaction problems (CSPs) models many important intractable
NP-hard problems such as propositional satisfiability problem (SAT). Algorithms
with non-trivial upper bounds on running time for restricted SAT with bounded
clause length k (k-SAT) can be classified into three styles: DPLL-like,
PPSZ-like and Local Search, with local search algorithms having already been
generalized to CSP with bounded constraint arity k (k-CSP). We generalize a
DPLL-like algorithm in its simplest form and a PPSZ-like algorithm from k-SAT
to k-CSP. As far as we know, this is the first attempt to use PPSZ-like
strategy to solve k-CSP, and before little work has been focused on the
DPLL-like or PPSZ-like strategies for k-CSP.



Over the last decade, a new idea challenging the classical self-non-self
viewpoint has become popular amongst immunologists. It is called the Danger
Theory. In this conceptual paper, we look at this theory from the perspective
of Artificial Immune System practitioners. An overview of the Danger Theory is
presented with particular emphasis on analogies in the Artificial Immune
Systems world. A number of potential application areas are then used to provide
a framing for a critical assessment of the concept, and its relevance for
Artificial Immune Systems.



This paper analyzes the scaling window of a random CSP model (i.e. model RB)
for which we can identify the threshold points exactly, denoted by $r_{cr}$ or
$p_{cr}$. For this model, we establish the scaling window
$W(n,\delta)=(r_{-}(n,\delta), r_{+}(n,\delta))$ such that the probability of a
random instance being satisfiable is greater than $1-\delta$ for
$r<r_{-}(n,\delta)$ and is less than $\delta$ for $r>r_{+}(n,\delta)$.
Specifically, we obtain the following result
$$W(n,\delta)=(r_{cr}-\Theta(\frac{1}{n^{1-\epsilon}\ln n}), \
r_{cr}+\Theta(\frac{1}{n\ln n})),$$ where $0\leq\epsilon<1$ is a constant. A
similar result with respect to the other parameter $p$ is also obtained. Since
the instances generated by model RB have been shown to be hard at the
threshold, this is the first attempt, as far as we know, to analyze the scaling
window of such a model with hard instances.



We combine Artificial Immune Systems 'AIS', technology with Collaborative
Filtering 'CF' and use it to build a movie recommendation system. We already
know that Artificial Immune Systems work well as movie recommenders from
previous work by Cayzer and Aickelin 3, 4, 5. Here our aim is to investigate
the effect of different affinity measure algorithms for the AIS. Two different
affinity measures, Kendalls Tau and Weighted Kappa, are used to calculate the
correlation coefficients for the movie recommender. We compare the results with
those published previously and show that Weighted Kappa is more suitable than
others for movie problems. We also show that AIS are generally robust movie
recommenders and that, as long as a suitable affinity measure is chosen,
results are good.



Consider a class $\mH$ of binary functions $h: X\to\{-1, +1\}$ on a finite
interval $X=[0, B]\subset \Real$. Define the {\em sample width} of $h$ on a
finite subset (a sample) $S\subset X$ as $\w_S(h) \equiv \min_{x\in S}
|\w_h(x)|$, where $\w_h(x) = h(x) \max\{a\geq 0: h(z)=h(x), x-a\leq z\leq
x+a\}$. Let $\mathbb{S}_\ell$ be the space of all samples in $X$ of cardinality
$\ell$ and consider sets of wide samples, i.e., {\em hypersets} which are
defined as $A_{\beta, h} = \{S\in \mathbb{S}_\ell: \w_{S}(h) \geq \beta\}$.
Through an application of the Sauer-Shelah result on the density of sets an
upper estimate is obtained on the growth function (or trace) of the class
$\{A_{\beta, h}: h\in\mH\}$, $\beta>0$, i.e., on the number of possible
dichotomies obtained by intersecting all hypersets with a fixed collection of
samples $S\in\mathbb{S}_\ell$ of cardinality $m$. The estimate is
$2\sum_{i=0}^{2\lfloor B/(2\beta)\rfloor}{m-\ell\choose i}$.



The methods used to establish PSPACE-bounds for modal logics can roughly be
grouped into two classes: syntax driven methods establish that exhaustive proof
search can be performed in polynomial space whereas semantic approaches
directly construct shallow models. In this paper, we follow the latter approach
and establish generic PSPACE-bounds for a large and heterogeneous class of
modal logics in a coalgebraic framework. In particular, no complete
axiomatisation of the logic under scrutiny is needed. This does not only
complement our earlier, syntactic, approach conceptually, but also covers a
wide variety of new examples which are difficult to harness by purely syntactic
means. Apart from re-proving known complexity bounds for a large variety of
structurally different logics, we apply our method to obtain previously unknown
PSPACE-bounds for Elgesem's logic of agency and for graded modal logic over
reflexive frames.



We explore a simple mathematical model of network computation, based on
Markov chains. Similar models apply to a broad range of computational
phenomena, arising in networks of computers, as well as in genetic, and neural
nets, in social networks, and so on. The main problem of interaction with such
spontaneously evolving computational systems is that the data are not uniformly
structured. An interesting approach is to try to extract the semantical content
of the data from their distribution among the nodes. A concept is then
identified by finding the community of nodes that share it. The task of data
structuring is thus reduced to the task of finding the network communities, as
groups of nodes that together perform some non-local data processing. Towards
this goal, we extend the ranking methods from nodes to paths. This allows us to
extract some information about the likely flow biases from the available static
information about the network.



The convergence properties of the stationary Fokker-Planck algorithm for the
estimation of the asymptotic density of stochastic search processes is studied.
Theoretical and empirical arguments for the characterization of convergence of
the estimation in the case of separable and nonseparable nonlinear optimization
problems are given. Some implications of the convergence of stationary
Fokker-Planck learning for the inference of parameters in artificial neural
network models are outlined.



A survey is given summarizing the state of the art of describing information
processing in Quantum Decision Theory, which has been recently advanced as a
novel variant of decision making, based on the mathematical theory of separable
Hilbert spaces. This mathematical structure captures the effect of
superposition of composite prospects, including many incorporated intended
actions. The theory characterizes entangled decision making, non-commutativity
of subsequent decisions, and intention interference. The self-consistent
procedure of decision making, in the frame of the quantum decision theory,
takes into account both the available objective information as well as
subjective contextual effects. This quantum approach avoids any paradox typical
of classical decision theory. Conditional maximization of entropy, equivalent
to the minimization of an information functional, makes it possible to connect
the quantum and classical decision theories, showing that the latter is the
limit of the former under vanishing interference terms.



Several technologies are emerging that provide new ways to capture, store,
present and use knowledge. This book is the first to provide a comprehensive
introduction to five of the most important of these technologies: Knowledge
Engineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and
Semantic Webs. For each of these, answers are given to a number of key
questions (What is it? How does it operate? How is a system developed? What can
it be used for? What tools are available? What are the main issues?). The book
is aimed at students, researchers and practitioners interested in Knowledge
Management, Artificial Intelligence, Design Engineering and Web Technologies.
  During the 1990s, Nick worked at the University of Nottingham on the
application of AI techniques to knowledge management and on various knowledge
acquisition projects to develop expert systems for military applications. In
1999, he joined Epistemics where he worked on numerous knowledge projects and
helped establish knowledge management programmes at large organisations in the
engineering, technology and legal sectors. He is author of the book "Knowledge
Acquisition in Practice", which describes a step-by-step procedure for
acquiring and implementing expertise. He maintains strong links with leading
research organisations working on knowledge technologies, such as
knowledge-based engineering, ontologies and semantic technologies.



We discuss a generic model of Bayesian inference with binary variables
defined on edges of a planar graph. The Loop Calculus approach of [1, 2] is
used to evaluate the resulting series expansion for the partition function. We
show that, for planar graphs, truncating the series at single-connected loops
reduces, via a map reminiscent of the Fisher transformation [3], to evaluating
the partition function of the dimer matching model on an auxiliary planar
graph. Thus, the truncated series can be easily re-summed, using the Pfaffian
formula of Kasteleyn [4]. This allows to identify a big class of
computationally tractable planar models reducible to a dimer model via the
Belief Propagation (gauge) transformation. The Pfaffian representation can also
be extended to the full Loop Series, in which case the expansion becomes a sum
of Pfaffian contributions, each associated with dimer matchings on an extension
to a subgraph of the original graph. Algorithmic consequences of the Pfaffian
representation, as well as relations to quantum and non-planar models, are
discussed.



Fifty years ago, John von Neumann compared the architecture of the brain with
that of computers that he invented and which is still in use today. In those
days, the organisation of computers was based on concepts of brain
organisation. Here, we give an update on current results on the global
organisation of neural systems. For neural systems, we outline how the spatial
and topological architecture of neuronal and cortical networks facilitates
robustness against failures, fast processing, and balanced network activation.
Finally, we discuss mechanisms of self-organization for such architectures.
After all, the organization of the brain might again inspire computer
architecture.



This research report introduces the generation of textual entailment within
the project CSIEC (Computer Simulation in Educational Communication), an
interactive web-based human-computer dialogue system with natural language for
English instruction. The generation of textual entailment (GTE) is critical to
the further improvement of CSIEC project. Up to now we have found few
literatures related with GTE. Simulating the process that a human being learns
English as a foreign language we explore our naive approach to tackle the GTE
problem and its algorithm within the framework of CSIEC, i.e. rule annotation
in NLML, pattern recognition (matching), and entailment transformation. The
time and space complexity of our algorithm is tested with some entailment
examples. Further works include the rules annotation based on the English
textbooks and a GUI interface for normal users to edit the entailment rules.



There are two kinds of approaches for termination analysis of logic programs:
"transformational" and "direct" ones. Direct approaches prove termination
directly on the basis of the logic program. Transformational approaches
transform a logic program into a term rewrite system (TRS) and then analyze
termination of the resulting TRS instead. Thus, transformational approaches
make all methods previously developed for TRSs available for logic programs as
well. However, the applicability of most existing transformations is quite
restricted, as they can only be used for certain subclasses of logic programs.
(Most of them are restricted to well-moded programs.) In this paper we improve
these transformations such that they become applicable for any definite logic
program. To simulate the behavior of logic programs by TRSs, we slightly modify
the notion of rewriting by permitting infinite terms. We show that our
transformation results in TRSs which are indeed suitable for automated
termination analysis. In contrast to most other methods for termination of
logic programs, our technique is also sound for logic programming without occur
check, which is typically used in practice. We implemented our approach in the
termination prover AProVE and successfully evaluated it on a large collection
of examples.



In this paper, we implement an anomaly detection system using the
Dempster-Shafer method. Using two standard benchmark problems we show that by
combining multiple signals it is possible to achieve better results than by
using a single signal. We further show that by applying this approach to a
real-world email dataset the algorithm works for email worm detection.
Dempster-Shafer can be a promising method for anomaly detection problems with
multiple features (data sources), and two or more classes.



We present ideas about creating a next generation Intrusion Detection System
based on the latest immunological theories. The central challenge with computer
security is determining the difference between normal and potentially harmful
activity. For half a century, developers have protected their systems by coding
rules that identify and block specific events. However, the nature of current
and future threats in conjunction with ever larger IT systems urgently requires
the development of automated and adaptive defensive tools. A promising solution
is emerging in the form of Artificial Immune Systems. The Human Immune System
can detect and defend against harmful and previously unseen invaders, so can we
not build a similar Intrusion Detection System for our computers.



We develop an incremental tableau-based decision procedures for the
  Alternating-time temporal logic ATL and some of its variants.
  While running within the theoretically established complexity upper bound, we
claim that our tableau is practically more efficient in the average case than
other decision procedures for ATL known so far. Besides, the ease of its
adaptation to variants of ATL demonstrates the flexibility of the proposed
procedure.



Jerne's idiotypic network theory postulates that the immune response involves
inter-antibody stimulation and suppression as well as matching to antigens. The
theory has proved the most popular Artificial Immune System (ais) model for
incorporation into behavior-based robotics but guidelines for implementing
idiotypic selection are scarce. Furthermore, the direct effects of employing
the technique have not been demonstrated in the form of a comparison with
non-idiotypic systems. This paper aims to address these issues. A method for
integrating an idiotypic ais network with a Reinforcement Learning based
control system (rl) is described and the mechanisms underlying antibody
stimulation and suppression are explained in detail. Some hypotheses that
account for the network advantage are put forward and tested using three
systems with increasing idiotypic complexity. The basic rl, a simplified hybrid
ais-rl that implements idiotypic selection independently of derived
concentration levels and a full hybrid ais-rl scheme are examined. The test bed
takes the form of a simulated Pioneer robot that is required to navigate
through maze worlds detecting and tracking door markers.



The biological immune system is a robust, complex, adaptive system that
defends the body from foreign pathogens. It is able to categorize all cells (or
molecules) within the body as self-cells or non-self cells. It does this with
the help of a distributed task force that has the intelligence to take action
from a local and also a global perspective using its network of chemical
messengers for communication. There are two major branches of the immune
system. The innate immune system is an unchanging mechanism that detects and
destroys certain invading organisms, whilst the adaptive immune system responds
to previously unknown foreign cells and builds a response to them that can
remain in the body over a long period of time. This remarkable information
processing biological system has caught the attention of computer science in
recent years. A novel computational intelligence technique, inspired by
immunology, has emerged, called Artificial Immune Systems. Several concepts
from the immune have been extracted and applied for solution to real world
science and engineering problems. In this tutorial, we briefly describe the
immune system metaphors that are relevant to existing Artificial Immune Systems
methods. We will then show illustrative real-world problems suitable for
Artificial Immune Systems and give a step-by-step algorithm walkthrough for one
such problem. A comparison of the Artificial Immune Systems to other well-known
algorithms, areas for future work, tips & tricks and a list of resources will
round this tutorial off. It should be noted that as Artificial Immune Systems
is still a young and evolving field, there is not yet a fixed algorithm
template and hence actual implementations might differ somewhat from time to
time and from those examples given here.



We provide deterministic, polynomial-time computable voting rules that
approximate Dodgson's and (the ``minimization version'' of) Young's scoring
rules to within a logarithmic factor. Our approximation of Dodgson's rule is
tight up to a constant factor, as Dodgson's rule is $\NP$-hard to approximate
to within some logarithmic factor. The ``maximization version'' of Young's rule
is known to be $\NP$-hard to approximate by any constant factor. Both
approximations are simple, and natural as rules in their own right: Given a
candidate we wish to score, we can regard either its Dodgson or Young score as
the edit distance between a given set of voter preferences and one in which the
candidate to be scored is the Condorcet winner. (The difference between the two
scoring rules is the type of edits allowed.) We regard the marginal cost of a
sequence of edits to be the number of edits divided by the number of reductions
(in the candidate's deficit against any of its opponents in the pairwise race
against that opponent) that the edits yield. Over a series of rounds, our
scoring rules greedily choose a sequence of edits that modify exactly one
voter's preferences and whose marginal cost is no greater than any other such
single-vote-modifying sequence.



A first-order conditional logic is considered, with semantics given by a
variant of epsilon-semantics, where p -> q means that Pr(q | p) approaches 1
super-polynomially --faster than any inverse polynomial. This type of
convergence is needed for reasoning about security protocols. A complete
axiomatization is provided for this semantics, and it is shown how a
qualitative proof of the correctness of a security protocol can be
automatically converted to a quantitative proof appropriate for reasoning about
concrete security.



We study the performance of approximate Nash equilibria for linear congestion
games. We consider how much the price of anarchy worsens and how much the price
of stability improves as a function of the approximation factor $\epsilon$. We
give (almost) tight upper and lower bounds for both the price of anarchy and
the price of stability for atomic and non-atomic congestion games. Our results
not only encompass and generalize the existing results of exact equilibria to
$\epsilon$-Nash equilibria, but they also provide a unified approach which
reveals the common threads of the atomic and non-atomic price of anarchy
results. By expanding the spectrum, we also cast the existing results in a new
light. For example, the Pigou network, which gives tight results for exact Nash
equilibria of selfish routing, remains tight for the price of stability of
$\epsilon$-Nash equilibria but not for the price of anarchy.



In this study, under general frame of MAny Connected Intelligent Particles
Systems (MACIPS), we reproduce two new simple subsets of such intelligent
complex network, namely hybrid intelligent systems, involved a few prominent
intelligent computing and approximate reasoning methods: self organizing
feature map (SOM), Neuro-Fuzzy Inference System and Rough Set Theory (RST).
Over this, we show how our algorithms can be construed as a linkage of
government-society interaction, where government catches various fashions of
behavior: solid (absolute) or flexible. So, transition of such society, by
changing of connectivity parameters (noise) from order to disorder is inferred.
Add to this, one may find an indirect mapping among financial systems and
eventual market fluctuations with MACIPS.



Several researchers have recently investigated the connection between
reinforcement learning and classification. We are motivated by proposals of
approximate policy iteration schemes without value functions which focus on
policy representation using classifiers and address policy learning as a
supervised learning problem. This paper proposes variants of an improved policy
iteration scheme which addresses the core sampling problem in evaluating a
policy through simulation as a multi-armed bandit machine. The resulting
algorithm offers comparable performance to the previous algorithm achieved,
however, with significantly less computational effort. An order of magnitude
improvement is demonstrated experimentally in two standard reinforcement
learning domains: inverted pendulum and mountain-car.



Computability logic (CL) (see http://www.cis.upenn.edu/~giorgi/cl.html) is a
recently launched program for redeveloping logic as a formal theory of
computability, as opposed to the formal theory of truth that logic has more
traditionally been. Formulas in it represent computational problems, "truth"
means existence of an algorithmic solution, and proofs encode such solutions.
Within the line of research devoted to finding axiomatizations for ever more
expressive fragments of CL, the present paper introduces a new deductive system
CL12 and proves its soundness and completeness with respect to the semantics of
CL. Conservatively extending classical predicate calculus and offering
considerable additional expressive and deductive power, CL12 presents a
reasonable, computationally meaningful, constructive alternative to classical
logic as a basis for applied theories. To obtain a model example of such
theories, this paper rebuilds the traditional, classical-logic-based Peano
arithmetic into a computability-logic-based counterpart. Among the purposes of
the present contribution is to provide a starting point for what, as the author
wishes to hope, might become a new line of research with a potential of
interesting findings -- an exploration of the presumably quite unusual
metatheory of CL-based arithmetic and other CL-based applied systems.



The seabed characterization from sonar images is a very hard task because of
the produced data and the unknown environment, even for an human expert. In
this work we propose an original approach in order to combine binary
classifiers arising from different kinds of strategies such as one-versus-one
or one-versus-rest, usually used in the SVM-classification. The decision
functions coming from these binary classifiers are interpreted in terms of
belief functions in order to combine these functions with one of the numerous
operators of the belief functions theory. Moreover, this interpretation of the
decision function allows us to propose a process of decisions by taking into
account the rejected observations too far removed from the learning data, and
the imprecise decisions given in unions of classes. This new approach is
illustrated and evaluated with a SVM in order to classify the different kinds
of sediment on image sonar.



Requirements about the quality of clinical guidelines can be represented by
schemata borrowed from the theory of abductive diagnosis, using temporal logic
to model the time-oriented aspects expressed in a guideline. Previously, we
have shown that these requirements can be verified using interactive theorem
proving techniques. In this paper, we investigate how this approach can be
mapped to the facilities of a resolution-based theorem prover, Otter, and a
complementary program that searches for finite models of first-order
statements, Mace. It is shown that the reasoning required for checking the
quality of a guideline can be mapped to such fully automated theorem-proving
facilities. The medical quality of an actual guideline concerning diabetes
mellitus 2 is investigated in this way.



The interface for the next generation of Unmanned Vehicle Systems should be
an interface with multi-modal displays and input controls. Then, the role of
the interface will not be restricted to be a support of the interactions
between the ground operator and vehicles. Interface must take part in the
interaction management too. In this paper, we show that recent works in
pragmatics and philosophy provide a suitable theoretical framework for the next
generation of UV System's interface. We concentrate on two main aspects of the
collaborative model of interaction based on acceptance: multi-strategy approach
for communicative act generation and interpretation and communicative
alignment.



The data-complexity of both satisfiability and finite satisfiability for the
two-variable fragment with counting is NP-complete; the data-complexity of both
query-answering and finite query-answering for the two-variable guarded
fragment with counting is co-NP-complete.



Nash equilibrium is the most commonly-used notion of equilibrium in game
theory. However, it suffers from numerous problems. Some are well known in the
game theory community; for example, the Nash equilibrium of repeated prisoner's
dilemma is neither normatively nor descriptively reasonable. However, new
problems arise when considering Nash equilibrium from a computer science
perspective: for example, Nash equilibrium is not robust (it does not tolerate
``faulty'' or ``unexpected'' behavior), it does not deal with coalitions, it
does not take computation cost into account, and it does not deal with cases
where players are not aware of all aspects of the game. Solution concepts that
try to address these shortcomings of Nash equilibrium are discussed.



We study random instances of the weighted $d$-CNF satisfiability problem
(WEIGHTED $d$-SAT), a generic W[1]-complete problem. A random instance of the
problem consists of a fixed parameter $k$ and a random $d$-CNF formula
$\weicnf{n}{p}{k, d}$ generated as follows: for each subset of $d$ variables
and with probability $p$, a clause over the $d$ variables is selected uniformly
at random from among the $2^d - 1$ clauses that contain at least one negated
literals.
  We show that random instances of WEIGHTED $d$-SAT can be solved in $O(k^2n +
n^{O(1)})$-time with high probability, indicating that typical instances of
WEIGHTED $d$-SAT under this instance distribution are fixed-parameter
tractable. The result also hold for random instances from the model
$\weicnf{n}{p}{k,d}(d')$ where clauses containing less than $d' (1 < d' < d)$
negated literals are forbidden, and for random instances of the renormalized
(miniaturized) version of WEIGHTED $d$-SAT in certain range of the random
model's parameter $p(n)$. This, together with our previous results on the
threshold behavior and the resolution complexity of unsatisfiable instances of
$\weicnf{n}{p}{k, d}$, provides an almost complete characterization of the
typical-case behavior of random instances of WEIGHTED $d$-SAT.



Algorithm selection is typically based on models of algorithm performance,
learned during a separate offline training sequence, which can be prohibitively
expensive. In recent work, we adopted an online approach, in which a
performance model is iteratively updated and used to guide selection on a
sequence of problem instances. The resulting exploration-exploitation trade-off
was represented as a bandit problem with expert advice, using an existing
solver for this game, but this required the setting of an arbitrary bound on
algorithm runtimes, thus invalidating the optimal regret of the solver. In this
paper, we propose a simpler framework for representing algorithm selection as a
bandit problem, with partial information, and an unknown bound on losses. We
adapt an existing solver to this game, proving a bound on its expected regret,
which holds also for the resulting algorithm selection technique. We present
preliminary experiments with a set of SAT solvers on a mixed SAT-UNSAT
benchmark.



The practical applications based on recurrent spiking neurons are limited due
to their non-trivial learning algorithms. The temporal nature of spiking
neurons is more favorable for hardware implementation where signals can be
represented in binary form and communication can be done through the use of
spikes. This work investigates the potential of recurrent spiking neurons
implementations on reconfigurable platforms and their applicability in temporal
based applications. A theoretical framework of reservoir computing is
investigated for hardware/software implementation. In this framework, only
readout neurons are trained which overcomes the burden of training at the
network level. These recurrent neural networks are termed as microcircuits
which are viewed as basic computational units in cortical computation. This
paper investigates the potential of recurrent neural reservoirs and presents a
novel hardware/software strategy for their implementation on FPGAs. The design
is implemented and the functionality is tested in the context of speech
recognition application.



This paper studies how to verify the conformity of a program with its
specification and proposes a novel constraint-programming framework for bounded
program verification (CPBPV). The CPBPV framework uses constraint stores to
represent the specification and the program and explores execution paths
nondeterministically. The input program is partially correct if each constraint
store so produced implies the post-condition. CPBPV does not explore spurious
execution paths as it incrementally prunes execution paths early by detecting
that the constraint store is not consistent. CPBPV uses the rich language of
constraint programming to express the constraint store. Finally, CPBPV is
parametrized with a list of solvers which are tried in sequence, starting with
the least expensive and less general. Experimental results often produce orders
of magnitude improvements over earlier approaches, running times being often
independent of the variable domains. Moreover, CPBPV was able to detect subtle
errors in some programs while other frameworks based on model checking have
failed.



Our aim is to build a set of rules, such that reasoning over temporal
dependencies within gene regulatory networks is possible. The underlying
transitions may be obtained by discretizing observed time series, or they are
generated based on existing knowledge, e.g. by Boolean networks or their
nondeterministic generalization. We use the mathematical discipline of formal
concept analysis (FCA), which has been applied successfully in domains as
knowledge representation, data mining or software engineering. By the attribute
exploration algorithm, an expert or a supporting computer program is enabled to
decide about the validity of a minimal set of implications and thus to
construct a sound and complete knowledge base. From this all valid implications
are derivable that relate to the selected properties of a set of genes. We
present results of our method for the initiation of sporulation in Bacillus
subtilis. However the formal structures are exhibited in a most general manner.
Therefore the approach may be adapted to signal transduction or metabolic
networks, as well as to discrete temporal transitions in many biological and
nonbiological areas.



We investigate the use of message-passing algorithms for the problem of
finding the max-weight independent set (MWIS) in a graph. First, we study the
performance of the classical loopy max-product belief propagation. We show that
each fixed point estimate of max-product can be mapped in a natural way to an
extreme point of the LP polytope associated with the MWIS problem. However,
this extreme point may not be the one that maximizes the value of node weights;
the particular extreme point at final convergence depends on the initialization
of max-product. We then show that if max-product is started from the natural
initialization of uninformative messages, it always solves the correct LP -- if
it converges. This result is obtained via a direct analysis of the iterative
algorithm, and cannot be obtained by looking only at fixed points.
  The tightness of the LP relaxation is thus necessary for max-product
optimality, but it is not sufficient. Motivated by this observation, we show
that a simple modification of max-product becomes gradient descent on (a
convexified version of) the dual of the LP, and converges to the dual optimum.
We also develop a message-passing algorithm that recovers the primal MWIS
solution from the output of the descent algorithm. We show that the MWIS
estimate obtained using these two algorithms in conjunction is correct when the
graph is bipartite and the MWIS is unique.
  Finally, we show that any problem of MAP estimation for probability
distributions over finite domains can be reduced to an MWIS problem. We believe
this reduction will yield new insights and algorithms for MAP estimation.



One of the most complex systems is the human brain whose formalized
functioning is characterized by decision theory. We present a "Quantum Decision
Theory" of decision making, based on the mathematical theory of separable
Hilbert spaces. This mathematical structure captures the effect of
superposition of composite prospects, including many incorporated intentions,
which allows us to explain a variety of interesting fallacies and anomalies
that have been reported to particularize the decision making of real human
beings. The theory describes entangled decision making, non-commutativity of
subsequent decisions, and intention interference of composite prospects. We
demonstrate how the violation of the Savage's sure-thing principle (disjunction
effect) can be explained as a result of the interference of intentions, when
making decisions under uncertainty. The conjunction fallacy is also explained
by the presence of the interference terms. We demonstrate that all known
anomalies and paradoxes, documented in the context of classical decision
theory, are reducible to just a few mathematical archetypes, all of which
finding straightforward explanations in the frame of the developed quantum
approach.



This report describes experimental results for a set of benchmarks on program
verification. It compares the capabilities of CPBVP "Constraint Programming
framework for Bounded Program Verification" [4] with the following frameworks:
ESC/Java, CBMC, Blast, EUREKA and Why.



Just as war is sometimes fallaciously represented as a zero sum game -- when
in fact war is a negative sum game - stock market trading, a positive sum game
over time, is often erroneously represented as a zero sum game. This is called
the "zero sum fallacy" -- the erroneous belief that one trader in a stock
market exchange can only improve their position provided some other trader's
position deteriorates. However, a positive sum game in absolute terms can be
recast as a zero sum game in relative terms. Similarly it appears that negative
sum games in absolute terms have been recast as zero sum games in relative
terms: otherwise, why would zero sum games be used to represent situations of
war? Such recasting may have heuristic or pedagogic interest but recasting must
be clearly explicited or risks generating confusion.
  Keywords: Game theory, stock trading and agent based AI.



This paper studies peek arc consistency, a reasoning technique that extends
the well-known arc consistency technique for constraint satisfaction. In
contrast to other more costly extensions of arc consistency that have been
studied in the literature, peek arc consistency requires only linear space and
quadratic time and can be parallelized in a straightforward way such that it
runs in linear time with a linear number of processors. We demonstrate that for
various constraint languages, peek arc consistency gives a polynomial-time
decision procedure for the constraint satisfaction problem. We also present an
algebraic characterization of those constraint languages that can be solved by
peek arc consistency, and study the robustness of the algorithm.



Consider a 0-1 observation matrix M, where rows correspond to entities and
columns correspond to signals; a value of 1 (or 0) in cell (i,j) of M indicates
that signal j has been observed (or not observed) in entity i. Given such a
matrix we study the problem of inferring the underlying directed links between
entities (rows) and finding which entries in the matrix are initiators.
  We formally define this problem and propose an MCMC framework for estimating
the links and the initiators given the matrix of observations M. We also show
how this framework can be extended to incorporate a temporal aspect; instead of
considering a single observation matrix M we consider a sequence of observation
matrices M1,..., Mt over time.
  We show the connection between our problem and several problems studied in
the field of social-network analysis. We apply our method to paleontological
and ecological data and show that our algorithms work well in practice and give
reasonable results.



This paper generalizes the traditional statistical concept of prediction
intervals for arbitrary probability density functions in high-dimensional
feature spaces by introducing significance level distributions, which provides
interval-independent probabilities for continuous random variables. The
advantage of the transformation of a probability density function into a
significance level distribution is that it enables one-class classification or
outlier detection in a direct manner.



The Baum-Welsh algorithm together with its derivatives and variations has
been the main technique for learning Hidden Markov Models (HMM) from
observational data. We present an HMM learning algorithm based on the
non-negative matrix factorization (NMF) of higher order Markovian statistics
that is structurally different from the Baum-Welsh and its associated
approaches. The described algorithm supports estimation of the number of
recurrent states of an HMM and iterates the non-negative matrix factorization
(NMF) algorithm to improve the learned HMM parameters. Numerical examples are
provided as well.



Wikipedia is a goldmine of information; not just for its many readers, but
also for the growing community of researchers who recognize it as a resource of
exceptional scale and utility. It represents a vast investment of manual effort
and judgment: a huge, constantly evolving tapestry of concepts and relations
that is being applied to a host of tasks.
  This article provides a comprehensive description of this work. It focuses on
research that extracts and makes use of the concepts, relations, facts and
descriptions found in Wikipedia, and organizes the work into four broad
categories: applying Wikipedia to natural language processing; using it to
facilitate information retrieval and information extraction; and as a resource
for ontology building. The article addresses how Wikipedia is being used as is,
how it is being improved and adapted, and how it is being combined with other
structures to create entirely new resources. We identify the research groups
and individuals involved, and how their work has developed in the last few
years. We provide a comprehensive list of the open-source software they have
produced.



Recent advances in neurosciences and psychology have provided evidence that
affective phenomena pervade intelligence at many levels, being inseparable from
the cognitionaction loop. Perception, attention, memory, learning,
decisionmaking, adaptation, communication and social interaction are some of
the aspects influenced by them. This work draws its inspirations from
neurobiology, psychophysics and sociology to approach the problem of building
autonomous robots capable of interacting with each other and building
strategies based on temperamental decision mechanism. Modelling emotions is a
relatively recent focus in artificial intelligence and cognitive modelling.
Such models can ideally inform our understanding of human behavior. We may see
the development of computational models of emotion as a core research focus
that will facilitate advances in the large array of computational systems that
model, interpret or influence human behavior. We propose a model based on a
scalable, flexible and modular approach to emotion which allows runtime
evaluation between emotional quality and performance. The results achieved
showed that the strategies based on temperamental decision mechanism strongly
influence the system performance and there are evident dependency between
emotional state of the agents and their temperamental type, as well as the
dependency between the team performance and the temperamental configuration of
the team members, and this enable us to conclude that the modular approach to
emotional programming based on temperamental theory is the good choice to
develop computational mind models for emotional behavioral Multi-Agent systems.



We prove the following results for task allocation of indivisible resources:
  - The problem of finding a leximin-maximal resource allocation is in P if the
agents have max-utility functions and atomic demands.
  - Deciding whether a resource allocation is Pareto-optimal is coNP-complete
for agents with (1-)additive utility functions.
  - Deciding whether there exists a Pareto-optimal and envy-free resource
allocation is Sigma_2^p-complete for agents with (1-)additive utility
functions.



We obtain a new upper bound on the capacity of a class of discrete memoryless
relay channels. For this class of relay channels, the relay observes an i.i.d.
sequence $T$, which is independent of the channel input $X$. The channel is
described by a set of probability transition functions $p(y|x,t)$ for all
$(x,t,y)\in \mathcal{X}\times \mathcal{T}\times \mathcal{Y}$. Furthermore, a
noiseless link of finite capacity $R_{0}$ exists from the relay to the
receiver. Although the capacity for these channels is not known in general, the
capacity of a subclass of these channels, namely when $T=g(X,Y)$, for some
deterministic function $g$, was obtained in [1] and it was shown to be equal to
the cut-set bound. Another instance where the capacity was obtained was in [2],
where the channel output $Y$ can be written as $Y=X\oplus Z$, where $\oplus$
denotes modulo-$m$ addition, $Z$ is independent of $X$,
$|\mathcal{X}|=|\mathcal{Y}|=m$, and $T$ is some stochastic function of $Z$.
The compress-and-forward (CAF) achievability scheme [3] was shown to be
capacity achieving in both cases.
  Using our upper bound we recover the capacity results of [1] and [2]. We also
obtain the capacity of a class of channels which does not fall into either of
the classes studied in [1] and [2]. For this class of channels, CAF scheme is
shown to be optimal but capacity is strictly less than the cut-set bound for
certain values of $R_{0}$. We also evaluate our outer bound for a particular
relay channel with binary multiplicative states and binary additive noise for
which the channel is given as $Y=TX+N$. We show that our upper bound is
strictly better than the cut-set upper bound for certain values of $R_{0}$ but
it lies strictly above the rates yielded by the CAF achievability scheme.



We propose to improve medical decision making and reduce global health care
costs by employing a free Internet-based medical information system with two
main target groups: practicing physicians and medical researchers. After
acquiring patients' consent, physicians enter medical histories, physiological
data and symptoms or disorders into the system; an integrated expert system can
then assist in diagnosis and statistical software provides a list of the most
promising treatment options and medications, tailored to the patient.
Physicians later enter information about the outcomes of the chosen treatments,
data the system uses to optimize future treatment recommendations. Medical
researchers can analyze the aggregate data to compare various drugs or
treatments in defined patient populations on a large scale.



Coalitional games are mathematical models suited to analyze scenarios where
players can collaborate by forming coalitions in order to obtain higher worths
than by acting in isolation. A fundamental problem for coalitional games is to
single out the most desirable outcomes in terms of appropriate notions of worth
distributions, which are usually called solution concepts. Motivated by the
fact that decisions taken by realistic players cannot involve unbounded
resources, recent computer science literature reconsidered the definition of
such concepts by advocating the relevance of assessing the amount of resources
needed for their computation in terms of their computational complexity. By
following this avenue of research, the paper provides a complete picture of the
complexity issues arising with three prominent solution concepts for
coalitional games with transferable utility, namely, the core, the kernel, and
the bargaining set, whenever the game worth-function is represented in some
reasonable compact form (otherwise, if the worths of all coalitions are
explicitly listed, the input sizes are so large that complexity problems
are---artificially---trivial). The starting investigation point is the setting
of graph games, about which various open questions were stated in the
literature. The paper gives an answer to these questions, and in addition
provides new insights on the setting, by characterizing the computational
complexity of the three concepts in some relevant generalizations and
specializations.



This paper has been withdrawn.



The exploration-exploitation dilemma has been an intriguing and unsolved
problem within the framework of reinforcement learning. "Optimism in the face
of uncertainty" and model building play central roles in advanced exploration
methods. Here, we integrate several concepts and obtain a fast and simple
algorithm. We show that the proposed algorithm finds a near-optimal policy in
polynomial time, and give experimental evidence that it is robust and efficient
compared to its ascendants.



In this paper entropy based methods are compared and used to measure
structural diversity of an ensemble of 21 classifiers. This measure is mostly
applied in ecology, whereby species counts are used as a measure of diversity.
The measures used were Shannon entropy, Simpsons and the Berger Parker
diversity indexes. As the diversity indexes increased so did the accuracy of
the ensemble. An ensemble dominated by classifiers with the same structure
produced poor accuracy. Uncertainty rule from information theory was also used
to further define diversity. Genetic algorithms were used to find the optimal
ensemble by using the diversity indices as the cost function. The method of
voting was used to aggregate the decisions.



The key approaches for machine learning, especially learning in unknown
probabilistic environments are new representations and computation mechanisms.
In this paper, a novel quantum reinforcement learning (QRL) method is proposed
by combining quantum theory and reinforcement learning (RL). Inspired by the
state superposition principle and quantum parallelism, a framework of value
updating algorithm is introduced. The state (action) in traditional RL is
identified as the eigen state (eigen action) in QRL. The state (action) set can
be represented with a quantum superposition state and the eigen state (eigen
action) can be obtained by randomly observing the simulated quantum state
according to the collapse postulate of quantum measurement. The probability of
the eigen action is determined by the probability amplitude, which is
parallelly updated according to rewards. Some related characteristics of QRL
such as convergence, optimality and balancing between exploration and
exploitation are also analyzed, which shows that this approach makes a good
tradeoff between exploration and exploitation using the probability amplitude
and can speed up learning through the quantum parallelism. To evaluate the
performance and practicability of QRL, several simulated experiments are given
and the results demonstrate the effectiveness and superiority of QRL algorithm
for some complex problems. The present work is also an effective exploration on
the application of quantum computation to artificial intelligence.



We introduce a modified model of random walk, and then develop two novel
clustering algorithms based on it. In the algorithms, each data point in a
dataset is considered as a particle which can move at random in space according
to the preset rules in the modified model. Further, this data point may be also
viewed as a local control subsystem, in which the controller adjusts its
transition probability vector in terms of the feedbacks of all data points, and
then its transition direction is identified by an event-generating function.
Finally, the positions of all data points are updated. As they move in space,
data points collect gradually and some separating parts emerge among them
automatically. As a consequence, data points that belong to the same class are
located at a same position, whereas those that belong to different classes are
away from one another. Moreover, the experimental results have demonstrated
that data points in the test datasets are clustered reasonably and efficiently,
and the comparison with other algorithms also provides an indication of the
effectiveness of the proposed algorithms.



We address the problem of reinforcement learning in which observations may
exhibit an arbitrary form of stochastic dependence on past observations and
actions, i.e. environments more general than (PO)MDPs. The task for an agent is
to attain the best possible asymptotic reward where the true generating
environment is unknown but belongs to a known countable family of environments.
We find some sufficient conditions on the class of environments under which an
agent exists which attains the best asymptotic reward for any environment in
the class. We analyze how tight these conditions are and how they relate to
different probabilistic assumptions known in reinforcement learning and related
fields, such as Markov Decision Processes and mixing conditions.



This paper presents the current state of a work in progress, whose objective
is to better understand the effects of factors that significantly influence the
performance of Latent Semantic Analysis (LSA). A difficult task, which consists
in answering (French) biology Multiple Choice Questions, is used to test the
semantic properties of the truncated singular space and to study the relative
influence of main parameters. A dedicated software has been designed to fine
tune the LSA semantic space for the Multiple Choice Questions task. With
optimal parameters, the performances of our simple model are quite surprisingly
equal or superior to those of 7th and 8th grades students. This indicates that
semantic spaces were quite good despite their low dimensions and the small
sizes of training data sets. Besides, we present an original entropy global
weighting of answers' terms of each question of the Multiple Choice Questions
which was necessary to achieve the model's success.



After presenting the broad context of authority sharing, we outline how
introducing more natural interaction in the design of the ground operator
interface of UV systems should help in allowing a single operator to manage the
complexity of his/her task. Introducing new modalities is one one of the means
in the realization of our vision of next- generation GOI. A more fundamental
aspect resides in the interaction manager which should help balance the
workload of the operator between mission and interaction, notably by applying a
multi-strategy approach to generation and interpretation. We intend to apply
these principles to the context of the Smaart prototype, and in this
perspective, we illustrate how to characterize the workload associated with a
particular operational situation.



In this paper, a practical power detection scheme for OFDM terminals, based
on recent free probability tools, is proposed. The objective is for the
receiving terminal to determine the transmission power and the number of the
surrounding base stations in the network. However, thesystem dimensions of the
network model turn energy detection into an under-determined problem. The focus
of this paper is then twofold: (i) discuss the maximum amount of information
that an OFDM terminal can gather from the surrounding base stations in the
network, (ii) propose a practical solution for blind cell detection using the
free deconvolution tool. The efficiency of this solution is measured through
simulations, which show better performance than the classical power detection
methods.



This paper introduces a Bayesian framework to detect multiple signals
embedded in noisy observations from a sensor array. For various states of
knowledge on the communication channel and the noise at the receiving sensors,
a marginalization procedure based on recent tools of finite random matrix
theory, in conjunction with the maximum entropy principle, is used to compute
the hypothesis selection criterion. Quite remarkably, explicit expressions for
the Bayesian detector are derived which enable to decide on the presence of
signal sources in a noisy wireless environment. The proposed Bayesian detector
is shown to outperform the classical power detector when the noise power is
known and provides very good performance for limited knowledge on the noise
power. Simulations corroborate the theoretical results and quantify the gain
achieved using the proposed Bayesian framework.



It has previously been an open problem whether all Boolean submodular
functions can be decomposed into a sum of binary submodular functions over a
possibly larger set of variables. This problem has been considered within
several different contexts in computer science, including computer vision,
artificial intelligence, and pseudo-Boolean optimisation. Using a connection
between the expressive power of valued constraints and certain algebraic
properties of functions, we answer this question negatively.
  Our results have several corollaries. First, we characterise precisely which
submodular functions of arity 4 can be expressed by binary submodular
functions. Next, we identify a novel class of submodular functions of arbitrary
arities which can be expressed by binary submodular functions, and therefore
minimised efficiently using a so-called expressibility reduction to the Min-Cut
problem. More importantly, our results imply limitations on this kind of
reduction and establish for the first time that it cannot be used in general to
minimise arbitrary submodular functions. Finally, we refute a conjecture of
Promislow and Young on the structure of the extreme rays of the cone of Boolean
submodular functions.



EVOC (for EVOlution of Culture) is a computer model of culture that enables
us to investigate how various factors such as barriers to cultural diffusion,
the presence and choice of leaders, or changes in the ratio of innovation to
imitation affect the diversity and effectiveness of ideas. It consists of
neural network based agents that invent ideas for actions, and imitate
neighbors' actions. The model is based on a theory of culture according to
which what evolves through culture is not memes or artifacts, but the internal
models of the world that give rise to them, and they evolve not through a
Darwinian process of competitive exclusion but a Lamarckian process involving
exchange of innovation protocols. EVOC shows an increase in mean fitness of
actions over time, and an increase and then decrease in the diversity of
actions. Diversity of actions is positively correlated with population size and
density, and with barriers between populations. Slowly eroding borders increase
fitness without sacrificing diversity by fostering specialization followed by
sharing of fit actions. Introducing a leader that broadcasts its actions
throughout the population increases the fitness of actions but reduces
diversity of actions. Increasing the number of leaders reduces this effect.
Efforts are underway to simulate the conditions under which an agent
immigrating from one culture to another contributes new ideas while still
fitting in.



Backtracking is a basic strategy to solve constraint satisfaction problems
(CSPs). A satisfiable CSP instance is backtrack-free if a solution can be found
without encountering any dead-end during a backtracking search, implying that
the instance is easy to solve. We prove an exact phase transition of
backtrack-free search in some random CSPs, namely in Model RB and in Model RD.
This is the first time an exact phase transition of backtrack-free search can
be identified on some random CSPs. Our technical results also have interesting
implications on the power of greedy algorithms, on the width of random
hypergraphs and on the exact satisfiability threshold of random CSPs.



This paper proposes a novel solution to spam detection inspired by a model of
the adaptive immune system known as the crossregulation model. We report on the
testing of a preliminary algorithm on six e-mail corpora. We also compare our
results statically and dynamically with those obtained by the Naive Bayes
classifier and another binary classification method we developed previously for
biomedical text-mining applications. We show that the cross-regulation model is
competitive against those and thus promising as a bio-inspired algorithm for
spam detection in particular, and binary classification in general.



The Quantum Decision Theory, developed recently by the authors, is applied to
clarify the role of risk and uncertainty in decision making and in particular
in relation to the phenomenon of dynamic inconsistency. By formulating this
notion in precise mathematical terms, we distinguish three types of
inconsistency: time inconsistency, planning paradox, and inconsistency
occurring in some discounting effects. While time inconsistency is well
accounted for in classical decision theory, the planning paradox is in
contradiction with classical utility theory. It finds a natural explanation in
the frame of the Quantum Decision Theory. Different types of discounting
effects are analyzed and shown to enjoy a straightforward explanation within
the suggested theory. We also introduce a general methodology based on
self-similar approximation theory for deriving the evolution equations for the
probabilities of future prospects. This provides a novel classification of
possible discount factors, which include the previously known cases
(exponential or hyperbolic discounting), but also predicts a novel class of
discount factors that decay to a strictly positive constant for very large
future time horizons. This class may be useful to deal with very long-term
discounting situations associated with intergenerational public policy choices,
encompassing issues such as global warming and nuclear waste disposal.



It has recently been discovered that both quantum and classical propositional
logics can be modelled by classes of non-orthomodular and thus non-distributive
lattices that properly contain standard orthomodular and Boolean classes,
respectively. In this paper we prove that these logics are complete even for
those classes of the former lattices from which the standard orthomodular
lattices and Boolean algebras are excluded. We also show that neither quantum
nor classical computers can be founded on the latter models. It follows that
logics are "valuation-nonmonotonic" in the sense that their possible models
(corresponding to their possible hardware implementations) and the valuations
for them drastically change when we add new conditions to their defining
conditions. These valuations can even be completely separated by putting them
into disjoint lattice classes by a technique presented in the paper.



Two well-known databases of semantic relationships between pairs of words
used in psycholinguistics, feature-based and association-based, are studied as
complex networks. We propose an algorithm to disentangle feature based
relationships from free association semantic networks. The algorithm uses the
rich topology of the free association semantic network to produce a new set of
relationships between words similar to those observed in feature production
norms.



Many AI researchers and cognitive scientists have argued that analogy is the
core of cognition. The most influential work on computational modeling of
analogy-making is Structure Mapping Theory (SMT) and its implementation in the
Structure Mapping Engine (SME). A limitation of SME is the requirement for
complex hand-coded representations. We introduce the Latent Relation Mapping
Engine (LRME), which combines ideas from SME and Latent Relational Analysis
(LRA) in order to remove the requirement for hand-coded representations. LRME
builds analogical mappings between lists of words, using a large corpus of raw
text to automatically discover the semantic relations among the words. We
evaluate LRME on a set of twenty analogical mapping problems, ten based on
scientific analogies and ten based on common metaphors. LRME achieves
human-level performance on the twenty problems. We compare LRME with a variety
of alternative approaches and find that they are not able to reach the same
level of performance.



The advent of the Semantic Web necessitates paradigm shifts away from
centralized client/server architectures towards decentralization and
peer-to-peer computation, making the existence of central authorities
superfluous and even impossible. At the same time, recommender systems are
gaining considerable impact in e-commerce, providing people with
recommendations that are personalized and tailored to their very needs. These
recommender systems have traditionally been deployed with stark centralized
scenarios in mind, operating in closed communities detached from their host
network's outer perimeter. We aim at marrying these two worlds, i.e.,
decentralized peer-to-peer computing and recommender systems, in one
agent-based framework. Our architecture features an epidemic-style protocol
maintaining neighborhoods of like-minded peers in a robust, selforganizing
fashion. In order to demonstrate our architecture's ability to retain
scalability, robustness and to allow for convergence towards high-quality
recommendations, we conduct offline experiments on top of the popular MovieLens
dataset.



General purpose intelligent learning agents cycle through (complex,non-MDP)
sequences of observations, actions, and rewards. On the other hand,
reinforcement learning is well-developed for small finite state Markov Decision
Processes (MDPs). So far it is an art performed by human designers to extract
the right state representation out of the bare observations, i.e. to reduce the
agent setup to the MDP framework. Before we can think of mechanizing this
search for suitable MDPs, we need a formal objective criterion. The main
contribution of this article is to develop such a criterion. I also integrate
the various parts into one learning algorithm. Extensions to more realistic
dynamic Bayesian networks are developed in a companion article.



Feature Markov Decision Processes (PhiMDPs) are well-suited for learning
agents in general environments. Nevertheless, unstructured (Phi)MDPs are
limited to relatively simple environments. Structured MDPs like Dynamic
Bayesian Networks (DBNs) are used for large-scale real-world problems. In this
article I extend PhiMDP to PhiDBN. The primary contribution is to derive a cost
criterion that allows to automatically extract the most relevant features from
the environment, leading to the "best" DBN representation. I discuss all
building blocks required for a complete general learning algorithm.



The logic which describes quantum robots is not orthodox quantum logic, but a
deductive calculus which reproduces the quantum tasks (computational processes,
and actions) taking into account quantum superposition and quantum
entanglement. A way toward the realization of intelligent quantum robots is to
adopt a quantum metalanguage to control quantum robots. A physical
implementation of a quantum metalanguage might be the use of coherent states in
brain signals.



We have proposed a model based upon flocking on a complex network, and then
developed two clustering algorithms on the basis of it. In the algorithms,
firstly a \textit{k}-nearest neighbor (knn) graph as a weighted and directed
graph is produced among all data points in a dataset each of which is regarded
as an agent who can move in space, and then a time-varying complex network is
created by adding long-range links for each data point. Furthermore, each data
point is not only acted by its \textit{k} nearest neighbors but also \textit{r}
long-range neighbors through fields established in space by them together, so
it will take a step along the direction of the vector sum of all fields. It is
more important that these long-range links provides some hidden information for
each data point when it moves and at the same time accelerate its speed
converging to a center. As they move in space according to the proposed model,
data points that belong to the same class are located at a same position
gradually, whereas those that belong to different classes are away from one
another. Consequently, the experimental results have demonstrated that data
points in datasets are clustered reasonably and efficiently, and the rates of
convergence of clustering algorithms are fast enough. Moreover, the comparison
with other algorithms also provides an indication of the effectiveness of the
proposed approach.



Artificial Chemistries (ACs) are symbolic chemical metaphors for the
exploration of Artificial Life, with specific focus on the problem of
biogenesis or the origin of life. This paper presents authors thoughts towards
defining a unified framework to characterize and classify symbolic artificial
chemistries by devising appropriate formalism to capture semantic and
organizational information. We identify three basic high level abstractions in
initial proposal for this framework viz., information, computation, and
communication. We present an analysis of two important notions of information,
namely, Shannon's Entropy and Algorithmic Information, and discuss inductive
and deductive approaches for defining the framework.



This paper investigates the relationship between the Logical Algorithms
language (LA) of Ganzinger and McAllester and Constraint Handling Rules (CHR).
We present a translation schema from LA to CHR-rp: CHR with rule priorities,
and show that the meta-complexity theorem for LA can be applied to a subset of
CHR-rp via inverse translation. Inspired by the high-level implementation
proposal for Logical Algorithm by Ganzinger and McAllester and based on a new
scheduling algorithm, we propose an alternative implementation for CHR-rp that
gives strong complexity guarantees and results in a new and accurate
meta-complexity theorem for CHR-rp. It is furthermore shown that the
translation from Logical Algorithms to CHR-rp combined with the new CHR-rp
implementation, satisfies the required complexity for the Logical Algorithms
meta-complexity result to hold.



We study constraint satisfaction problems on the so-called 'planted' random
ensemble. We show that for a certain class of problems, e.g. graph coloring,
many of the properties of the usual random ensemble are quantitatively
identical in the planted random ensemble. We study the structural phase
transitions, and the easy/hard/easy pattern in the average computational
complexity. We also discuss the finite temperature phase diagram, finding a
close connection with the liquid/glass/solid phenomenology.



The promise of e-Science will only be realized when data is discoverable,
accessible, and comprehensible within distributed teams, across disciplines,
and over the long-term--without reliance on out-of-band (non-digital) means. We
have developed the open-source Tupelo semantic content management framework and
are employing it to manage a wide range of e-Science entities (including data,
documents, workflows, people, and projects) and a broad range of metadata
(including provenance, social networks, geospatial relationships, temporal
relations, and domain descriptions). Tupelo couples the use of global
identifiers and resource description framework (RDF) statements with an
aggregatable content repository model to provide a unified space for securely
managing distributed heterogeneous content and relationships.



The present article introduces ptarithmetic (short for "polynomial time
arithmetic") -- a formal number theory similar to the well known Peano
arithmetic, but based on the recently born computability logic (see
http://www.cis.upenn.edu/~giorgi/cl.html) instead of classical logic. The
formulas of ptarithmetic represent interactive computational problems rather
than just true/false statements, and their "truth" is understood as existence
of a polynomial time solution. The system of ptarithmetic elaborated in this
article is shown to be sound and complete. Sound in the sense that every
theorem T of the system represents an interactive number-theoretic
computational problem with a polynomial time solution and, furthermore, such a
solution can be effectively extracted from a proof of T. And complete in the
sense that every interactive number-theoretic problem with a polynomial time
solution is represented by some theorem T of the system.
  The paper is self-contained, and can be read without any previous familiarity
with computability logic.



The Linked Data community is focused on integrating Resource Description
Framework (RDF) data sets into a single unified representation known as the Web
of Data. The Web of Data can be traversed by both man and machine and shows
promise as the \textit{de facto} standard for integrating data world wide much
like the World Wide Web is the \textit{de facto} standard for integrating
documents. On February 27$^\text{th}$ of 2009, an updated Linked Data cloud
visualization was made publicly available. This visualization represents the
various RDF data sets currently in the Linked Data cloud and their interlinking
relationships. For the purposes of this article, this visual representation was
manually transformed into a directed graph and analyzed.



In this paper, we investigate the deductive inference for the interiors and
exteriors of Horn knowledge bases, where the interiors and exteriors were
introduced by Makino and Ibaraki to study stability properties of knowledge
bases. We present a linear time algorithm for the deduction for the interiors
and show that it is co-NP-complete for the deduction for the exteriors. Under
model-based representation, we show that the deduction problem for interiors is
NP-complete while the one for exteriors is co-NP-complete. As for Horn
envelopes of the exteriors, we show that it is linearly solvable under
model-based representation, while it is co-NP-complete under formula-based
representation. We also discuss the polynomially solvable cases for all the
intractable problems.



Affective computing has proven to be a viable field of research comprised of
a large number of multidisciplinary researchers resulting in work that is
widely published. The majority of this work consists of computational models of
emotion recognition, computational modeling of causal factors of emotion and
emotion expression through rendered and robotic faces. A smaller part is
concerned with modeling the effects of emotion, formal modeling of cognitive
appraisal theory and models of emergent emotions. Part of the motivation for
affective computing as a field is to better understand emotional processes
through computational modeling. One of the four major topics in affective
computing is computers that have emotions (the others are recognizing,
expressing and understanding emotions). A critical and neglected aspect of
having emotions is the experience of emotion (Barrett, Mesquita, Ochsner, and
Gross, 2007): what does the content of an emotional episode look like, how does
this content change over time and when do we call the episode emotional. Few
modeling efforts have these topics as primary focus. The launch of a journal on
synthetic emotions should motivate research initiatives in this direction, and
this research should have a measurable impact on emotion research in
psychology. I show that a good way to do so is to investigate the psychological
core of what an emotion is: an experience. I present ideas on how the
experience of emotion could be modeled and provide evidence that several
computational models of emotion are already addressing the issue.



Complexity theory is a useful tool to study computational issues surrounding
the elicitation of preferences, as well as the strategic manipulation of
elections aggregating together preferences of multiple agents. We study here
the complexity of determining when we can terminate eliciting preferences, and
prove that the complexity depends on the elicitation strategy. We show, for
instance, that it may be better from a computational perspective to elicit all
preferences from one agent at a time than to elicit individual preferences from
multiple agents. We also study the connection between the strategic
manipulation of an election and preference elicitation. We show that what we
can manipulate affects the computational complexity of manipulation. In
particular, we prove that there are voting rules which are easy to manipulate
if we can change all of an agent's vote, but computationally intractable if we
can change only some of their preferences. This suggests that, as with
preference elicitation, a fine-grained view of manipulation may be informative.
Finally, we study the connection between predicting the winner of an election
and preference elicitation. Based on this connection, we identify a voting rule
where it is computationally difficult to decide the probability of a candidate
winning given a probability distribution over the votes.



This chapter defines a new concept and framework for constructing fusion
rules for evidences. This framework is based on a referee function, which does
a decisional arbitrament conditionally to basic decisions provided by the
several sources of information. A simple sampling method is derived from this
framework. The purpose of this sampling approach is to avoid the combinatorics
which are inherent to the definition of fusion rules of evidences. This
definition of the fusion rule by the means of a sampling process makes possible
the construction of several rules on the basis of an algorithmic implementation
of the referee function, instead of a mathematical formulation. Incidentally,
it is a versatile and intuitive way for defining rules. The framework is
implemented for various well known evidence rules. On the basis of this
framework, new rules for combining evidences are proposed, which takes into
account a consensual evaluation of the sources of information.



The Sudoku puzzle has achieved worldwide popularity recently, and attracted
great attention of the computational intelligence community. Sudoku is always
considered as Satisfiability Problem or Constraint Satisfaction Problem. In
this paper, we propose to focus on the essential graph structure underlying the
Sudoku puzzle. First, we formalize Sudoku as a graph. Then a solving algorithm
based on heuristic reasoning on the graph is proposed. The related r-Reduction
theorem, inference theorem and their properties are proved, providing the
formal basis for developments of Sudoku solving systems. In order to evaluate
the difficulty levels of puzzles, a quantitative measurement of the complexity
level of Sudoku puzzles based on the graph structure and information theory is
proposed. Experimental results show that all the puzzles can be solved fast
using the proposed heuristic reasoning, and that the proposed game complexity
metrics can discriminate difficulty levels of puzzles perfectly.



We study the tracking problem, namely, estimating the hidden state of an
object over time, from unreliable and noisy measurements. The standard
framework for the tracking problem is the generative framework, which is the
basis of solutions such as the Bayesian algorithm and its approximation, the
particle filters. However, the problem with these solutions is that they are
very sensitive to model mismatches. In this paper, motivated by online
learning, we introduce a new framework -- an {\em explanatory} framework -- for
tracking. We provide an efficient tracking algorithm for this framework. We
provide experimental results comparing our algorithm to the Bayesian algorithm
on simulated data. Our experiments show that when there are slight model
mismatches, our algorithm vastly outperforms the Bayesian algorithm.



In this work, we first show that feature selection methods other than
boosting can also be used for training an efficient object detector. In
particular, we introduce Greedy Sparse Linear Discriminant Analysis (GSLDA)
\cite{Moghaddam2007Fast} for its conceptual simplicity and computational
efficiency; and slightly better detection performance is achieved compared with
\cite{Viola2004Robust}. Moreover, we propose a new technique, termed Boosted
Greedy Sparse Linear Discriminant Analysis (BGSLDA), to efficiently train a
detection cascade. BGSLDA exploits the sample re-weighting property of boosting
and the class-separability criterion of GSLDA.



In this paper we treat both forms of probabilistic inference, estimating
marginal probabilities of the joint distribution and finding the most probable
assignment, through a unified message-passing algorithm architecture. We
generalize the Belief Propagation (BP) algorithms of sum-product and
max-product and tree-rewaighted (TRW) sum and max product algorithms (TRBP) and
introduce a new set of convergent algorithms based on "convex-free-energy" and
Linear-Programming (LP) relaxation as a zero-temprature of a
convex-free-energy. The main idea of this work arises from taking a general
perspective on the existing BP and TRBP algorithms while observing that they
all are reductions from the basic optimization formula of $f + \sum_i h_i$
where the function $f$ is an extended-valued, strictly convex but non-smooth
and the functions $h_i$ are extended-valued functions (not necessarily convex).
We use tools from convex duality to present the "primal-dual ascent" algorithm
which is an extension of the Bregman successive projection scheme and is
designed to handle optimization of the general type $f + \sum_i h_i$. Mapping
the fractional-free-energy variational principle to this framework introduces
the "norm-product" message-passing. Special cases include sum-product and
max-product (BP algorithms) and the TRBP algorithms. When the
fractional-free-energy is set to be convex (convex-free-energy) the
norm-product is globally convergent for estimating of marginal probabilities
and for approximating the LP-relaxation. We also introduce another branch of
the norm-product, the "convex-max-product". The convex-max-product is
convergent (unlike max-product) and aims at solving the LP-relaxation.



Every encoding has priori information if the encoding represents any semantic
information of the unverse or object. Encoding means mapping from the unverse
to the string or strings of digits. The semantic here is used in the
model-theoretic sense or denotation of the object. If encoding or strings of
symbols is the adequate and true mapping of model or object, and the mapping is
recursive or computable, the distance between two strings (text) is mapping the
distance between models. We then are able to measure the distance by computing
the distance between the two strings. Otherwise, we may take a misleading
course. "Language tree" may not be a family tree in the sense of historical
linguistics. Rather it just means the similarity.



Semantic memory is the subsystem of human memory that stores knowledge of
concepts or meanings, as opposed to life specific experiences. The organization
of concepts within semantic memory can be understood as a semantic network,
where the concepts (nodes) are associated (linked) to others depending on
perceptions, similarities, etc. Lexical access is the complementary part of
this system and allows the retrieval of such organized knowledge. While
conceptual information is stored under certain underlying organization (and
thus gives rise to a specific topology), it is crucial to have an accurate
access to any of the information units, e.g. the concepts, for efficiently
retrieving semantic information for real-time needings. An example of an
information retrieval process occurs in verbal fluency tasks, and it is known
to involve two different mechanisms: -clustering-, or generating words within a
subcategory, and, when a subcategory is exhausted, -switching- to a new
subcategory. We extended this approach to random-walking on a network
(clustering) in combination to jumping (switching) to any node with certain
probability and derived its analytical expression based on Markov chains.
Results show that this dual mechanism contributes to optimize the exploration
of different network models in terms of the mean first passage time.
Additionally, this cognitive inspired dual mechanism opens a new framework to
better understand and evaluate exploration, propagation and transport phenomena
in other complex systems where switching-like phenomena are feasible.



For a wide variety of regularization methods, algorithms computing the entire
solution path have been developed recently. Solution path algorithms do not
only compute the solution for one particular value of the regularization
parameter but the entire path of solutions, making the selection of an optimal
parameter much easier. Most of the currently used algorithms are not robust in
the sense that they cannot deal with general or degenerate input. Here we
present a new robust, generic method for parametric quadratic programming. Our
algorithm directly applies to nearly all machine learning applications, where
so far every application required its own different algorithm.
  We illustrate the usefulness of our method by applying it to a very low rank
problem which could not be solved by existing path tracking methods, namely to
compute part-worth values in choice based conjoint analysis, a popular
technique from market research to estimate consumers preferences on a class of
parameterized options.



A technique for speeding up reinforcement learning algorithms by using time
manipulation is proposed. It is applicable to failure-avoidance control
problems running in a computer simulation. Turning the time of the simulation
backwards on failure events is shown to speed up the learning by 260% and
improve the state space exploration by 12% on the cart-pole balancing task,
compared to the conventional Q-learning and Actor-Critic algorithms.



Owners of a web-site are often interested in analysis of groups of users of
their site. Information on these groups can help optimizing the structure and
contents of the site. In this paper we use an approach based on formal concepts
for constructing taxonomies of user groups. For decreasing the huge amount of
concepts that arise in applications, we employ stability index of a concept,
which describes how a group given by a concept extent differs from other such
groups. We analyze resulting taxonomies of user groups for three target
websites.



We present a heuristic framework for attacking the undecidable termination
problem of logic programs, as an alternative to current
termination/non-termination proof approaches. We introduce an idea of
termination prediction, which predicts termination of a logic program in case
that neither a termination nor a non-termination proof is applicable. We
establish a necessary and sufficient characterization of infinite (generalized)
SLDNF-derivations with arbitrary (concrete or moded) queries, and develop an
algorithm that predicts termination of general logic programs with arbitrary
non-floundering queries. We have implemented a termination prediction tool and
obtained quite satisfactory experimental results. Except for five programs
which break the experiment time limit, our prediction is 100% correct for all
296 benchmark programs of the Termination Competition 2007, of which eighteen
programs cannot be proved by any of the existing state-of-the-art analyzers
like AProVE07, NTI, Polytool and TALP.



This paper further introduces and formalizes a novel concept of
self-forensics for automotive vehicles, specified in the Forensic Lucid
language. We argue that self-forensics, with the forensics taken out of the
cybercrime domain, is applicable to "self-dissection" of intelligent vehicles
and hardware systems for automated incident and anomaly analysis and event
reconstruction by the software with or without the aid of the engineering teams
in a variety of forensic scenarios. We propose a formal design, requirements,
and specification of the self-forensic enabled units (similar to blackboxes) in
vehicles that will help investigation of incidents and also automated reasoning
and verification of theories along with the events reconstruction in a formal
model. We argue such an analysis is beneficial to improve the safety of the
passengers and their vehicles, like the airline industry does for planes.



Graded modal logic is the formal language obtained from ordinary
(propositional) modal logic by endowing its modal operators with cardinality
constraints. Under the familiar possible-worlds semantics, these augmented
modal operators receive interpretations such as "It is true at no fewer than 15
accessible worlds that...", or "It is true at no more than 2 accessible worlds
that...". We investigate the complexity of satisfiability for this language
over some familiar classes of frames. This problem is more challenging than its
ordinary modal logic counterpart--especially in the case of transitive frames,
where graded modal logic lacks the tree-model property. We obtain tight
complexity bounds for the problem of determining the satisfiability of a given
graded modal logic formula over the classes of frames characterized by any
combination of reflexivity, seriality, symmetry, transitivity and the Euclidean
property.



The problem is sequence prediction in the following setting. A sequence
$x_1,...,x_n,...$ of discrete-valued observations is generated according to
some unknown probabilistic law (measure) $\mu$. After observing each outcome,
it is required to give the conditional probabilities of the next observation.
The measure $\mu$ belongs to an arbitrary class $\C$ of stochastic processes.
We are interested in predictors $\rho$ whose conditional probabilities converge
to the "true" $\mu$-conditional probabilities if any $\mu\in\C$ is chosen to
generate the data. We show that if such a predictor exists, then a predictor
can also be obtained as a convex combination of a countably many elements of
$\C$. In other words, it can be obtained as a Bayesian predictor whose prior is
concentrated on a countable set. This result is established for two very
different measures of performance of prediction, one of which is very strong,
namely, total variation, and the other is very weak, namely, prediction in
expected average Kullback-Leibler divergence.



Formal Concept Analysis (FCA) is a mathematical theory based on the
formalization of the notions of concept and concept hierarchies. It has been
successfully applied to several Computer Science fields such as data
mining,software engineering, and knowledge engineering, and in many domains
like medicine, psychology, linguistics and ecology. For instance, it has been
exploited for the design, mapping and refinement of ontologies. In this paper,
we show how FCA can benefit from a given domain ontology by analyzing the
impact of a taxonomy (on objects and/or attributes) on the resulting concept
lattice. We willmainly concentrate on the usage of a taxonomy to extract
generalized patterns (i.e., knowledge generated from data when elements of a
given domain ontology are used) in the form of concepts and rules, and improve
navigation through these patterns. To that end, we analyze three generalization
cases and show their impact on the size of the generalized pattern set.
Different scenarios of simultaneous generalizations on both objects and
attributes are also discussed



Online Social Networks (OSNs) have exploded in terms of scale and scope over
the last few years. The unprecedented growth of these networks present
challenges in terms of system design and maintenance. One way to cope with this
is by partitioning such large networks and assigning these partitions to
different machines. However, social networks possess unique properties that
make the partitioning problem non-trivial. The main contribution of this paper
is to understand different properties of social networks and how these
properties can guide the choice of a partitioning algorithm. Using large scale
measurements representing real OSNs, we first characterize different properties
of social networks, and then we evaluate qualitatively different partitioning
methods that cover the design space. We expose different trade-offs involved
and understand them in light of properties of social networks. We show that a
judicious choice of a partitioning scheme can help improve performance.



In this paper, we present an application of neural networks in the renewable
energy domain. We have developed a methodology for the daily prediction of
global solar radiation on a horizontal surface. We use an ad-hoc time series
preprocessing and a Multi-Layer Perceptron (MLP) in order to predict solar
radiation at daily horizon. First results are promising with nRMSE < 21% and
RMSE < 998 Wh/m2. Our optimized MLP presents prediction similar to or even
better than conventional methods such as ARIMA techniques, Bayesian inference,
Markov chains and k-Nearest-Neighbors approximators. Moreover we found that our
data preprocessing approach can reduce significantly forecasting errors.



General-purpose, intelligent, learning agents cycle through sequences of
observations, actions, and rewards that are complex, uncertain, unknown, and
non-Markovian. On the other hand, reinforcement learning is well-developed for
small finite state Markov decision processes (MDPs). Up to now, extracting the
right state representations out of bare observations, that is, reducing the
general agent setup to the MDP framework, is an art that involves significant
effort by designers. The primary goal of this work is to automate the reduction
process and thereby significantly expand the scope of many existing
reinforcement learning algorithms and the agents that employ them. Before we
can think of mechanizing this search for suitable MDPs, we need a formal
objective criterion. The main contribution of this article is to develop such a
criterion. I also integrate the various parts into one learning algorithm.
Extensions to more realistic dynamic Bayesian networks are developed in Part
II. The role of POMDPs is also considered there.



Computability logic (CoL) (see http://www.cis.upenn.edu/~giorgi/cl.html) is a
recently introduced semantical platform and ambitious program for redeveloping
logic as a formal theory of computability, as opposed to the formal theory of
truth that logic has more traditionally been. Its expressions represent
interactive computational tasks seen as games played by a machine against the
environment, and "truth" is understood as existence of an algorithmic winning
strategy. With logical operators standing for operations on games, the
formalism of CoL is open-ended, and has already undergone series of extensions.
This article extends the expressive power of CoL in a qualitatively new way,
generalizing formulas (to which the earlier languages of CoL were limited) to
circuit-style structures termed cirquents. The latter, unlike formulas, are
able to account for subgame/subtask sharing between different parts of the
overall game/task. Among the many advantages offered by this ability is that it
allows us to capture, refine and generalize the well known
independence-friendly logic which, after the present leap forward, naturally
becomes a conservative fragment of CoL, just as classical logic had been known
to be a conservative fragment of the formula-based version of CoL. Technically,
this paper is self-contained, and can be read without any prior familiarity
with CoL.



Among many existing distance measures for time series data, Dynamic Time
Warping (DTW) distance has been recognized as one of the most accurate and
suitable distance measures due to its flexibility in sequence alignment.
However, DTW distance calculation is computationally intensive. Especially in
very large time series databases, sequential scan through the entire database
is definitely impractical, even with random access that exploits some index
structures since high dimensionality of time series data incurs extremely high
I/O cost. More specifically, a sequential structure consumes high CPU but low
I/O costs, while an index structure requires low CPU but high I/O costs. In
this work, we therefore propose a novel indexed sequential structure called
TWIST (Time Warping in Indexed Sequential sTructure) which benefits from both
sequential access and index structure. When a query sequence is issued, TWIST
calculates lower bounding distances between a group of candidate sequences and
the query sequence, and then identifies the data access order in advance, hence
reducing a great number of both sequential and random accesses. Impressively,
our indexed sequential structure achieves significant speedup in a querying
process by a few orders of magnitude. In addition, our method shows superiority
over existing rival methods in terms of query processing time, number of page
accesses, and storage requirement with no false dismissal guaranteed.



A sensor network is a collection of wireless devices that are able to monitor
physical or environmental conditions. These devices (nodes) are expected to
operate autonomously, be battery powered and have very limited computational
capabilities. This makes the task of protecting a sensor network against
misbehavior or possible malfunction a challenging problem. In this document we
discuss performance of Artificial immune systems (AIS) when used as the
mechanism for detecting misbehavior.
  We show that (i) mechanism of the AIS have to be carefully applied in order
to avoid security weaknesses, (ii) the choice of genes and their interaction
have a profound influence on the performance of the AIS, (iii) randomly created
detectors do not comply with limitations imposed by communications protocols
and (iv) the data traffic pattern seems not to impact significantly the overall
performance.
  We identified a specific MAC layer based gene that showed to be especially
useful for detection; genes measure a network's performance from a node's
viewpoint. Furthermore, we identified an interesting complementarity property
of genes; this property exploits the local nature of sensor networks and moves
the burden of excessive communication from normally behaving nodes to
misbehaving nodes. These results have a direct impact on the design of AIS for
sensor networks and on engineering of sensor networks.



We propose a new model-based computer-aided diagnosis (CAD) system for tumor
detection and classification (cancerous v.s. benign) in breast images.
Specifically, we show that (x-ray, ultrasound and MRI) images can be accurately
modeled by two-dimensional autoregressive-moving average (ARMA) random fields.
We derive a two-stage Yule-Walker Least-Squares estimates of the model
parameters, which are subsequently used as the basis for statistical inference
and biophysical interpretation of the breast image. We use a k-means classifier
to segment the breast image into three regions: healthy tissue, benign tumor,
and cancerous tumor. Our simulation results on ultrasound breast images
illustrate the power of the proposed approach.



A general framework is proposed for integration of rules and external first
order theories. It is based on the well-founded semantics of normal logic
programs and inspired by ideas of Constraint Logic Programming (CLP) and
constructive negation for logic programs. Hybrid rules are normal clauses
extended with constraints in the bodies; constraints are certain formulae in
the language of the external theory. A hybrid program is a pair of a set of
hybrid rules and an external theory. Instances of the framework are obtained by
specifying the class of external theories, and the class of constraints. An
example instance is integration of (non-disjunctive) Datalog with ontologies
formalized as description logics.
  The paper defines a declarative semantics of hybrid programs and a
goal-driven formal operational semantics. The latter can be seen as a
generalization of SLS-resolution. It provides a basis for hybrid
implementations combining Prolog with constraint solvers. Soundness of the
operational semantics is proven. Sufficient conditions for decidability of the
declarative semantics, and for completeness of the operational semantics are
given.



In earlier work, we proposed a logic that extends the Logic of General
Awareness of Fagin and Halpern [1988] by allowing quantification over primitive
propositions. This makes it possible to express the fact that an agent knows
that there are some facts of which he is unaware. In that logic, it is not
possible to model an agent who is uncertain about whether he is aware of all
formulas. To overcome this problem, we keep the syntax of the earlier paper,
but allow models where, with each world, a possibly different language is
associated. We provide a sound and complete axiomatization for this logic and
show that, under natural assumptions, the quantifier-free fragment of the logic
is characterized by exactly the same axioms as the logic of Heifetz, Meier, and
Schipper [2008].



Brandenburger, Friedenberg, and Keisler provide an epistemic characterization
of iterated admissibility (i.e., iterated deletion of weakly dominated
strategies) where uncertainty is represented using LPSs (lexicographic
probability sequences). Their characterization holds in a rich structure called
a complete structure, where all types are possible. Here, a logical
charaacterization of iterated admisibility is given that involves only standard
probability and holds in all structures, not just complete structures. A
stronger notion of strong admissibility is then defined. Roughly speaking,
strong admissibility is meant to capture the intuition that "all the agent
knows" is that the other agents satisfy the appropriate rationality
assumptions. Strong admissibility makes it possible to relate admissibility,
canonical structures (as typically considered in completeness proofs in modal
logic), complete structures, and the notion of ``all I know''.



The problem of detecting terms that can be interesting to the advertiser is
considered. If a company has already bought some advertising terms which
describe certain services, it is reasonable to find out the terms bought by
competing companies. A part of them can be recommended as future advertising
terms to the company. The goal of this work is to propose better interpretable
recommendations based on FCA and association rules.



Recent work in data mining and related areas has highlighted the importance
of the statistical assessment of data mining results. Crucial to this endeavour
is the choice of a non-trivial null model for the data, to which the found
patterns can be contrasted. The most influential null models proposed so far
are defined in terms of invariants of the null distribution. Such null models
can be used by computation intensive randomization approaches in estimating the
statistical significance of data mining results.
  Here, we introduce a methodology to construct non-trivial probabilistic
models based on the maximum entropy (MaxEnt) principle. We show how MaxEnt
models allow for the natural incorporation of prior information. Furthermore,
they satisfy a number of desirable properties of previously introduced
randomization approaches. Lastly, they also have the benefit that they can be
represented explicitly. We argue that our approach can be used for a variety of
data types. However, for concreteness, we have chosen to demonstrate it in
particular for databases and networks.



Understanding how systems can be designed to be evolvable is fundamental to
research in optimization, evolution, and complex systems science. Many
researchers have thus recognized the importance of evolvability, i.e. the
ability to find new variants of higher fitness, in the fields of biological
evolution and evolutionary computation. Recent studies by Ciliberti et al
(Proc. Nat. Acad. Sci., 2007) and Wagner (Proc. R. Soc. B., 2008) propose a
potentially important link between the robustness and the evolvability of a
system. In particular, it has been suggested that robustness may actually lead
to the emergence of evolvability. Here we study two design principles,
redundancy and degeneracy, for achieving robustness and we show that they have
a dramatically different impact on the evolvability of the system. In
particular, purely redundant systems are found to have very little evolvability
while systems with degeneracy, i.e. distributed robustness, can be orders of
magnitude more evolvable. These results offer insights into the general
principles for achieving evolvability and may prove to be an important step
forward in the pursuit of evolvable representations in evolutionary
computation.



Specialized intelligent systems can be found everywhere: finger print,
handwriting, speech, and face recognition, spam filtering, chess and other game
programs, robots, et al. This decade the first presumably complete mathematical
theory of artificial intelligence based on universal
induction-prediction-decision-action has been proposed. This
information-theoretic approach solidifies the foundations of inductive
inference and artificial intelligence. Getting the foundations right usually
marks a significant progress and maturing of a field. The theory provides a
gold standard and guidance for researchers working on intelligent algorithms.
The roots of universal induction have been laid exactly half-a-century ago and
the roots of universal intelligence exactly one decade ago. So it is timely to
take stock of what has been achieved and what remains to be done. Since there
are already good recent surveys, I describe the state-of-the-art only in
passing and refer the reader to the literature. This article concentrates on
the open problems in universal induction and its extension to universal
intelligence.



Nodes in an ad hoc wireless network incur certain costs for forwarding
packets since packet forwarding consumes the resources of the nodes. If the
nodes are rational, free packet forwarding by the nodes cannot be taken for
granted and incentive based protocols are required to stimulate cooperation
among the nodes. Existing incentive based approaches are based on the VCG
(Vickrey-Clarke-Groves) mechanism which leads to high levels of incentive
budgets and restricted applicability to only certain topologies of networks.
Moreover, the existing approaches have only focused on unicast and multicast.
Motivated by this, we propose an incentive based broadcast protocol that
satisfies Bayesian incentive compatibility and minimizes the incentive budgets
required by the individual nodes. The proposed protocol, which we call {\em
BIC-B} (Bayesian incentive compatible broadcast) protocol, also satisfies
budget balance. We also derive a necessary and sufficient condition for the
ex-post individual rationality of the BIC-B protocol. The {\em BIC-B} protocol
exhibits superior performance in comparison to a dominant strategy incentive
compatible broadcast protocol.



There have been several highway traffic models proposed based on cellular
automata. The simplest one is elementary cellular automaton rule 184. We extend
this model to city traffic with cellular automata coupled at intersections
using only rules 184, 252, and 136. The simplicity of the model offers a clear
understanding of the main properties of city traffic and its phase transitions.
  We use the proposed model to compare two methods for coordinating traffic
lights: a green-wave method that tries to optimize phases according to expected
flows and a self-organizing method that adapts to the current traffic
conditions. The self-organizing method delivers considerable improvements over
the green-wave method. For low densities, the self-organizing method promotes
the formation and coordination of platoons that flow freely in four directions,
i.e. with a maximum velocity and no stops. For medium densities, the method
allows a constant usage of the intersections, exploiting their maximum flux
capacity. For high densities, the method prevents gridlocks and promotes the
formation and coordination of "free-spaces" that flow in the opposite direction
of traffic.



The evolution of the web server contents and the emergence of new kinds of
intrusions make necessary the adaptation of the intrusion detection systems
(IDS). Nowadays, the adaptation of the IDS requires manual -- tedious and
unreactive -- actions from system administrators. In this paper, we present a
self-adaptive intrusion detection system which relies on a set of local
model-based diagnosers. The redundancy of diagnoses is exploited, online, by a
meta-diagnoser to check the consistency of computed partial diagnoses, and to
trigger the adaptation of defective diagnoser models (or signatures) in case of
inconsistency. This system is applied to the intrusion detection from a stream
of HTTP requests. Our results show that our system 1) detects intrusion
occurrences sensitively and precisely, 2) accurately self-adapts diagnoser
model, thus improving its detection accuracy.



Dendritic cells are the crime scene investigators of the human immune system.
Their function is to correlate potentially anomalous invading entities with
observed damage to the body. The detection of such invaders by dendritic cells
results in the activation of the adaptive immune system, eventually leading to
the removal of the invader from the host body. This mechanism has provided
inspiration for the development of a novel bio-inspired algorithm, the
Dendritic Cell Algorithm. This algorithm processes information at multiple
levels of resolution, resulting in the creation of information granules of
variable structure. In this chapter we examine the multi-faceted nature of
immunology and how research in this field has shaped the function of the
resulting Dendritic Cell Algorithm. A brief overview of the algorithm is given
in combination with the details of the processes used for its development. The
chapter is concluded with a discussion of the parallels between our
understanding of the human immune system and how such knowledge influences the
design of artificial immune systems.



A key question in cooperative game theory is that of coalitional stability,
usually captured by the notion of the \emph{core}--the set of outcomes such
that no subgroup of players has an incentive to deviate. However, some
coalitional games have empty cores, and any outcome in such a game is unstable.
  In this paper, we investigate the possibility of stabilizing a coalitional
game by using external payments. We consider a scenario where an external
party, which is interested in having the players work together, offers a
supplemental payment to the grand coalition (or, more generally, a particular
coalition structure). This payment is conditional on players not deviating from
their coalition(s). The sum of this payment plus the actual gains of the
coalition(s) may then be divided among the agents so as to promote stability.
We define the \emph{cost of stability (CoS)} as the minimal external payment
that stabilizes the game.
  We provide general bounds on the cost of stability in several classes of
games, and explore its algorithmic properties. To develop a better intuition
for the concepts we introduce, we provide a detailed algorithmic study of the
cost of stability in weighted voting games, a simple but expressive class of
games which can model decision-making in political bodies, and cooperation in
multiagent settings. Finally, we extend our model and results to games with
coalition structures.



We consider the task of opportunistic channel access in a primary system
composed of independent Gilbert-Elliot channels where the secondary (or
opportunistic) user does not dispose of a priori information regarding the
statistical characteristics of the system. It is shown that this problem may be
cast into the framework of model-based learning in a specific class of
Partially Observed Markov Decision Processes (POMDPs) for which we introduce an
algorithm aimed at striking an optimal tradeoff between the exploration (or
estimation) and exploitation requirements. We provide finite horizon regret
bounds for this algorithm as well as a numerical evaluation of its performance
in the single channel model as well as in the case of stochastically identical
channels.



The Web community has introduced a set of standards and technologies for
representing, querying, and manipulating a globally distributed data structure
known as the Web of Data. The proponents of the Web of Data envision much of
the world's data being interrelated and openly accessible to the general
public. This vision is analogous in many ways to the Web of Documents of common
knowledge, but instead of making documents and media openly accessible, the
focus is on making data openly accessible. In providing data for public use,
there has been a stimulated interest in a movement dubbed Open Data. Open Data
is analogous in many ways to the Open Source movement. However, instead of
focusing on software, Open Data is focused on the legal and licensing issues
around publicly exposed data. Together, various technological and legal tools
are laying the groundwork for the future of global-scale data management on the
Web. As of today, in its early form, the Web of Data hosts a variety of data
sets that include encyclopedic facts, drug and protein data, metadata on music,
books and scholarly articles, social network representations, geospatial
information, and many other types of information. The size and diversity of the
Web of Data is a demonstration of the flexibility of the underlying standards
and the overall feasibility of the project as a whole. The purpose of this
article is to provide a review of the technological underpinnings of the Web of
Data as well as some of the hurdles that need to be overcome if the Web of Data
is to emerge as the defacto medium for data representation, distribution, and
ultimately, processing.



We suggest an approach to use memristors (resistors with memory) in
programmable analog circuits. Our idea consists in a circuit design in which
low voltages are applied to memristors during their operation as analog circuit
elements and high voltages are used to program the memristor's states. This
way, as it was demonstrated in recent experiments, the state of memristors does
not essentially change during analog mode operation. As an example of our
approach, we have built several programmable analog circuits demonstrating
memristor-based programming of threshold, gain and frequency.



Traditional recommendation systems make recommendations based solely on the
customer's past purchases, product ratings and demographic data without
considering the profitability the items being recommended. In this work we
study the question of how a vendor can directly incorporate the profitability
of items into its recommender so as to maximize its expected profit while still
providing accurate recommendations. Our approach uses the output of any
traditional recommender system and adjust them according to item
profitabilities. Our approach is parameterized so the vendor can control how
much the recommendation incorporating profits can deviate from the traditional
recommendation. We study our approach under two settings and show that it
achieves approximately 22% more profit than traditional recommendations.



This paper introduces a principled approach for the design of a scalable
general reinforcement learning agent. Our approach is based on a direct
approximation of AIXI, a Bayesian optimality notion for general reinforcement
learning agents. Previously, it has been unclear whether the theory of AIXI
could motivate the design of practical algorithms. We answer this hitherto open
question in the affirmative, by providing the first computationally feasible
approximation to the AIXI agent. To develop our approximation, we introduce a
new Monte-Carlo Tree Search algorithm along with an agent-specific extension to
the Context Tree Weighting algorithm. Empirically, we present a set of
encouraging results on a variety of stochastic and partially observable
domains. We conclude by proposing a number of directions for future research.



A general approach describing quantum decision procedures is developed. The
approach can be applied to quantum information processing, quantum computing,
creation of artificial quantum intelligence, as well as to analyzing decision
processes of human decision makers. Our basic point is to consider an active
quantum system possessing its own strategic state. Processing information by
such a system is analogous to the cognitive processes associated to decision
making by humans. The algebra of probability operators, associated with the
possible options available to the decision maker, plays the role of the algebra
of observables in quantum theory of measurements. A scheme is advanced for a
practical realization of decision procedures by thinking quantum systems. Such
thinking quantum systems can be realized by using spin lattices, systems of
magnetic molecules, cold atoms trapped in optical lattices, ensembles of
quantum dots, or multilevel atomic systems interacting with electromagnetic
field.



Regularized risk minimization with the binary hinge loss and its variants
lies at the heart of many machine learning problems. Bundle methods for
regularized risk minimization (BMRM) and the closely related SVMStruct are
considered the best general purpose solvers to tackle this problem. It was
recently shown that BMRM requires $O(1/\epsilon)$ iterations to converge to an
$\epsilon$ accurate solution. In the first part of the paper we use the
Hadamard matrix to construct a regularized risk minimization problem and show
that these rates cannot be improved. We then show how one can exploit the
structure of the objective function to devise an algorithm for the binary hinge
loss which converges to an $\epsilon$ accurate solution in
$O(1/\sqrt{\epsilon})$ iterations.



This paper describes a new approach to solving some stochastic optimization
problems for linear dynamic system with various parametric uncertainties.
Proposed approach is based on application of tensor formalism for creation the
mathematical model of parametric uncertainties. Within proposed approach
following problems are considered: prediction, data processing and optimal
control. Outcomes of carried out simulation are used as illustration of
properties and effectiveness of proposed methods.



We investigate a population of binary mistake sequences that result from
learning with parametric models of different order. We obtain estimates of
their error, algorithmic complexity and divergence from a purely random
Bernoulli sequence. We study the relationship of these variables to the
learner's information density parameter which is defined as the ratio between
the lengths of the compressed to uncompressed files that contain the learner's
decision rule. The results indicate that good learners have a low information
density$\rho$ while bad learners have a high $\rho$. Bad learners generate
mistake sequences that are atypically complex or diverge stochastically from a
purely random Bernoulli sequence. Good learners generate typically complex
sequences with low divergence from Bernoulli sequences and they include mistake
sequences generated by the Bayes optimal predictor. Based on the static
algorithmic interference model of \cite{Ratsaby_entropy} the learner here acts
as a static structure which "scatters" the bits of an input sequence (to be
predicted) in proportion to its information density $\rho$ thereby deforming
its randomness characteristics.



We consider multi-agent systems where agents' preferences are aggregated via
sequential majority voting: each decision is taken by performing a sequence of
pairwise comparisons where each comparison is a weighted majority vote among
the agents. Incompleteness in the agents' preferences is common in many
real-life settings due to privacy issues or an ongoing elicitation process. In
addition, there may be uncertainty about how the preferences are aggregated.
For example, the agenda (a tree whose leaves are labelled with the decisions
being compared) may not yet be known or fixed. We therefore study how to
determine collectively optimal decisions (also called winners) when preferences
may be incomplete, and when the agenda may be uncertain. We show that it is
computationally easy to determine if a candidate decision always wins, or may
win, whatever the agenda. On the other hand, it is computationally hard to know
wheth er a candidate decision wins in at least one agenda for at least one
completion of the agents' preferences. These results hold even if the agenda
must be balanced so that each candidate decision faces the same number of
majority votes. Such results are useful for reasoning about preference
elicitation. They help understand the complexity of tasks such as determining
if a decision can be taken collectively, as well as knowing if the winner can
be manipulated by appropriately ordering the agenda.



This paper proposes an intrusion detection and prediction system based on
uncertain and imprecise inference networks and its implementation. Giving a
historic of sessions, it is about proposing a method of supervised learning
doubled of a classifier permitting to extract the necessary knowledge in order
to identify the presence or not of an intrusion in a session and in the
positive case to recognize its type and to predict the possible intrusions that
will follow it. The proposed system takes into account the uncertainty and
imprecision that can affect the statistical data of the historic. The
systematic utilization of an unique probability distribution to represent this
type of knowledge supposes a too rich subjective information and risk to be in
part arbitrary. One of the first objectives of this work was therefore to
permit the consistency between the manner of which we represent information and
information which we really dispose.



The universal-algebraic approach has proved a powerful tool in the study of
the complexity of CSPs. This approach has previously been applied to the study
of CSPs with finite or (infinite) omega-categorical templates, and relies on
two facts. The first is that in finite or omega-categorical structures A, a
relation is primitive positive definable if and only if it is preserved by the
polymorphisms of A. The second is that every finite or omega-categorical
structure is homomorphically equivalent to a core structure. In this paper, we
present generalizations of these facts to infinite structures that are not
necessarily omega-categorical. (This abstract has been severely curtailed by
the space constraints of arXiv -- please read the full abstract in the
article.) Finally, we present applications of our general results to the
description and analysis of the complexity of CSPs. In particular, we give
general hardness criteria based on the absence of polymorphisms that depend on
more than one argument, and we present a polymorphism-based description of
those CSPs that are first-order definable (and therefore can be solved in
polynomial time).



This short note reviews briefly three algorithms for finding the set of
dispensable variables of a boolean formula. The presentation is light on proofs
and heavy on intuitions.



Machine learning, data mining and artificial intelligence (AI) based methods
have been used to determine the relations between chemical structure and
biological activity, called quantitative structure activity relationships
(QSARs) for the compounds. Pre-processing of the dataset, which includes the
mapping from a large number of molecular descriptors in the original high
dimensional space to a small number of components in the lower dimensional
space while retaining the features of the original data, is the first step in
this process. A common practice is to use a mapping method for a dataset
without prior analysis. This pre-analysis has been stressed in our work by
applying it to two important classes of QSAR prediction problems: drug design
(predicting anti-HIV-1 activity) and predictive toxicology (estimating
hepatocarcinogenicity of chemicals). We apply one linear and two nonlinear
mapping methods on each of the datasets. Based on this analysis, we conclude
the nature of the inherent relationships between the elements of each dataset,
and hence, the mapping method best suited for it. We also show that proper
preprocessing can help us in choosing the right feature extraction tool as well
as give an insight about the type of classifier pertinent for the given
problem.



This work presents a novel learning method in the context of embodied
artificial intelligence and self-organization, which has as few assumptions and
restrictions as possible about the world and the underlying model. The learning
rule is derived from the principle of maximizing the predictive information in
the sensorimotor loop. It is evaluated on robot chains of varying length with
individually controlled, non-communicating segments. The comparison of the
results shows that maximizing the predictive information per wheel leads to a
higher coordinated behavior of the physically connected robots compared to a
maximization per robot. Another focus of this paper is the analysis of the
effect of the robot chain length on the overall behavior of the robots. It will
be shown that longer chains with less capable controllers outperform those of
shorter length and more complex controllers. The reason is found and discussed
in the information-geometric interpretation of the learning process.



The quest for robust heuristics that are able to solve more than one problem
is ongoing. In this paper, we present, discuss and analyse a technique called
Evolutionary Squeaky Wheel Optimisation and apply it to two different personnel
scheduling problems. Evolutionary Squeaky Wheel Optimisation improves the
original Squeaky Wheel Optimisation's effectiveness and execution speed by
incorporating two extra steps (Selection and Mutation) for added evolution. In
the Evolutionary Squeaky Wheel Optimisation, a cycle of
Analysis-Selection-Mutation-Prioritization-Construction continues until
stopping conditions are reached. The aim of the Analysis step is to identify
below average solution components by calculating a fitness value for all
components. The Selection step then chooses amongst these underperformers and
discards some probabilistically based on fitness. The Mutation step further
discards a few components at random. Solutions can become incomplete and thus
repairs may be required. The repairs are carried out by using the
Prioritization to first produce priorities that determine an order by which the
following Construction step then schedules the remaining components. Therefore,
improvement in the Evolutionary Squeaky Wheel Optimisation is achieved by
selective solution disruption mixed with interative improvement and
constructive repair. Strong experimental results are reported on two different
domains of personnel scheduling: bus and rail driver scheduling and hospital
nurse scheduling.



A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to
solving mobile robot navigation problems is presented and tested in both real
and simulated environments. The LTL consists of rapid simulations that use a
Genetic Algorithm to derive diverse sets of behaviours. These sets are then
transferred to an idiotypic Artificial Immune System (AIS), which forms the STL
phase, and the system is said to be seeded. The combined LTL-STL approach is
compared with using STL only, and with using a handdesigned controller. In
addition, the STL phase is tested when the idiotypic mechanism is turned off.
The results provide substantial evidence that the best option is the seeded
idiotypic system, i.e. the architecture that merges LTL with an idiotypic AIS
for the STL. They also show that structurally different environments can be
used for the two phases without compromising transferability



The immune system provides a rich metaphor for computer security: anomaly
detection that works in nature should work for machines. However, early
artificial immune system approaches for computer security had only limited
success. Arguably, this was due to these artificial systems being based on too
simplistic a view of the immune system. We present here a second generation
artificial immune system for process anomaly detection. It improves on earlier
systems by having different artificial cell types that process information.
Following detailed information about how to build such second generation
systems, we find that communication between cells types is key to performance.
Through realistic testing and validation we show that second generation
artificial immune systems are capable of anomaly detection beyond generic
system policies. The paper concludes with a discussion and outline of the next
steps in this exciting area of computer security.



Network Intrusion Detection Systems (NIDS) are computer systems which monitor
a network with the aim of discerning malicious from benign activity on that
network. While a wide range of approaches have met varying levels of success,
most IDSs rely on having access to a database of known attack signatures which
are written by security experts. Nowadays, in order to solve problems with
false positive alerts, correlation algorithms are used to add additional
structure to sequences of IDS alerts. However, such techniques are of no help
in discovering novel attacks or variations of known attacks, something the
human immune system (HIS) is capable of doing in its own specialised domain.
This paper presents a novel immune algorithm for application to the IDS
problem. The goal is to discover packets containing novel variations of attacks
covered by an existing signature base.



Medical Informatics and the application of modern signal processing in the
assistance of the diagnostic process in medical imaging is one of the more
recent and active research areas today. This thesis addresses a variety of
issues related to the general problem of medical image analysis, specifically
in mammography, and presents a series of algorithms and design approaches for
all the intermediate levels of a modern system for computer-aided diagnosis
(CAD). The diagnostic problem is analyzed with a systematic approach, first
defining the imaging characteristics and features that are relevant to probable
pathology in mammo-grams. Next, these features are quantified and fused into
new, integrated radio-logical systems that exhibit embedded digital signal
processing, in order to improve the final result and minimize the radiological
dose for the patient. In a higher level, special algorithms are designed for
detecting and encoding these clinically interest-ing imaging features, in order
to be used as input to advanced pattern classifiers and machine learning
models. Finally, these approaches are extended in multi-classifier models under
the scope of Game Theory and optimum collective deci-sion, in order to produce
efficient solutions for combining classifiers with minimum computational costs
for advanced diagnostic systems. The material covered in this thesis is related
to a total of 18 published papers, 6 in scientific journals and 12 in
international conferences.



When configuring customizable software, it is useful to provide interactive
tool-support that ensures that the configuration does not breach given
constraints.
  But, when is a configuration complete and how can the tool help the user to
complete it?
  We formalize this problem and relate it to concepts from non-monotonic
reasoning well researched in Artificial Intelligence. The results are
interesting for both practitioners and theoreticians. Practitioners will find a
technique facilitating an interactive configuration process and experiments
supporting feasibility of the approach. Theoreticians will find links between
well-known formal concepts and a concrete practical application.



In sports competitions, teams can manipulate the result by, for instance,
throwing games. We show that we can decide how to manipulate round robin and
cup competitions, two of the most popular types of sporting competitions in
polynomial time. In addition, we show that finding the minimal number of games
that need to be thrown to manipulate the result can also be determined in
polynomial time. Finally, we show that there are several different variations
of standard cup competitions where manipulation remains polynomial.



There are both benefits and drawbacks to creativity. In a social group it is
not necessary for all members to be creative to benefit from creativity; some
merely imitate or enjoy the fruits of others' creative efforts. What proportion
should be creative? This paper contains a very preliminary investigation of
this question carried out using a computer model of cultural evolution referred
to as EVOC (for EVOlution of Culture). EVOC is composed of neural network based
agents that evolve fitter ideas for actions by (1) inventing new ideas through
modification of existing ones, and (2) imitating neighbors' ideas. The ideal
proportion with respect to fitness of ideas occurs when thirty to forty percent
of the individuals is creative. When creators are inventing 50% of iterations
or less, mean fitness of actions in the society is a positive function of the
ratio of creators to imitators; otherwise mean fitness of actions starts to
drop when the ratio of creators to imitators exceeds approximately 30%. For all
levels or creativity, the diversity of ideas in a population is positively
correlated with the ratio of creative agents.



It is noted that some unusual moves against a strong chess program greatly
weaken its ability to see the serious targets of the game, and its whole level
of play... It is suggested to create programs with different weaknesses in
order to analyze similar human behavior. Finally, a new version of chess,
"Chess Corrida" is suggested.



For many voting rules, it is NP-hard to compute a successful manipulation.
However, NP-hardness only bounds the worst-case complexity. Recent theoretical
results suggest that manipulation may often be easy in practice. We study
empirically the cost of manipulating the single transferable vote (STV) rule.
This was one of the first rules shown to be NP-hard to manipulate. It also
appears to be one of the harder rules to manipulate since it involves multiple
rounds and since, unlike many other rules, it is NP-hard for a single agent to
manipulate without weights on the votes or uncertainty about how the other
agents have voted. In almost every election in our experiments, it was easy to
compute how a single agent could manipulate the election or to prove that
manipulation by a single agent was impossible. It remains an interesting open
question if manipulation by a coalition of agents is hard to compute in
practice.



Sets of desirable gambles constitute a quite general type of uncertainty
model with an interesting geometrical interpretation. We give a general
discussion of such models and their rationality criteria. We study
exchangeability assessments for them, and prove counterparts of de Finetti's
finite and infinite representation theorems. We show that the finite
representation in terms of count vectors has a very nice geometrical
interpretation, and that the representation in terms of frequency vectors is
tied up with multivariate Bernstein (basis) polynomials. We also lay bare the
relationships between the representations of updated exchangeable models, and
discuss conservative inference (natural extension) under exchangeability and
the extension of exchangeable sequences.



Rewards typically express desirabilities or preferences over a set of
alternatives. Here we propose that rewards can be defined for any probability
distribution based on three desiderata, namely that rewards should be
real-valued, additive and order-preserving, where the latter implies that more
probable events should also be more desirable. Our main result states that
rewards are then uniquely determined by the negative information content. To
analyze stochastic processes, we define the utility of a realization as its
reward rate. Under this interpretation, we show that the expected utility of a
stochastic process is its negative entropy rate. Furthermore, we apply our
results to analyze agent-environment interactions. We show that the expected
utility that will actually be achieved by the agent is given by the negative
cross-entropy from the input-output (I/O) distribution of the coupled
interaction system and the agent's I/O distribution. Thus, our results allow
for an information-theoretic interpretation of the notion of utility and the
characterization of agent-environment interactions in terms of entropy
dynamics.



Images can be segmented by first using a classifier to predict an affinity
graph that reflects the degree to which image pixels must be grouped together
and then partitioning the graph to yield a segmentation. Machine learning has
been applied to the affinity classifier to produce affinity graphs that are
good in the sense of minimizing edge misclassification rates. However, this
error measure is only indirectly related to the quality of segmentations
produced by ultimately partitioning the affinity graph. We present the first
machine learning algorithm for training a classifier to produce affinity graphs
that are good in the sense of producing segmentations that directly minimize
the Rand index, a well known segmentation performance measure. The Rand index
measures segmentation performance by quantifying the classification of the
connectivity of image pixel pairs after segmentation. By using the simple graph
partitioning algorithm of finding the connected components of the thresholded
affinity graph, we are able to train an affinity classifier to directly
minimize the Rand index of segmentations resulting from the graph partitioning.
Our learning algorithm corresponds to the learning of maximin affinities
between image pixel pairs, which are predictive of the pixel-pair connectivity.



There is currently an intersection in the research of game theory and
cryptography. Generally speaking, there are two aspects to this partnership.
First there is the application of game theory to cryptography. Yet, the purpose
of this paper is to focus on the second aspect, the converse of the first, the
application of cryptography to game theory. Chiefly, there exist a branch of
non-cooperative games which have a correlated equilibrium as their solution.
These equilibria tend to be superior to the conventional Nash equilibria. The
primary condition for a correlated equilibrium is the presence of a mediator
within the game. This is simply a neutral and mutually trusted entity. It is
the role of the mediator to make recommendations in terms of strategy profiles
to all players, who then act (supposedly) on this advice. Each party privately
provides the mediator with the necessary information, and the referee responds
privately with their optimized strategy set. However, there seem to be a
multitude of situations in which no mediator could exist. Thus, games modeling
these sorts of cases could not use these entities as tools for analysis. Yet,
if these equilibria are in the best interest of players, it would be rational
to construct a machine, or protocol, to calculate them. Of course, this machine
would need to satisfy some standard for secure transmission between a player
and itself. The requirement that no third party could detect either the input
or strategy profile would need to be satisfied by this scheme. Here is the
synthesis of cryptography into game theory; analyzing the ability of the
players to construct a protocol which can be used successfully in the place of
a mediator.



This paper describes the approach taken to the XML Mining track at INEX 2008
by a group at the Queensland University of Technology. We introduce the K-tree
clustering algorithm in an Information Retrieval context by adapting it for
document clustering. Many large scale problems exist in document clustering.
K-tree scales well with large inputs due to its low complexity. It offers
promising results both in terms of efficiency and quality. Document
classification was completed using Support Vector Machines.



We introduce K-tree in an information retrieval context. It is an efficient
approximation of the k-means clustering algorithm. Unlike k-means it forms a
hierarchy of clusters. It has been extended to address issues with sparse
representations. We compare performance and quality to CLUTO using document
collections. The K-tree has a low time complexity that is suitable for large
document collections. This tree structure allows for efficient disk based
implementations where space requirements exceed that of main memory.



Random Indexing (RI) K-tree is the combination of two algorithms for
clustering. Many large scale problems exist in document clustering. RI K-tree
scales well with large inputs due to its low complexity. It also exhibits
features that are useful for managing a changing collection. Furthermore, it
solves previous issues with sparse document vectors when using K-tree. The
algorithms and data structures are defined, explained and motivated. Specific
modifications to K-tree are made for use with RI. Experiments have been
executed to measure quality. The results indicate that RI K-tree improves
document cluster quality over the original K-tree algorithm.



This empirical study is mainly devoted to comparing four tree-based boosting
algorithms: mart, abc-mart, robust logitboost, and abc-logitboost, for
multi-class classification on a variety of publicly available datasets. Some of
those datasets have been thoroughly tested in prior studies using a broad range
of classification algorithms including SVM, neural nets, and deep learning.
  In terms of the empirical classification errors, our experiment results
demonstrate:
  1. Abc-mart considerably improves mart. 2. Abc-logitboost considerably
improves (robust) logitboost. 3. Robust) logitboost} considerably improves mart
on most datasets. 4. Abc-logitboost considerably improves abc-mart on most
datasets. 5. These four boosting algorithms (especially abc-logitboost)
outperform SVM on many datasets. 6. Compared to the best deep learning methods,
these four boosting algorithms (especially abc-logitboost) are competitive.



A perceived limitation of evolutionary art and design algorithms is that they
rely on human intervention; the artist selects the most aesthetically pleasing
variants of one generation to produce the next. This paper discusses how
computer generated art and design can become more creatively human-like with
respect to both process and outcome. As an example of a step in this direction,
we present an algorithm that overcomes the above limitation by employing an
automatic fitness function. The goal is to evolve abstract portraits of Darwin,
using our 2nd generation fitness function which rewards genomes that not just
produce a likeness of Darwin but exhibit certain strategies characteristic of
human artists. We note that in human creativity, change is less choosing
amongst randomly generated variants and more capitalizing on the associative
structure of a conceptual network to hone in on a vision. We discuss how to
achieve this fluidity algorithmically.



There are at least two ways to interpret numerical degrees of belief in terms
of betting: (1) you can offer to bet at the odds defined by the degrees of
belief, or (2) you can judge that a strategy for taking advantage of such
betting offers will not multiply the capital it risks by a large factor. Both
interpretations can be applied to ordinary additive probabilities and used to
justify updating by conditioning. Only the second can be applied to
Dempster-Shafer degrees of belief and used to justify Dempster's rule of
combination.



We propose a variation of the standard genetic algorithm that incorporates
social interaction between the individuals in the population. Our goal is to
understand the evolutionary role of social systems and its possible application
as a non-genetic new step in evolutionary algorithms. In biological
populations, ie animals, even human beings and microorganisms, social
interactions often affect the fitness of individuals. It is conceivable that
the perturbation of the fitness via social interactions is an evolutionary
strategy to avoid trapping into local optimum, thus avoiding a fast convergence
of the population. We model the social interactions according to Game Theory.
The population is, therefore, composed by cooperator and defector individuals
whose interactions produce payoffs according to well known game models
(prisoner's dilemma, chicken game, and others). Our results on Knapsack
problems show, for some game models, a significant performance improvement as
compared to a standard genetic algorithm.



The role of T-cells within the immune system is to confirm and assess
anomalous situations and then either respond to or tolerate the source of the
effect. To illustrate how these mechanisms can be harnessed to solve real-world
problems, we present the blueprint of a T-cell inspired algorithm for computer
security worm detection. We show how the three central T-cell processes, namely
T-cell maturation, differentiation and proliferation, naturally map into this
domain and further illustrate how such an algorithm fits into a complete immune
inspired computer security system and framework.



Ensuring the security of computers is a non-trivial task, with many
techniques used by malicious users to compromise these systems. In recent years
a new threat has emerged in the form of networks of hijacked zombie machines
used to perform complex distributed attacks such as denial of service and to
obtain sensitive data such as password information. These zombie machines are
said to be infected with a 'bot' - a malicious piece of software which is
installed on a host machine and is controlled by a remote attacker, termed the
'botmaster of a botnet'. In this work, we use the biologically inspired
Dendritic Cell Algorithm (DCA) to detect the existence of a single bot on a
compromised host machine. The DCA is an immune-inspired algorithm based on an
abstract model of the behaviour of the dendritic cells of the human body. The
basis of anomaly detection performed by the DCA is facilitated using the
correlation of behavioural attributes such as keylogging and packet flooding
behaviour. The results of the application of the DCA to the detection of a
single bot show that the algorithm is a successful technique for the detection
of such malicious software without responding to normally running programs.



We propose and evaluate an immuno-inspired approach to misbehavior detection
in ad hoc wireless networks. Node misbehavior can be the result of an
intrusion, or a software or hardware failure. Our approach is motivated by
co-stimulatory signals present in the Biological immune system. The results
show that co-stimulation in ad hoc wireless networks can both substantially
improve energy efficiency of detection and, at the same time, help achieve low
false positives rates. The energy efficiency improvement is almost two orders
of magnitude, if compared to misbehavior detection based on watchdogs.
  We provide a characterization of the trade-offs between detection approaches
executed by a single node and by several nodes in cooperation. Additionally, we
investigate several feature sets for misbehavior detection. These feature sets
impose different requirements on the detection system, most notably from the
energy efficiency point of view.



When users rate objects, a sophisticated algorithm that takes into account
ability or reputation may produce a fairer or more accurate aggregation of
ratings than the straightforward arithmetic average. Recently a number of
authors have proposed different co-determination algorithms where estimates of
user and object reputation are refined iteratively together, permitting
accurate measures of both to be derived directly from the rating data. However,
simulations demonstrating these methods' efficacy assumed a continuum of rating
values, consistent with typical physical modelling practice, whereas in most
actual rating systems only a limited range of discrete values (such as a 5-star
system) is employed. We perform a comparative test of several co-determination
algorithms with different scales of discrete ratings and show that this
seemingly minor modification in fact has a significant impact on algorithms'
performance. Paradoxically, where rating resolution is low, increased noise in
users' ratings may even improve the overall performance of the system.



We propose a formal framework that supports a model of agent-based Virtual
Organisations (VOs) for service grids and provides an associated operational
model for the creation of VOs. The framework is intended to be used for
describing different service grid applications based on multiple agents and, as
a result, it abstracts away from any realisation choices of the service grid
application, the agents involved to support the applications and their
interactions. Within the proposed framework VOs are seen as emerging from
societies of agents, where agents are abstractly characterised by goals and
roles they can play within VOs. In turn, VOs are abstractly characterised by
the agents participating in them with specific roles, as well as the workflow
of services and corresponding contracts suitable for achieving the goals of the
participating agents. We illustrate the proposed framework with an earth
observation scenario.



This paper presents the Computing Networks (CNs) framework. CNs are used to
generalize neural and swarm architectures. Artificial neural networks, ant
colony optimization, particle swarm optimization, and realistic biological
models are used as examples of instantiations of CNs. The description of these
architectures as CNs allows their comparison. Their differences and
similarities allow the identification of properties that enable neural and
swarm architectures to perform complex computations and exhibit complex
cognitive abilities. In this context, the most relevant characteristics of CNs
are the existence multiple dynamical and functional scales. The relationship
between multiple dynamical and functional scales with adaptation, cognition (of
brains and swarms) and computation is discussed.



Artificial immune systems have previously been applied to the problem of
intrusion detection. The aim of this research is to develop an intrusion
detection system based on the function of Dendritic Cells (DCs). DCs are
antigen presenting cells and key to activation of the human immune system,
behaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA).
In algorithmic terms, individual DCs perform multi-sensor data fusion,
asynchronously correlating the the fused data signals with a secondary data
stream. Aggregate output of a population of cells, is analysed and forms the
basis of an anomaly detection system. In this paper the DCA is applied to the
detection of outgoing port scans using TCP SYN packets. Results show that
detection can be achieved with the DCA, yet some false positives can be
encountered when simultaneously scanning and using other network services.
Suggestions are made for using adaptive signals to alleviate this uncovered
problem.



Auctions play an important role in electronic commerce, and have been used to
solve problems in distributed computing. Automated approaches to designing
effective auction mechanisms are helpful in reducing the burden of traditional
game theoretic, analytic approaches and in searching through the large space of
possible auction mechanisms. This paper presents an approach to automated
mechanism design (AMD) in the domain of double auctions. We describe a novel
parametrized space of double auctions, and then introduce an evolutionary
search method that searches this space of parameters. The approach evaluates
auction mechanisms using the framework of the TAC Market Design Game and
relates the performance of the markets in that game to their constituent parts
using reinforcement learning. Experiments show that the strongest mechanisms we
found using this approach not only win the Market Design Game against known,
strong opponents, but also exhibit desirable economic properties when they run
in isolation.



The search for patterns or motifs in data represents an area of key interest
to many researchers. In this paper we present the Motif Tracking Algorithm, a
novel immune inspired pattern identification tool that is able to identify
unknown motifs which repeat within time series data. The power of the algorithm
is derived from its use of a small number of parameters with minimal
assumptions. The algorithm searches from a completely neutral perspective that
is independent of the data being analysed, and the underlying motifs. In this
paper the motif tracking algorithm is applied to the search for patterns within
sequences of low level system calls between the Linux kernel and the operating
system's user space. The MTA is able to compress data found in large system
call data sets to a limited number of motifs which summarise that data. The
motifs provide a resource from which a profile of executed processes can be
built. The potential for these profiles and new implications for security
research are highlighted. A higher level call system language for measuring
similarity between patterns of such calls is also suggested.



In recent years computer systems have become increasingly complex and
consequently the challenge of protecting these systems has become increasingly
difficult. Various techniques have been implemented to counteract the misuse of
computer systems in the form of firewalls, anti-virus software and intrusion
detection systems. The complexity of networks and dynamic nature of computer
systems leaves current methods with significant room for improvement. Computer
scientists have recently drawn inspiration from mechanisms found in biological
systems and, in the context of computer security, have focused on the human
immune system (HIS). The human immune system provides a high level of
protection from constant attacks. By examining the precise mechanisms of the
human immune system, it is hoped the paradigm will improve the performance of
real intrusion detection systems. This paper presents an introduction to recent
developments in the field of immunology. It discusses the incorporation of a
novel immunological paradigm, Danger Theory, and how this concept is inspiring
artificial immune systems (AIS). Applications within the context of computer
security are outlined drawing direct reference to the underlying principles of
Danger Theory and finally, the current state of intrusion detection systems is
discussed and improvements suggested.



A bot is a piece of software that is usually installed on an infected machine
without the user's knowledge. A bot is controlled remotely by the attacker
under a Command and Control structure. Recent statistics show that bots
represent one of the fastest growing threats to our network by performing
malicious activities such as email spamming or keylogging. However, few bot
detection techniques have been developed to date. In this paper, we investigate
a behavioural algorithm to detect a single bot that uses keylogging activity.
Our approach involves the use of function calls analysis for the detection of
the bot with a keylogging component. Correlation of the frequency of a
specified time-window is performed to enhance he detection scheme. We perform a
range of experiments with the spybot. Our results show that there is a high
correlation between some function calls executed by this bot which indicates
abnormal activity in our system.



Adaptive control problems are notoriously difficult to solve even in the
presence of plant-specific controllers. One way to by-pass the intractable
computation of the optimal policy is to restate the adaptive control as the
minimization of the relative entropy of a controller that ignores the true
plant dynamics from an informed controller. The solution is given by the
Bayesian control rule-a set of equations characterizing a stochastic adaptive
controller for the class of possible plant dynamics. Here, the Bayesian control
rule is applied to derive BCR-MDP, a controller to solve undiscounted Markov
decision processes with finite state and action spaces and unknown dynamics. In
particular, we derive a non-parametric conjugate prior distribution over the
policy space that encapsulates the agent's whole relevant history and we
present a Gibbs sampler to draw random policies from this distribution.
Preliminary results show that BCR-MDP successfully avoids sub-optimal limit
cycles due to its built-in mechanism to balance exploration versus
exploitation.



We extend the Chow-Liu algorithm for general random variables while the
previous versions only considered finite cases. In particular, this paper
applies the generalization to Suzuki's learning algorithm that generates from
data forests rather than trees based on the minimum description length by
balancing the fitness of the data to the forest and the simplicity of the
forest. As a result, we successfully obtain an algorithm when both of the
Gaussian and finite random variables are present.



The max-product algorithm, a local message-passing scheme that attempts to
compute the most probable assignment (MAP) of a given probability distribution,
has been successfully employed as a method of approximate inference for
applications arising in coding theory, computer vision, and machine learning.
However, the max-product algorithm is not guaranteed to converge to the MAP
assignment, and if it does, is not guaranteed to recover the MAP assignment.
  Alternative convergent message-passing schemes have been proposed to overcome
these difficulties. This work provides a systematic study of such
message-passing algorithms that extends the known results by exhibiting new
sufficient conditions for convergence to local and/or global optima, providing
a combinatorial characterization of these optima based on graph covers, and
describing a new convergent and correct message-passing algorithm whose
derivation unifies many of the known convergent message-passing algorithms.
  While convergent and correct message-passing algorithms represent a step
forward in the analysis of max-product style message-passing algorithms, the
conditions needed to guarantee convergence to a global optimum can be too
restrictive in both theory and practice. This limitation of convergent and
correct message-passing schemes is characterized by graph covers and
illustrated by example.



We propose a new approach to the analysis of Loopy Belief Propagation (LBP)
by establishing a formula that connects the Hessian of the Bethe free energy
with the edge zeta function. The formula has a number of theoretical
implications on LBP. It is applied to give a sufficient condition that the
Hessian of the Bethe free energy is positive definite, which shows
non-convexity for graphs with multiple cycles. The formula clarifies the
relation between the local stability of a fixed point of LBP and local minima
of the Bethe free energy. We also propose a new approach to the uniqueness of
LBP fixed point, and show various conditions of uniqueness.



Aim of this paper is to address the problem of learning Boolean functions
from training data with missing values. We present an extension of the BRAIN
algorithm, called U-BRAIN (Uncertainty-managing Batch Relevance-based
Artificial INtelligence), conceived for learning DNF Boolean formulas from
partial truth tables, possibly with uncertain values or missing bits.
  Such an algorithm is obtained from BRAIN by introducing fuzzy sets in order
to manage uncertainty. In the case where no missing bits are present, the
algorithm reduces to the original BRAIN.



We proposed a learning algorithm for nonparametric estimation and on-line
prediction for general stationary ergodic sources. We prepare histograms each
of which estimates the probability as a finite distribution, and mixture them
with weights to construct an estimator. The whole analysis is based on measure
theory. The estimator works whether the source is discrete or continuous. If it
is stationary ergodic, then the measure theoretically given Kullback-Leibler
information divided by the sequence length $n$ converges to zero as $n$ goes to
infinity. In particular, for continuous sources, the method does not require
existence of a probability density function.



The syntactic topic model (STM) is a Bayesian nonparametric model of language
that discovers latent distributions of words (topics) that are both
semantically and syntactically coherent. The STM models dependency parsed
corpora where sentences are grouped into documents. It assumes that each word
is drawn from a latent topic chosen by combining document-level features and
the local syntactic context. Each document has a distribution over latent
topics, as in topic models, which provides the semantic consistency. Each
element in the dependency parse tree also has a distribution over the topics of
its children, as in latent-state syntax models, which provides the syntactic
consistency. These distributions are convolved so that the topic of each word
is likely under both its document and syntactic context. We derive a fast
posterior inference algorithm based on variational methods. We report
qualitative and quantitative studies on both synthetic data and hand-parsed
documents. We show that the STM is a more predictive model of language than
current models based only on syntax or only on topics.



Personalized web services strive to adapt their services (advertisements,
news articles, etc) to individual users by making use of both content and user
information. Despite a few recent advances, this problem remains challenging
for at least two reasons. First, web service is featured with dynamically
changing pools of content, rendering traditional collaborative filtering
methods inapplicable. Second, the scale of most web services of practical
interest calls for solutions that are both fast in learning and computation.
  In this work, we model personalized recommendation of news articles as a
contextual bandit problem, a principled approach in which a learning algorithm
sequentially selects articles to serve users based on contextual information
about the users and articles, while simultaneously adapting its
article-selection strategy based on user-click feedback to maximize total user
clicks.
  The contributions of this work are three-fold. First, we propose a new,
general contextual bandit algorithm that is computationally efficient and well
motivated from learning theory. Second, we argue that any bandit algorithm can
be reliably evaluated offline using previously recorded random traffic.
Finally, using this offline evaluation method, we successfully applied our new
algorithm to a Yahoo! Front Page Today Module dataset containing over 33
million events. Results showed a 12.5% click lift compared to a standard
context-free bandit algorithm, and the advantage becomes even greater when data
gets more scarce.



As an immune-inspired algorithm, the Dendritic Cell Algorithm (DCA), produces
promising performances in the field of anomaly detection. This paper presents
the application of the DCA to a standard data set, the KDD 99 data set. The
results of different implementation versions of the DXA, including the antigen
multiplier and moving time windows are reported. The real-valued Negative
Selection Algorithm (NSA) using constant-sized detectors and the C4.5 decision
tree algorithm are used, to conduct a baseline comparison. The results suggest
that the DCA is applicable to KDD 99 data set, and the antigen multiplier and
moving time windows have the same effect on the DCA for this particular data
set. The real-valued NSA with constant-sized detectors is not applicable to the
data set, and the C4.5 decision tree algorithm provides a benchmark of the
classification performance for this data set.



Dendritic cells are antigen presenting cells that provide a vital link
between the innate and adaptive immune system, providing the initial detection
of pathogenic invaders. Research into this family of cells has revealed that
they perform information fusion which directs immune responses. We have derived
a Dendritic Cell Algorithm based on the functionality of these cells, by
modelling the biological signals and differentiation pathways to build a
control mechanism for an artificial immune system. We present algorithmic
details in addition to experimental results, when the algorithm was applied to
anomaly detection for the detection of port scans. The results show the
Dendritic Cell Algorithm is sucessful at detecting port scans.



Network Intrusion Detection Systems (NDIS) monitor a network with the aim of
discerning malicious from benign activity on that network. While a wide range
of approaches have met varying levels of success, most IDS's rely on having
access to a database of known attack signatures which are written by security
experts. Nowadays, in order to solve problems with false positive alters,
correlation algorithms are used to add additional structure to sequences of IDS
alerts. However, such techniques are of no help in discovering novel attacks or
variations of known attacks, something the human immune system (HIS) is capable
of doing in its own specialised domain. This paper presents a novel immune
algorithm for application to an intrusion detection problem. The goal is to
discover packets containing novel variations of attacks covered by an existing
signature base.



In Newcomb's paradox you choose to receive either the contents of a
particular closed box, or the contents of both that closed box and another one.
Before you choose, a prediction algorithm deduces your choice, and fills the
two boxes based on that deduction. Newcomb's paradox is that game theory
appears to provide two conflicting recommendations for what choice you should
make in this scenario. We analyze Newcomb's paradox using a recent extension of
game theory in which the players set conditional probability distributions in a
Bayes net. We show that the two game theory recommendations in Newcomb's
scenario have different presumptions for what Bayes net relates your choice and
the algorithm's prediction. We resolve the paradox by proving that these two
Bayes nets are incompatible. We also show that the accuracy of the algorithm's
prediction, the focus of much previous work, is irrelevant. In addition we show
that Newcomb's scenario only provides a contradiction between game theory's
expected utility and dominance principles if one is sloppy in specifying the
underlying Bayes net. We also show that Newcomb's paradox is time-reversal
invariant; both the paradox and its resolution are unchanged if the algorithm
makes its `prediction' after you make your choice rather than before.



We study online social networks in which relationships can be either positive
(indicating relations such as friendship) or negative (indicating relations
such as opposition or antagonism). Such a mix of positive and negative links
arise in a variety of online settings; we study datasets from Epinions,
Slashdot and Wikipedia. We find that the signs of links in the underlying
social networks can be predicted with high accuracy, using models that
generalize across this diverse range of sites. These models provide insight
into some of the fundamental principles that drive the formation of signed
links in networks, shedding light on theories of balance and status from social
psychology; they also suggest social computing applications by which the
attitude of one user toward another can be estimated from evidence provided by
their relationships with other members of the surrounding social network.



In this paper we address an issue that has been brought to the attention of
the database community with the advent of the Semantic Web, i.e. the issue of
how ontologies (and semantics conveyed by them) can help solving typical
database problems, through a better understanding of KR aspects related to
databases. In particular, we investigate this issue from the ILP perspective by
considering two database problems, (i) the definition of views and (ii) the
definition of constraints, for a database whose schema is represented also by
means of an ontology. Both can be reformulated as ILP problems and can benefit
from the expressive and deductive power of the KR framework DL+log. We
illustrate the application scenarios by means of examples. Keywords: Inductive
Logic Programming, Relational Databases, Ontologies, Description Logics, Hybrid
Knowledge Representation and Reasoning Systems. Note: To appear in Theory and
Practice of Logic Programming (TPLP).



Multi-agent systems offer a new and exciting way of understanding the world
of work. We apply agent-based modeling and simulation to investigate a set of
problems in a retail context. Specifically, we are working to understand the
relationship between people management practices on the shop-floor and retail
performance. Despite the fact we are working within a relatively novel and
complex domain, it is clear that using an agent-based approach offers great
potential for improving organizational capabilities in the future. Our
multi-disciplinary research team has worked closely with one of the UK's top
ten retailers to collect data and build an understanding of shop-floor
operations and the key actors in a department (customers, staff, and managers).
Based on this case study we have built and tested our first version of a retail
branch agent-based simulation model where we have focused on how we can
simulate the effects of people management practices on customer satisfaction
and sales. In our experiments we have looked at employee development and
cashier empowerment as two examples of shop floor management practices. In this
paper we describe the underlying conceptual ideas and the features of our
simulation model. We present a selection of experiments we have conducted in
order to validate our simulation model and to show its potential for answering
"what-if" questions in a retail context. We also introduce a novel performance
measure which we have created to quantify customers' satisfaction with service,
based on their individual shopping experiences.



Intelligent agents offer a new and exciting way of understanding the world of
work. Agent-Based Simulation (ABS), one way of using intelligent agents,
carries great potential for progressing our understanding of management
practices and how they link to retail performance. We have developed simulation
models based on research by a multi-disciplinary team of economists, work
psychologists and computer scientists. We will discuss our experiences of
implementing these concepts working with a well-known retail department store.
There is no doubt that management practices are linked to the performance of an
organisation (Reynolds et al., 2005; Wall & Wood, 2005). Best practices have
been developed, but when it comes down to the actual application of these
guidelines considerable ambiguity remains regarding their effectiveness within
particular contexts (Siebers et al., forthcoming a). Most Operational Research
(OR) methods can only be used as analysis tools once management practices have
been implemented. Often they are not very useful for giving answers to
speculative 'what-if' questions, particularly when one is interested in the
development of the system over time rather than just the state of the system at
a certain point in time. Simulation can be used to analyse the operation of
dynamic and stochastic systems. ABS is particularly useful when complex
interactions between system entities exist, such as autonomous decision making
or negotiation. In an ABS model the researcher explicitly describes the
decision process of simulated actors at the micro level. Structures emerge at
the macro level as a result of the actions of the agents and their interactions
with other agents and the environment. 3 We will show how ABS experiments can
deal with testing and optimising management practices such as training,
empowerment or teamwork. Hence, questions such as "will staff setting their own
break times improve performance?" can be investigated.



We consider a living organism as an observer of the evolution of its
environment recording sensory information about the state space X of the
environment in real time. Sensory information is sampled and then processed on
two levels. On the biological level, the organism serves as an evaluation
mechanism of the subjective relevance of the incoming data to the observer: the
observer assigns excitation values to events in X it could recognize using its
sensory equipment. On the algorithmic level, sensory input is used for updating
a database, the memory of the observer whose purpose is to serve as a
geometric/combinatorial model of X, whose nodes are weighted by the excitation
values produced by the evaluation mechanism. These values serve as a guidance
system for deciding how the database should transform as observation data
mounts. We define a searching problem for the proposed model and discuss the
model's flexibility and its computational efficiency, as well as the
possibility of implementing it as a dynamic network of neuron-like units. We
show how various easily observable properties of the human memory and thought
process can be explained within the framework of this model. These include:
reasoning (with efficiency bounds), errors, temporary and permanent loss of
information. We are also able to define general learning problems in terms of
the new model, such as the language acquisition problem.



Solving stochastic optimization problems under partial observability, where
one needs to adaptively make decisions with uncertain outcomes, is a
fundamental but notoriously difficult challenge. In this paper, we introduce
the concept of adaptive submodularity, generalizing submodular set functions to
adaptive policies. We prove that if a problem satisfies this property, a simple
adaptive greedy algorithm is guaranteed to be competitive with the optimal
policy. In addition to providing performance guarantees for both stochastic
maximization and coverage, adaptive submodularity can be exploited to
drastically speed up the greedy algorithm by using lazy evaluations. We
illustrate the usefulness of the concept by giving several examples of adaptive
submodular objectives arising in diverse applications including sensor
placement, viral marketing and active learning. Proving adaptive submodularity
for these problems allows us to recover existing results in these applications
as special cases, improve approximation guarantees and handle natural
generalizations.



As an immune inspired algorithm, the Dendritic Cell Algorithm (DCA) has been
applied to a range of problems, particularly in the area of intrusion
detection. Ideally, the intrusion detection should be performed in real-time,
to continuously detect misuses as soon as they occur. Consequently, the
analysis process performed by an intrusion detection system must operate in
real-time or near-to real-time. The analysis process of the DCA is currently
performed offline, therefore to improve the algorithm's performance we suggest
the development of a real-time analysis component. The initial step of the
development is to apply segmentation to the DCA. This involves segmenting the
current output of the DCA into slices and performing the analysis in various
ways. Two segmentation approaches are introduced and tested in this paper,
namely antigen based segmentation (ABS) and time based segmentation (TBS). The
results of the corresponding experiments suggest that applying segmentation
produces different and significantly better results in some cases, when
compared to the standard DCA without segmentation. Therefore, we conclude that
the segmentation is applicable to the DCA for the purpose of real-time
analysis.



In this paper, we investigate output accuracy for a Discrete Event Simulation
(DES) model and Agent Based Simulation (ABS) model. The purpose of this
investigation is to find out which of these simulation techniques is the best
one for modelling human reactive behaviour in the retail sector. In order to
study the output accuracy in both models, we have carried out a validation
experiment in which we compared the results from our simulation models to the
performance of a real system. Our experiment was carried out using a large UK
department store as a case study. We had to determine an efficient
implementation of management policy in the store's fitting room using DES and
ABS. Overall, we have found that both simulation models were a good
representation of the real system when modelling human reactive behaviour.



The analysis of system calls is one method employed by anomaly detection
systems to recognise malicious code execution. Similarities can be drawn
between this process and the behaviour of certain cells belonging to the human
immune system, and can be applied to construct an artificial immune system. A
recently developed hypothesis in immunology, the Danger Theory, states that our
immune system responds to the presence of intruders through sensing molecules
belonging to those invaders, plus signals generated by the host indicating
danger and damage. We propose the incorporation of this concept into a
responsive intrusion detection system, where behavioural information of the
system and running processes is combined with information regarding individual
system calls.



Previous work has shown that robot navigation systems that employ an
architecture based upon the idiotypic network theory of the immune system have
an advantage over control techniques that rely on reinforcement learning only.
This is thought to be a result of intelligent behaviour selection on the part
of the idiotypic robot. In this paper an attempt is made to imitate idiotypic
dynamics by creating controllers that use reinforcement with a number of
different probabilistic schemes to select robot behaviour. The aims are to show
that the idiotypic system is not merely performing some kind of periodic random
behaviour selection, and to try to gain further insight into the processes that
govern the idiotypic mechanism. Trials are carried out using simulated Pioneer
robots that undertake navigation exercises. Results show that a scheme that
boosts the probability of selecting highly-ranked alternative behaviours to 50%
during stall conditions comes closest to achieving the properties of the
idiotypic system, but remains unable to match it in terms of all round
performance.



The efficiency of current cargo screening processes at sea and air ports is
largely unknown as few benchmarks exists against which they could be measured.
Some manufacturers provide benchmarks for individual sensors but we found no
benchmarks that take a holistic view of the overall screening procedures and no
benchmarks that take operator variability into account. Just adding up
resources and manpower used is not an effective way for assessing systems where
human decision-making and operator compliance to rules play a vital role. Our
aim is to develop a decision support tool (cargo-screening system simulator)
that will map the right technology and manpower to the right commodity-threat
combination in order to maximise detection rates. In this paper we present our
ideas for developing such a system and highlight the research challenges we
have identified. Then we introduce our first case study and report on the
progress we have made so far.



Current statistical models for structured prediction make simplifying
assumptions about the underlying output graph structure, such as assuming a
low-order Markov chain, because exact inference becomes intractable as the
tree-width of the underlying graph increases. Approximate inference algorithms,
on the other hand, force one to trade off representational power with
computational efficiency. In this paper, we propose two new types of
probabilistic graphical models, large margin Boltzmann machines (LMBMs) and
large margin sigmoid belief networks (LMSBNs), for structured prediction.
LMSBNs in particular allow a very fast inference algorithm for arbitrary graph
structures that runs in polynomial time with a high probability. This
probability is data-distribution dependent and is maximized in learning. The
new approach overcomes the representation-efficiency trade-off in previous
models and allows fast structured prediction with complicated graph structures.
We present results from applying a fully connected model to multi-label scene
classification and demonstrate that the proposed approach can yield significant
performance gains over current state-of-the-art methods.



Contextual bandit algorithms have become popular for online recommendation
systems such as Digg, Yahoo! Buzz, and news recommendation in general.
\emph{Offline} evaluation of the effectiveness of new algorithms in these
applications is critical for protecting online user experiences but very
challenging due to their "partial-label" nature. Common practice is to create a
simulator which simulates the online environment for the problem at hand and
then run an algorithm against this simulator. However, creating simulator
itself is often difficult and modeling bias is usually unavoidably introduced.
In this paper, we introduce a \emph{replay} methodology for contextual bandit
algorithm evaluation. Different from simulator-based approaches, our method is
completely data-driven and very easy to adapt to different applications. More
importantly, our method can provide provably unbiased evaluations. Our
empirical results on a large-scale news article recommendation dataset
collected from Yahoo! Front Page conform well with our theoretical results.
Furthermore, comparisons between our offline replay and online bucket
evaluation of several contextual bandit algorithms show accuracy and
effectiveness of our offline evaluation method.



In the past few years, IRC bots, malicious programs which are remotely
controlled by the attacker through IRC servers, have become a major threat to
the Internet and users. These bots can be used in different malicious ways such
as issuing distributed denial of services attacks to shutdown other networks
and services, keystrokes logging, spamming, traffic sniffing cause serious
disruption on networks and users. New bots use peer to peer (P2P) protocols
start to appear as the upcoming threat to Internet security due to the fact
that P2P bots do not have a centralized point to shutdown or traceback, thus
making the detection of P2P bots is a real challenge. In response to these
threats, we present an algorithm to detect an individual P2P bot running on a
system by correlating its activities. Our evaluation shows that correlating
different activities generated by P2P bots within a specified time period can
detect these kind of bots.



This thesis investigates the use of problem-specific knowledge to enhance a
genetic algorithm approach to multiple-choice optimisation problems.It shows
that such information can significantly enhance performance, but that the
choice of information and the way it is included are important factors for
success.Two multiple-choice problems are considered.The first is constructing a
feasible nurse roster that considers as many requests as possible.In the second
problem, shops are allocated to locations in a mall subject to constraints and
maximising the overall income.Genetic algorithms are chosen for their
well-known robustness and ability to solve large and complex discrete
optimisation problems.However, a survey of the literature reveals room for
further research into generic ways to include constraints into a genetic
algorithm framework.Hence, the main theme of this work is to balance
feasibility and cost of solutions.In particular, co-operative co-evolution with
hierarchical sub-populations, problem structure exploiting repair schemes and
indirect genetic algorithms with self-adjusting decoder functions are
identified as promising approaches.The research starts by applying standard
genetic algorithms to the problems and explaining the failure of such
approaches due to epistasis.To overcome this, problem-specific information is
added in a variety of ways, some of which are designed to increase the number
of feasible solutions found whilst others are intended to improve the quality
of such solutions.As well as a theoretical discussion as to the underlying
reasons for using each operator,extensive computational experiments are carried
out on a variety of data.These show that the indirect approach relies less on
problem structure and hence is easier to implement and superior in solution
quality.



We mark up a corpus of LaTeX lecture notes semantically and expose them as
Linked Data in XHTML+MathML+RDFa. Our application makes the resulting documents
interactively browsable for students. Our ontology helps to answer queries from
students and lecturers, and paves the path towards an integration of our corpus
with external sites.



Inter-subject parcellation of functional Magnetic Resonance Imaging (fMRI)
data based on a standard General Linear Model (GLM)and spectral clustering was
recently proposed as a means to alleviate the issues associated with spatial
normalization in fMRI. However, for all its appeal, a GLM-based parcellation
approach introduces its own biases, in the form of a priori knowledge about the
shape of Hemodynamic Response Function (HRF) and task-related signal changes,
or about the subject behaviour during the task. In this paper, we introduce a
data-driven version of the spectral clustering parcellation, based on
Independent Component Analysis (ICA) and Partial Least Squares (PLS) instead of
the GLM. First, a number of independent components are automatically selected.
Seed voxels are then obtained from the associated ICA maps and we compute the
PLS latent variables between the fMRI signal of the seed voxels (which covers
regional variations of the HRF) and the principal components of the signal
across all voxels. Finally, we parcellate all subjects data with a spectral
clustering of the PLS latent variables. We present results of the application
of the proposed method on both single-subject and multi-subject fMRI datasets.
Preliminary experimental results, evaluated with intra-parcel variance of GLM
t-values and PLS derived t-values, indicate that this data-driven approach
offers improvement in terms of parcellation accuracy over GLM based techniques.



Crisis response requires information intensive efforts utilized for reducing
uncertainty, calculating and comparing costs and benefits, and managing
resources in a fashion beyond those regularly available to handle routine
problems. This paper presents an Artificial Immune Systems (AIS) metaphor for
agent based modeling of crisis response operations. The presented model
proposes integration of hybrid set of aspects (multi-agent systems, built-in
defensive model of AIS, situation management, and intensity-based learning) for
crisis response operations. In addition, the proposed response model is applied
on the spread of pandemic influenza in Egypt as a case study.



The search for patterns or motifs in data represents an area of key interest
to many researchers. In this paper we present the Motif Tracking Algorithm, a
novel immune inspired pattern identification tool that is able to identify
variable length unknown motifs which repeat within time series data. The
algorithm searches from a completely neutral perspective that is independent of
the data being analysed and the underlying motifs. In this paper we test the
flexibility of the motif tracking algorithm by applying it to the search for
patterns in two industrial data sets. The algorithm is able to identify a
population of motifs successfully in both cases, and the value of these motifs
is discussed.



Malicious users try to compromise systems using new techniques. One of the
recent techniques used by the attacker is to perform complex distributed
attacks such as denial of service and to obtain sensitive data such as password
information. These compromised machines are said to be infected with malicious
software termed a "bot". In this paper, we investigate the correlation of
behavioural attributes such as keylogging and packet flooding behaviour to
detect the existence of a single bot on a compromised machine by applying (1)
Spearman's rank correlation (SRC) algorithm and (2) the Dendritic Cell
Algorithm (DCA). We also compare the output results generated from these two
methods to the detection of a single bot. The results show that the DCA has a
better performance in detecting malicious activities.



Accurate immunological models offer the possibility of performing
highthroughput experiments in silico that can predict, or at least suggest, in
vivo phenomena. In this chapter, we compare various models of immunological
memory. We first validate an experimental immunological simulator, developed by
the authors, by simulating several theories of immunological memory with known
results. We then use the same system to evaluate the predicted effects of a
theory of immunological memory. The resulting model has not been explored
before in artificial immune systems research, and we compare the simulated in
silico output with in vivo measurements. Although the theory appears valid, we
suggest that there are a common set of reasons why immunological memory models
are a useful support tool; not conclusive in themselves.



In this paper we outline initial concepts for an immune inspired algorithm to
evaluate price time series data. The proposed solution evolves a short term
pool of trackers dynamically through a process of proliferation and mutation,
with each member attempting to map to trends in price movements. Successful
trackers feed into a long term memory pool that can generalise across repeating
trend patterns. Tests are performed to examine the algorithm's ability to
successfully identify trends in a small data set. The influence of the long
term memory pool is then examined. We find the algorithm is able to identify
price trends presented successfully and efficiently.



We study the formalization of a collection of documents created for a
Software Engineering project from an MKM perspective. We analyze how document
and collection markup formats can cope with an open-ended, multi-dimensional
space of primary and secondary classifications and relationships. We show that
RDFa-based extensions of MKM formats, employing flexible "metadata"
relationships referencing specific vocabularies for distinct dimensions, are
well-suited to encode this and to put it into service. This formalized
knowledge can be used for enriching interactive document browsing, for enabling
multi-dimensional metadata queries over documents and collections, and for
exporting Linked Data to the Semantic Web and thus enabling further reuse.



The dendritic cell algorithm is an immune-inspired technique for processing
time-dependant data. Here we propose it as a possible solution for a robotic
classification problem. The dendritic cell algorithm is implemented on a real
robot and an investigation is performed into the effects of varying the
migration threshold median for the cell population. The algorithm performs well
on a classification task with very little tuning. Ways of extending the
implementation to allow it to be used as a classifier within the field of
robotic security are suggested.



We introduce a class of neural networks derived from probabilistic models in
the form of Bayesian networks. By imposing additional assumptions about the
nature of the probabilistic models represented in the networks, we derive
neural networks with standard dynamics that require no training to determine
the synaptic weights, that perform accurate calculation of the mean values of
the random variables, that can pool multiple sources of evidence, and that deal
cleanly and consistently with inconsistent or contradictory evidence. The
presented neural networks capture many properties of Bayesian networks,
providing distributed versions of probabilistic models.



One of the objectives of designing feature selection learning algorithms is
to obtain classifiers that depend on a small number of attributes and have
verifiable future performance guarantees. There are few, if any, approaches
that successfully address the two goals simultaneously. Performance guarantees
become crucial for tasks such as microarray data analysis due to very small
sample sizes resulting in limited empirical evaluation. To the best of our
knowledge, such algorithms that give theoretical bounds on the future
performance have not been proposed so far in the context of the classification
of gene expression data. In this work, we investigate the premise of learning a
conjunction (or disjunction) of decision stumps in Occam's Razor, Sample
Compression, and PAC-Bayes learning settings for identifying a small subset of
attributes that can be used to perform reliable classification tasks. We apply
the proposed approaches for gene identification from DNA microarray data and
compare our results to those of well known successful approaches proposed for
the task. We show that our algorithm not only finds hypotheses with much
smaller number of genes while giving competitive classification accuracy but
also have tight risk guarantees on future performance unlike other approaches.
The proposed approaches are general and extensible in terms of both designing
novel algorithms and application to other domains.



We present in this paper an evolution of a tool from a user interface for a
concrete Computer Algebra system for Algebraic Topology (the Kenzo system), to
a front-end allowing the interoperability among different sources for
computation and deduction. The architecture allows the system not only to
interface several systems, but also to make them cooperate in shared
calculations.



ECG Feature Extraction plays a significant role in diagnosing most of the
cardiac diseases. One cardiac cycle in an ECG signal consists of the P-QRS-T
waves. This feature extraction scheme determines the amplitudes and intervals
in the ECG signal for subsequent analysis. The amplitudes and intervals value
of P-QRS-T segment determines the functioning of heart of every human.
Recently, numerous research and techniques have been developed for analyzing
the ECG signal. The proposed schemes were mostly based on Fuzzy Logic Methods,
Artificial Neural Networks (ANN), Genetic Algorithm (GA), Support Vector
Machines (SVM), and other Signal Analysis techniques. All these techniques and
algorithms have their advantages and limitations. This proposed paper discusses
various techniques and transformations proposed earlier in literature for
extracting feature from an ECG signal. In addition this paper also provides a
comparative study of various methods proposed by researchers in extracting the
feature from ECG signal.



We present tropical games, a generalization of combinatorial min-max games
based on tropical algebras. Our model breaks the traditional symmetry of
rational zero-sum games where players have exactly opposed goals (min vs. max),
is more widely applicable than min-max and also supports a form of pruning,
despite it being less effective than alpha-beta. Actually, min-max games may be
seen as particular cases where both the game and its dual are tropical: when
the dual of a tropical game is also tropical, the power of alpha-beta is
completely recovered. We formally develop the model and prove that the tropical
pruning strategy is correct, then conclude by showing how the problem of
approximated parsing can be modeled as a tropical game, profiting from pruning.



Plasmodium of \emph{Physarum polycephalum} is a single cell visible by
unaided eye. The plasmodium's foraging behaviour is interpreted in terms of
computation. Input data is a configuration of nutrients, result of computation
is a network of plasmodium's cytoplasmic tubes spanning sources of nutrients.
Tsuda et al (2004) experimentally demonstrated that basic logical gates can be
implemented in foraging behaviour of the plasmodium. We simplify the original
designs of the gates and show --- in computer models --- that the plasmodium is
capable for computation of two-input two-output gate $<x, y> \to <xy, x+y>$ and
three-input two-output $<x, y, z> \to < \bar{x}yz, x+y+z>$. We assemble the
gates in a binary one-bit adder and demonstrate validity of the design using
computer simulation.



Cross-document coreference, the problem of resolving entity mentions across
multi-document collections, is crucial to automated knowledge base construction
and data mining tasks. However, the scarcity of large labeled data sets has
hindered supervised machine learning research for this task. In this paper we
develop and demonstrate an approach based on ``distantly-labeling'' a data set
from which we can train a discriminative cross-document coreference model. In
particular we build a dataset of more than a million people mentions extracted
from 3.5 years of New York Times articles, leverage Wikipedia for distant
labeling with a generative model (and measure the reliability of such
labeling); then we train and evaluate a conditional random field coreference
model that has factors on cross-document entities as well as mention-pairs.
This coreference model obtains high accuracy in resolving mentions and entities
that are not present in the training data, indicating applicability to
non-Wikipedia data. Given the large amount of data, our work is also an
exercise demonstrating the scalability of our approach.



In logic there is a clear concept of what constitutes a proof and what not. A
proof is essentially defined as a finite sequence of formulae which are either
axioms or derived by proof rules from formulae earlier in the sequence.
Sociologically, however, it is more difficult to say what should constitute a
proof and what not. In this paper we will look at different forms of proofs and
try to clarify the concept of proof in the wider meaning of the term. This has
implications on how proofs should be represented formally.



The contribution of this paper is to provide a semantic model (using soft
constraints) of the words used by web-users to describe objects in a language
game; a game in which one user describes a selected object of those composing
the scene, and another user has to guess which object has been described. The
given description needs to be non ambiguous and accurate enough to allow other
users to guess the described shape correctly.
  To build these semantic models the descriptions need to be analyzed to
extract the syntax and words' classes used. We have modeled the meaning of
these descriptions using soft constraints as a way for grounding the meaning.
  The descriptions generated by the system took into account the context of the
object to avoid ambiguous descriptions, and allowed users to guess the
described object correctly 72% of the times.



Voting is a simple mechanism to combine together the preferences of multiple
agents. Agents may try to manipulate the result of voting by mis-reporting
their preferences. One barrier that might exist to such manipulation is
computational complexity. In particular, it has been shown that it is NP-hard
to compute how to manipulate a number of different voting rules. However,
NP-hardness only bounds the worst-case complexity. Recent theoretical results
suggest that manipulation may often be easy in practice. In this paper, we
study empirically the manipulability of single transferable voting (STV) to
determine if computational complexity is really a barrier to manipulation. STV
was one of the first voting rules shown to be NP-hard. It also appears one of
the harder voting rules to manipulate. We sample a number of distributions of
votes including uniform and real world elections. In almost every election in
our experiments, it was easy to compute how a single agent could manipulate the
election or to prove that manipulation by a single agent was impossible.



Standard hybrid learners that use domain knowledge require stronger knowledge
that is hard and expensive to acquire. However, weaker domain knowledge can
benefit from prior knowledge while being cost effective. Weak knowledge in the
form of feature relative importance (FRI) is presented and explained. Feature
relative importance is a real valued approximation of a feature's importance
provided by experts. Advantage of using this knowledge is demonstrated by IANN,
a modified multilayer neural network algorithm. IANN is a very simple
modification of standard neural network algorithm but attains significant
performance gains. Experimental results in the field of molecular biology show
higher performance over other empirical learning algorithms including standard
backpropagation and support vector machines. IANN performance is even
comparable to a theory refinement system KBANN that uses stronger domain
knowledge. This shows Feature relative importance can improve performance of
existing empirical learning algorithms significantly with minimal effort.



Class prediction is an important application of microarray gene expression
data analysis. The high-dimensionality of microarray data, where number of
genes (variables) is very large compared to the number of samples (obser-
vations), makes the application of many prediction techniques (e.g., logistic
regression, discriminant analysis) difficult. An efficient way to solve this
prob- lem is by using dimension reduction statistical techniques. Increasingly
used in psychology-related applications, Rasch model (RM) provides an appealing
framework for handling high-dimensional microarray data. In this paper, we
study the potential of RM-based modeling in dimensionality reduction with
binarized microarray gene expression data and investigate its prediction ac-
curacy in the context of class prediction using linear discriminant analysis.
Two different publicly available microarray data sets are used to illustrate a
general framework of the approach. Performance of the proposed method is
assessed by re-randomization scheme using principal component analysis (PCA) as
a benchmark method. Our results show that RM-based dimension reduction is as
effective as PCA-based dimension reduction. The method is general and can be
applied to the other high-dimensional data problems.



Representing distributions over permutations can be a daunting task due to
the fact that the number of permutations of $n$ objects scales factorially in
$n$. One recent way that has been used to reduce storage complexity has been to
exploit probabilistic independence, but as we argue, full independence
assumptions impose strong sparsity constraints on distributions and are
unsuitable for modeling rankings. We identify a novel class of independence
structures, called \emph{riffled independence}, encompassing a more expressive
family of distributions while retaining many of the properties necessary for
performing efficient inference and reducing sample complexity. In riffled
independence, one draws two permutations independently, then performs the
\emph{riffle shuffle}, common in card games, to combine the two permutations to
form a single permutation. Within the context of ranking, riffled independence
corresponds to ranking disjoint sets of objects independently, then
interleaving those rankings. In this paper, we provide a formal introduction to
riffled independence and present algorithms for using riffled independence
within Fourier-theoretic frameworks which have been explored by a number of
recent papers. Additionally, we propose an automated method for discovering
sets of items which are riffle independent from a training set of rankings. We
show that our clustering-like algorithms can be used to discover meaningful
latent coalitions from real preference ranking datasets and to learn the
structure of hierarchically decomposable models based on riffled independence.



The Dendritic Cell Algorithm (DCA) is an immune-inspired algorithm, developed
for the purpose of anomaly detection. The algorithm performs multi-sensor data
fusion and correlation which results in a 'context aware' detection system.
Previous applications of the DCA have included the detection of potentially
malicious port scanning activity, where it has produced high rates of true
positives and low rates of false positives. In this work we aim to compare the
performance of the DCA and of a Self-Organizing Map (SOM) when applied to the
detection of SYN port scans, through experimental analysis. A SOM is an ideal
candidate for comparison as it shares similarities with the DCA in terms of the
data fusion method employed. It is shown that the results of the two systems
are comparable, and both produce false positives for the same processes. This
shows that the DCA can produce anomaly detection results to the same standard
as an established technique.



The search for patterns or motifs in data represents a problem area of key
interest to finance and economic researchers. In this paper we introduce the
Motif Tracking Algorithm, a novel immune inspired pattern identification tool
that is able to identify unknown motifs of a non specified length which repeat
within time series data. The power of the algorithm comes from the fact that it
uses a small number of parameters with minimal assumptions regarding the data
being examined or the underlying motifs. Our interest lies in applying the
algorithm to financial time series data to identify unknown patterns that
exist. The algorithm is tested using three separate data sets. Particular
suitability to financial data is shown by applying it to oil price data. In all
cases the algorithm identifies the presence of a motif population in a fast and
efficient manner due to the utilisation of an intuitive symbolic
representation. The resulting population of motifs is shown to have
considerable potential value for other applications such as forecasting and
algorithm seeding.



A new emerging paradigm of Uncertain Risk of Suspicion, Threat and Danger,
observed across the field of information security, is described. Based on this
paradigm a novel approach to anomaly detection is presented. Our approach is
based on a simple yet powerful analogy from the innate part of the human immune
system, the Toll-Like Receptors. We argue that such receptors incorporated as
part of an anomaly detector enhance the detector's ability to distinguish
normal and anomalous behaviour. In addition we propose that Toll-Like Receptors
enable the classification of detected anomalies based on the types of attacks
that perpetrate the anomalous behaviour. Classification of such type is either
missing in existing literature or is not fit for the purpose of reducing the
burden of an administrator of an intrusion detection system. For our model to
work, we propose the creation of a taxonomy of the digital Acytota, based on
which our receptors are created.



We present an application of Artificial Intelligence techniques to the field
of Information Security. The problem of remote Operating System (OS) Detection,
also called OS Fingerprinting, is a crucial step of the penetration testing
process, since the attacker (hacker or security professional) needs to know the
OS of the target host in order to choose the exploits that he will use. OS
Detection is accomplished by passively sniffing network packets and actively
sending test packets to the target host, to study specific variations in the
host responses revealing information about its operating system.
  The first fingerprinting implementations were based on the analysis of
differences between TCP/IP stack implementations. The next generation focused
the analysis on application layer data such as the DCE RPC endpoint
information. Even though more information was analyzed, some variation of the
"best fit" algorithm was still used to interpret this new information. Our new
approach involves an analysis of the composition of the information collected
during the OS identification process to identify key elements and their
relations. To implement this approach, we have developed tools using Neural
Networks and techniques from the field of Statistics. These tools have been
successfully integrated in a commercial software (Core Impact).



A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to
solving mobile-robot navigation problems is presented and tested in both the
real and virtual domains. The LTL phase consists of rapid simulations that use
a Genetic Algorithm to derive diverse sets of behaviours, encoded as variable
sets of attributes, and the STL phase is an idiotypic Artificial Immune System.
Results from the LTL phase show that sets of behaviours develop very rapidly,
and significantly greater diversity is obtained when multiple autonomous
populations are used, rather than a single one. The architecture is assessed
under various scenarios, including removal of the LTL phase and switching off
the idiotypic mechanism in the STL phase. The comparisons provide substantial
evidence that the best option is the inclusion of both the LTL phase and the
idiotypic system. In addition, this paper shows that structurally different
environments can be used for the two phases without compromising
transferability.



Functional constraints and bi-functional constraints are an important
constraint class in Constraint Programming (CP) systems, in particular for
Constraint Logic Programming (CLP) systems. CP systems with finite domain
constraints usually employ CSP-based solvers which use local consistency, for
example, arc consistency. We introduce a new approach which is based instead on
variable substitution. We obtain efficient algorithms for reducing systems
involving functional and bi-functional constraints together with other
non-functional constraints. It also solves globally any CSP where there exists
a variable such that any other variable is reachable from it through a sequence
of functional constraints. Our experiments on random problems show that
variable elimination can significantly improve the efficiency of solving
problems with functional constraints.



Previous work has shown that robot navigation systems that employ an
architecture based upon the idiotypic network theory of the immune system have
an advantage over control techniques that rely on reinforcement learning only.
This is thought to be a result of intelligent behaviour selection on the part
of the idiotypic robot. In this paper an attempt is made to imitate idiotypic
dynamics by creating controllers that use reinforcement with a number of
different probabilistic schemes to select robot behaviour. The aims are to show
that the idiotypic system is not merely performing some kind of periodic random
behaviour selection, and to try to gain further insight into the processes that
govern the idiotypic mechanism. Trials are carried out using simulated Pioneer
robots that undertake navigation exercises. Results show that a scheme that
boosts the probability of selecting highly-ranked alternative behaviours to 50%
during stall conditions comes closest to achieving the properties of the
idiotypic system, but remains unable to match it in terms of all round
performance.



This research investigated the simulation model behaviour of a traditional
and combined discrete event as well as agent based simulation models when
modelling human reactive and proactive behaviour in human centric complex
systems. A departmental store was chosen as human centric complex case study
where the operation system of a fitting room in WomensWear department was
investigated. We have looked at ways to determine the efficiency of new
management policies for the fitting room operation through simulating the
reactive and proactive behaviour of staff towards customers. Once development
of the simulation models and their verification had been done, we carried out a
validation experiment in the form of a sensitivity analysis. Subsequently, we
executed a statistical analysis where the mixed reactive and proactive
behaviour experimental results were compared with some reactive experimental
results from previously published works. Generally, this case study discovered
that simple proactive individual behaviour could be modelled in both simulation
models. In addition, we found the traditional discrete event model performed
similar in the simulation model output compared to the combined discrete event
and agent based simulation when modelling similar human behaviour.



Artificial Immune Systems have been successfully applied to a number of
problem domains including fault tolerance and data mining, but have been shown
to scale poorly when applied to computer intrusion detec- tion despite the fact
that the biological immune system is a very effective anomaly detector. This
may be because AIS algorithms have previously been based on the adaptive immune
system and biologically-naive mod- els. This paper focuses on describing and
testing a more complex and biologically-authentic AIS model, inspired by the
interactions between the innate and adaptive immune systems. Its performance on
a realistic process anomaly detection problem is shown to be better than
standard AIS methods (negative-selection), policy-based anomaly detection
methods (systrace), and an alternative innate AIS approach (the DCA). In
addition, it is shown that runtime information can be used in combination with
system call information to enhance detection capability.



Often models for understanding the impact of management practices on retail
performance are developed under the assumption of stability, equilibrium and
linearity, whereas retail operations are considered in reality to be dynamic,
non-linear and complex. Alternatively, discrete event and agent-based modelling
are approaches that allow the development of simulation models of heterogeneous
non-equilibrium systems for testing out different scenarios. When developing
simulation models one has to abstract and simplify from the real world, which
means that one has to try and capture the 'essence' of the system required for
developing a representation of the mechanisms that drive the progression in the
real system. Simulation models can be developed at different levels of
abstraction. To know the appropriate level of abstraction for a specific
application is often more of an art than a science. We have developed a retail
branch simulation model to investigate which level of model accuracy is
required for such a model to obtain meaningful results for practitioners.



Feature selection refers to the problem of selecting relevant features which
produce the most predictive outcome. In particular, feature selection task is
involved in datasets containing huge number of features. Rough set theory has
been one of the most successful methods used for feature selection. However,
this method is still not able to find optimal subsets. This paper proposes a
new feature selection method based on Rough set theory hybrid with Bee Colony
Optimization (BCO) in an attempt to combat this. This proposed work is applied
in the medical domain to find the minimal reducts and experimentally compared
with the Quick Reduct, Entropy Based Reduct, and other hybrid Rough Set methods
such as Genetic Algorithm (GA), Ant Colony Optimization (ACO) and Particle
Swarm Optimization (PSO).



The sequential parameter optimization (SPOT) package for R is a toolbox for
tuning and understanding simulation and optimization algorithms. Model-based
investigations are common approaches in simulation and optimization. Sequential
parameter optimization has been developed, because there is a strong need for
sound statistical analysis of simulation and optimization algorithms. SPOT
includes methods for tuning based on classical regression and analysis of
variance techniques; tree-based models such as CART and random forest; Gaussian
process models (Kriging), and combinations of different meta-modeling
approaches. This article exemplifies how SPOT can be used for automatic and
interactive tuning.



The human immune system has numerous properties that make it ripe for
exploitation in the computational domain, such as robustness and fault
tolerance, and many different algorithms, collectively termed Artificial Immune
Systems (AIS), have been inspired by it. Two generations of AIS are currently
in use, with the first generation relying on simplified immune models and the
second generation utilising interdisciplinary collaboration to develop a deeper
understanding of the immune system and hence produce more complex models. Both
generations of algorithms have been successfully applied to a variety of
problems, including anomaly detection, pattern recognition, optimisation and
robotics. In this chapter an overview of AIS is presented, its evolution is
discussed, and it is shown that the diversification of the field is linked to
the diversity of the immune system itself, leading to a number of algorithms as
opposed to one archetypal system. Two case studies are also presented to help
provide insight into the mechanisms of AIS; these are the idiotypic network
approach and the Dendritic Cell Algorithm.



The Dendritic Cell Algorithm (DCA) is inspired by the function of the
dendritic cells of the human immune system. In nature, dendritic cells are the
intrusion detection agents of the human body, policing the tissue and organs
for potential invaders in the form of pathogens. In this research, and abstract
model of DC behaviour is developed and subsequently used to form an algorithm,
the DCA. The abstraction process was facilitated through close collaboration
with laboratory- based immunologists, who performed bespoke experiments, the
results of which are used as an integral part of this algorithm. The DCA is a
population based algorithm, with each agent in the system represented as an
'artificial DC'. Each DC has the ability to combine multiple data streams and
can add context to data suspected as anomalous. In this chapter the abstraction
process and details of the resultant algorithm are given. The algorithm is
applied to numerous intrusion detection problems in computer security including
the detection of port scans and botnets, where it has produced impressive
results with relatively low rates of false positives.



Although the Music Sight Reading process has been studied from the cognitive
psychology view points, but the computational learning methods like the
Reinforcement Learning have not yet been used to modeling of such processes. In
this paper, with regards to essential properties of our specific problem, we
consider the value function concept and will indicate that the optimum policy
can be obtained by the method we offer without to be getting involved with
computing of the complex value functions. Also, we will offer a normative
behavioral model for the interaction of the agent with the musical pitch
environment and by using a slightly different version of Partially observable
Markov decision processes we will show that our method helps for faster
learning of state-action pairs in our implemented agents.



When agents are acting together, they may need a simple mechanism to decide
on joint actions. One possibility is to have the agents express their
preferences in the form of a ballot and use a voting rule to decide the winning
action(s). Unfortunately, agents may try to manipulate such an election by
misreporting their preferences. Fortunately, it has been shown that it is
NP-hard to compute how to manipulate a number of different voting rules.
However, NP-hardness only bounds the worst-case complexity. Recent theoretical
results suggest that manipulation may often be easy in practice. To address
this issue, I suggest studying empirically if computational complexity is in
practice a barrier to manipulation. The basic tool used in my investigations is
the identification of computational "phase transitions". Such an approach has
been fruitful in identifying hard instances of propositional satisfiability and
other NP-hard problems. I show that phase transition behaviour gives insight
into the hardness of manipulating voting rules, increasing concern that
computational complexity is indeed any sort of barrier. Finally, I look at the
problem of computing manipulation of other, related problems like stable
marriage and tournament problems.



We describe how to use propositional model counting for a quantitative
analysis of product configuration data. Our approach computes valuable meta
information such as the total number of valid configurations or the relative
frequency of components. This information can be used to assess the severity of
documentation errors or to measure documentation quality. As an application
example we show how we apply these methods to product documentation formulas of
the Mercedes-Benz line of vehicles. In order to process these large formulas we
developed and implemented a new model counter for non-CNF formulas. Our model
counter can process formulas, whose CNF representations could not be processed
up till now.



PRISM is an extension of Prolog with probabilistic predicates and built-in
support for expectation-maximization learning. Constraint Handling Rules (CHR)
is a high-level programming language based on multi-headed multiset rewrite
rules.
  In this paper, we introduce a new probabilistic logic formalism, called
CHRiSM, based on a combination of CHR and PRISM. It can be used for high-level
rapid prototyping of complex statistical models by means of "chance rules". The
underlying PRISM system can then be used for several probabilistic inference
tasks, including probability computation and parameter learning. We define the
CHRiSM language in terms of syntax and operational semantics, and illustrate it
with examples. We define the notion of ambiguous programs and define a
distribution semantics for unambiguous programs. Next, we describe an
implementation of CHRiSM, based on CHR(PRISM). We discuss the relation between
CHRiSM and other probabilistic logic programming languages, in particular PCHR.
Finally we identify potential application domains.



This paper presents new results for the (partial) maximum a posteriori (MAP)
problem in Bayesian networks, which is the problem of querying the most
probable state configuration of some of the network variables given evidence.
First, it is demonstrated that the problem remains hard even in networks with
very simple topology, such as binary polytrees and simple trees (including the
Naive Bayes structure). Such proofs extend previous complexity results for the
problem. Inapproximability results are also derived in the case of trees if the
number of states per variable is not bounded. Although the problem is shown to
be hard and inapproximable even in very simple scenarios, a new exact algorithm
is described that is empirically fast in networks of bounded treewidth and
bounded number of states per variable. The same algorithm is used as basis of a
Fully Polynomial Time Approximation Scheme for MAP under such assumptions.
Approximation schemes were generally thought to be impossible for this problem,
but we show otherwise for classes of networks that are important in practice.
The algorithms are extensively tested using some well-known networks as well as
random generated cases to show their effectiveness.



One possible escape from the Gibbard-Satterthwaite theorem is computational
complexity. For example, it is NP-hard to compute if the STV rule can be
manipulated. However, there is increasing concern that such results may not re
ect the difficulty of manipulation in practice. In this tutorial, I survey
recent results in this area.



The stable marriage problem is a well-known problem of matching men to women
so that no man and woman, who are not married to each other, both prefer each
other. Such a problem has a wide variety of practical applications, ranging
from matching resident doctors to hospitals, to matching students to schools or
more generally to any two-sided market. In the classical stable marriage
problem, both men and women express a strict preference order over the members
of the other sex, in a qualitative way. Here we consider stable marriage
problems with quantitative preferences: each man (resp., woman) provides a
score for each woman (resp., man). Such problems are more expressive than the
classical stable marriage problems. Moreover, in some real-life situations it
is more natural to express scores (to model, for example, profits or costs)
rather than a qualitative preference ordering. In this context, we define new
notions of stability and optimality, and we provide algorithms to find
marriages which are stable and/or optimal according to these notions. While
expressivity greatly increases by adopting quantitative preferences, we show
that in most cases the desired solutions can be found by adapting existing
algorithms for the classical stable marriage problem.



The paper investigates a novel approach, based on Constraint Logic
Programming (CLP), to predict the 3D conformation of a protein via fragments
assembly. The fragments are extracted by a preprocessor-also developed for this
work- from a database of known protein structures that clusters and classifies
the fragments according to similarity and frequency. The problem of assembling
fragments into a complete conformation is mapped to a constraint solving
problem and solved using CLP. The constraint-based model uses a medium
discretization degree Ca-side chain centroid protein model that offers
efficiency and a good approximation for space filling. The approach adapts
existing energy models to the protein representation used and applies a large
neighboring search strategy. The results shows the feasibility and efficiency
of the method. The declarative nature of the solution allows to include future
extensions, e.g., different size fragments for better accuracy.



A Hidden Markov Model (HMM) is a common statistical model which is widely
used for analysis of biological sequence data and other sequential phenomena.
In the present paper we show how HMMs can be extended with side-constraints and
present constraint solving techniques for efficient inference. Defining HMMs
with side-constraints in Constraint Logic Programming have advantages in terms
of more compact expression and pruning opportunities during inference.
  We present a PRISM-based framework for extending HMMs with side-constraints
and show how well-known constraints such as cardinality and all different are
integrated. We experimentally validate our approach on the biologically
motivated problem of global pairwise alignment.



Markov models are extensively used in the analysis of molecular evolution. A
recent line of research suggests that pairs of proteins with functional and
physical interactions co-evolve with each other. Here, by analyzing hundreds of
orthologous sets of three fungi and their co-evolutionary relations, we
demonstrate that co-evolutionary assumption may violate the Markov assumption.
Our results encourage developing alternative probabilistic models for the cases
of extreme co-evolution.



We discuss the problems of modeling, control, and decision support in complex
dynamic systems from a general system theoretic point of view. The main
characteristics of complex systems and of system approach to complex system
study are considered. We provide an overview and analysis of known existing
paradigms and methods of mathematical modeling and simulation of complex
systems, which support the processes of control and decision making. Then we
continue with the general dynamic modeling and simulation technique for complex
hierarchical systems functioning in control loop. Architectural and structural
models of computer information system intended for simulation and decision
support in complex systems are presented.



In context of efforts of composing category-theoretic and logical methods in
the area of knowledge representation we propose the notion of conceptory. We
consider intersection/union and other constructions in conceptories as
expressive alternative to category-theoretic (co)limits and show they have
features similar to (pro-, in-)jections. Then we briefly discuss approaches to
development of formal systems built on the base of conceptories and describe
possible application of such system to the specific ontology.



This paper proposes a nonparametric Bayesian method for exploratory data
analysis and feature construction in continuous time series. Our method focuses
on understanding shared features in a set of time series that exhibit
significant individual variability. Our method builds on the framework of
latent Diricihlet allocation (LDA) and its extension to hierarchical Dirichlet
processes, which allows us to characterize each series as switching between
latent ``topics'', where each topic is characterized as a distribution over
``words'' that specify the series dynamics. However, unlike standard
applications of LDA, we discover the words as we learn the model. We apply this
model to the task of tracking the physiological signals of premature infants;
our model obtains clinically significant insights as well as useful features
for supervised learning tasks.



This paper deals with chain graphs under the classic
Lauritzen-Wermuth-Frydenberg interpretation. We prove that the regular Gaussian
distributions that factorize with respect to a chain graph $G$ with $d$
parameters have positive Lebesgue measure with respect to $\mathbb{R}^d$,
whereas those that factorize with respect to $G$ but are not faithful to it
have zero Lebesgue measure with respect to $\mathbb{R}^d$. This means that, in
the measure-theoretic sense described, almost all the regular Gaussian
distributions that factorize with respect to $G$ are faithful to it.



We focus on credal nets, which are graphical models that generalise Bayesian
nets to imprecise probability. We replace the notion of strong independence
commonly used in credal nets with the weaker notion of epistemic irrelevance,
which is arguably more suited for a behavioural theory of probability. Focusing
on directed trees, we show how to combine the given local uncertainty models in
the nodes of the graph into a global model, and we use this to construct and
justify an exact message-passing algorithm that computes updated beliefs for a
variable in the tree. The algorithm, which is linear in the number of nodes, is
formulated entirely in terms of coherent lower previsions, and is shown to
satisfy a number of rationality requirements. We supply examples of the
algorithm's operation, and report an application to on-line character
recognition that illustrates the advantages of our approach for prediction. We
comment on the perspectives, opened by the availability, for the first time, of
a truly efficient algorithm based on epistemic irrelevance.



We extend the mixtures of Gaussians (MOG) model to the projected mixture of
Gaussians (PMOG) model. In the PMOG model, we assume that q dimensional input
data points z_i are projected by a q dimensional vector w into 1-D variables
u_i. The projected variables u_i are assumed to follow a 1-D MOG model. In the
PMOG model, we maximize the likelihood of observing u_i to find both the model
parameters for the 1-D MOG as well as the projection vector w. First, we derive
an EM algorithm for estimating the PMOG model. Next, we show how the PMOG model
can be applied to the problem of blind source separation (BSS). In contrast to
conventional BSS where an objective function based on an approximation to
differential entropy is minimized, PMOG based BSS simply minimizes the
differential entropy of projected sources by fitting a flexible MOG model in
the projected 1-D space while simultaneously optimizing the projection vector
w. The advantage of PMOG over conventional BSS algorithms is the more flexible
fitting of non-Gaussian source densities without assuming near-Gaussianity (as
in conventional BSS) and still retaining computational feasibility.



In this paper we analyze judgement aggregation problems in which a group of
agents independently votes on a set of complex propositions that has some
interdependency constraint between them(e.g., transitivity when describing
preferences). We consider the issue of judgement aggregation from the
perspective of approximation. That is, we generalize the previous results by
studying approximate judgement aggregation. We relax the main two constraints
assumed in the current literature, Consistency and Independence and consider
mechanisms that only approximately satisfy these constraints, that is, satisfy
them up to a small portion of the inputs. The main question we raise is whether
the relaxation of these notions significantly alters the class of satisfying
aggregation mechanisms. The recent works for preference aggregation of Kalai,
Mossel, and Keller fit into this framework. The main result of this paper is
that, as in the case of preference aggregation, in the case of a subclass of a
natural class of aggregation problems termed `truth-functional agendas', the
set of satisfying aggregation mechanisms does not extend non-trivially when
relaxing the constraints. Our proof techniques involve Boolean Fourier
transform and analysis of voter influences for voting protocols. The question
we raise for Approximate Aggregation can be stated in terms of Property
Testing. For instance, as a corollary from our result we get a generalization
of the classic result for property testing of linearity of Boolean functions.
  An updated version (RePEc:huj:dispap:dp574R) is available at
http://www.ratio.huji.ac.il/dp_files/dp574R.pdf



It is well known that text compression can be achieved by predicting the next
symbol in the stream of text data based on the history seen up to the current
symbol. The better the prediction the more skewed the conditional probability
distribution of the next symbol and the shorter the codeword that needs to be
assigned to represent this next symbol. What about the opposite direction ?
suppose we have a black box that can compress text stream. Can it be used to
predict the next symbol in the stream ? We introduce a criterion based on the
length of the compressed data and use it to predict the next symbol. We examine
empirically the prediction error rate and its dependency on some compression
parameters.



Ink Drop Spread (IDS) is the engine of Active Learning Method (ALM), which is
the methodology of soft computing. IDS, as a pattern-based processing unit,
extracts useful information from a system subjected to modeling. In spite of
its excellent potential in solving problems such as classification and modeling
compared to other soft computing tools, finding its simple and fast hardware
implementation is still a challenge. This paper describes a new hardware
implementation of IDS method based on the memristor crossbar structure. In
addition of simplicity, being completely real-time, having low latency and the
ability to continue working after the occurrence of power breakdown are some of
the advantages of our proposed circuit.



The ubiquitous role of the cyber-infrastructures, such as the WWW, provides
myriad opportunities for machine learning and its broad spectrum of application
domains taking advantage of digital communication. Pattern classification and
feature extraction are among the first applications of machine learning that
have received extensive attention. The most remarkable achievements have
addressed data sets of moderate-to-large size. The 'data deluge' in the last
decade or two has posed new challenges for AI researchers to design new,
effective and accurate algorithms for similar tasks using ultra-massive data
sets and complex (natural or synthetic) dynamical systems. We propose a novel
principled approach to feature extraction in hybrid architectures comprised of
humans and machines in networked communication, who collaborate to solve a
pre-assigned pattern recognition (feature extraction) task. There are two
practical considerations addressed below: (1) Human experts, such as plant
biologists or astronomers, often use their visual perception and other implicit
prior knowledge or expertise without any obvious constraints to search for the
significant features, whereas machines are limited to a pre-programmed set of
criteria to work with; (2) in a team collaboration of collective problem
solving, the human experts have diverse abilities that are complementary, and
they learn from each other to succeed in cognitively complex tasks in ways that
are still impossible imitate by machines.



This paper presents our investigations on emotional state categorization from
speech signals with a psychologically inspired computational model against
human performance under the same experimental setup. Based on psychological
studies, we propose a multistage categorization strategy which allows
establishing an automatic categorization model flexibly for a given emotional
speech categorization task. We apply the strategy to the Serbian Emotional
Speech Corpus (GEES) and the Danish Emotional Speech Corpus (DES), where human
performance was reported in previous psychological studies. Our work is the
first attempt to apply machine learning to the GEES corpus where the human
recognition rates were only available prior to our study. Unlike the previous
work on the DES corpus, our work focuses on a comparison to human performance
under the same experimental settings. Our studies suggest that
psychology-inspired systems yield behaviours that, to a great extent, resemble
what humans perceived and their performance is close to that of humans under
the same experimental setup. Furthermore, our work also uncovers some
differences between machine and humans in terms of emotional state recognition
from speech.



Low-rank matrix approximations are often used to help scale standard machine
learning algorithms to large-scale problems. Recently, matrix coherence has
been used to characterize the ability to extract global information from a
subset of matrix entries in the context of these low-rank approximations and
other sampling-based algorithms, e.g., matrix com- pletion, robust PCA. Since
coherence is defined in terms of the singular vectors of a matrix and is
expensive to compute, the practical significance of these results largely
hinges on the following question: Can we efficiently and accurately estimate
the coherence of a matrix? In this paper we address this question. We propose a
novel algorithm for estimating coherence from a small number of columns,
formally analyze its behavior, and derive a new coherence-based matrix
approximation bound based on this analysis. We then present extensive
experimental results on synthetic and real datasets that corroborate our
worst-case theoretical analysis, yet provide strong support for the use of our
proposed algorithm whenever low-rank approximation is being considered. Our
algorithm efficiently and accurately estimates matrix coherence across a wide
range of datasets, and these coherence estimates are excellent predictors of
the effectiveness of sampling-based matrix approximation on a case-by-case
basis.



In May 1, 2008, researchers at Hewlett Packard (HP) announced the first
physical realization of a fundamental circuit element called memristor that
attracted so much interest worldwide. This newly found element can easily be
combined with crossbar interconnect technology which this new structure has
opened a new field in designing configurable or programmable electronic
systems. These systems in return can have applications in signal processing and
artificial intelligence. In this paper, based on the simple memristor crossbar
structure, we propose new and simple circuits for hardware implementation of
fuzzy membership functions. In our proposed circuits, these fuzzy membership
functions can have any shapes and resolutions. In addition, these circuits can
be used as a basis in the construction of evolutionary systems.



Ensuring sufficient liquidity is one of the key challenges for designers of
prediction markets. Various market making algorithms have been proposed in the
literature and deployed in practice, but there has been little effort to
evaluate their benefits and disadvantages in a systematic manner. We introduce
a novel experimental design for comparing market structures in live trading
that ensures fair comparison between two different microstructures with the
same trading population. Participants trade on outcomes related to a
two-dimensional random walk that they observe on their computer screens. They
can simultaneously trade in two markets, corresponding to the independent
horizontal and vertical random walks. We use this experimental design to
compare the popular inventory-based logarithmic market scoring rule (LMSR)
market maker and a new information based Bayesian market maker (BMM). Our
experiments reveal that BMM can offer significant benefits in terms of price
stability and expected loss when controlling for liquidity; the caveat is that,
unlike LMSR, BMM does not guarantee bounded loss. Our investigation also
elucidates some general properties of market makers in prediction markets. In
particular, there is an inherent tradeoff between adaptability to market shocks
and convergence during market equilibrium.



Over the past few decades, non-monotonic reasoning has developed to be one of
the most important topics in computational logic and artificial intelligence.
Different ways to introduce non-monotonic aspects to classical logic have been
considered, e.g., extension with default rules, extension with modal belief
operators, or modification of the semantics. In this survey we consider a
logical formalism from each of the above possibilities, namely Reiter's default
logic, Moore's autoepistemic logic and McCarthy's circumscription.
Additionally, we consider abduction, where one is not interested in inferences
from a given knowledge base but in computing possible explanations for an
observation with respect to a given knowledge base.
  Complexity results for different reasoning tasks for propositional variants
of these logics have been studied already in the nineties. In recent years,
however, a renewed interest in complexity issues can be observed. One current
focal approach is to consider parameterized problems and identify reasonable
parameters that allow for FPT algorithms. In another approach, the emphasis
lies on identifying fragments, i.e., restriction of the logical language, that
allow more efficient algorithms for the most important reasoning tasks. In this
survey we focus on this second aspect. We describe complexity results for
fragments of logical languages obtained by either restricting the allowed set
of operators (e.g., forbidding negations one might consider only monotone
formulae) or by considering only formulae in conjunctive normal form but with
generalized clause types.
  The algorithmic problems we consider are suitable variants of satisfiability
and implication in each of the logics, but also counting problems, where one is
not only interested in the existence of certain objects (e.g., models of a
formula) but asks for their number.



Complex network theory aims to model and analyze complex systems that consist
of multiple and interdependent components. Among all studies on complex
networks, topological structure analysis is of the most fundamental importance,
as it represents a natural route to understand the dynamics, as well as to
synthesize or optimize the functions, of networks. A broad spectrum of network
structural patterns have been respectively reported in the past decade, such as
communities, multipartites, hubs, authorities, outliers, bow ties, and others.
Here, we show that most individual real-world networks demonstrate multiplex
structures. That is, a multitude of known or even unknown (hidden) patterns can
simultaneously situate in the same network, and moreover they may be overlapped
and nested with each other to collaboratively form a heterogeneous, nested or
hierarchical organization, in which different connective phenomena can be
observed at different granular levels. In addition, we show that the multiplex
structures hidden in exploratory networks can be well defined as well as
effectively recognized within an unified framework consisting of a set of
proposed concepts, models, and algorithms. Our findings provide a strong
evidence that most real-world complex systems are driven by a combination of
heterogeneous mechanisms that may collaboratively shape their ubiquitous
multiplex structures as we observe currently. This work also contributes a
mathematical tool for analyzing different sources of networks from a new
perspective of unveiling multiplex structures, which will be beneficial to
multiple disciplines including sociology, economics and computer science.



Artificial neural networks built from two-state neurons are powerful
computational substrates, whose computational ability is well understood by
analogy with statistical mechanics. In this work, we introduce similar
analogies in the context of spiking neurons in a fixed time window, where
excitatory and inhibitory inputs drawn from a Poisson distribution play the
role of temperature. For single neurons with a "bandgap" between their inputs
and the spike threshold, this temperature allows for stochastic spiking. By
imposing a global inhibitory rhythm over the fixed time windows, we connect
neurons into a network that exhibits synchronous, clock-like updating akin to
neural networks. We implement a single-layer Boltzmann machine without learning
to demonstrate our model.



We present Mantis, a new framework that automatically predicts program
performance with high accuracy. Mantis integrates techniques from programming
language and machine learning for performance modeling, and is a radical
departure from traditional approaches. Mantis extracts program features, which
are information about program execution runs, through program instrumentation.
It uses machine learning techniques to select features relevant to performance
and creates prediction models as a function of the selected features. Through
program analysis, it then generates compact code slices that compute these
feature values for prediction. Our evaluation shows that Mantis can achieve
more than 93% accuracy with less than 10% training data set, which is a
significant improvement over models that are oblivious to program features. The
system generates code slices that are cheap to compute feature values.



This short note demonstrates how one can define a transformation of a
non-zero sum game into a zero sum, so that the optimal mixed strategy achieving
equilibrium always exists. The transformation is equivalent to introduction of
a passive player into a game (a player with a singleton set of pure
strategies), whose payoff depends on the actions of the active players, and it
is justified by the law of conservation of utility in a game. In a transformed
game, each participant plays against all other players, including the passive
player. The advantage of this approach is that the transformed game is zero-sum
and has an equilibrium solution. The optimal strategy and the value of the new
game, however, can be different from strategies that are rational in the
original game. We demonstrate the principle using the Prisoner's Dilemma
example.



We tackle the fundamental problem of Bayesian active learning with noise,
where we need to adaptively select from a number of expensive tests in order to
identify an unknown hypothesis sampled from a known prior distribution. In the
case of noise-free observations, a greedy algorithm called generalized binary
search (GBS) is known to perform near-optimally. We show that if the
observations are noisy, perhaps surprisingly, GBS can perform very poorly. We
develop EC2, a novel, greedy active learning algorithm and prove that it is
competitive with the optimal policy, thus obtaining the first competitiveness
guarantees for Bayesian active learning with noisy observations. Our bounds
rely on a recently discovered diminishing returns property called adaptive
submodularity, generalizing the classical notion of submodular set functions to
adaptive policies. Our results hold even if the tests have non-uniform cost and
their noise is correlated. We also propose EffECXtive, a particularly fast
approximation of EC2, and evaluate it on a Bayesian experimental design problem
involving human subjects, intended to tease apart competing economic theories
of how people make decisions under uncertainty.



We consider the problem of learning about and comparing the consequences of
dynamic treatment strategies on the basis of observational data. We formulate
this within a probabilistic decision-theoretic framework. Our approach is
compared with related work by Robins and others: in particular, we show how
Robins's 'G-computation' algorithm arises naturally from this
decision-theoretic perspective. Careful attention is paid to the mathematical
and substantive conditions required to justify the use of this formula. These
conditions revolve around a property we term stability, which relates the
probabilistic behaviours of observational and interventional regimes. We show
how an assumption of 'sequential randomization' (or 'no unmeasured
confounders'), or an alternative assumption of 'sequential irrelevance', can be
used to infer stability. Probabilistic influence diagrams are used to simplify
manipulations, and their power and limitations are discussed. We compare our
approach with alternative formulations based on causal DAGs or potential
response models. We aim to show that formulating the problem of assessing
dynamic treatment strategies as a problem of decision analysis brings clarity,
simplicity and generality.



The covariance graph (aka bi-directed graph) of a probability distribution
$p$ is the undirected graph $G$ where two nodes are adjacent iff their
corresponding random variables are marginally dependent in $p$. In this paper,
we present a graphical criterion for reading dependencies from $G$, under the
assumption that $p$ satisfies the graphoid properties as well as weak
transitivity and composition. We prove that the graphical criterion is sound
and complete in certain sense. We argue that our assumptions are not too
restrictive. For instance, all the regular Gaussian probability distributions
satisfy them.



The purpose of this article is to introduce a new iterative algorithm with
properties resembling real life bipartite graphs. The algorithm enables us to
generate wide range of random bigraphs, which features are determined by a set
of parameters.We adapt the advances of last decade in unipartite complex
networks modeling to the bigraph setting. This data structure can be observed
in several situations. However, only a few datasets are freely available to
test the algorithms (e.g. community detection, influential nodes
identification, information retrieval) which operate on such data. Therefore,
artificial datasets are needed to enhance development and testing of the
algorithms. We are particularly interested in applying the generator to the
analysis of recommender systems. Therefore, we focus on two characteristics
that, besides simple statistics, are in our opinion responsible for the
performance of neighborhood based collaborative filtering algorithms. The
features are node degree distribution and local clustering coeficient.



The purpose of this article is to introduce a new analytical framework
dedicated to measuring performance of recommender systems. The standard
approach is to assess the quality of a system by means of accuracy related
statistics. However, the specificity of the environments in which recommender
systems are deployed requires to pay much attention to speed and memory
requirements of the algorithms. Unfortunately, it is implausible to assess
accurately the complexity of various algorithms with formal tools. This can be
attributed to the fact that such analyses are usually based on an assumption of
dense representation of underlying data structures. Whereas, in real life the
algorithms operate on sparse data and are implemented with collections
dedicated for them. Therefore, we propose to measure the complexity of
recommender systems with artificial datasets that posses real-life properties.
We utilize recently developed bipartite graph generator to evaluate how
state-of-the-art recommender systems' behavior is determined and diversified by
topological properties of the generated datasets.



This report outlines the use of a relational representation in a Multi-Agent
domain to model the behaviour of the whole system. A desired property in this
systems is the ability of the team members to work together to achieve a common
goal in a cooperative manner. The aim is to define a systematic method to
verify the effective collaboration among the members of a team and comparing
the different multi-agent behaviours. Using external observations of a
Multi-Agent System to analyse, model, recognize agent behaviour could be very
useful to direct team actions. In particular, this report focuses on the
challenge of autonomous unsupervised sequential learning of the team's
behaviour from observations. Our approach allows to learn a symbolic sequence
(a relational representation) to translate raw multi-agent, multi-variate
observations of a dynamic, complex environment, into a set of sequential
behaviours that are characteristic of the team in question, represented by a
set of sequences expressed in first-order logic atoms. We propose to use a
relational learning algorithm to mine meaningful frequent patterns among the
relational sequences to characterise team behaviours. We compared the
performance of two teams in the RoboCup four-legged league environment, that
have a very different approach to the game. One uses a Case Based Reasoning
approach, the other uses a pure reactive behaviour.



An effective procedure to determine the optimal parameters appearing in
artificial flockings is proposed in terms of optimization problems. We
numerically examine genetic algorithms (GAs) to determine the optimal set of
parameters such as the weights for three essential interactions in BOIDS by
Reynolds (1987) under `zero-collision' and `no-breaking-up' constraints. As a
fitness function (the energy function) to be maximized by the GA, we choose the
so-called the $\gamma$-value of anisotropy which can be observed empirically in
typical flocks of starling. We confirm that the GA successfully finds the
solution having a large $\gamma$-value leading-up to a strong anisotropy. The
numerical experience shows that the procedure might enable us to make more
realistic and efficient artificial flocking of starling even in our personal
computers. We also evaluate two distinct types of interactions in agents,
namely, metric and topological definitions of interactions. We confirmed that
the topological definition can explain the empirical evidence much better than
the metric definition does.



Sequential prediction problems such as imitation learning, where future
observations depend on previous predictions (actions), violate the common
i.i.d. assumptions made in statistical learning. This leads to poor performance
in theory and often in practice. Some recent approaches provide stronger
guarantees in this setting, but remain somewhat unsatisfactory as they train
either non-stationary or stochastic policies and require a large number of
iterations. In this paper, we propose a new iterative algorithm, which trains a
stationary deterministic policy, that can be seen as a no regret algorithm in
an online learning setting. We show that any such no regret algorithm, combined
with additional reduction assumptions, must find a policy with good performance
under the distribution of observations it induces in such sequential settings.
We demonstrate that this new approach outperforms previous approaches on two
challenging imitation learning problems and a benchmark sequence labeling
problem.



Our work has focused on support for film or television scriptwriting. Since
this involves potentially varied story-lines, we note the implicit or latent
support for interactivity. Furthermore the film, television, games, publishing
and other sectors are converging, so that cross-over and re-use of one form of
product in another of these sectors is ever more common. Technically our work
has been largely based on mathematical algorithms for data clustering and
display. Operationally, we also discuss how our algorithms can support
collective, distributed problem-solving.



The development of discursive knowledge presumes the communication of meaning
as analytically different from the communication of information. Knowledge can
then be considered as a meaning which makes a difference. Whereas the
communication of information is studied in the information sciences and
scientometrics, the communication of meaning has been central to Luhmann's
attempts to make the theory of autopoiesis relevant for sociology. Analytical
techniques such as semantic maps and the simulation of anticipatory systems
enable us to operationalize the distinctions which Luhmann proposed as relevant
to the elaboration of Husserl's "horizons of meaning" in empirical research:
interactions among communications, the organization of meaning in
instantiations, and the self-organization of interhuman communication in terms
of symbolically generalized media such as truth, love, and power. Horizons of
meaning, however, remain uncertain orders of expectations, and one should
caution against reification from the meta-biological perspective of systems
theory.



The Reflexive Game Theory (RGT) has been recently proposed by Vladimir
Lefebvre to model behavior of individuals in groups. The goal of this study is
to introduce the Inverse task. We consider methods of solution together with
practical applications. We present a brief overview of the RGT for easy
understanding of the problem. We also develop the schematic representation of
the RGT inference algorithms to create the basis for soft- and hardware
solutions of the RGT tasks. We propose a unified hierarchy of schemas to
represent humans and robots. This hierarchy is considered as a unified
framework to solve the entire spectrum of the RGT tasks. We conclude by
illustrating how this framework can be applied for modeling of mixed groups of
humans and robots. All together this provides the exhaustive solution of the
Inverse task and clearly illustrates its role and relationships with other
issues considered in the RGT.



Learning structured representations has emerged as an important problem in
many domains, including document and Web data mining, bioinformatics, and image
analysis. One approach to learning complex structures is to integrate many
smaller, incomplete and noisy structure fragments. In this work, we present an
unsupervised probabilistic approach that extends affinity propagation to
combine the small ontological fragments into a collection of integrated,
consistent, and larger folksonomies. This is a challenging task because the
method must aggregate similar structures while avoiding structural
inconsistencies and handling noise. We validate the approach on a real-world
social media dataset, comprised of shallow personal hierarchies specified by
many individual users, collected from the photosharing website Flickr. Our
empirical results show that our proposed approach is able to construct deeper
and denser structures, compared to an approach using only the standard affinity
propagation algorithm. Additionally, the approach yields better overall
integration quality than a state-of-the-art approach based on incremental
relational clustering.



The semi-automatic or automatic synthesis of robot controller software is
both desirable and challenging. Synthesis of rather simple behaviors such as
collision avoidance by applying artificial evolution has been shown multiple
times. However, the difficulty of this synthesis increases heavily with
increasing complexity of the task that should be performed by the robot. We try
to tackle this problem of complexity with Artificial Homeostatic Hormone
Systems (AHHS), which provide both intrinsic, homeostatic processes and
(transient) intrinsic, variant behavior. By using AHHS the need for pre-defined
controller topologies or information about the field of application is
minimized. We investigate how the principle design of the controller and the
hormone network size affects the overall performance of the artificial
evolution (i.e., evolvability). This is done by comparing two variants of AHHS
that show different effects when mutated. We evolve a controller for a robot
built from five autonomous, cooperating modules. The desired behavior is a form
of gait resulting in fast locomotion by using the modules' main hinges.



A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where
observed data $\mathbf{Y}$ is modeled as a linear superposition, $\mathbf{G}$,
of a potentially infinite number of hidden factors, $\mathbf{X}$. The Indian
Buffet Process (IBP) is used as a prior on $\mathbf{G}$ to incorporate sparsity
and to allow the number of latent features to be inferred. The model's utility
for modeling gene expression data is investigated using randomly generated data
sets based on a known sparse connectivity matrix for E. Coli, and on three
biological data sets of increasing complexity.



Coecke, Sadrzadeh, and Clark (arXiv:1003.4394v1 [cs.CL]) developed a
compositional model of meaning for distributional semantics, in which each word
in a sentence has a meaning vector and the distributional meaning of the
sentence is a function of the tensor products of the word vectors. Abstractly
speaking, this function is the morphism corresponding to the grammatical
structure of the sentence in the category of finite dimensional vector spaces.
In this paper, we provide a concrete method for implementing this linear
meaning map, by constructing a corpus-based vector space for the type of
sentence. Our construction method is based on structured vector spaces whereby
meaning vectors of all sentences, regardless of their grammatical structure,
live in the same vector space. Our proposed sentence space is the tensor
product of two noun spaces, in which the basis vectors are pairs of words each
augmented with a grammatical role. This enables us to compare meanings of
sentences by simply taking the inner product of their vectors.



Suppose that multiple experts (or learning algorithms) provide us with
alternative Bayesian network (BN) structures over a domain, and that we are
interested in combining them into a single consensus BN structure.
Specifically, we are interested in that the consensus BN structure only
represents independences all the given BN structures agree upon and that it has
as few parameters associated as possible. In this paper, we prove that there
may exist several non-equivalent consensus BN structures and that finding one
of them is NP-hard. Thus, we decide to resort to heuristics to find an
approximated consensus BN structure. In this paper, we consider the heuristic
proposed in
\citep{MatzkevichandAbramson1992,MatzkevichandAbramson1993a,MatzkevichandAbramson1993b}.
This heuristic builds upon two algorithms, called Methods A and B, for
efficiently deriving the minimal directed independence map of a BN structure
relative to a given node ordering. Methods A and B are claimed to be correct
although no proof is provided (a proof is just sketched). In this paper, we
show that Methods A and B are not correct and propose a correction of them.



In a Role-Playing Game, finding optimal trajectories is one of the most
important tasks. In fact, the strategy decision system becomes a key component
of a game engine. Determining the way in which decisions are taken (online,
batch or simulated) and the consumed resources in decision making (e.g.
execution time, memory) will influence, in mayor degree, the game performance.
When classical search algorithms such as A* can be used, they are the very
first option. Nevertheless, such methods rely on precise and complete models of
the search space, and there are many interesting scenarios where their
application is not possible. Then, model free methods for sequential decision
making under uncertainty are the best choice. In this paper, we propose a
heuristic planning strategy to incorporate the ability of heuristic-search in
path-finding into a Dyna agent. The proposed Dyna-H algorithm, as A* does,
selects branches more likely to produce outcomes than other branches. Besides,
it has the advantages of being a model-free online reinforcement learning
algorithm. The proposal was evaluated against the one-step Q-Learning and
Dyna-Q algorithms obtaining excellent experimental results: Dyna-H
significantly overcomes both methods in all experiments. We suggest also, a
functional analogy between the proposed sampling from worst trajectories
heuristic and the role of dreams (e.g. nightmares) in human behavior.



Chaotic neural networks have received a great deal of attention these last
years. In this paper we establish a precise correspondence between the
so-called chaotic iterations and a particular class of artificial neural
networks: global recurrent multi-layer perceptrons. We show formally that it is
possible to make these iterations behave chaotically, as defined by Devaney,
and thus we obtain the first neural networks proven chaotic. Several neural
networks with different architectures are trained to exhibit a chaotical
behavior.



External information propagates in the cell mainly through signaling cascades
and transcriptional activation, allowing it to react to a wide spectrum of
environmental changes. High throughput experiments identify numerous molecular
components of such cascades that may, however, interact through unknown
partners. Some of them may be detected using data coming from the integration
of a protein-protein interaction network and mRNA expression profiles. This
inference problem can be mapped onto the problem of finding appropriate optimal
connected subgraphs of a network defined by these datasets. The optimization
procedure turns out to be computationally intractable in general. Here we
present a new distributed algorithm for this task, inspired from statistical
physics, and apply this scheme to alpha factor and drug perturbations data in
yeast. We identify the role of the COS8 protein, a member of a gene family of
previously unknown function, and validate the results by genetic experiments.
The algorithm we present is specially suited for very large datasets, can run
in parallel, and can be adapted to other problems in systems biology. On
renowned benchmarks it outperforms other algorithms in the field.



Different features have different relevance to a particular learning problem.
Some features are less relevant; while some very important. Instead of
selecting the most relevant features using feature selection, an algorithm can
be given this knowledge of feature importance based on expert opinion or prior
learning. Learning can be faster and more accurate if learners take feature
importance into account. Correlation aided Neural Networks (CANN) is presented
which is such an algorithm. CANN treats feature importance as the correlation
coefficient between the target attribute and the features. CANN modifies normal
feed-forward Neural Network to fit both correlation values and training data.
Empirical evaluation shows that CANN is faster and more accurate than applying
the two step approach of feature selection and then using normal learning
algorithms.



Hybrid learning methods use theoretical knowledge of a domain and a set of
classified examples to develop a method for classification. Methods that use
domain knowledge have been shown to perform better than inductive learners.
However, there is no general method to include domain knowledge into all
inductive learning algorithms as all hybrid methods are highly specialized for
a particular algorithm. We present an algorithm that will take domain knowledge
in the form of propositional rules, generate artificial examples from the rules
and also remove instances likely to be flawed. This enriched dataset then can
be used by any learning algorithm. Experimental results of different scenarios
are shown that demonstrate this method to be more effective than simple
inductive learning.



Recent research in multi-robot exploration and mapping has focused on
sampling environmental fields, which are typically modeled using the Gaussian
process (GP). Existing information-theoretic exploration strategies for
learning GP-based environmental field maps adopt the non-Markovian problem
structure and consequently scale poorly with the length of history of
observations. Hence, it becomes computationally impractical to use these
strategies for in situ, real-time active sampling. To ease this computational
burden, this paper presents a Markov-based approach to efficient
information-theoretic path planning for active sampling of GP-based fields. We
analyze the time complexity of solving the Markov-based path planning problem,
and demonstrate analytically that it scales better than that of deriving the
non-Markovian strategies with increasing length of planning horizon. For a
class of exploration tasks called the transect sampling task, we provide
theoretical guarantees on the active sampling performance of our Markov-based
policy, from which ideal environmental field conditions and sampling task
settings can be established to limit its performance degradation due to
violation of the Markov assumption. Empirical evaluation on real-world
temperature and plankton density field data shows that our Markov-based policy
can generally achieve active sampling performance comparable to that of the
widely-used non-Markovian greedy policies under less favorable realistic field
conditions and task settings while enjoying significant computational gain over
them.



Dynamical systems theory and complexity science provide powerful tools for
analysing artificial agents and robots. Furthermore, they have been recently
proposed also as a source of design principles and guidelines. Boolean networks
are a prominent example of complex dynamical systems and they have been shown
to effectively capture important phenomena in gene regulation. From an
engineering perspective, these models are very compelling, because they can
exhibit rich and complex behaviours, in spite of the compactness of their
description. In this paper, we propose the use of Boolean networks for
controlling robots' behaviour. The network is designed by means of an automatic
procedure based on stochastic local search techniques. We show that this
approach makes it possible to design a network which enables the robot to
accomplish a task that requires the capability of navigating the space using a
light stimulus, as well as the formation and use of an internal memory.



In this paper we present a new approach to solve the satisfiability problem
(SAT), based on boolean networks (BN). We define a mapping between a SAT
instance and a BN, and we solve SAT problem by simulating the BN dynamics. We
prove that BN fixed points correspond to the SAT solutions. The mapping
presented allows to develop a new class of algorithms to solve SAT. Moreover,
this new approach suggests new ways to combine symbolic and connectionist
computation and provides a general framework for local search algorithms.



The constraint satisfaction problem (CSP) is a general problem central to
computer science and artificial intelligence. Although the CSP is NP-hard in
general, considerable effort has been spent on identifying tractable
subclasses. The main two approaches consider structural properties
(restrictions on the hypergraph of constraint scopes) and relational properties
(restrictions on the language of constraint relations). Recently, some authors
have considered hybrid properties that restrict the constraint hypergraph and
the relations simultaneously.
  Our key contribution is the novel concept of a CSP pattern and classes of
problems defined by forbidden patterns (which can be viewed as forbidding
generic subproblems). We describe the theoretical framework which can be used
to reason about classes of problems defined by forbidden patterns. We show that
this framework generalises relational properties and allows us to capture known
hybrid tractable classes.
  Although we are not close to obtaining a dichotomy concerning the
tractability of general forbidden patterns, we are able to make some progress
in a special case: classes of problems that arise when we can only forbid
binary negative patterns (generic subproblems in which only inconsistent tuples
are specified). In this case we are able to characterise very large classes of
tractable and NP-hard forbidden patterns. This leaves the complexity of just
one case unresolved and we conjecture that this last case is tractable.



In a minimal binary constraint network, every tuple of a constraint relation
can be extended to a solution. The tractability or intractability of computing
a solution to such a minimal network was a long standing open question. Dechter
conjectured this computation problem to be NP-hard. We prove this conjecture.
We also prove a conjecture by Dechter and Pearl stating that for k\geq2 it is
NP-hard to decide whether a single constraint can be decomposed into an
equivalent k-ary constraint network. We show that this holds even in case of
bi-valued constraints where k\geq3, which proves another conjecture of Dechter
and Pearl. Finally, we establish the tractability frontier for this problem
with respect to the domain cardinality and the parameter k.



One of the hallmarks of biological organisms is their ability to integrate
disparate information sources to optimize their behavior in complex
environments. How this capability can be quantified and related to the
functional complexity of an organism remains a challenging problem, in
particular since organismal functional complexity is not well-defined. We
present here several candidate measures that quantify information and
integration, and study their dependence on fitness as an artificial agent
("animat") evolves over thousands of generations to solve a navigation task in
a simple, simulated environment. We compare the ability of these measures to
predict high fitness with more conventional information-theoretic processing
measures. As the animat adapts by increasing its "fit" to the world,
information integration and processing increase commensurately along the
evolutionary line of descent. We suggest that the correlation of fitness with
information integration and with processing measures implies that high fitness
requires both information processing as well as integration, but that
information integration may be a better measure when the task requires memory.
A correlation of measures of information integration (but also information
processing) and fitness strongly suggests that these measures reflect the
functional complexity of the animat, and that such measures can be used to
quantify functional complexity even in the absence of fitness data.



Dictionaries are inherently circular in nature. A given word is linked to a
set of alternative words (the definition) which in turn point to further
descendants. Iterating through definitions in this way, one typically finds
that definitions loop back upon themselves. The graph formed by such
definitional relations is our object of study. By eliminating those links which
are not in loops, we arrive at a core subgraph of highly connected nodes.
  We observe that definitional loops are conveniently classified by length,
with longer loops usually emerging from semantic misinterpretation. By breaking
the long loops in the graph of the dictionary, we arrive at a set of
disconnected clusters. We find that the words in these clusters constitute
semantic units, and moreover tend to have been introduced into the English
language at similar times, suggesting a possible mechanism for language
evolution.



Traditional machine-learned ranking systems for web search are often trained
to capture stationary relevance of documents to queries, which has limited
ability to track non-stationary user intention in a timely manner. In recency
search, for instance, the relevance of documents to a query on breaking news
often changes significantly over time, requiring effective adaptation to user
intention. In this paper, we focus on recency search and study a number of
algorithms to improve ranking results by leveraging user click feedback. Our
contributions are three-fold. First, we use real search sessions collected in a
random exploration bucket for \emph{reliable} offline evaluation of these
algorithms, which provides an unbiased comparison across algorithms without
online bucket tests. Second, we propose a re-ranking approach to improve search
results for recency queries using user clicks. Third, our empirical comparison
of a dozen algorithms on real-life search data suggests importance of a few
algorithmic choices in these applications, including generalization across
different query-document pairs, specialization to popular queries, and
real-time adaptation of user clicks.



The competitive MNIST handwritten digit recognition benchmark has a long
history of broken records since 1998. The most recent substantial improvement
by others dates back 7 years (error rate 0.4%) . Recently we were able to
significantly improve this result, using graphics cards to greatly speed up
training of simple but deep MLPs, which achieved 0.35%, outperforming all the
previous more complex methods. Here we report another substantial improvement:
0.31% obtained using a committee of MLPs.



Social computation, whether in the form of searches performed by swarms of
agents or collective predictions of markets, often supplies remarkably good
solutions to complex problems. In many examples, individuals trying to solve a
problem locally can aggregate their information and work together to arrive at
a superior global solution. This suggests that there may be general principles
of information aggregation and coordination that can transcend particular
applications. Here we show that the general structure of this problem can be
cast in terms of information theory and derive mathematical conditions that
lead to optimal multi-agent searches. Specifically, we illustrate the problem
in terms of local search algorithms for autonomous agents looking for the
spatial location of a stochastic source. We explore the types of search
problems, defined in terms of the statistical properties of the source and the
nature of measurements at each agent, for which coordination among multiple
searchers yields an advantage beyond that gained by having the same number of
independent searchers. We show that effective coordination corresponds to
synergy and that ineffective coordination corresponds to independence as
defined using information theory. We classify explicit types of sources in
terms of their potential for synergy. We show that sources that emit
uncorrelated signals provide no opportunity for synergetic coordination while
sources that emit signals that are correlated in some way, do allow for strong
synergy between searchers. These general considerations are crucial for
designing optimal algorithms for particular search problems in real world
settings.



Spatial search problems abound in the real world, from locating hidden
nuclear or chemical sources to finding skiers after an avalanche. We exemplify
the formalism and solution for spatial searches involving two agents that may
or may not choose to share information during a search. For certain classes of
tasks, sharing information between multiple searchers makes cooperative
searching advantageous. In some examples, agents are able to realize synergy by
aggregating information and moving based on local judgments about maximal
information gathering expectations. We also explore one- and two-dimensional
simplified situations analytically and numerically to provide a framework for
analyzing more complex problems. These general considerations provide a guide
for designing optimal algorithms for real-world search problems.



The paper proposes an approach to modeling users of large Web sites based on
combining different data sources: access logs and content of the accessed pages
are combined with semantic information about the Web pages, the users and the
accesses of the users to the Web site. The assumption is that we are dealing
with a large Web site providing content to a large number of users accessing
the site. The proposed approach represents each user by a set of features
derived from the different data sources, where some feature values may be
missing for some users. It further enables user modeling based on the provided
characteristics of the targeted user subset. The approach is evaluated on
real-world data where we compare performance of the automatic assignment of a
user to a predefined user segment when different data sources are used to
represent the users.



Understanding how users tailor their SPARQL queries is crucial when designing
query evaluation engines or fine-tuning RDF stores with performance in mind. In
this paper we analyze 3 million real-world SPARQL queries extracted from logs
of the DBPedia and SWDF public endpoints. We aim at finding which are the most
used language elements both from syntactical and structural perspectives,
paying special attention to triple patterns and joins, since they are indeed
some of the most expensive SPARQL operations at evaluation phase. We have
determined that most of the queries are simple and include few triple patterns
and joins, being Subject-Subject, Subject-Object and Object-Object the most
common join types. The graph patterns are usually star-shaped and despite
triple pattern chains exist, they are generally short.



Research shows that comment spamming (comments which are unsolicited,
unrelated, abusive, hateful, commercial advertisements etc) in online
discussion forums has become a common phenomenon in Web 2.0 applications and
there is a strong need to counter or combat comment spamming. We present a
method to automatically detect comment spammer in YouTube (largest and a
popular video sharing website) forums. The proposed technique is based on
mining comment activity log of a user and extracting patterns (such as time
interval between subsequent comments, presence of exactly same comment across
multiple unrelated videos) indicating spam behavior. We perform empirical
analysis on data crawled from YouTube and demonstrate that the proposed method
is effective for the task of comment spammer detection.



The Semantic Web initiative puts emphasis not primarily on putting data on
the Web, but rather on creating links in a way that both humans and machines
can explore the Web of data. When such users access the Web, they leave a trail
as Web servers maintain a history of requests. Web usage mining approaches have
been studied since the beginning of the Web given the log's huge potential for
purposes such as resource annotation, personalization, forecasting etc.
However, the impact of any such efforts has not really gone beyond generating
statistics detailing who, when, and how Web pages maintained by a Web server
were visited.



There are two distinct approaches to solving reinforcement learning problems,
namely, searching in value function space and searching in policy space.
Temporal difference methods and evolutionary algorithms are well-known examples
of these approaches. Kaelbling, Littman and Moore recently provided an
informative survey of temporal difference methods. This article focuses on the
application of evolutionary algorithms to the reinforcement learning problem,
emphasizing alternative policy representations, credit assignment methods, and
problem-specific genetic operators. Strengths and weaknesses of the
evolutionary approach to reinforcement learning are presented, along with a
survey of representative applications.



This paper introduces an elemental building block which combines Dictionary
Learning and Dimension Reduction (DRDL). We show how this foundational element
can be used to iteratively construct a Hierarchical Sparse Representation (HSR)
of a sensory stream. We compare our approach to existing models showing the
generality of our simple prescription. We then perform preliminary experiments
using this framework, illustrating with the example of an object recognition
task using standard datasets. This work introduces the very first steps towards
an integrated framework for designing and analyzing various computational tasks
from learning to attention to action. The ultimate goal is building a
mathematically rigorous, integrated theory of intelligence.



In this paper, a possibilistic disjunctive logic programming approach for
modeling uncertain, incomplete and inconsistent information is defined. This
approach introduces the use of possibilistic disjunctive clauses which are able
to capture incomplete information and incomplete states of a knowledge base at
the same time.
  By considering a possibilistic logic program as a possibilistic logic theory,
a construction of a possibilistic logic programming semantic based on answer
sets and the proof theory of possibilistic logic is defined. It shows that this
possibilistic semantics for disjunctive logic programs can be characterized by
a fixed-point operator. It is also shown that the suggested possibilistic
semantics can be computed by a resolution algorithm and the consideration of
optimal refutations from a possibilistic logic theory.
  In order to manage inconsistent possibilistic logic programs, a preference
criterion between inconsistent possibilistic models is defined; in addition,
the approach of cuts for restoring consistency of an inconsistent possibilistic
knowledge base is adopted. The approach is illustrated in a medical scenario.



The structure representation of data distribution plays an important role in
understanding the underlying mechanism of generating data. In this paper, we
propose nearest prime simplicial complex approaches (NSC) by utilizing
persistent homology to capture such structures. Assuming that each class is
represented with a prime simplicial complex, we classify unlabeled samples
based on the nearest projection distances from the samples to the simplicial
complexes. We also extend the extrapolation ability of these complexes with a
projection constraint term. Experiments in simulated and practical datasets
indicate that compared with several published algorithms, the proposed NSC
approaches achieve promising performance without losing the structure
representation.



The use of L1 regularisation for sparse learning has generated immense
research interest, with successful application in such diverse areas as signal
acquisition, image coding, genomics and collaborative filtering. While existing
work highlights the many advantages of L1 methods, in this paper we find that
L1 regularisation often dramatically underperforms in terms of predictive
performance when compared with other methods for inferring sparsity. We focus
on unsupervised latent variable models, and develop L1 minimising factor
models, Bayesian variants of "L1", and Bayesian models with a stronger L0-like
sparsity induced through spike-and-slab distributions. These spike-and-slab
Bayesian factor models encourage sparsity while accounting for uncertainty in a
principled manner and avoiding unnecessary shrinkage of non-zero values. We
demonstrate on a number of data sets that in practice spike-and-slab Bayesian
methods outperform L1 minimisation, even on a computational budget. We thus
highlight the need to re-assess the wide use of L1 methods in sparsity-reliant
applications, particularly when we care about generalising to previously unseen
data, and provide an alternative that, over many varying conditions, provides
improved generalisation performance.



We address the problem of learning in an online setting where the learner
repeatedly observes features, selects among a set of actions, and receives
reward for the action taken. We provide the first efficient algorithm with an
optimal regret. Our algorithm uses a cost sensitive classification learner as
an oracle and has a running time $\mathrm{polylog}(N)$, where $N$ is the number
of classification rules among which the oracle might choose. This is
exponentially faster than all previous algorithms that achieve optimal regret
in this setting. Our formulation also enables us to create an algorithm with
regret that is additive rather than multiplicative in feedback delay as in all
previous work.



Scoring rules for eliciting expert predictions of random variables are
usually developed assuming that experts derive utility only from the quality of
their predictions (e.g., score awarded by the rule, or payoff in a prediction
market). We study a more realistic setting in which (a) the principal is a
decision maker and will take a decision based on the expert's prediction; and
(b) the expert has an inherent interest in the decision. For example, in a
corporate decision market, the expert may derive different levels of utility
from the actions taken by her manager. As a consequence the expert will usually
have an incentive to misreport her forecast to influence the choice of the
decision maker if typical scoring rules are used. We develop a general model
for this setting and introduce the concept of a compensation rule. When
combined with the expert's inherent utility for decisions, a compensation rule
induces a net scoring rule that behaves like a normal scoring rule. Assuming
full knowledge of expert utility, we provide a complete characterization of all
(strictly) proper compensation rules. We then analyze the situation where the
expert's utility function is not fully known to the decision maker. We show
bounds on: (a) expert incentive to misreport; (b) the degree to which an expert
will misreport; and (c) decision maker loss in utility due to such uncertainty.
These bounds depend in natural ways on the degree of uncertainty, the local
degree of convexity of net scoring function, and natural properties of the
decision maker's utility function. They also suggest optimization procedures
for the design of compensation rules. Finally, we briefly discuss the use of
compensation rules as market scoring rules for self-interested experts in a
prediction market.



In this article, a survey of several important equilibrium concepts for
decentralized networks is presented. The term decentralized is used here to
refer to scenarios where decisions (e.g., choosing a power allocation policy)
are taken autonomously by devices interacting with each other (e.g., through
mutual interference). The iterative long-term interaction is characterized by
stable points of the wireless network called equilibria. The interest in these
equilibria stems from the relevance of network stability and the fact that they
can be achieved by letting radio devices to repeatedly interact over time. To
achieve these equilibria, several learning techniques, namely, the best
response dynamics, fictitious play, smoothed fictitious play, reinforcement
learning algorithms, and regret matching, are discussed in terms of information
requirements and convergence properties. Most of the notions introduced here,
for both equilibria and learning schemes, are illustrated by a simple case
study, namely, an interference channel with two transmitter-receiver pairs.



We propose a purely extensional semantics for higher-order logic programming.
In this semantics program predicates denote sets of ordered tuples, and two
predicates are equal iff they are equal as sets. Moreover, every program has a
unique minimum Herbrand model which is the greatest lower bound of all Herbrand
models of the program and the least fixed-point of an immediate consequence
operator. We also propose an SLD-resolution proof procedure which is proven
sound and complete with respect to the minimum model semantics. In other words,
we provide a purely extensional theoretical framework for higher-order logic
programming which generalizes the familiar theory of classical (first-order)
logic programming.



A sound and complete embedding of conditional logics into classical
higher-order logic is presented. This embedding enables the application of
off-the-shelf higher-order automated theorem provers and model finders for
reasoning within and about conditional logics.



We consider the setting of ontological database access, where an Abox is
given in form of a relational database D and where a Boolean conjunctive query
q has to be evaluated against D modulo a Tbox T formulated in DL-Lite or Linear
Datalog+/-. It is well-known that (T,q) can be rewritten into an equivalent
nonrecursive Datalog program P that can be directly evaluated over D. However,
for Linear Datalog? or for DL-Lite versions that allow for role inclusion, the
rewriting methods described so far result in a nonrecursive Datalog program P
of size exponential in the joint size of T and q. This gives rise to the
interesting question of whether such a rewriting necessarily needs to be of
exponential size. In this paper we show that it is actually possible to
translate (T,q) into a polynomially sized equivalent nonrecursive Datalog
program P.



Using the Hilbert-Bernays account as a spring-board, we first define four
ways in which two objects can be discerned from one another, using the
non-logical vocabulary of the language concerned. (These definitions are based
on definitions made by Quine and Saunders.) Because of our use of the
Hilbert-Bernays account, these definitions are in terms of the syntax of the
language. But we also relate our definitions to the idea of permutations on the
domain of quantification, and their being symmetries. These relations turn out
to be subtle---some natural conjectures about them are false. We will see in
particular that the idea of symmetry meshes with a species of indiscernibility
that we will call `absolute indiscernibility'. We then report all the logical
implications between our four kinds of discernibility. We use these four kinds
as a resource for stating four metaphysical theses about identity. Three of
these theses articulate two traditional philosophical themes: viz. the
principle of the identity of indiscernibles (which will come in two versions),
and haecceitism. The fourth is recent. Its most notable feature is that it
makes diversity (i.e. non-identity) weaker than what we will call individuality
(being an individual): two objects can be distinct but not individuals. For
this reason, it has been advocated both for quantum particles and for spacetime
points. Finally, we locate this fourth metaphysical thesis in a broader
position, which we call structuralism. We conclude with a discussion of the
semantics suitable for a structuralist, with particular reference to physical
theories as well as elementary model theory.



Refinement is a powerful mechanism for mastering the complexities that arise
when formally modelling systems. Refinement also brings with it additional
proof obligations -- requiring a developer to discover properties relating to
their design decisions. With the goal of reducing this burden, we have
investigated how a general purpose theory formation tool, HR, can be used to
automate the discovery of such properties within the context of Event-B. Here
we develop a heuristic approach to the automatic discovery of invariants and
report upon a series of experiments that we undertook in order to evaluate our
approach. The set of heuristics developed provides systematic guidance in
tailoring HR for a given Event-B development. These heuristics are based upon
proof-failure analysis, and have given rise to some promising results.



Given a point cloud, we consider inferring kinematic models of 3D articulated
objects such as boxes for the purpose of manipulating them. While previous work
has shown how to extract a planar kinematic model (often represented as a
linear chain), such planar models do not apply to 3D objects that are composed
of segments often linked to the other segments in cyclic configurations. We
present an approach for building a model that captures the relation between the
input point cloud features and the object segment as well as the relation
between the neighboring object segments. We use a conditional random field that
allows us to model the dependencies between different segments of the object.
We test our approach on inferring the kinematic structure from partial and
noisy point cloud data for a wide variety of boxes including cake boxes, pizza
boxes, and cardboard cartons of several sizes. The inferred structure enables
our robot to successfully close these boxes by manipulating the flaps.



We propose an online form of the cake cutting problem. This models situations
where agents arrive and depart during the process of dividing a resource. We
show that well known fair division procedures like cut-and-choose and the
Dubins-Spanier moving knife procedure can be adapted to apply to such online
problems. We propose some fairness properties that online cake cutting
procedures can possess like online forms of proportionality and envy-freeness.
We also consider the impact of collusion between agents. Finally, we study
theoretically and empirically the competitive ratio of these online cake
cutting procedures. Based on its resistance to collusion, and its good
performance in practice, our results favour the online version of the
cut-and-choose procedure over the online version of the moving knife procedure.



We present a method for estimating pose information from a single depth image
given an arbitrary kinematic structure without prior training. For an arbitrary
skeleton and depth image, an evolutionary algorithm is used to find the optimal
kinematic configuration to explain the observed image. Results show that our
approach can correctly estimate poses of 39 and 78 degree-of-freedom models
from a single depth image, even in cases of significant self-occlusion.



We consider manipulation problems when the manipulator only has partial
information about the votes of the nonmanipulators. Such partial information is
described by an information set, which is the set of profiles of the
nonmanipulators that are indistinguishable to the manipulator. Given such an
information set, a dominating manipulation is a non-truthful vote that the
manipulator can cast which makes the winner at least as preferable (and
sometimes more preferable) as the winner when the manipulator votes truthfully.
When the manipulator has full information, computing whether or not there
exists a dominating manipulation is in P for many common voting rules (by known
results). We show that when the manipulator has no information, there is no
dominating manipulation for many common voting rules. When the manipulator's
information is represented by partial orders and only a small portion of the
preferences are unknown, computing a dominating manipulation is NP-hard for
many common voting rules. Our results thus throw light on whether we can
prevent strategic behavior by limiting information about the votes of other
voters.



We discuss the problem in which an autonomous vehicle must classify an object
based on multiple views. We focus on the active classification setting, where
the vehicle controls which views to select to best perform the classification.
The problem is formulated as an extension to Bayesian active learning, and we
show connections to recent theoretical guarantees in this area. We formally
analyze the benefit of acting adaptively as new information becomes available.
The analysis leads to a probabilistic algorithm for determining the best views
to observe based on information theoretic costs. We validate our approach in
two ways, both related to underwater inspection: 3D polyhedra recognition in
synthetic depth maps and ship hull inspection with imaging sonar. These tasks
encompass both the planning and recognition aspects of the active
classification problem. The results demonstrate that actively planning for
informative views can reduce the number of necessary views by up to 80% when
compared to passive methods.



The relation between self awareness and intelligence is an open problem these
days. Despite the fact that self awarness is usually related to Emotional
Intelligence, this is not the case here. The problem described in this paper is
how to model an agent which knows (Cognitive) Binary Logic and which is also
able to pass (without any mistake) a certain family of Turing Tests designed to
verify its knowledge and its discourse about the modal states of truth
corresponding to well-formed formulae within the language of Propositional
Binary Logic.



The AdaBoost algorithm was designed to combine many "weak" hypotheses that
perform slightly better than random guessing into a "strong" hypothesis that
has very low error. We study the rate at which AdaBoost iteratively converges
to the minimum of the "exponential loss." Unlike previous work, our proofs do
not require a weak-learning assumption, nor do they require that minimizers of
the exponential loss are finite. Our first result shows that at iteration $t$,
the exponential loss of AdaBoost's computed parameter vector will be at most
$\epsilon$ more than that of any parameter vector of $\ell_1$-norm bounded by
$B$ in a number of rounds that is at most a polynomial in $B$ and $1/\epsilon$.
We also provide lower bounds showing that a polynomial dependence on these
parameters is necessary. Our second result is that within $C/\epsilon$
iterations, AdaBoost achieves a value of the exponential loss that is at most
$\epsilon$ more than the best possible value, where $C$ depends on the dataset.
We show that this dependence of the rate on $\epsilon$ is optimal up to
constant factors, i.e., at least $\Omega(1/\epsilon)$ rounds are necessary to
achieve within $\epsilon$ of the optimal exponential loss.



Kernel methods are among the most popular techniques in machine learning.
From a frequentist/discriminative perspective they play a central role in
regularization theory as they provide a natural choice for the hypotheses space
and the regularization functional through the notion of reproducing kernel
Hilbert spaces. From a Bayesian/generative perspective they are the key in the
context of Gaussian processes, where the kernel function is also known as the
covariance function. Traditionally, kernel methods have been used in supervised
learning problem with scalar outputs and indeed there has been a considerable
amount of work devoted to designing and learning kernels. More recently there
has been an increasing interest in methods that deal with multiple outputs,
motivated partly by frameworks like multitask learning. In this paper, we
review different methods to design or learn valid kernel functions for multiple
outputs, paying particular attention to the connection between probabilistic
and functional methods.



Using the probability theory-based approach, this paper reveals the
equivalence of an arbitrary NP-complete problem to a problem of checking
whether a level set of a specifically constructed harmonic cost function (with
all diagonal entries of its Hessian matrix equal to zero) intersects with a
unit hypercube in many-dimensional Euclidean space. This connection suggests
the possibility that methods of continuous mathematics can provide crucial
insights into the most intriguing open questions in modern complexity theory.



Induction is the process by which we obtain predictive laws or theories or
models of the world. We consider the structural aspect of induction. We answer
the question as to whether we can find a finite and minmalistic set of
operations on structural elements in terms of which any theory can be
expressed. We identify abstraction (grouping similar entities) and
super-structuring (combining topologically e.g., spatio-temporally close
entities) as the essential structural operations in the induction process. We
show that only two more structural operations, namely, reverse abstraction and
reverse super-structuring (the duals of abstraction and super-structuring
respectively) suffice in order to exploit the full power of Turing-equivalent
generative grammars in induction. We explore the implications of this theorem
with respect to the nature of hidden variables, radical positivism and the
2-century old claim of David Hume about the principles of connexion among
ideas.



We represent agents as sets of strings. Each string encodes a potential
interaction with another agent or environment. We represent the total set of
dynamics between two agents as the intersection of their respective strings, we
prove complexity properties of player interactions using Algorithmic
Information Theory. We show how the proposed construction is compatible with
Universal Artificial Intelligence, in that the AIXI model can be seen as
universal with respect to interaction.



We propose to model the text classification process as a sequential decision
process. In this process, an agent learns to classify documents into topics
while reading the document sentences sequentially and learns to stop as soon as
enough information was read for deciding. The proposed algorithm is based on a
modelisation of Text Classification as a Markov Decision Process and learns by
using Reinforcement Learning. Experiments on four different classical
mono-label corpora show that the proposed approach performs comparably to
classical SVM approaches for large training sets, and better for small training
sets. In addition, the model automatically adapts its reading process to the
quantity of training information provided.



In this paper we propose task swapping networks for task reassignments by
using task swappings in distributed systems. Some classes of task reassignments
are achieved by using iterative local task swappings between software agents in
distributed systems. We use group-theoretic methods to find a minimum-length
sequence of adjacent task swappings needed from a source task assignment to a
target task assignment in a task swapping network of several well-known
topologies.



Research in the field of Artificial Intelligence is continually progressing
to simulate the human knowledge into automated intelligent knowledge base,
which can encode and retrieve knowledge efficiently along with the capability
of being is consistent and scalable at all times. However, there is no system
at hand that can match the diversified abilities of human knowledge base. In
this position paper, we put forward a theoretical model of a different system
that intends to integrate pieces of knowledge, Informledge System (ILS). ILS
would encode the knowledge, by virtue of knowledge units linked across
diversified domains. The proposed ILS comprises of autonomous knowledge units
termed as Knowledge Network Node (KNN), which would help in efficient
cross-linking of knowledge units to encode fresh knowledge. These links are
reasoned and inferred by the Parser and Link Manager, which are part of KNN.



We reminisce and discuss applications of algorithmic probability to a wide
range of problems in artificial intelligence, philosophy and technological
society. We propose that Solomonoff has effectively axiomatized the field of
artificial intelligence, therefore establishing it as a rigorous scientific
discipline. We also relate to our own work in incremental machine learning and
philosophy of complexity.



The aim of this paper is to present the principles and results about
case-based reasoning adapted to real- time interactive simulations, more
precisely concerning retrieval mechanisms. The article begins by introducing
the constraints involved in interactive multiagent-based simulations. The
second section pre- sents a framework stemming from case-based reasoning by
autonomous agents. Each agent uses a case base of local situations and, from
this base, it can choose an action in order to interact with other auton- omous
agents or users' avatars. We illustrate this framework with an example
dedicated to the study of dynamic situations in football. We then go on to
address the difficulties of conducting such simulations in real-time and
propose a model for case and for case base. Using generic agents and adequate
case base structure associated with a dedicated recall algorithm, we improve
retrieval performance under time pressure compared to classic CBR techniques.
We present some results relating to the performance of this solution. The
article concludes by outlining future development of our project.



We develop a new approach that computes approximate equilibrium strategies in
Jotto, a popular word game. Jotto is an extremely large two-player game of
imperfect information; its game tree has many orders of magnitude more states
than games previously studied, including no-limit Texas hold 'em. To address
the fact that the game is so large, we propose a novel strategy representation
called oracular form, in which we do not explicitly represent a strategy, but
rather appeal to an oracle that quickly outputs a sample move from the
strategy's distribution. Our overall approach is based on an extension of the
fictitious play algorithm to this oracular setting. We demonstrate the
superiority of our computed strategies over the strategies computed by a
benchmark algorithm, both in terms of head-to-head and worst-case performance.



We present a rigorous mathematical framework for analyzing dynamics of a
broad class of Boolean network models. We use this framework to provide the
first formal proof of many of the standard critical transition results in
Boolean network analysis, and offer analogous characterizations for novel
classes of random Boolean networks. We precisely connect the short-run dynamic
behavior of a Boolean network to the average influence of the transfer
functions. We show that some of the assumptions traditionally made in the more
common mean-field analysis of Boolean networks do not hold in general.
  For example, we offer some evidence that imbalance, or expected internal
inhomogeneity, of transfer functions is a crucial feature that tends to drive
quiescent behavior far more strongly than previously observed.



Harmonic theory provides a mathematical framework to describe the structure,
behavior, evolution and emergence of harmonic systems. A harmonic system is
context aware, contains elements that manifest characteristics either
collaboratively or independently according to system's expression and can
interact with its environment. This theory provides a fresh way to analyze
emergence and collaboration of "ad-hoc" and complex systems.



It has been argued that analogy is the core of cognition. In AI research,
algorithms for analogy are often limited by the need for hand-coded high-level
representations as input. An alternative approach is to use high-level
perception, in which high-level representations are automatically generated
from raw data. Analogy perception is the process of recognizing analogies using
high-level perception. We present PairClass, an algorithm for analogy
perception that recognizes lexical proportional analogies using representations
that are automatically generated from a large corpus of raw textual data. A
proportional analogy is an analogy of the form A:B::C:D, meaning "A is to B as
C is to D". A lexical proportional analogy is a proportional analogy with
words, such as carpenter:wood::mason:stone. PairClass represents the semantic
relations between two words using a high-dimensional feature vector, in which
the elements are based on frequencies of patterns in the corpus. PairClass
recognizes analogies by applying standard supervised machine learning
techniques to the feature vectors. We show how seven different tests of word
comprehension can be framed as problems of analogy perception and we then apply
PairClass to the seven resulting sets of analogy perception problems. We
achieve competitive results on all seven tests. This is the first time a
uniform approach has handled such a range of tests of word comprehension.



Many real world domains require the representation of a measure of
uncertainty. The most common such representation is probability, and the
combination of probability with logic programs has given rise to the field of
Probabilistic Logic Programming (PLP), leading to languages such as the
Independent Choice Logic, Logic Programs with Annotated Disjunctions (LPADs),
Problog, PRISM and others. These languages share a similar distribution
semantics, and methods have been devised to translate programs between these
languages. The complexity of computing the probability of queries to these
general PLP programs is very high due to the need to combine the probabilities
of explanations that may not be exclusive. As one alternative, the PRISM system
reduces the complexity of query answering by restricting the form of programs
it can evaluate. As an entirely different alternative, Possibilistic Logic
Programs adopt a simpler metric of uncertainty than probability. Each of these
approaches -- general PLP, restricted PLP, and Possibilistic Logic Programming
-- can be useful in different domains depending on the form of uncertainty to
be represented, on the form of programs needed to model problems, and on the
scale of the problems to be solved. In this paper, we show how the PITA system,
which originally supported the general PLP language of LPADs, can also
efficiently support restricted PLP and Possibilistic Logic Programs. PITA
relies on tabling with answer subsumption and consists of a transformation
along with an API for library functions that interface with answer subsumption.



In this paper we propose a use-case-driven iterative design methodology for
normative frameworks, also called virtual institutions, which are used to
govern open systems. Our computational model represents the normative framework
as a logic program under answer set semantics (ASP). By means of an inductive
logic programming approach, implemented using ASP, it is possible to synthesise
new rules and revise the existing ones. The learning mechanism is guided by the
designer who describes the desired properties of the framework through use
cases, comprising (i) event traces that capture possible scenarios, and (ii) a
state that describes the desired outcome. The learning process then proposes
additional rules, or changes to current rules, to satisfy the constraints
expressed in the use cases. Thus, the contribution of this paper is a process
for the elaboration and revision of a normative framework by means of a
semi-automatic and iterative process driven from specifications of
(un)desirable behaviour. The process integrates a novel and general methodology
for theory revision based on ASP.



We present a new system for simultaneous estimation of keys, chords, and bass
notes from music audio. It makes use of a novel chromagram representation of
audio that takes perception of loudness into account. Furthermore, it is fully
based on machine learning (instead of expert knowledge), such that it is
potentially applicable to a wider range of genres as long as training data is
available. As compared to other models, the proposed system is fast and memory
efficient, while achieving state-of-the-art performance.



High dimensional time series are endemic in applications of machine learning
such as robotics (sensor data), computational biology (gene expression data),
vision (video sequences) and graphics (motion capture data). Practical
nonlinear probabilistic approaches to this data are required. In this paper we
introduce the variational Gaussian process dynamical system. Our work builds on
recent variational approximations for Gaussian process latent variable models
to allow for nonlinear dimensionality reduction simultaneously with learning a
dynamical prior in the latent space. The approach also allows for the
appropriate dimensionality of the latent space to be automatically determined.
We demonstrate the model on a human motion capture data set and a series of
high resolution video sequences.



Learning to operate a vehicle is generally accomplished by forming a new
cognitive map between the body motions and extrapersonal space. Here, we
consider the challenge of remapping movement-to-space representations in
survivors of spinal cord injury, for the control of powered wheelchairs. Our
goal is to facilitate this remapping by developing interfaces between residual
body motions and navigational commands that exploit the degrees of freedom that
disabled individuals are most capable to coordinate. We present a new framework
for allowing spinal cord injured persons to control powered wheelchairs through
signals derived from their residual mobility. The main novelty of this approach
lies in substituting the more common joystick controllers of powered
wheelchairs with a sensor shirt. This allows the whole upper body of the user
to operate as an adaptive joystick. Considerations about learning and risks
have lead us to develop a safe testing environment in 3D Virtual Reality. A
Personal Augmented Reality Immersive System (PARIS) allows us to analyse
learning skills and provide users with an adequate training to control a
simulated wheelchair through the signals generated by body motions in a safe
environment. We provide a description of the basic theory, of the development
phases and of the operation of the complete system. We also present preliminary
results illustrating the processing of the data and supporting of the
feasibility of this approach.



A possibly immortal agent tries to maximise its summed discounted rewards
over time, where discounting is used to avoid infinite utilities and encourage
the agent to value current rewards more than future ones. Some commonly used
discount functions lead to time-inconsistent behavior where the agent changes
its plan over time. These inconsistencies can lead to very poor behavior. We
generalise the usual discounted utility model to one where the discount
function changes with the age of the agent. We then give a simple
characterisation of time-(in)consistent discount functions and show the
existence of a rational policy for an agent that knows its discount function is
time-inconsistent.



Specifying and implementing flexible human-computer dialogs, such as those
used in kiosks and smart phone apps, is challenging because of the numerous and
varied directions in which each user might steer a dialog. The objective of
this research is to improve dialog specification and implementation. To do so
we enriched a notation based on concepts from programming languages, especially
partial evaluation, for specifying a variety of unsolicited reporting,
mixed-initiative dialogs in a concise representation that serves as a design
for dialog implementation. We also built a dialog mining system that extracts a
specification in this notation from requirements. To demonstrate that such a
specification provides a design for dialog implementation, we built a system
that automatically generates an implementation of the dialog, called a stager,
from it. These two components constitute a dialog modeling toolkit that
automates dialog specification and implementation. These results provide a
proof of concept and demonstrate the study of dialog specification and
implementation from a programming languages perspective. The ubiquity of
dialogs in domains such as travel, education, and health care combined with the
demand for smart phone apps provide a landscape for further investigation of
these results.



Fingertips detection has been used in many applications, and it is very
popular and commonly used in the area of Human Computer Interaction these days.
This paper presents a novel time efficient method that will lead to fingertip
detection after cropping the irrelevant parts of input image. Binary silhouette
of the input image is generated using HSV color space based skin filter and
hand cropping done based on histogram of the hand image. The cropped image will
be used to figure out the fingertips.



For the general problem of minimizing a convex function over a compact convex
domain, we will investigate a simple iterative approximation algorithm based on
the method by Frank & Wolfe 1956, that does not need projection steps in order
to stay inside the optimization domain. Instead of a projection step, the
linearized problem defined by a current subgradient is solved, which gives a
step direction that will naturally stay in the domain. Our framework
generalizes the sparse greedy algorithm of Frank & Wolfe and its primal-dual
analysis by Clarkson 2010 (and the low-rank SDP approach by Hazan 2008) to
arbitrary convex domains. We give a convergence proof guaranteeing
{\epsilon}-small duality gap after O(1/{\epsilon}) iterations.
  The method allows us to understand the sparsity of approximate solutions for
any l1-regularized convex optimization problem (and for optimization over the
simplex), expressed as a function of the approximation quality. We obtain
matching upper and lower bounds of {\Theta}(1/{\epsilon}) for the sparsity for
l1-problems. The same bounds apply to low-rank semidefinite optimization with
bounded trace, showing that rank O(1/{\epsilon}) is best possible here as well.
As another application, we obtain sparse matrices of O(1/{\epsilon}) non-zero
entries as {\epsilon}-approximate solutions when optimizing any convex function
over a class of diagonally dominant symmetric matrices.
  We show that our proposed first-order method also applies to nuclear norm and
max-norm matrix optimization problems. For nuclear norm regularized
optimization, such as matrix completion and low-rank recovery, we demonstrate
the practical efficiency and scalability of our algorithm for large matrix
problems, as e.g. the Netflix dataset. For general convex optimization over
bounded matrix max-norm, our algorithm is the first with a convergence
guarantee, to the best of our knowledge.



Crowdsourcing websites (e.g. Yahoo! Answers, Amazon Mechanical Turk, and
etc.) emerged in recent years that allow requesters from all around the world
to post tasks and seek help from an equally global pool of workers. However,
intrinsic incentive problems reside in crowdsourcing applications as workers
and requester are selfish and aim to strategically maximize their own benefit.
In this paper, we propose to provide incentives for workers to exert effort
using a novel game-theoretic model based on repeated games. As there is always
a gap in the social welfare between the non-cooperative equilibria emerging
when workers pursue their self-interests and the desirable Pareto efficient
outcome, we propose a novel class of incentive protocols based on social norms
which integrates reputation mechanisms into the existing pricing schemes
currently implemented on crowdsourcing websites, in order to improve the
performance of the non-cooperative equilibria emerging in such applications. We
first formulate the exchanges on a crowdsourcing website as a two-sided market
where requesters and workers are matched and play gift-giving games repeatedly.
Subsequently, we study the protocol designer's problem of finding an optimal
and sustainable (equilibrium) protocol which achieves the highest social
welfare for that website. We prove that the proposed incentives protocol can
make the website operate close to Pareto efficiency. Moreover, we also examine
an alternative scenario, where the protocol designer aims at maximizing the
revenue of the website and evaluate the performance of the optimal protocol.



Given a set of several inputs into a system (e.g., independent variables
characterizing stimuli) and a set of several stochastically non-independent
outputs (e.g., random variables describing different aspects of responses), how
can one determine, for each of the outputs, which of the inputs it is
influenced by? The problem has applications ranging from modeling pairwise
comparisons to reconstructing mental processing architectures to conjoint
testing. A necessary and sufficient condition for a given pattern of selective
influences is provided by the Joint Distribution Criterion, according to which
the problem of "what influences what" is equivalent to that of the existence of
a joint distribution for a certain set of random variables. For inputs and
outputs with finite sets of values this criterion translates into a test of
consistency of a certain system of linear equations and inequalities (Linear
Feasibility Test) which can be performed by means of linear programming. The
Joint Distribution Criterion also leads to a metatheoretical principle for
generating a broad class of necessary conditions (tests) for diagrams of
selective influences. Among them is the class of distance-type tests based on
the observation that certain functionals on jointly distributed random
variables satisfy triangle inequality.



There is little research concerning comparisons and combination of System
Dynamics Simulation (SDS) and Agent Based Simulation (ABS). ABS is a paradigm
used in many levels of abstraction, including those levels covered by SDS. We
believe that the establishment of frameworks for the choice between these two
simulation approaches would contribute to the simulation research. Hence, our
work aims for the establishment of directions for the choice between SDS and
ABS approaches for immune system-related problems. Previously, we compared the
use of ABS and SDS for modelling agents' behaviour in an environment with
nomovement or interactions between these agents. We concluded that for these
types of agents it is preferable to use SDS, as it takes up less computational
resources and produces the same results as those obtained by the ABS model. In
order to move this research forward, our next research question is: if we
introduce interactions between these agents will SDS still be the most
appropriate paradigm to be used? To answer this question for immune system
simulation problems, we will use, as case studies, models involving
interactions between tumour cells and immune effector cells. Experiments show
that there are cases where SDS and ABS can not be used interchangeably, and
therefore, their comparison is not straightforward.



Multi-step ahead forecasting is still an open challenge in time series
forecasting. Several approaches that deal with this complex problem have been
proposed in the literature but an extensive comparison on a large number of
tasks is still missing. This paper aims to fill this gap by reviewing existing
strategies for multi-step ahead forecasting and comparing them in theoretical
and practical terms. To attain such an objective, we performed a large scale
comparison of these different strategies using a large experimental benchmark
(namely the 111 series from the NN5 forecasting competition). In addition, we
considered the effects of deseasonalization, input variable selection, and
forecast combination on these strategies and on multi-step ahead forecasting at
large. The following three findings appear to be consistently supported by the
experimental results: Multiple-Output strategies are the best performing
approaches, deseasonalization leads to uniformly improved forecast accuracy,
and input selection is more effective when performed in conjunction with
deseasonalization.



For some computational problems (e.g., product configuration, planning,
diagnosis, query answering, phylogeny reconstruction) computing a set of
similar/diverse solutions may be desirable for better decision-making. With
this motivation, we studied several decision/optimization versions of this
problem in the context of Answer Set Programming (ASP), analyzed their
computational complexity, and introduced offline/online methods to compute
similar/diverse solutions of such computational problems with respect to a
given distance function. All these methods rely on the idea of computing
solutions to a problem by means of finding the answer sets for an ASP program
that describes the problem. The offline methods compute all solutions in
advance using the ASP formulation of the problem with an ASP solver, like
Clasp, and then identify similar/diverse solutions using clustering methods.
The online methods compute similar/diverse solutions following one of the three
approaches: by reformulating the ASP representation of the problem to compute
similar/diverse solutions at once using an ASP solver; by computing
similar/diverse solutions iteratively (one after other) using an ASP solver; by
modifying the search algorithm of an ASP solver to compute similar/diverse
solutions incrementally. We modified Clasp to implement the last online method
and called it Clasp-NK. In the first two online methods, the given distance
function is represented in ASP; in the last one it is implemented in C++. We
showed the applicability and the effectiveness of these methods on
reconstruction of similar/diverse phylogenies for Indo-European languages, and
on several planning problems in Blocks World. We observed that in terms of
computational efficiency the last online method outperforms the others; also it
allows us to compute similar/diverse solutions when the distance function
cannot be represented in ASP.



In this work we introduce a mixture of GPs to address the data association
problem, i.e. to label a group of observations according to the sources that
generated them. Unlike several previously proposed GP mixtures, the novel
mixture has the distinct characteristic of using no gating function to
determine the association of samples and mixture components. Instead, all the
GPs in the mixture are global and samples are clustered following
"trajectories" across input space. We use a non-standard variational Bayesian
algorithm to efficiently recover sample labels and learn the hyperparameters.
We show how multi-object tracking problems can be disambiguated and also
explore the characteristics of the model in traditional regression settings.



We present a system capable of automatically solving combinatorial logic
puzzles given in (simplified) English. It involves translating the English
descriptions of the puzzles into answer set programming(ASP) and using ASP
solvers to provide solutions of the puzzles. To translate the descriptions, we
use a lambda-calculus based approach using Probabilistic Combinatorial
Categorial Grammars (PCCG) where the meanings of words are associated with
parameters to be able to distinguish between multiple meanings of the same
word. Meaning of many words and the parameters are learned. The puzzles are
represented in ASP using an ontology which is applicable to a large set of
logic puzzles.



Genetic algorithms are considered as an original way to solve problems,
probably because of their generality and of their "blind" nature. But GAs are
also unusual since the features of many implementations (among all that could
be thought of) are principally led by the biological metaphor, while efficiency
measurements intervene only afterwards. We propose here to examine the
relevance of these biomimetic aspects, by pointing out some fundamental
similarities and divergences between GAs and the genome of living beings shaped
by natural selection. One of the main differences comes from the fact that GAs
rely principally on the so-called implicit parallelism, while giving to the
mutation/selection mechanism the second role. Such differences could suggest
new ways of employing GAs on complex problems, using complex codings and
starting from nearly homogeneous populations.



This article describes an exemplary robot exercise which was conducted in a
class for mechatronics students. The goal of this exercise was to engage
students in scientific thinking and reasoning, activities which do not always
play an important role in their curriculum. The robotic platform presented here
is simple in its construction and is customizable to the needs of the teacher.
Therefore, it can be used for exercises in many different fields of science,
not necessarily related to robotics. Here we present a situation where the
robot is used like an alien creature from which we want to understand its
behavior, resembling an ethological research activity. This robot exercise is
suited for a wide range of courses, from general introduction to science, to
hardware oriented lectures.



We present a framework which constructs an event-style dis- course semantics.
The discourse dynamics are encoded in continuation semantics and various
rhetorical relations are embedded in the resulting interpretation of the
framework. We assume discourse and sentence are distinct semantic objects, that
play different roles in meaning evalua- tion. Moreover, two sets of composition
functions, for handling different discourse relations, are introduced. The
paper first gives the necessary background and motivation for event and dynamic
semantics, then the framework with detailed examples will be introduced.



This article presents an extension of Minimalist Categorial Gram- mars (MCG)
to encode Chomsky's phases. These grammars are based on Par- tially Commutative
Logic (PCL) and encode properties of Minimalist Grammars (MG) of Stabler. The
first implementation of MCG were using both non- commutative properties (to
respect the linear word order in an utterance) and commutative ones (to model
features of different constituents). Here, we pro- pose to adding Chomsky's
phases with the non-commutative tensor product of the logic. Then we could give
account of the PIC just by using logical prop- erties of the framework.



This paper begins the discussion of how the Information Flow Framework can be
used to provide a principled foundation for the metalevel (or structural level)
of the Standard Upper Ontology (SUO). This SUO structural level can be used as
a logical framework for manipulating collections of ontologies in the object
level of the SUO or other middle level or domain ontologies. From the
Information Flow perspective, the SUO structural level resolves into several
metalevel ontologies. This paper discusses a KIF formalization for one of those
metalevel categories, the Category Theory Ontology. In particular, it discusses
its category and colimit sub-namespaces.



When formalizing proofs with interactive theorem provers, it often happens
that extra background knowledge (declarative or procedural) about mathematical
concepts is employed without the formalizer explicitly invoking it, to help the
formalizer focus on the relevant details of the proof. In the contexts of
producing and studying a formalized mathematical argument, such mechanisms are
clearly valuable. But we may not always wish to suppress background knowledge.
For certain purposes, it is important to know, as far as possible, precisely
what background knowledge was implicitly employed in a formal proof. In this
note we describe an experiment conducted on the MIZAR Mathematical Library of
formal mathematical proofs to elicit one such class of implicitly employed
background knowledge: properties of functions and relations (e.g.,
commutativity, asymmetry, etc.).



Multiclass prediction is the problem of classifying an object into a relevant
target class. We consider the problem of learning a multiclass predictor that
uses only few features, and in particular, the number of used features should
increase sub-linearly with the number of possible classes. This implies that
features should be shared by several classes. We describe and analyze the
ShareBoost algorithm for learning a multiclass predictor that uses few shared
features. We prove that ShareBoost efficiently finds a predictor that uses few
shared features (if such a predictor exists) and that it has a small
generalization error. We also describe how to use ShareBoost for learning a
non-linear predictor that has a fast evaluation time. In a series of
experiments with natural data sets we demonstrate the benefits of ShareBoost
and evaluate its success relatively to other state-of-the-art approaches.



We present a new algorithm for exactly solving decision making problems
represented as influence diagrams. We do not require the usual assumptions of
no forgetting and regularity; this allows us to solve problems with
simultaneous decisions and limited information. The algorithm is empirically
shown to outperform a state-of-the-art algorithm on randomly generated problems
of up to 150 variables and $10^{64}$ solutions. We show that the problem is
NP-hard even if the underlying graph structure of the problem has small
treewidth and the variables take on a bounded number of states, but that a
fully polynomial time approximation scheme exists for these cases. Moreover, we
show that the bound on the number of states is a necessary condition for any
efficient approximation scheme.



A lot of research effort has been put into community detection from all
corners of academic interest such as physics, mathematics and computer science.
In this paper I have proposed a Bi-Objective Genetic Algorithm for community
detection which maximizes modularity and community score. Then the results
obtained for both benchmark and real life data sets are compared with other
algorithms using the modularity and MNI performance metrics. The results show
that the BOCD algorithm is capable of successfully detecting community
structure in both real life and synthetic datasets, as well as improving upon
the performance of previous techniques.



Metrics specifying distances between data points can be learned in a
discriminative manner or from generative models. In this paper, we show how to
unify generative and discriminative learning of metrics via a kernel learning
framework. Specifically, we learn local metrics optimized from parametric
generative models. These are then used as base kernels to construct a global
kernel that minimizes a discriminative training criterion. We consider both
linear and nonlinear combinations of local metric kernels. Our empirical
results show that these combinations significantly improve performance on
classification tasks. The proposed learning algorithm is also very efficient,
achieving order of magnitude speedup in training time compared to previous
discriminative baseline methods.



In answer-set programming (ASP), the solutions of a problem are encoded in
dedicated models, called answer sets, of a logical theory. These answer sets
are computed from the program that represents the theory by means of an ASP
solver and returned to the user as sets of ground first-order literals. As this
type of representation is often cumbersome for the user to interpret, tools
like ASPVIZ and IDPDraw were developed that allow for visualising answer sets.
The tool Kara, introduced in this paper, follows these approaches, using ASP
itself as a language for defining visualisations of interpretations. Unlike
existing tools that position graphic primitives according to static coordinates
only, Kara allows for more high-level specifications, supporting graph
structures, grids, and relative positioning of graphical elements. Moreover,
generalising the functionality of previous tools, Kara provides modifiable
visualisations such that interpretations can be manipulated by graphically
editing their visualisations. This is realised by resorting to abductive
reasoning techniques. Kara is part of SeaLion, a forthcoming integrated
development environment (IDE) for ASP.



Fuzzy inference systems always suffer from the lack of efficient structures
or platforms for their hardware implementation. In this paper, we tried to
overcome this problem by proposing new method for the implementation of those
fuzzy inference systems which use fuzzy rule base to make inference. To achieve
this goal, we have designed a multi-layer neuro-fuzzy computing system based on
the memristor crossbar structure by introducing some new concepts like fuzzy
minterms. Although many applications can be realized through the use of our
proposed system, in this study we show how the fuzzy XOR function can be
constructed and how it can be used to extract edges from grayscale images. Our
memristive fuzzy edge detector (implemented in analog form) compared with other
common edge detectors has this advantage that it can extract edges of any given
image all at once in real-time.



In default theories, outliers denote sets of literals featuring unexpected
properties. In previous papers, we have defined outliers in default logics and
investigated their formal properties. Specifically, we have looked into the
computational complexity of outlier detection problems and proved that while
they are generally intractable, interesting tractable cases can be singled out.
Following those results, we study here the tractability frontier in outlier
detection problems, by analyzing it with respect to (i) the considered outlier
detection problem, (ii) the reference default logic fragment, and (iii) the
adopted notion of outlier. As for point (i), we shall consider three problems
of increasing complexity, called Outlier-Witness Recognition, Outlier
Recognition and Outlier Existence, respectively. As for point (ii), as we look
for conditions under which outlier detection can be done efficiently, attention
will be limited to subsets of Disjunction-free propositional default theories.
As for point (iii), we shall refer to both the notion of outlier of [ABP08] and
a new and more restrictive one, called strong outlier. After complexity
results, we present a polynomial time algorithm for enumerating all strong
outliers of bounded size in an quasi-acyclic normal unary default theory. Some
of our tractability results rely on the Incremental Lemma that provides
conditions for a deafult logic fragment to have a monotonic behavior. Finally,
in order to show that the simple fragments of DL we deal with are still rich
enough to solve interesting problems and, therefore, the tractability results
that we prove are interesting not only on the mere theoretical side, insights
into the expressive capabilities of these fragments are provided, by showing
that normal unary theories express all NL queries, hereby indirectly answering
a question raised by Kautz and Selman.



This paper proposes a novel latent semantic learning method for extracting
high-level features (i.e. latent semantics) from a large vocabulary of abundant
mid-level features (i.e. visual keywords) with structured sparse
representation, which can help to bridge the semantic gap in the challenging
task of human action recognition. To discover the manifold structure of
midlevel features, we develop a spectral embedding approach to latent semantic
learning based on L1-graph, without the need to tune any parameter for graph
construction as a key step of manifold learning. More importantly, we construct
the L1-graph with structured sparse representation, which can be obtained by
structured sparse coding with its structured sparsity ensured by novel L1-norm
hypergraph regularization over mid-level features. In the new embedding space,
we learn latent semantics automatically from abundant mid-level features
through spectral clustering. The learnt latent semantics can be readily used
for human action recognition with SVM by defining a histogram intersection
kernel. Different from the traditional latent semantic analysis based on topic
models, our latent semantic learning method can explore the manifold structure
of mid-level features in both L1-graph construction and spectral embedding,
which results in compact but discriminative high-level features. The
experimental results on the commonly used KTH action dataset and unconstrained
YouTube action dataset show the superior performance of our method.



This paper studies the topic modeling problem of tagged documents and images.
Higher-order relations among tagged documents and images are major and
ubiquitous characteristics, and play positive roles in extracting reliable and
interpretable topics. In this paper, we propose the tag-topic models (TTM) to
depict such higher-order topic structural dependencies within the Markov random
field (MRF) framework. First, we use the novel factor graph representation of
latent Dirichlet allocation (LDA)-based topic models from the MRF perspective,
and present an efficient loopy belief propagation (BP) algorithm for
approximate inference and parameter estimation. Second, we propose the factor
hypergraph representation of TTM, and focus on both pairwise and higher-order
relation modeling among tagged documents and images. Efficient loopy BP
algorithm is developed to learn TTM, which encourages the topic labeling
smoothness among tagged documents and images. Extensive experimental results
confirm the incorporation of higher-order relations to be effective in
enhancing the overall topic modeling performance, when compared with current
state-of-the-art topic models, in many text and image mining tasks of broad
interests such as word and link prediction, document classification, and tag
recommendation.



In this paper, we extend Meek's conjecture (Meek 1997) from directed and
acyclic graphs to chain graphs, and prove that the extended conjecture is true.
Specifically, we prove that if a chain graph H is an independence map of the
independence model induced by another chain graph G, then (i) G can be
transformed into H by a sequence of directed and undirected edge additions and
feasible splits and mergings, and (ii) after each operation in the sequence H
remains an independence map of the independence model induced by G. Our result
has the same important consequence for learning chain graphs from data as the
proof of Meek's conjecture in (Chickering 2002) had for learning Bayesian
networks from data: It makes it possible to develop efficient and
asymptotically correct learning algorithms under mild assumptions.



While belief functions may be seen formally as a generalization of
probabilistic distributions, the question of the interactions between belief
functions and probability is still an issue in practice. This question is
difficult, since the contexts of use of these theory are notably different and
the semantics behind these theories are not exactly the same. A prominent issue
is increasingly regarded by the community, that is the management of the
conflicting information. Recent works have introduced new rules for handling
the conflict redistribution while combining belief functions. The notion of
conflict, or its cancellation by an hypothesis of open world, seems by itself
to prevent a direct interpretation of belief function in a probabilistic
framework. This paper addresses the question of a probabilistic interpretation
of belief functions. It first introduces and implements a theoretically
grounded rule, which is in essence an adaptive conjunctive rule. It is shown,
how this rule is derived from a logical interpretation of the belief functions
by means of a probabilistic multimodal logic; in addition, a concept of source
independence is introduced, based on a principle of entropy maximization.



This work contributes to the domains of Boolean algebra and of Bayesian
probability, by proposing an algebraic extension of Boolean algebras, which
implements an operator for the Bayesian conditional inference and is closed
under this operator. It is known since the work of Lewis (Lewis' triviality)
that it is not possible to construct such conditional operator within the space
of events. Nevertheless, this work proposes an answer which complements Lewis'
triviality, by the construction of a conditional operator outside the space of
events, thus resulting in an algebraic extension. In particular, it is proved
that any probability defined on a Boolean algebra may be extended to its
algebraic extension in compliance with the multiplicative definition of the
conditional probability. In the last part of this paper, a new bivalent logic
is introduced on the basis of this algebraic extension, and basic properties
are derived.



A fundamental operation in many vision tasks, including motion understanding,
stereopsis, visual odometry, or invariant recognition, is establishing
correspondences between images or between images and data from other
modalities. We present an analysis of the role that multiplicative interactions
play in learning such correspondences, and we show how learning and inferring
relationships between images can be viewed as detecting rotations in the
eigenspaces shared among a set of orthogonal matrices. We review a variety of
recent multiplicative sparse coding methods in light of this observation. We
also review how the squaring operation performed by energy models and by models
of complex cells can be thought of as a way to implement multiplicative
interactions. This suggests that the main utility of including complex cells in
computational models of vision may be that they can encode relations not
invariances.



Artificial Neural Network is among the most popular algorithm for supervised
learning. However, Neural Networks have a well-known drawback of being a "Black
Box" learner that is not comprehensible to the Users. This lack of transparency
makes it unsuitable for many high risk tasks such as medical diagnosis that
requires a rational justification for making a decision. Rule Extraction
methods attempt to curb this limitation by extracting comprehensible rules from
a trained Network. Many such extraction algorithms have been developed over the
years with their respective strengths and weaknesses. They have been broadly
categorized into three types based on their approach to use internal model of
the Network. Eclectic Methods are hybrid algorithms that combine the other
approaches to attain more performance. In this paper, we present an Eclectic
method called HERETIC. Our algorithm uses Inductive Decision Tree learning
combined with information of the neural network structure for extracting
logical rules. Experiments and theoretical analysis show HERETIC to be better
in terms of speed and performance.



This paper applies machine learning and the mathematics of chaos to the task
of designing indoor rock-climbing routes. Chaotic variation has been used to
great advantage on music and dance, but the challenges here are quite
different, beginning with the representation. We present a formalized system
for transcribing rock climbing problems, then describe a variation generator
that is designed to support human route-setters in designing new and
interesting climbing problems. This variation generator, termed Strange Beta,
combines chaos and machine learning, using the former to introduce novelty and
the latter to smooth transitions in a manner that is consistent with the style
of the climbs This entails parsing the domain-specific natural language that
rock climbers use to describe routes and movement and then learning the
patterns in the results. We validated this approach with a pilot study in a
small university rock climbing gym, followed by a large blinded study in a
commercial climbing gym, in cooperation with experienced climbers and expert
route setters. The results show that {\sc Strange Beta} can help a human setter
produce routes that are at least as good as, and in some cases better than,
those produced in the traditional manner.



The paper presents a knowledge representation formalism, in the form of a
high-level Action Description Language for multi-agent systems, where
autonomous agents reason and act in a shared environment. Agents are
autonomously pursuing individual goals, but are capable of interacting through
a shared knowledge repository. In their interactions through shared portions of
the world, the agents deal with problems of synchronization and concurrency;
the action language allows the description of strategies to ensure a consistent
global execution of the agents' autonomously derived plans. A distributed
planning problem is formalized by providing the declarative specifications of
the portion of the problem pertaining a single agent. Each of these
specifications is executable by a stand-alone CLP-based planner. The
coordination among agents exploits a Linda infrastructure. The proposal is
validated in a prototype implementation developed in SICStus Prolog.
  To appear in Theory and Practice of Logic Programming (TPLP).



The distribution semantics is one of the most prominent approaches for the
combination of logic programming and probability theory. Many languages follow
this semantics, such as Independent Choice Logic, PRISM, pD, Logic Programs
with Annotated Disjunctions (LPADs) and ProbLog. When a program contains
functions symbols, the distribution semantics is well-defined only if the set
of explanations for a query is finite and so is each explanation.
Well-definedness is usually either explicitly imposed or is achieved by
severely limiting the class of allowed programs. In this paper we identify a
larger class of programs for which the semantics is well-defined together with
an efficient procedure for computing the probability of queries. Since LPADs
offer the most general syntax, we present our results for them, but our results
are applicable to all languages under the distribution semantics. We present
the algorithm "Probabilistic Inference with Tabling and Answer subsumption"
(PITA) that computes the probability of queries by transforming a probabilistic
program into a normal program and then applying SLG resolution with answer
subsumption. PITA has been implemented in XSB and tested on six domains: two
with function symbols and four without. The execution times are compared with
those of ProbLog, cplint and CVE, PITA was almost always able to solve larger
problems in a shorter time, on domains with and without function symbols.



We revisit the additive model learning literature and adapt a penalized
spline formulation due to Eilers and Marx, to train additive classifiers
efficiently. We also propose two new embeddings based two classes of orthogonal
basis with orthogonal derivatives, which can also be used to efficiently learn
additive classifiers. This paper follows the popular theme in the current
literature where kernel SVMs are learned much more efficiently using a
approximate embedding and linear machine. In this paper we show that spline
basis are especially well suited for learning additive models because of their
sparsity structure and the ease of computing the embedding which enables one to
train these models in an online manner, without incurring the memory overhead
of precomputing the storing the embeddings. We show interesting connections
between B-Spline basis and histogram intersection kernel and show that for a
particular choice of regularization and degree of the B-Splines, our proposed
learning algorithm closely approximates the histogram intersection kernel SVM.
This enables one to learn additive models with almost no memory overhead
compared to fast a linear solver, such as LIBLINEAR, while being only 5-6X
slower on average. On two large scale image classification datasets, MNIST and
Daimler Chrysler pedestrians, the proposed additive classifiers are as accurate
as the kernel SVM, while being two orders of magnitude faster to train.



We present a method for the automated verification of temporal properties of
infinite state systems. Our verification method is based on the specialization
of constraint logic programs (CLP) and works in two phases: (1) in the first
phase, a CLP specification of an infinite state system is specialized with
respect to the initial state of the system and the temporal property to be
verified, and (2) in the second phase, the specialized program is evaluated by
using a bottom-up strategy. The effectiveness of the method strongly depends on
the generalization strategy which is applied during the program specialization
phase. We consider several generalization strategies obtained by combining
techniques already known in the field of program analysis and program
transformation, and we also introduce some new strategies. Then, through many
verification experiments, we evaluate the effectiveness of the generalization
strategies we have considered. Finally, we compare the implementation of our
specialization-based verification method to other constraint-based model
checking tools. The experimental results show that our method is competitive
with the methods used by those other tools. To appear in Theory and Practice of
Logic Programming (TPLP).



Given a collection of objects and an associated similarity measure, the
all-pairs similarity search problem asks us to find all pairs of objects with
similarity greater than a certain user-specified threshold. Locality-sensitive
hashing (LSH) based methods have become a very popular approach for this
problem. However, most such methods only use LSH for the first phase of
similarity search - i.e. efficient indexing for candidate generation. In this
paper, we present BayesLSH, a principled Bayesian algorithm for the subsequent
phase of similarity search - performing candidate pruning and similarity
estimation using LSH. A simpler variant, BayesLSH-Lite, which calculates
similarities exactly, is also presented. BayesLSH is able to quickly prune away
a large majority of the false positive candidate pairs, leading to significant
speedups over baseline approaches. For BayesLSH, we also provide probabilistic
guarantees on the quality of the output, both in terms of accuracy and recall.
Finally, the quality of BayesLSH's output can be easily tuned and does not
require any manual setting of the number of hashes to use for similarity
estimation, unlike standard approaches. For two state-of-the-art candidate
generation algorithms, AllPairs and LSH, BayesLSH enables significant speedups,
typically in the range 2x-20x for a wide variety of datasets.



We introduce matrix and its block to the Dung's theory of argumentation
frameworks. It is showed that each argumentation framework has a matrix
representation, and the common extension-based semantics of argumentation
framework can be characterized by blocks of matrix and their relations. In
contrast with traditional method of directed graph, the matrix way has the
advantage of computability. Therefore, it has an extensive perspective to bring
the theory of matrix into the research of argumentation frameworks and related
areas.



In this paper, we present a supervised learning approach to training
submodular scoring functions for extractive multi-document summarization. By
taking a structured predicition approach, we provide a large-margin method that
directly optimizes a convex relaxation of the desired performance measure. The
learning method applies to all submodular summarization methods, and we
demonstrate its effectiveness for both pairwise as well as coverage-based
scoring functions on multiple datasets. Compared to state-of-the-art functions
that were tuned manually, our method significantly improves performance and
enables high-fidelity models with numbers of parameters well beyond what could
reasonbly be tuned by hand.



This paper addresses the estimation of parameters of a Bayesian network from
incomplete data. The task is usually tackled by running the
Expectation-Maximization (EM) algorithm several times in order to obtain a high
log-likelihood estimate. We argue that choosing the maximum log-likelihood
estimate (as well as the maximum penalized log-likelihood and the maximum a
posteriori estimate) has severe drawbacks, being affected both by overfitting
and model uncertainty. Two ideas are discussed to overcome these issues: a
maximum entropy approach and a Bayesian model averaging approach. Both ideas
can be easily applied on top of EM, while the entropy idea can be also
implemented in a more sophisticated way, through a dedicated non-linear solver.
A vast set of experiments shows that these ideas produce significantly better
estimates and inferences than the traditional and widely used maximum
(penalized) log-likelihood and maximum a posteriori estimates. In particular,
if EM is adopted as optimization engine, the model averaging approach is the
best performing one; its performance is matched by the entropy approach when
implemented using the non-linear solver. The results suggest that the
applicability of these ideas is immediate (they are easy to implement and to
integrate in currently available inference engines) and that they constitute a
better way to learn Bayesian network parameters.



This paper presents an improvement to model learning when using multi-class
LogitBoost for classification. Motivated by the statistical view, LogitBoost
can be seen as additive tree regression. Two important factors in this setting
are: 1) coupled classifier output due to a sum-to-zero constraint, and 2) the
dense Hessian matrices that arise when computing tree node split gain and node
value fittings. In general, this setting is too complicated for a tractable
model learning algorithm. However, too aggressive simplification of the setting
may lead to degraded performance. For example, the original LogitBoost is
outperformed by ABC-LogitBoost due to the latter's more careful treatment of
the above two factors.
  In this paper we propose techniques to address the two main difficulties of
the LogitBoost setting: 1) we adopt a vector tree (i.e. each node value is
vector) that enforces a sum-to-zero constraint, and 2) we use an adaptive block
coordinate descent that exploits the dense Hessian when computing tree split
gain and node values. Higher classification accuracy and faster convergence
rates are observed for a range of public data sets when compared to both the
original and the ABC-LogitBoost implementations.



Scene understanding includes many related sub-tasks, such as scene
categorization, depth estimation, object detection, etc. Each of these
sub-tasks is often notoriously hard, and state-of-the-art classifiers already
exist for many of them. These classifiers operate on the same raw image and
provide correlated outputs. It is desirable to have an algorithm that can
capture such correlation without requiring any changes to the inner workings of
any classifier.
  We propose Feedback Enabled Cascaded Classification Models (FE-CCM), that
jointly optimizes all the sub-tasks, while requiring only a `black-box'
interface to the original classifier for each sub-task. We use a two-layer
cascade of classifiers, which are repeated instantiations of the original ones,
with the output of the first layer fed into the second layer as input. Our
training method involves a feedback step that allows later classifiers to
provide earlier classifiers information about which error modes to focus on. We
show that our method significantly improves performance in all the sub-tasks in
the domain of scene understanding, where we consider depth estimation, scene
categorization, event categorization, object detection, geometric labeling and
saliency detection. Our method also improves performance in two robotic
applications: an object-grasping robot and an object-finding robot.



When people explore and manage information, they think in terms of topics and
themes. However, the software that supports information exploration sees text
at only the surface level. In this paper we show how topic modeling -- a
technique for identifying latent themes across large collections of documents
-- can support semantic exploration. We present TopicViz, an interactive
environment for information exploration. TopicViz combines traditional search
and citation-graph functionality with a range of novel interactive
visualizations, centered around a force-directed layout that links documents to
the latent themes discovered by the topic model. We describe several use
scenarios in which TopicViz supports rapid sensemaking on large document
collections.



Backdoor sets, a notion introduced by Williams et al. in 2003, are certain
sets of key variables of a CNF formula F that make it easy to solve the
formula; by assigning truth values to the variables in a backdoor set, the
formula gets reduced to one or several polynomial-time solvable formulas. More
specifically, a weak backdoor set of F is a set X of variables such that there
exits a truth assignment t to X that reduces F to a satisfiable formula F[t]
that belongs to a polynomial-time decidable base class C. A strong backdoor set
is a set X of variables such that for all assignments t to X, the reduced
formula F[t] belongs to C.
  We study the problem of finding backdoor sets of size at most k with respect
to the base class of CNF formulas with acyclic incidence graphs, taking k as
the parameter. We show that
  1. the detection of weak backdoor sets is W[2]-hard in general but
fixed-parameter tractable for r-CNF formulas, for any fixed r>=3, and
  2. the detection of strong backdoor sets is fixed-parameter approximable.
  Result 1 is the the first positive one for a base class that does not have a
characterization with obstructions of bounded size. Result 2 is the first
positive one for a base class for which strong backdoor sets are more powerful
than deletion backdoor sets.
  Not only SAT, but also #SAT can be solved in polynomial time for CNF formulas
with acyclic incidence graphs. Hence Result 2 establishes a new structural
parameter that makes #SAT fixed-parameter tractable and that is incomparable
with known parameters such as treewidth and clique-width.
  We obtain the algorithms by a combination of an algorithmic version of the
Erd\"os-P\'osa Theorem, Courcelle's model checking for monadic second order
logic, and new combinatorial results on how disjoint cycles can interact with
the backdoor set.



A backdoor set is a set of variables of a propositional formula such that
fixing the truth values of the variables in the backdoor set moves the formula
into some polynomial-time decidable class. If we know a small backdoor set we
can reduce the question of whether the given formula is satisfiable to the same
question for one or several easy formulas that belong to the tractable class
under consideration. In this survey we review parameterized complexity results
for problems that arise in the context of backdoor sets, such as the problem of
finding a backdoor set of size at most k, parameterized by k. We also discuss
recent results on backdoor sets for problems that are beyond NP.



This paper sets out to resolve how agents ought to act in the Sleeping Beauty
problem and various related anthropic (self-locating belief) problems, not
through the calculation of anthropic probabilities, but through finding the
correct decision to make. It creates an anthropic decision theory (ADT) that
decides these problems from a small set of principles. By doing so, it
demonstrates that the attitude of agents with regards to each other (selfish or
altruistic) changes the decisions they reach, and that it is very important to
take this into account. To illustrate ADT, it is then applied to two major
anthropic problems and paradoxes, the Presumptuous Philosopher and Doomsday
problems, thus resolving some issues about the probability of human extinction.



The main topic discussed in this paper is how to use intelligence for
biometric decision defuzzification. A neural training model is proposed and
tested here as a possible solution for dealing with natural fuzzification that
appears between the intra- and inter-class distribution of scores computed
during iris recognition tests. It is shown here that the use of proposed neural
network support leads to an improvement in the artificial perception of the
separation between the intra- and inter-class score distributions by moving
them away from each other.



This work proposes a way to align statistical modeling with decision making.
We provide a method that propagates the uncertainty in predictive modeling to
the uncertainty in operational cost, where operational cost is the amount spent
by the practitioner in solving the problem. The method allows us to explore the
range of operational costs associated with the set of reasonable statistical
models, so as to provide a useful way for practitioners to understand
uncertainty. To do this, the operational cost is cast as a regularization term
in a learning algorithm's objective function, allowing either an optimistic or
pessimistic view of possible costs, depending on the regularization parameter.
From another perspective, if we have prior knowledge about the operational
cost, for instance that it should be low, this knowledge can help to restrict
the hypothesis space, and can help with generalization. We provide a
theoretical generalization bound for this scenario. We also show that learning
with operational costs is related to robust optimization.



This paper studies the coupling of internally guided learning and social
interaction, and more specifically the improvement owing to demonstrations of
the learning by intrinsic motivation. We present Socially Guided Intrinsic
Motivation by Demonstration (SGIM-D), an algorithm for learning in continuous,
unbounded and non-preset environments. After introducing social learning and
intrinsic motivation, we describe the design of our algorithm, before showing
through a fishing experiment that SGIM-D efficiently combines the advantages of
social learning and intrinsic motivation to gain a wide repertoire while being
specialised in specific subspaces.



The basis of the method proposed in this article is the idea that information
is one of the most important factors in strategic decisions, including
decisions in computer chess and other strategy games. The model proposed in
this article and the algorithm described are based on the idea of a information
theoretic basis of decision in strategy games . The model generalizes and
provides a mathematical justification for one of the most popular search
algorithms used in leading computer chess programs, the fractional ply scheme.
However, despite its success in leading computer chess applications, until now
few has been published about this method. The article creates a fundamental
basis for this method in the axioms of information theory, then derives the
principles used in programming the search and describes mathematically the form
of the coefficients. One of the most important parameters of the fractional ply
search is derived from fundamental principles. Until now this coefficient has
been usually handcrafted or determined from intuitive elements or data mining.
There is a deep, information theoretical justification for such a parameter. In
one way the method proposed is a generalization of previous methods. More
important, it shows why the fractional depth ply scheme is so powerful. It is
because the algorithm navigates along the lines where the highest information
gain is possible. A working and original implementation has been written and
tested for this algorithm and is provided in the appendix. The article is
essentially self-contained and gives proper background knowledge and
references. The assumptions are intuitive and in the direction expected and
described intuitively by great champions of chess.



The article describes a model of chess based on information theory. A
mathematical model of the partial depth scheme is outlined and a formula for
the partial depth added for each ply is calculated from the principles of the
model. An implementation of alpha-beta with partial depth is given. The method
is tested using an experimental strategy having as objective to show the effect
of allocation of a higher amount of search resources on areas of the search
tree with higher information. The search proceeds in the direction of lines
with higher information gain. The effects on search performance of allocating
higher search resources on lines with higher information gain are tested
experimentaly and conclusive results are obtained. In order to isolate the
effects of the partial depth scheme no other heuristic is used.



Adaptation to changing environments is a hallmark of biological systems.
Diversity in traits is necessary for adaptation and can influence the survival
of a population faced with novelty. In habitats that remain stable over many
generations, stabilizing selection reduces trait differences within
populations, thereby appearing to remove the diversity needed for heritable
adaptive responses in new environments. Paradoxically, field studies have
documented numerous populations under long periods of stabilizing selection and
evolutionary stasis that have rapidly evolved under changed environmental
conditions. In this article, we review how cryptic genetic variation (CGV)
resolves this diversity paradox by allowing populations in a stable environment
to gradually accumulate hidden genetic diversity that is revealed as trait
differences when environments change. Instead of being in conflict,
environmental stasis supports CGV accumulation and thus appears to facilitate
rapid adaptation in new environments as suggested by recent CGV studies.
Similarly, degeneracy has been found to support both genetic and non-genetic
adaptation at many levels of biological organization. Degenerate, as opposed to
diverse or redundant, ensembles appear functionally redundant in certain
environmental contexts but functionally diverse in others. CGV and degeneracy
paradigms for adaptation are integrated in this review, revealing a common set
of principles that support adaptation at multiple levels of biological
organization. Though a discussion of simulation studies, molecular-based
experimental systems, principles from population genetics, and field
experiments, we demonstrate that CGV and degeneracy reflect complementary
top-down and bottom-up, respectively, conceptualizations of the same basic
phenomenon and arguably capture a universal feature of biological adaptive
processes.



Nowadays, computer scientists have shown the interest in the study of social
insect's behaviour in neural networks area for solving different combinatorial
and statistical problems. Chief among these is the Artificial Bee Colony (ABC)
algorithm. This paper investigates the use of ABC algorithm that simulates the
intelligent foraging behaviour of a honey bee swarm. Multilayer Perceptron
(MLP) trained with the standard back propagation algorithm normally utilises
computationally intensive training algorithms. One of the crucial problems with
the backpropagation (BP) algorithm is that it can sometimes yield the networks
with suboptimal weights because of the presence of many local optima in the
solution space. To overcome ABC algorithm used in this work to train MLP
learning the complex behaviour of earthquake time series data trained by BP,
the performance of MLP-ABC is benchmarked against MLP training with the
standard BP. The experimental result shows that MLP-ABC performance is better
than MLP-BP for time series data.



A new approach to data compression is developed and applied to multimedia
content. This method separates messages into components suitable for both
lossless coding and 'lossy' or statistical coding techniques, compressing
complex objects by separately encoding signals and noise. This is demonstrated
by compressing the most significant bits of data exactly, since they are
typically redundant and compressible, and either fitting a maximally likely
noise function to the residual bits or compressing them using lossy methods.
Upon decompression, the significant bits are decoded and added to a noise
function, whether sampled from a noise model or decompressed from a lossy code.
This results in compressed data similar to the original. For many test images,
a two-part image code using JPEG2000 for lossy coding and PAQ8l for lossless
coding produces less mean-squared error than an equal length of JPEG2000.
Computer-generated images typically compress better using this method than
through direct lossy coding, as do many black and white photographs and most
color photographs at sufficiently high quality levels. Examples applying the
method to audio and video coding are also demonstrated. Since two-part codes
are efficient for both periodic and chaotic data, concatenations of roughly
similar objects may be encoded efficiently, which leads to improved inference.
Applications to artificial intelligence are demonstrated, showing that signals
using an economical lossless code have a critical level of redundancy which
leads to better description-based inference than signals which encode either
insufficient data or too much detail.



In this paper, we study CPU utilization time patterns of several Map-Reduce
applications. After extracting running patterns of several applications, the
patterns with their statistical information are saved in a reference database
to be later used to tweak system parameters to efficiently execute unknown
applications in future. To achieve this goal, CPU utilization patterns of new
applications along with its statistical information are compared with the
already known ones in the reference database to find/predict their most
probable execution patterns. Because of different patterns lengths, the Dynamic
Time Warping (DTW) is utilized for such comparison; a statistical analysis is
then applied to DTWs' outcomes to select the most suitable candidates.
Moreover, under a hypothesis, another algorithm is proposed to classify
applications under similar CPU utilization patterns. Three widely used text
processing applications (WordCount, Distributed Grep, and Terasort) and another
application (Exim Mainlog parsing) are used to evaluate our hypothesis in
tweaking system parameters in executing similar applications. Results were very
promising and showed effectiveness of our approach on 5-node Map-Reduce
platform



This paper studies the use of the Tsallis Entropy versus the classic
Boltzmann-Gibbs-Shannon entropy for classifying image patterns. Given a
database of 40 pattern classes, the goal is to determine the class of a given
image sample. Our experiments show that the Tsallis entropy encoded in a
feature vector for different $q$ indices has great advantage over the
Boltzmann-Gibbs-Shannon entropy for pattern classification, boosting
recognition rates by a factor of 3. We discuss the reasons behind this success,
shedding light on the usefulness of the Tsallis entropy.



This work is about diagrammatic languages, how they can be represented, and
what they in turn can be used to represent. More specifically, it focuses on
representations and applications of string diagrams. String diagrams are used
to represent a collection of processes, depicted as "boxes" with multiple
(typed) inputs and outputs, depicted as "wires". If we allow plugging input and
output wires together, we can intuitively represent complex compositions of
processes, formalised as morphisms in a monoidal category.
  [...] The first major contribution of this dissertation is the introduction
of a discretised version of a string diagram called a string graph. String
graphs form a partial adhesive category, so they can be manipulated using
double-pushout graph rewriting. Furthermore, we show how string graphs modulo a
rewrite system can be used to construct free symmetric traced and compact
closed categories on a monoidal signature.
  The second contribution is in the application of graphical languages to
quantum information theory. We use a mixture of diagrammatic and algebraic
techniques to prove a new classification result for strongly complementary
observables. [...] We also introduce a graphical language for multipartite
entanglement and illustrate a simple graphical axiom that distinguishes the two
maximally-entangled tripartite qubit states: GHZ and W. [...]
  The third contribution is a description of two software tools developed in
part by the author to implement much of the theoretical content described here.
The first tool is Quantomatic, a desktop application for building string graphs
and graphical theories, as well as performing automated graph rewriting
visually. The second is QuantoCoSy, which performs fully automated,
model-driven theory creation using a procedure called conjecture synthesis.



Language evolution might have preferred certain prior social configurations
over others. Experiments conducted with models of different social structures
(varying subgroup interactions and the role of a dominant interlocutor) suggest
that having isolated agent groups rather than an interconnected agent is more
advantageous for the emergence of a social communication system. Distinctive
groups that are closely connected by communication yield systems less like
natural language than fully isolated groups inhabiting the same world.
Furthermore, the addition of a dominant male who is asymmetrically favoured as
a hearer, and equally likely to be a speaker has no positive influence on the
disjoint groups.



In this paper, we claim that language is likely to have emerged as a
mechanism for coordinating the solution of complex tasks. To confirm this
thesis, computer simulations are performed based on the coordination task
presented by Garrod & Anderson (1987). The role of success in task-oriented
dialogue is analytically evaluated with the help of performance measurements
and a thorough lexical analysis of the emergent communication system.
Simulation results confirm a strong effect of success mattering on both
reliability and dispersion of linguistic conventions.



In the paper, frameworks for electronic shopping of composite (modular)
products are described: (a) multicriteria selection (product is considered as a
whole system, it is a traditional approach), (b) combinatorial synthesis
(composition) of the product from its components, (c) aggregation of the
product from several selected products/prototypes. The following product model
is examined: (i) general tree-like structure, (ii) set of system
parts/components (leaf nodes), (iii) design alternatives (DAs) for each
component, (iv) ordinal priorities for DAs, and (v) estimates of compatibility
between DAs for different components. The combinatorial synthesis is realized
as morphological design of a composite (modular) product or an extended
composite product (e.g., product and support services as financial
instruments). Here the solving process is based on Hierarchical Morphological
Multicriteria Design (HMMD): (i) multicriteria selection of alternatives for
system parts, (ii) composing the selected alternatives into a resultant
combination (while taking into account ordinal quality of the alternatives
above and their compatibility). The aggregation framework is based on
consideration of aggregation procedures, for example: (i) addition procedure:
design of a products substructure or an extended substructure ('kernel') and
addition of elements, and (ii) design procedure: design of the composite
solution based on all elements of product superstructure. Applied numerical
examples (e.g., composite product, extended composite product, product repair
plan, and product trajectory) illustrate the proposed approaches.



We consider unsupervised estimation of mixtures of discrete graphical models,
where the class variable corresponding to the mixture components is hidden and
each mixture component over the observed variables can have a potentially
different Markov graph structure and parameters. We propose a novel approach
for estimating the mixture components, and our output is a tree-mixture model
which serves as a good approximation to the underlying graphical model mixture.
Our method is efficient when the union graph, which is the union of the Markov
graphs of the mixture components, has sparse vertex separators between any pair
of observed variables. This includes tree mixtures and mixtures of bounded
degree graphs. For such models, we prove that our method correctly recovers the
union graph structure and the tree structures corresponding to
maximum-likelihood tree approximations of the mixture components. The sample
and computational complexities of our method scale as $\poly(p, r)$, for an
$r$-component mixture of $p$-variate graphical models. We further extend our
results to the case when the union graph has sparse local separators between
any pair of observed variables, such as mixtures of locally tree-like graphs,
and the mixture components are in the regime of correlation decay.



A fundamental problem in control is to learn a model of a system from
observations that is useful for controller synthesis. To provide good
performance guarantees, existing methods must assume that the real system is in
the class of models considered during learning. We present an iterative method
with strong guarantees even in the agnostic case where the system is not in the
class. In particular, we show that any no-regret online learning algorithm can
be used to obtain a near-optimal policy, provided some model achieves low
training error and access to a good exploration distribution. Our approach
applies to both discrete and continuous domains. We demonstrate its efficacy
and scalability on a challenging helicopter domain from the literature.



To understand the structural dynamics of a large-scale social, biological or
technological network, it may be useful to discover behavioral roles
representing the main connectivity patterns present over time. In this paper,
we propose a scalable non-parametric approach to automatically learn the
structural dynamics of the network and individual nodes. Roles may represent
structural or behavioral patterns such as the center of a star, peripheral
nodes, or bridge nodes that connect different communities. Our novel approach
learns the appropriate structural role dynamics for any arbitrary network and
tracks the changes over time. In particular, we uncover the specific global
network dynamics and the local node dynamics of a technological, communication,
and social network. We identify interesting node and network patterns such as
stationary and non-stationary roles, spikes/steps in role-memberships (perhaps
indicating anomalies), increasing/decreasing role trends, among many others.
Our results indicate that the nodes in each of these networks have distinct
connectivity patterns that are non-stationary and evolve considerably over
time. Overall, the experiments demonstrate the effectiveness of our approach
for fast mining and tracking of the dynamics in large networks. Furthermore,
the dynamic structural representation provides a basis for building more
sophisticated models and tools that are fast for exploring large dynamic
networks.



The Turing Test (TT) checks for human intelligence, rather than any putative
general intelligence. It involves repeated interaction requiring learning in
the form of adaption to the human conversation partner. It is a macro-level
post-hoc test in contrast to the definition of a Turing Machine (TM), which is
a prior micro-level definition. This raises the question of whether learning is
just another computational process, i.e. can be implemented as a TM. Here we
argue that learning or adaption is fundamentally different from computation,
though it does involve processes that can be seen as computations. To
illustrate this difference we compare (a) designing a TM and (b) learning a TM,
defining them for the purpose of the argument. We show that there is a
well-defined sequence of problems which are not effectively designable but are
learnable, in the form of the bounded halting problem. Some characteristics of
human intelligence are reviewed including it's: interactive nature, learning
abilities, imitative tendencies, linguistic ability and context-dependency. A
story that explains some of these is the Social Intelligence Hypothesis. If
this is broadly correct, this points to the necessity of a considerable period
of acculturation (social learning in context) if an artificial intelligence is
to pass the TT. Whilst it is always possible to 'compile' the results of
learning into a TM, this would not be a designed TM and would not be able to
continually adapt (pass future TTs). We conclude three things, namely that: a
purely "designed" TM will never pass the TT; that there is no such thing as a
general intelligence since it necessary involves learning; and that
learning/adaption and computation should be clearly distinguished.



We study the tracking problem, namely, estimating the hidden state of an
object over time, from unreliable and noisy measurements. The standard
framework for the tracking problem is the generative framework, which is the
basis of solutions such as the Bayesian algorithm and its approximation, the
particle filters. However, these solutions can be very sensitive to model
mismatches. In this paper, motivated by online learning, we introduce a new
framework for tracking. We provide an efficient tracking algorithm for this
framework. We provide experimental results comparing our algorithm to the
Bayesian algorithm on simulated data. Our experiments show that when there are
slight model mismatches, our algorithm outperforms the Bayesian algorithm.



Cyber-physical systems, such as mobile robots, must respond adaptively to
dynamic operating conditions. Effective operation of these systems requires
that sensing and actuation tasks are performed in a timely manner.
Additionally, execution of mission specific tasks such as imaging a room must
be balanced against the need to perform more general tasks such as obstacle
avoidance. This problem has been addressed by maintaining relative utilization
of shared resources among tasks near a user-specified target level. Producing
optimal scheduling strategies requires complete prior knowledge of task
behavior, which is unlikely to be available in practice. Instead, suitable
scheduling strategies must be learned online through interaction with the
system. We consider the sample complexity of reinforcement learning in this
domain, and demonstrate that while the problem state space is countably
infinite, we may leverage the problem's structure to guarantee efficient
learning.



Over the past two decades, several consistent procedures have been designed
to infer causal conclusions from observational data. We prove that if the true
causal network might be an arbitrary, linear Gaussian network or a discrete
Bayes network, then every unambiguous causal conclusion produced by a
consistent method from non-experimental data is subject to reversal as the
sample size increases any finite number of times. That result, called the
causal flipping theorem, extends prior results to the effect that causal
discovery cannot be reliable on a given sample size. We argue that since
repeated flipping of causal conclusions is unavoidable in principle for
consistent methods, the best possible discovery methods are consistent methods
that retract their earlier conclusions no more than necessary. A series of
simulations of various methods across a wide range of sample sizes illustrates
concretely both the theorem and the principle of comparing methods in terms of
retractions.



In many fields observations are performed irregularly along time, due to
either measurement limitations or lack of a constant immanent rate. While
discrete-time Markov models (as Dynamic Bayesian Networks) introduce either
inefficient computation or an information loss to reasoning about such
processes, continuous-time Markov models assume either a discrete state space
(as Continuous-Time Bayesian Networks), or a flat continuous state space (as
stochastic differential equations). To address these problems, we present a new
modeling class called Irregular-Time Bayesian Networks (ITBNs), generalizing
Dynamic Bayesian Networks, allowing substantially more compact representations,
and increasing the expressivity of the temporal dynamics. In addition, a
globally optimal solution is guaranteed when learning temporal systems,
provided that they are fully observed at the same irregularly spaced
time-points, and a semiparametric subclass of ITBNs is introduced to allow
further adaptation to the irregular nature of the available data.



We present a probabilistic model of events in continuous time in which each
event triggers a Poisson process of successor events. The ensemble of observed
events is thereby modeled as a superposition of Poisson processes. Efficient
inference is feasible under this model with an EM algorithm. Moreover, the EM
algorithm can be implemented as a distributed algorithm, permitting the model
to be applied to very large datasets. We apply these techniques to the modeling
of Twitter messages and the revision history of Wikipedia.



The explore{exploit dilemma is one of the central challenges in Reinforcement
Learning (RL). Bayesian RL solves the dilemma by providing the agent with
information in the form of a prior distribution over environments; however,
full Bayesian planning is intractable. Planning with the mean MDP is a common
myopic approximation of Bayesian planning. We derive a novel reward bonus that
is a function of the posterior distribution over environments, which, when
added to the reward in planning with the mean MDP, results in an agent which
explores efficiently and effectively. Although our method is similar to
existing methods when given an uninformative or unstructured prior, unlike
existing methods, our method can exploit structured priors. We prove that our
method results in a polynomial sample complexity and empirically demonstrate
its advantages in a structured exploration task.



Monte-Carlo Tree Search (MCTS) methods are drawing great interest after
yielding breakthrough results in computer Go. This paper proposes a Bayesian
approach to MCTS that is inspired by distributionfree approaches such as UCT
[13], yet significantly differs in important respects. The Bayesian framework
allows potentially much more accurate (Bayes-optimal) estimation of node values
and node uncertainties from a limited number of simulation trials. We further
propose propagating inference in the tree via fast analytic Gaussian
approximation methods: this can make the overhead of Bayesian inference
manageable in domains such as Go, while preserving high accuracy of
expected-value estimates. We find substantial empirical outperformance of UCT
in an idealized bandit-tree test environment, where we can obtain valuable
insights by comparing with known ground truth. Additionally we rigorously prove
on-policy and off-policy convergence of the proposed methods.



We study the problem of learning Bayesian network structures from data. We
develop an algorithm for finding the k-best Bayesian network structures. We
propose to compute the posterior probabilities of hypotheses of interest by
Bayesian model averaging over the k-best Bayesian networks. We present
empirical results on structural discovery over several real and synthetic data
sets and show that the method outperforms the model selection method and the
state of-the-art MCMC methods.



It is known that fixed points of loopy belief propagation (BP) correspond to
stationary points of the Bethe variational problem, where we minimize the Bethe
free energy subject to normalization and marginalization constraints.
Unfortunately, this does not entirely explain BP because BP is a dual rather
than primal algorithm to solve the Bethe variational problem -- beliefs are
infeasible before convergence. Thus, we have no better understanding of BP than
as an algorithm to seek for a common zero of a system of non-linear functions,
not explicitly related to each other. In this theoretical paper, we show that
these functions are in fact explicitly related -- they are the partial
derivatives of a single function of reparameterizations. That means, BP seeks
for a stationary point of a single function, without any constraints. This
function has a very natural form: it is a linear combination of local
log-partition functions, exactly as the Bethe entropy is the same linear
combination of local entropies.



Learning algorithms normally assume that there is at most one annotation or
label per data point. However, in some scenarios, such as medical diagnosis and
on-line collaboration,multiple annotations may be available. In either case,
obtaining labels for data points can be expensive and time-consuming (in some
circumstances ground-truth may not exist). Semi-supervised learning approaches
have shown that utilizing the unlabeled data is often beneficial in these
cases. This paper presents a probabilistic semi-supervised model and algorithm
that allows for learning from both unlabeled and labeled data in the presence
of multiple annotators. We assume that it is known what annotator labeled which
data points. The proposed approach produces annotator models that allow us to
provide (1) estimates of the true label and (2) annotator variable expertise
for both labeled and unlabeled data. We provide numerical comparisons under
various scenarios and with respect to standard semi-supervised learning.
Experiments showed that the presented approach provides clear advantages over
multi-annotator methods that do not use the unlabeled data and over methods
that do not use multi-labeler information.



Multi-task learning is a learning paradigm which seeks to improve the
generalization performance of a learning task with the help of some other
related tasks. In this paper, we propose a regularization formulation for
learning the relationships between tasks in multi-task learning. This
formulation can be viewed as a novel generalization of the regularization
framework for single-task learning. Besides modeling positive task correlation,
our method, called multi-task relationship learning (MTRL), can also describe
negative task correlation and identify outlier tasks based on the same
underlying principle. Under this regularization framework, the objective
function of MTRL is convex. For efficiency, we use an alternating method to
learn the optimal model parameters for each task as well as the relationships
between tasks. We study MTRL in the symmetric multi-task learning setting and
then generalize it to the asymmetric setting as well. We also study the
relationships between MTRL and some existing multi-task learning methods.
Experiments conducted on a toy problem as well as several benchmark data sets
demonstrate the effectiveness of MTRL.



Deep Boltzmann machines are in principle powerful models for extracting the
hierarchical structure of data. Unfortunately, attempts to train layers jointly
(without greedy layer-wise pretraining) have been largely unsuccessful. We
propose a modification of the learning algorithm that initially recenters the
output of the activation functions to zero. This modification leads to a better
conditioned Hessian and thus makes learning easier. We test the algorithm on
real data and demonstrate that our suggestion, the centered deep Boltzmann
machine, learns a hierarchy of increasingly abstract representations and a
better generative model of data.



We propose a principled algorithm for robust Bayesian filtering and smoothing
in nonlinear stochastic dynamic systems when both the transition function and
the measurement function are described by non-parametric Gaussian process (GP)
models. GPs are gaining increasing importance in signal processing, machine
learning, robotics, and control for representing unknown system functions by
posterior probability distributions. This modern way of "system identification"
is more robust than finding point estimates of a parametric function
representation. In this article, we present a principled algorithm for robust
analytic smoothing in GP dynamic systems, which are increasingly used in
robotics and control. Our numerical evaluations demonstrate the robustness of
the proposed approach in situations where other state-of-the-art Gaussian
filters and smoothers can fail.



The deep Boltzmann machine (DBM) has been an important development in the
quest for powerful "deep" probabilistic models. To date, simultaneous or joint
training of all layers of the DBM has been largely unsuccessful with existing
training methods. We introduce a simple regularization scheme that encourages
the weight vectors associated with each hidden unit to have similar norms. We
demonstrate that this regularization can be easily combined with standard
stochastic maximum likelihood to yield an effective training strategy for the
simultaneous training of all layers of the deep Boltzmann machine.



An automated technique has recently been proposed to transfer learning in the
hierarchical Bayesian optimization algorithm (hBOA) based on distance-based
statistics. The technique enables practitioners to improve hBOA efficiency by
collecting statistics from probabilistic models obtained in previous hBOA runs
and using the obtained statistics to bias future hBOA runs on similar problems.
The purpose of this paper is threefold: (1) test the technique on several
classes of NP-complete problems, including MAXSAT, spin glasses and minimum
vertex cover; (2) demonstrate that the technique is effective even when
previous runs were done on problems of different size; (3) provide empirical
evidence that combining transfer learning with other efficiency enhancement
techniques can often yield nearly multiplicative speedups.



The paper addresses an approach to ordinal assessment of alternatives based
on assignment of elements into an ordinal scale. Basic versions of the
assessment problems are formulated while taking into account the number of
levels at a basic ordinal scale [1,2,...,l] and the number of assigned elements
(e.g., 1,2,3). The obtained estimates are multisets (or bags) (cardinality of
the multiset equals a constant). Scale-posets for the examined assessment
problems are presented. 'Interval multiset estimates' are suggested. Further,
operations over multiset estimates are examined: (a) integration of multiset
estimates, (b) proximity for multiset estimates, (c) comparison of multiset
estimates, (d) aggregation of multiset estimates, and (e) alignment of multiset
estimates. Combinatorial synthesis based on morphological approach is examined
including the modified version of the approach with multiset estimates of
design alternatives. Knapsack-like problems with multiset estimates are briefly
described as well. The assessment approach, multiset-estimates, and
corresponding combinatorial problems are illustrated by numerical examples.



A relatively recent advance in cognitive neuroscience has been multi-voxel
pattern analysis (MVPA), which enables researchers to decode brain states
and/or the type of information represented in the brain during a cognitive
operation. MVPA methods utilize machine learning algorithms to distinguish
among types of information or cognitive states represented in the brain, based
on distributed patterns of neural activity. In the current investigation, we
propose a new approach for representation of neural data for pattern analysis,
namely a Mesh Learning Model. In this approach, at each time instant, a star
mesh is formed around each voxel, such that the voxel corresponding to the
center node is surrounded by its p-nearest neighbors. The arc weights of each
mesh are estimated from the voxel intensity values by least squares method. The
estimated arc weights of all the meshes, called Mesh Arc Descriptors (MADs),
are then used to train a classifier, such as Neural Networks, k-Nearest
Neighbor, Na\"ive Bayes and Support Vector Machines. The proposed Mesh Model
was tested on neuroimaging data acquired via functional magnetic resonance
imaging (fMRI) during a recognition memory experiment using categorized word
lists, employing a previously established experimental paradigm (\"Oztekin &
Badre, 2011). Results suggest that the proposed Mesh Learning approach can
provide an effective algorithm for pattern analysis of brain activity during
cognitive processing.



This paper addresses the problem of community detection in networked data
that combines link and content analysis. Most existing work combines link and
content information by a generative model. There are two major shortcomings
with the existing approaches. First, they assume that the probability of
creating a link between two nodes is determined only by the community
memberships of the nodes; however other factors (e.g. popularity) could also
affect the link pattern. Second, they use generative models to model the
content of individual nodes, whereas these generative models are vulnerable to
the content attributes that are irrelevant to communities. We propose a
Bayesian framework for combining link and content information for community
detection that explicitly addresses these shortcomings. A new link model is
presented that introduces a random variable to capture the node popularity when
deciding the link between two nodes; a discriminative model is used to
determine the community membership of a node by its content. An approximate
inference algorithm is presented for efficient Bayesian inference. Our
empirical study shows that the proposed framework outperforms several
state-of-theart approaches in combining link and content information for
community detection.



Efficiently finding the maximum a posteriori (MAP) configuration of a
graphical model is an important problem which is often implemented using
message passing algorithms. The optimality of such algorithms is only well
established for singly-connected graphs and other limited settings. This
article extends the set of graphs where MAP estimation is in P and where
message passing recovers the exact solution to so-called perfect graphs. This
result leverages recent progress in defining perfect graphs (the strong perfect
graph theorem), linear programming relaxations of MAP estimation and recent
convergent message passing schemes. The article converts graphical models into
nand Markov random fields which are straightforward to relax into linear
programs. Therein, integrality can be established in general by testing for
graph perfection. This perfection test is performed efficiently using a
polynomial time algorithm. Alternatively, known decomposition tools from
perfect graph theory may be used to prove perfection for certain families of
graphs. Thus, a general graph framework is provided for determining when MAP
estimation in any graphical model is in P, has an integral linear programming
relaxation and is exactly recoverable by message passing.



Bayesian model-based reinforcement learning is a formally elegant approach to
learning optimal behaviour under model uncertainty, trading off exploration and
exploitation in an ideal way. Unfortunately, finding the resulting
Bayes-optimal policies is notoriously taxing, since the search space becomes
enormous. In this paper we introduce a tractable, sample-based method for
approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our
approach outperformed prior Bayesian model-based RL algorithms by a significant
margin on several well-known benchmark problems -- because it avoids expensive
applications of Bayes rule within the search tree by lazily sampling models
from the current beliefs. We illustrate the advantages of our approach by
showing it working in an infinite state space domain which is qualitatively out
of reach of almost all previous work in Bayesian exploration.



The goal of this paper is to discover a set of discriminative patches which
can serve as a fully unsupervised mid-level visual representation. The desired
patches need to satisfy two requirements: 1) to be representative, they need to
occur frequently enough in the visual world; 2) to be discriminative, they need
to be different enough from the rest of the visual world. The patches could
correspond to parts, objects, "visual phrases", etc. but are not restricted to
be any one of them. We pose this as an unsupervised discriminative clustering
problem on a huge dataset of image patches. We use an iterative procedure which
alternates between clustering and training discriminative classifiers, while
applying careful cross-validation at each step to prevent overfitting. The
paper experimentally demonstrates the effectiveness of discriminative patches
as an unsupervised mid-level visual representation, suggesting that it could be
used in place of visual words for many tasks. Furthermore, discriminative
patches can also be used in a supervised regime, such as scene classification,
where they demonstrate state-of-the-art performance on the MIT Indoor-67
dataset.



This paper deals with the distributed processing in the search for an optimum
classification model using evolutionary product unit neural networks. For this
distributed search we used a cluster of computers. Our objective is to obtain a
more efficient design than those net architectures which do not use a
distributed process and which thus result in simpler designs. In order to get
the best classification models we use evolutionary algorithms to train and
design neural networks, which require a very time consuming computation. The
reasons behind the need for this distribution are various. It is complicated to
train this type of nets because of the difficulty entailed in determining their
architecture due to the complex error surface. On the other hand, the use of
evolutionary algorithms involves running a great number of tests with different
seeds and parameters, thus resulting in a high computational cost



We introduce kLog, a novel approach to statistical relational learning.
Unlike standard approaches, kLog does not represent a probability distribution
directly. It is rather a language to perform kernel-based learning on
expressive logical and relational representations. kLog allows users to specify
learning problems declaratively. It builds on simple but powerful concepts:
learning from interpretations, entity/relationship data modeling, logic
programming, and deductive databases. Access by the kernel to the rich
representation is mediated by a technique we call graphicalization: the
relational representation is first transformed into a graph --- in particular,
a grounded entity/relationship diagram. Subsequently, a choice of graph kernel
defines the feature space. kLog supports mixed numerical and symbolic data, as
well as background knowledge in the form of Prolog or Datalog programs as in
inductive logic programming systems. The kLog framework can be applied to
tackle the same range of tasks that has made statistical relational learning so
popular, including classification, regression, multitask learning, and
collective classification. We also report about empirical comparisons, showing
that kLog can be either more accurate, or much faster at the same level of
accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at
http://klog.dinfo.unifi.it along with tutorials.



The free energy functional has recently been proposed as a variational
principle for bounded rational decision-making, since it instantiates a natural
trade-off between utility gains and information processing costs that can be
axiomatically derived. Here we apply the free energy principle to general
decision trees that include both adversarial and stochastic environments. We
derive generalized sequential optimality equations that not only include the
Bellman optimality equations as a limit case, but also lead to well-known
decision-rules such as Expectimax, Minimax and Expectiminimax. We show how
these decision-rules can be derived from a single free energy principle that
assigns a resource parameter to each node in the decision tree. These resource
parameters express a concrete computational cost that can be measured as the
amount of samples that are needed from the distribution that belongs to each
node. The free energy principle therefore provides the normative basis for
generalized optimality equations that account for both adversarial and
stochastic environments.



We propose Coactive Learning as a model of interaction between a learning
system and a human user, where both have the common goal of providing results
of maximum utility to the user. At each step, the system (e.g. search engine)
receives a context (e.g. query) and predicts an object (e.g. ranking). The user
responds by correcting the system if necessary, providing a slightly improved
-- but not necessarily optimal -- object as feedback. We argue that such
feedback can often be inferred from observable user behavior, for example, from
clicks in web-search. Evaluating predictions by their cardinal utility to the
user, we propose efficient learning algorithms that have ${\cal
O}(\frac{1}{\sqrt{T}})$ average regret, even though the learning algorithm
never observes cardinal utility values as in conventional online learning. We
demonstrate the applicability of our model and learning algorithms on a movie
recommendation task, as well as ranking for web-search.



OpenGM is a C++ template library for defining discrete graphical models and
performing inference on these models, using a wide range of state-of-the-art
algorithms. No restrictions are imposed on the factor graph to allow for
higher-order factors and arbitrary neighborhood structures. Large models with
repetitive structure are handled efficiently because (i) functions that occur
repeatedly need to be stored only once, and (ii) distinct functions can be
implemented differently, using different encodings alongside each other in the
same model. Several parametric functions (e.g. metrics), sparse and dense value
tables are provided and so is an interface for custom C++ code. Algorithms are
separated by design from the representation of graphical models and are easily
exchangeable. OpenGM, its algorithms, HDF5 file format and command line tools
are modular and extendible.



We analyze different aspects of our quantum modeling approach of human
concepts, and more specifically focus on the quantum effects of contextuality,
interference, entanglement and emergence, illustrating how each of them makes
its appearance in specific situations of the dynamics of human concepts and
their combinations. We point out the relation of our approach, which is based
on an ontology of a concept as an entity in a state changing under influence of
a context, with the main traditional concept theories, i.e. prototype theory,
exemplar theory and theory theory. We ponder about the question why quantum
theory performs so well in its modeling of human concepts, and shed light on
this question by analyzing the role of complex amplitudes, showing how they
allow to describe interference in the statistics of measurement outcomes, while
in the traditional theories statistics of outcomes originates in classical
probability weights, without the possibility of interference. The relevance of
complex numbers, the appearance of entanglement, and the role of Fock space in
explaining contextual emergence, all as unique features of the quantum
modeling, are explicitly revealed in this paper by analyzing human concepts and
their dynamics.



Agricultural research has been profited by technical advances such as
automation, data mining. Today, data mining is used in a vast areas and many
off-the-shelf data mining system products and domain specific data mining
application soft wares are available, but data mining in agricultural soil
datasets is a relatively a young research field. The large amounts of data that
are nowadays virtually harvested along with the crops have to be analyzed and
should be used to their full extent. This research aims at analysis of soil
dataset using data mining techniques. It focuses on classification of soil
using various algorithms available. Another important purpose is to predict
untested attributes using regression technique, and implementation of automated
soil sample classification.



The Generalized Traveling Salesman Problem (GTSP) is an extension of the
well-known Traveling Salesman Problem (TSP), where the node set is partitioned
into clusters, and the objective is to find the shortest cycle visiting each
cluster exactly once. In this paper, we present a new hybrid Ant Colony System
(ACS) algorithm for the symmetric GTSP. The proposed algorithm is a
modification of a simple ACS for the TSP improved by an efficient GTSP-specific
local search procedure. Our extensive computational experiments show that the
use of the local search procedure dramatically improves the performance of the
ACS algorithm, making it one of the most successful GTSP metaheuristics to
date.



Twitter introduced user lists in late 2009, allowing users to be grouped
according to meaningful topics or themes. Lists have since been adopted by
media outlets as a means of organising content around news stories. Thus the
curation of these lists is important - they should contain the key information
gatekeepers and present a balanced perspective on a story. Here we address this
list curation process from a recommender systems perspective. We propose a
variety of criteria for generating user list recommendations, based on content
analysis, network analysis, and the "crowdsourcing" of existing user lists. We
demonstrate that these types of criteria are often only successful for datasets
with certain characteristics. To resolve this issue, we propose the aggregation
of these different "views" of a news story on Twitter to produce more accurate
user recommendations to support the curation process.



We propose a novel Bayesian approach to solve stochastic optimization
problems that involve finding extrema of noisy, nonlinear functions. Previous
work has focused on representing possible functions explicitly, which leads to
a two-step procedure of first, doing inference over the function space and
second, finding the extrema of these functions. Here we skip the representation
step and directly model the distribution over extrema. To this end, we devise a
non-parametric conjugate prior based on a kernel regressor. The resulting
posterior distribution directly captures the uncertainty over the maximum of
the unknown function. We illustrate the effectiveness of our model by
optimizing a noisy, high-dimensional, non-convex objective function.



We present a suite of algorithms for Dimension Independent Similarity
Computation (DISCO) to compute all pairwise similarities between very high
dimensional sparse vectors. All of our results are provably independent of
dimension, meaning apart from the initial cost of trivially reading in the
data, all subsequent operations are independent of the dimension, thus the
dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard
similarity measures. For Jaccard similiarity we include an improved version of
MinHash. Our results are geared toward the MapReduce framework. We empirically
validate our theorems at large scale using data from the social networking site
Twitter. At time of writing, our algorithms are live in production at
twitter.com.



We describe a system for meta-analysis where a wiki stores numerical data in
a simple format and a web service performs the numerical computation.
  We initially apply the system on multiple meta-analyses of structural
neuroimaging data results. The described system allows for mass meta-analysis,
e.g., meta-analysis across multiple brain regions and multiple mental
disorders.



The associationist account for early word-learning is based on the
co-occurrence between objects and words. Here we examine the performance of a
simple associative learning algorithm for acquiring the referents of words in a
cross-situational scenario affected by noise produced by out-of-context words.
We find a critical value of the noise parameter $\gamma_c$ above which learning
is impossible. We use finite-size scaling to show that the sharpness of the
transition persists across a region of order $\tau^{-1/2}$ about $\gamma_c$,
where $\tau$ is the number of learning trials, as well as to obtain the
learning error (scaling function) in the critical region. In addition, we show
that the distribution of durations of periods when the learning error is zero
is a power law with exponent -3/2 at the critical point.



Assume that cause-effect relationships between variables can be described as
a directed acyclic graph and the corresponding linear structural equation
model.We consider the identification problem of total effects in the presence
of latent variables and selection bias between a treatment variable and a
response variable. Pearl and his colleagues provided the back door criterion,
the front door criterion (Pearl, 2000) and the conditional instrumental
variable method (Brito and Pearl, 2002) as identifiability criteria for total
effects in the presence of latent variables, but not in the presence of
selection bias. In order to solve this problem, we propose new graphical
identifiability criteria for total effects based on the identifiable factor
models. The results of this paper are useful to identify total effects in
observational studies and provide a new viewpoint to the identification
conditions of factor models.



We consider conditions that allow us to find an optimal strategy for
sequential decisions from a given data situation. For the case where all
interventions are unconditional (atomic), identifiability has been discussed by
Pearl & Robins (1995). We argue here that an optimal strategy must be
conditional, i.e. take the information available at each decision point into
account. We show that the identification of an optimal sequential decision
strategy is more restrictive, in the sense that conditional interventions might
not always be identified when atomic interventions are. We further demonstrate
that a simple graphical criterion for the identifiability of an optimal
strategy can be given.



We introduce Church, a universal language for describing stochastic
generative processes. Church is based on the Lisp model of lambda calculus,
containing a pure Lisp as its deterministic subset. The semantics of Church is
defined in terms of evaluation histories and conditional distributions on such
histories. Church also includes a novel language construct, the stochastic
memoizer, which enables simple description of many complex non-parametric
models. We illustrate language features through several examples, including: a
generalized Bayes net in which parameters cluster over trials, infinite PCFGs,
planning by inference, and various non-parametric clustering models. Finally,
we show how to implement query on any Church program, exactly and
approximately, using Monte Carlo techniques.



An important task in data analysis is the discovery of causal relationships
between observed variables. For continuous-valued data, linear acyclic causal
models are commonly used to model the data-generating process, and the
inference of such models is a well-studied problem. However, existing methods
have significant limitations. Methods based on conditional independencies
(Spirtes et al. 1993; Pearl 2000) cannot distinguish between
independence-equivalent models, whereas approaches purely based on Independent
Component Analysis (Shimizu et al. 2006) are inapplicable to data which is
partially Gaussian. In this paper, we generalize and combine the two
approaches, to yield a method able to learn the model structure in many cases
for which the previous methods provide answers that are either incorrect or are
not as informative as possible. We give exact graphical conditions for when two
distinct models represent the same family of distributions, and empirically
demonstrate the power of our method through thorough simulations.



We study a multiagent learning problem where agents can either learn via
repeated interactions, or can follow the advice of a mediator who suggests
possible actions to take. We present an algorithmthat each agent can use so
that, with high probability, they can verify whether or not the mediator's
advice is useful. In particular, if the mediator's advice is useful then agents
will reach a correlated equilibrium, but if the mediator's advice is not
useful, then agents are not harmed by using our test, and can fall back to
their original learning algorithm. We then generalize our algorithm and show
that in the limit it always correctly verifies the mediator's advice.



We consider the problem of efficiently learning optimal control policies and
value functions over large state spaces in an online setting in which estimates
must be available after each interaction with the world. This paper develops an
explicitly model-based approach extending the Dyna architecture to linear
function approximation. Dynastyle planning proceeds by generating imaginary
experience from the world model and then applying model-free reinforcement
learning algorithms to the imagined state transitions. Our main results are to
prove that linear Dyna-style planning converges to a unique solution
independent of the generating distribution, under natural conditions. In the
policy evaluation setting, we prove that the limit point is the least-squares
(LSTD) solution. An implication of our results is that prioritized-sweeping can
be soundly extended to the linear approximation case, backing up to preceding
features rather than to preceding states. We introduce two versions of
prioritized sweeping with linear Dyna and briefly illustrate their performance
empirically on the Mountain Car and Boyan Chain problems.



Linear Programming (LP) relaxations have become powerful tools for finding
the most probable (MAP) configuration in graphical models. These relaxations
can be solved efficiently using message-passing algorithms such as belief
propagation and, when the relaxation is tight, provably find the MAP
configuration. The standard LP relaxation is not tight enough in many
real-world problems, however, and this has lead to the use of higher order
cluster-based LP relaxations. The computational cost increases exponentially
with the size of the clusters and limits the number and type of clusters we can
use. We propose to solve the cluster selection problem monotonically in the
dual LP, iteratively selecting clusters with guaranteed improvement, and
quickly re-solving with the added clusters by reusing the existing solution.
Our dual message-passing algorithm finds the MAP configuration in protein
sidechain placement, protein design, and stereo problems, in cases where the
standard LP relaxation fails.



Recent works on cost based relaxations have improved Constraint Programming
(CP) models for the Traveling Salesman Problem (TSP). We provide a short survey
over solving asymmetric TSP with CP. Then, we suggest new implied propagators
based on general graph properties. We experimentally show that such implied
propagators bring robustness to pathological instances and highlight the fact
that graph structure can significantly improve search heuristics behavior.
Finally, we show that our approach outperforms current state of the art
results.



The main stated contribution of the Deformable Parts Model (DPM) detector of
Felzenszwalb et al. (over the Histogram-of-Oriented-Gradients approach of Dalal
and Triggs) is the use of deformable parts. A secondary contribution is the
latent discriminative learning. Tertiary is the use of multiple components. A
common belief in the vision community (including ours, before this study) is
that their ordering of contributions reflects the performance of detector in
practice. However, what we have experimentally found is that the ordering of
importance might actually be the reverse. First, we show that by increasing the
number of components, and switching the initialization step from their
aspect-ratio, left-right flipping heuristics to appearance-based clustering,
considerable improvement in performance is obtained. But more intriguingly, we
show that with these new components, the part deformations can now be
completely switched off, yet obtaining results that are almost on par with the
original DPM detector. Finally, we also show initial results for using multiple
components on a different problem -- scene classification, suggesting that this
idea might have wider applications in addition to object detection.



We systematically investigate the complexity of model checking the
existential positive fragment of first-order logic. In particular, for a set of
existential positive sentences, we consider model checking where the sentence
is restricted to fall into the set; a natural question is then to classify
which sentence sets are tractable and which are intractable. With respect to
fixed-parameter tractability, we give a general theorem that reduces this
classification question to the corresponding question for primitive positive
logic, for a variety of representations of structures. This general theorem
allows us to deduce that an existential positive sentence set having bounded
arity is fixed-parameter tractable if and only if each sentence is equivalent
to one in bounded-variable logic. We then use the lens of classical complexity
to study these fixed-parameter tractable sentence sets. We show that such a set
can be NP-complete, and consider the length needed by a translation from
sentences in such a set to bounded-variable logic; we prove superpolynomial
lower bounds on this length using the theory of compilability, obtaining an
interesting type of formula size lower bound. Overall, the tools, concepts, and
results of this article set the stage for the future consideration of the
complexity of model checking on more expressive logics.



In this work we present an algorithm for covering continuous connected
domains by ant-like robots with very limited capabilities. The robots can mark
visited places with pheromone marks and sense the level of the pheromone in
their local neighborhood. In case of multiple robots these pheromone marks can
be sensed by all robots and provide the only way of (indirect) communication
between the robots. The robots are assumed to be memoryless, and to have no
global information such as the domain map, their own position (either absolute
or relative), total marked area percentage, maximal pheromone level, etc..
Despite the robots' simplicity, we show that they are able, by running a very
simple rule of behavior, to ensure efficient covering of arbitrary connected
domains, including non-planar and multidimensional ones. The novelty of our
algorithm lies in the fact that, unlike previously proposed methods, our
algorithm works on continuous domains without relying on some "induced"
underlying graph, that effectively reduces the problem to a discrete case of
graph covering. The algorithm guarantees complete coverage of any connected
domain. We also prove that the algorithm is noise immune, i.e., it is able to
cope with any initial pheromone profile (noise). In addition the algorithm
provides a bounded constant time between two successive visits of the robot,
and thus, is suitable for patrolling or surveillance applications.



This paper revisits the problem of analyzing multiple ratings given by
different judges. Different from previous work that focuses on distilling the
true labels from noisy crowdsourcing ratings, we emphasize gaining diagnostic
insights into our in-house well-trained judges. We generalize the well-known
DawidSkene model (Dawid & Skene, 1979) to a spectrum of probabilistic models
under the same "TrueLabel + Confusion" paradigm, and show that our proposed
hierarchical Bayesian model, called HybridConfusion, consistently outperforms
DawidSkene on both synthetic and real-world data sets.



Model-based Bayesian Reinforcement Learning (BRL) allows a found
formalization of the problem of acting optimally while facing an unknown
environment, i.e., avoiding the exploration-exploitation dilemma. However,
algorithms explicitly addressing BRL suffer from such a combinatorial explosion
that a large body of work relies on heuristic algorithms. This paper introduces
BOLT, a simple and (almost) deterministic heuristic algorithm for BRL which is
optimistic about the transition function. We analyze BOLT's sample complexity,
and show that under certain parameters, the algorithm is near-optimal in the
Bayesian sense with high probability. Then, experimental results highlight the
key differences of this method compared to previous work.



Inverse optimal control, also known as inverse reinforcement learning, is the
problem of recovering an unknown reward function in a Markov decision process
from expert demonstrations of the optimal policy. We introduce a probabilistic
inverse optimal control algorithm that scales gracefully with task
dimensionality, and is suitable for large, continuous domains where even
computing a full policy is impractical. By using a local approximation of the
reward function, our method can also drop the assumption that the
demonstrations are globally optimal, requiring only local optimality. This
allows it to learn from examples that are unsuitable for prior methods.



We consider the problem of parameter estimation using weakly supervised
datasets, where a training sample consists of the input and a partially
specified annotation, which we refer to as the output. The missing information
in the annotation is modeled using latent variables. Previous methods
overburden a single distribution with two separate tasks: (i) modeling the
uncertainty in the latent variables during training; and (ii) making accurate
predictions for the output and the latent variables during testing. We propose
a novel framework that separates the demands of the two tasks using two
distributions: (i) a conditional distribution to model the uncertainty of the
latent variables for a given input-output pair; and (ii) a delta distribution
to predict the output and the latent variables for a given input. During
learning, we encourage agreement between the two distributions by minimizing a
loss-based dissimilarity coefficient. Our approach generalizes latent SVM in
two important ways: (i) it models the uncertainty over latent variables instead
of relying on a pointwise estimate; and (ii) it allows the use of loss
functions that depend on latent variables, which greatly increases its
applicability. We demonstrate the efficacy of our approach on two challenging
problems---object detection and action detection---using publicly available
datasets.



Effective learning of user preferences is critical to easing user burden in
various types of matching problems. Equally important is active query selection
to further reduce the amount of preference information users must provide. We
address the problem of active learning of user preferences for matching
problems, introducing a novel method for determining probabilistic matchings,
and developing several new active learning strategies that are sensitive to the
specific matching objective. Experiments with real-world data sets spanning
diverse domains demonstrate that matching-sensitive active learning



Belief Propagation (BP) is one of the most popular methods for inference in
probabilistic graphical models. BP is guaranteed to return the correct answer
for tree structures, but can be incorrect or non-convergent for loopy graphical
models. Recently, several new approximate inference algorithms based on cavity
distribution have been proposed. These methods can account for the effect of
loops by incorporating the dependency between BP messages. Alternatively,
region-based approximations (that lead to methods such as Generalized Belief
Propagation) improve upon BP by considering interactions within small clusters
of variables, thus taking small loops within these clusters into account. This
paper introduces an approach, Generalized Loop Correction (GLC), that benefits
from both of these types of loop correction. We show how GLC relates to these
two families of inference methods, then provide empirical evidence that GLC
works effectively in general, and can be significantly more accurate than both
correction schemes.



Much of current machine learning (ML) research has lost its connection to
problems of import to the larger world of science and society. From this
perspective, there exist glaring limitations in the data sets we investigate,
the metrics we employ for evaluation, and the degree to which results are
communicated back to their originating domains. What changes are needed to how
we conduct research to increase the impact that ML has? We present six Impact
Challenges to explicitly focus the field?s energy and attention, and we discuss
existing obstacles that must be addressed. We aim to inspire ongoing discussion
and focus on ML that matters.



Precision-recall (PR) curves and the areas under them are widely used to
summarize machine learning results, especially for data sets exhibiting class
skew. They are often used analogously to ROC curves and the area under ROC
curves. It is known that PR curves vary as class skew changes. What was not
recognized before this paper is that there is a region of PR space that is
completely unachievable, and the size of this region depends only on the skew.
This paper precisely characterizes the size of that region and discusses its
implications for empirical evaluation methodology in machine learning.



We propose a new method for parameter learning in Bayesian networks with
qualitative influences. This method extends our previous work from networks of
binary variables to networks of discrete variables with ordered values. The
specified qualitative influences correspond to certain order restrictions on
the parameters in the network. These parameters may therefore be estimated
using constrained maximum likelihood estimation. We propose an alternative
method, based on the isotonic regression. The constrained maximum likelihood
estimates are fairly complicated to compute, whereas computation of the
isotonic regression estimates only requires the repeated application of the
Pool Adjacent Violators algorithm for linear orders. Therefore, the isotonic
regression estimator is to be preferred from the viewpoint of computational
complexity. Through experiments on simulated and real data, we show that the
new learning method is competitive in performance to the constrained maximum
likelihood estimator, and that both estimators improve on the standard
estimator.



We present the mixture-of-parents maximum entropy Markov model (MoP-MEMM), a
class of directed graphical models extending MEMMs. The MoP-MEMM allows
tractable incorporation of long-range dependencies between nodes by restricting
the conditional distribution of each node to be a mixture of distributions
given the parents. We show how to efficiently compute the exact marginal
posterior node distributions, regardless of the range of the dependencies. This
enables us to model non-sequential correlations present within text documents,
as well as between interconnected documents, such as hyperlinked web pages. We
apply the MoP-MEMM to a named entity recognition task and a web page
classification task. In each, our model shows significant improvement over the
basic MEMM, and is competitive with other long-range sequence models that use
approximate inference.



We present a graphical criterion for reading dependencies from the minimal
directed independence map G of a graphoid p when G is a polytree and p
satisfies composition and weak transitivity. We prove that the criterion is
sound and complete. We argue that assuming composition and weak transitivity is
not too restrictive.



We analyze the generalized Mallows model, a popular exponential model over
rankings. Estimating the central (or consensus) ranking from data is NP-hard.
We obtain the following new results: (1) We show that search methods can
estimate both the central ranking pi0 and the model parameters theta exactly.
The search is n! in the worst case, but is tractable when the true distribution
is concentrated around its mode; (2) We show that the generalized Mallows model
is jointly exponential in (pi0; theta), and introduce the conjugate prior for
this model class; (3) The sufficient statistics are the pairwise marginal
probabilities that item i is preferred to item j. Preliminary experiments
confirm the theoretical predictions and compare the new algorithm and existing
heuristics.



The belief propagation (BP) algorithm is widely applied to perform
approximate inference on arbitrary graphical models, in part due to its
excellent empirical properties and performance. However, little is known
theoretically about when this algorithm will perform well. Using recent
analysis of convergence and stability properties in BP and new results on
approximations in binary systems, we derive a bound on the error in BP's
estimates for pairwise Markov random fields over discrete valued random
variables. Our bound is relatively simple to compute, and compares favorably
with a previous method of bounding the accuracy of BP.



Finding the most probable assignment (MAP) in a general graphical model is
known to be NP hard but good approximations have been attained with max-product
belief propagation (BP) and its variants. In particular, it is known that using
BP on a single-cycle graph or tree reweighted BP on an arbitrary graph will
give the MAP solution if the beliefs have no ties. In this paper we extend the
setting under which BP can be used to provably extract the MAP. We define
Convex BP as BP algorithms based on a convex free energy approximation and show
that this class includes ordinary BP with single-cycle, tree reweighted BP and
many other BP variants. We show that when there are no ties, fixed-points of
convex max-product BP will provably give the MAP solution. We also show that
convex sum-product BP at sufficiently small temperatures can be used to solve
linear programs that arise from relaxing the MAP problem. Finally, we derive a
novel condition that allows us to derive the MAP solution even if some of the
convex BP beliefs have ties. In experiments, we show that our theorems allow us
to find the MAP in many real-world instances of graphical models where exact
inference using junction-tree is impossible.



The goal of imitation learning is for an apprentice to learn how to behave in
a stochastic environment by observing a mentor demonstrating the correct
behavior. Accurate prior knowledge about the correct behavior can reduce the
need for demonstrations from the mentor. We present a novel approach to
encoding prior knowledge about the correct behavior, where we assume that this
prior knowledge takes the form of a Markov Decision Process (MDP) that is used
by the apprentice as a rough and imperfect model of the mentor's behavior.
Specifically, taking a Bayesian approach, we treat the value of a policy in
this modeling MDP as the log prior probability of the policy. In other words,
we assume a priori that the mentor's behavior is likely to be a high value
policy in the modeling MDP, though quite possibly different from the optimal
policy. We describe an efficient algorithm that, given a modeling MDP and a set
of demonstrations by a mentor, provably converges to a stationary point of the
log posterior of the mentor's policy, where the posterior is computed with
respect to the "value based" prior. We also present empirical evidence that
this prior does in fact speed learning of the mentor's policy, and is an
improvement in our experiments over similar previous methods.



Belief propagation and its variants are popular methods for approximate
inference, but their running time and even their convergence depend greatly on
the schedule used to send the messages. Recently, dynamic update schedules have
been shown to converge much faster on hard networks than static schedules,
namely the residual BP schedule of Elidan et al. [2006]. But that RBP algorithm
wastes message updates: many messages are computed solely to determine their
priority, and are never actually performed. In this paper, we show that
estimating the residual, rather than calculating it directly, leads to
significant decreases in the number of messages required for convergence, and
in the total running time. The residual is estimated using an upper bound based
on recent work on message errors in BP. On both synthetic and real-world
networks, this dramatically decreases the running time of BP, in some cases by
a factor of five, without affecting the quality of the solution.



We present a novel approach to detecting and utilizing symmetries in
probabilistic graphical models with two main contributions. First, we present a
scalable approach to computing generating sets of permutation groups
representing the symmetries of graphical models. Second, we introduce orbital
Markov chains, a novel family of Markov chains leveraging model symmetries to
reduce mixing times. We establish an insightful connection between model
symmetries and rapid mixing of orbital Markov chains. Thus, we present the
first lifted MCMC algorithm for probabilistic graphical models. Both analytical
and empirical results demonstrate the effectiveness and efficiency of the
approach.



Gaussian processes (GP) are powerful tools for probabilistic modeling
purposes. They can be used to define prior distributions over latent functions
in hierarchical Bayesian models. The prior over functions is defined implicitly
by the mean and covariance function, which determine the smoothness and
variability of the function. The inference can then be conducted directly in
the function space by evaluating or approximating the posterior process.
Despite their attractive theoretical properties GPs provide practical
challenges in their implementation. GPstuff is a versatile collection of
computational tools for GP models compatible with Linux and Windows MATLAB and
Octave. It includes, among others, various inference methods, sparse
approximations and tools for model assessment. In this work, we review these
tools and demonstrate the use of GPstuff in several models.



We propose a new probabilistic graphical model that jointly models the
difficulties of questions, the abilities of participants and the correct
answers to questions in aptitude testing and crowdsourcing settings. We devise
an active learning/adaptive testing scheme based on a greedy minimization of
expected model entropy, which allows a more efficient resource allocation by
dynamically choosing the next question to be asked based on the previous
responses. We present experimental results that confirm the ability of our
model to infer the required parameters and demonstrate that the adaptive
testing scheme requires fewer questions to obtain the same accuracy as a static
test scenario.



We consider the incorporation of causal knowledge about the presence or
absence of (possibly indirect) causal relations into a causal model. Such
causal relations correspond to directed paths in a causal model. This type of
knowledge naturally arises from experimental data, among others. Specifically,
we consider the formalisms of Causal Bayesian Networks and Maximal Ancestral
Graphs and their Markov equivalence classes: Partially Directed Acyclic Graphs
and Partially Oriented Ancestral Graphs. We introduce sound and complete
procedures which are able to incorporate causal prior knowledge in such models.
In simulated experiments, we show that often considering even a few causal
facts leads to a significant number of new inferences. In a case study, we also
show how to use real experimental data to infer causal knowledge and
incorporate it into a real biological causal network. The code is available at
mensxmachina.org.



Learning from electronic medical records (EMR) is challenging due to their
relational nature and the uncertain dependence between a patient's past and
future health status. Statistical relational learning is a natural fit for
analyzing EMRs but is less adept at handling their inherent latent structure,
such as connections between related medications or diseases. One way to capture
the latent structure is via a relational clustering of objects. We propose a
novel approach that, instead of pre-clustering the objects, performs a
demand-driven clustering during learning. We evaluate our algorithm on three
real-world tasks where the goal is to use EMRs to predict whether a patient
will have an adverse reaction to a medication. We find that our approach is
more accurate than performing no clustering, pre-clustering, and using
expert-constructed medical heterarchies.



In Passive POMDPs actions do not affect the world state, but still incur
costs. When the agent is bounded by information-processing constraints, it can
only keep an approximation of the belief. We present a variational principle
for the problem of maintaining the information which is most useful for
minimizing the cost, and introduce an efficient and simple algorithm for
finding an optimum.



We consider a framework for structured prediction based on search in the
space of complete structured outputs. Given a structured input, an output is
produced by running a time-bounded search procedure guided by a learned cost
function, and then returning the least cost output uncovered during the search.
This framework can be instantiated for a wide range of search spaces and search
procedures, and easily incorporates arbitrary structured-prediction loss
functions. In this paper, we make two main technical contributions. First, we
define the limited-discrepancy search space over structured outputs, which is
able to leverage powerful classification learning algorithms to improve the
search space quality. Second, we give a generic cost function learning
approach, where the key idea is to learn a cost function that attempts to mimic
the behavior of conducting searches guided by the true loss function. Our
experiments on six benchmark domains demonstrate that using our framework with
only a small amount of search is sufficient for significantly improving on
state-of-the-art structured-prediction performance.



We consider apprenticeship learning, i.e., having an agent learn a task by
observing an expert demonstrating the task in a partially observable
environment when the model of the environment is uncertain. This setting is
useful in applications where the explicit modeling of the environment is
difficult, such as a dialogue system. We show that we can extract information
about the environment model by inferring action selection process behind the
demonstration, under the assumption that the expert is choosing optimal actions
based on knowledge of the true model of the target environment. Proposed
algorithms can achieve more accurate estimates of POMDP parameters and better
policies from a short demonstration, compared to methods that learns only from
the reaction from the environment.



We describe an expert system, MAIES, developed for analysing forensic
identification problems involving DNA mixture traces using quantitative peak
area information. Peak area information is represented by conditional Gaussian
distributions, and inference based on exact junction tree propagation
ascertains whether individuals, whose profiles have been measured, have
contributed to the mixture. The system can also be used to predict DNA profiles
of unknown contributors by separating the mixture into its individual
components. The use of the system is illustrated with an application to a real
world example. The system implements a novel MAP (maximum a posteriori) search
algorithm that is described in an appendix.



We consider a Bayesian method for learning the Bayesian network structure
from complete data. Recently, Koivisto and Sood (2004) presented an algorithm
that for any single edge computes its marginal posterior probability in O(n
2^n) time, where n is the number of attributes; the number of parents per
attribute is bounded by a constant. In this paper we show that the posterior
probabilities for all the n (n - 1) potential edges can be computed in O(n 2^n)
total time. This result is achieved by a forward-backward technique and fast
Moebius transform algorithms, which are of independent interest. The resulting
speedup by a factor of about n^2 allows us to experimentally study the
statistical power of learning moderate-size networks. We report results from a
simulation study that covers data sets with 20 to 10,000 records over 5 to 25
discrete attributes



We investigate methods for parameter learning from incomplete data that is
not missing at random. Likelihood-based methods then require the optimization
of a profile likelihood that takes all possible missingness mechanisms into
account. Optimzing this profile likelihood poses two main difficulties:
multiple (local) maxima, and its very high-dimensional parameter space. In this
paper a new method is presented for optimizing the profile likelihood that
addresses the second difficulty: in the proposed AI&M (adjusting imputation and
mazimization) procedure the optimization is performed by operations in the
space of data completions, rather than directly in the parameter space of the
profile likelihood. We apply the AI&M method to learning parameters for
Bayesian networks. The method is compared against conservative inference, which
takes into account each possible data completion, and against EM. The results
indicate that likelihood-based inference is still feasible in the case of
unknown missingness mechanisms, and that conservative inference is
unnecessarily weak. On the other hand, our results also provide evidence that
the EM algorithm is still quite effective when the data is not missing at
random.



SDYNA is a general framework designed to address large stochastic
reinforcement learning problems. Unlike previous model based methods in FMDPs,
it incrementally learns the structure and the parameters of a RL problem using
supervised learning techniques. Then, it integrates decision-theoric planning
algorithms based on FMDPs to compute its policy. SPITI is an instanciation of
SDYNA that exploits ITI, an incremental decision tree algorithm, to learn the
reward function and the Dynamic Bayesian Networks with local structures
representing the transition function of the problem. These representations are
used by an incremental version of the Structured Value Iteration algorithm. In
order to learn the structure, SPITI uses Chi-Square tests to detect the
independence between two probability distributions. Thus, we study the relation
between the threshold used in the Chi-Square test, the size of the model built
and the relative error of the value function of the induced policy with respect
to the optimal value. We show that, on stochastic problems, one can tune the
threshold so as to generate both a compact model and an efficient policy. Then,
we show that SPITI, while keeping its model compact, uses the generalization
property of its learning method to perform better than a stochastic classical
tabular algorithm in large RL problem with an unknown structure. We also
introduce a new measure based on Chi-Square to qualify the accuracy of the
model learned by SPITI. We qualitatively show that the generalization property
in SPITI within the FMDP framework may prevent an exponential growth of the
time required to learn the structure of large stochastic RL problems.



One approach to monitoring a dynamic system relies on decomposition of the
system into weakly interacting subsystems. An earlier paper introduced a notion
of weak interaction called separability, and showed that it leads to exact
propagation of marginals for prediction. This paper addresses two questions
left open by the earlier paper: can we define a notion of approximate
separability that occurs naturally in practice, and do separability and
approximate separability lead to accurate monitoring? The answer to both
questions is afirmative. The paper also analyzes the structure of approximately
separable decompositions, and provides some explanation as to why these models
perform well.



We propose a method to identify all the nodes that are relevant to compute
all the conditional probability distributions for a given set of nodes. Our
method is simple, effcient, consistent, and does not require learning a
Bayesian network first. Therefore, our method can be applied to
high-dimensional databases, e.g. gene expression databases.



Collaborative data consist of ratings relating two distinct sets of objects:
users and items. Much of the work with such data focuses on filtering:
predicting unknown ratings for pairs of users and items. In this paper we focus
on the problem of visualizing the information. Given all of the ratings, our
task is to embed all of the users and items as points in the same Euclidean
space. We would like to place users near items that they have rated (or would
rate) high, and far away from those they would give a low rating. We pose this
problem as a real-valued non-linear Bayesian network and employ Markov chain
Monte Carlo and expectation maximization to find an embedding. We present a
metric by which to judge the quality of a visualization and compare our results
to local linear embedding and Eigentaste on three real-world datasets.



Previous work in hierarchical reinforcement learning has faced a dilemma:
either ignore the values of different possible exit states from a subroutine,
thereby risking suboptimal behavior, or represent those values explicitly
thereby incurring a possibly large representation cost because exit values
refer to nonlocal aspects of the world (i.e., all subsequent rewards). This
paper shows that, in many cases, one can avoid both of these problems. The
solution is based on recursively decomposing the exit value function in terms
of Q-functions at higher levels of the hierarchy. This leads to an intuitively
appealing runtime architecture in which a parent subroutine passes to its child
a value function on the exit states and the child reasons about how its choices
affect the exit value. We also identify structural conditions on the value
function and transition distributions that allow much more concise
representations of exit state distributions, leading to further state
abstraction. In essence, the only variables whose exit values need be
considered are those that the parent cares about and the child affects. We
demonstrate the utility of our algorithms on a series of increasingly complex
environments.



Traditional approaches to Bayes net structure learning typically assume
little regularity in graph structure other than sparseness. However, in many
cases, we expect more systematicity: variables in real-world systems often
group into classes that predict the kinds of probabilistic dependencies they
participate in. Here we capture this form of prior knowledge in a hierarchical
Bayesian framework, and exploit it to enable structure learning and type
discovery from small datasets. Specifically, we present a nonparametric
generative model for directed acyclic graphs as a prior for Bayes net structure
learning. Our model assumes that variables come in one or more classes and that
the prior probability of an edge existing between two variables is a function
only of their classes. We derive an MCMC algorithm for simultaneous inference
of the number of classes, the class assignments of variables, and the Bayes net
structure over variables. For several realistic, sparse datasets, we show that
the bias towards systematicity of connections provided by our model yields more
accurate learned networks than a traditional, uniform prior approach, and that
the classes found by our model are appropriate.



Bayesian Networks (BNs) are useful tools giving a natural and compact
representation of joint probability distributions. In many applications one
needs to learn a Bayesian Network (BN) from data. In this context, it is
important to understand the number of samples needed in order to guarantee a
successful learning. Previous work have studied BNs sample complexity, yet it
mainly focused on the requirement that the learned distribution will be close
to the original distribution which generated the data. In this work, we study a
different aspect of the learning, namely the number of samples needed in order
to learn the correct structure of the network. We give both asymptotic results,
valid in the large sample limit, and experimental results, demonstrating the
learning behavior for feasible sample sizes. We show that structure learning is
a more difficult task, compared to approximating the correct distribution, in
the sense that it requires a much larger number of samples, regardless of the
computational power available for the learner.



In many cases it makes sense to model a relationship symmetrically, not
implying any particular directionality. Consider the classical example of a
recommendation system where the rating of an item by a user should
symmetrically be dependent on the attributes of both the user and the item. The
attributes of the (known) relationships are also relevant for predicting
attributes of entities and for predicting attributes of new relations. In
recommendation systems, the exploitation of relational attributes is often
referred to as collaborative filtering. Again, in many applications one might
prefer to model the collaborative effect in a symmetrical way. In this paper we
present a relational model, which is completely symmetrical. The key innovation
is that we introduce for each entity (or object) an infinite-dimensional latent
variable as part of a Dirichlet process (DP) model. We discuss inference in the
model, which is based on a DP Gibbs sampler, i.e., the Chinese restaurant
process. We extend the Chinese restaurant process to be applicable to
relational modeling. Our approach is evaluated in three applications. One is a
recommendation system based on the MovieLens data set. The second application
concerns the prediction of the function of yeast genes/proteins on the data set
of KDD Cup 2001 using a multi-relational model. The third application involves
a relational medical domain. The experimental results show that our model gives
significantly improved estimates of attributes describing relationships or
entities in complex relational models.



We present a non-parametric Bayesian approach to structure learning with
hidden causes. Previous Bayesian treatments of this problem define a prior over
the number of hidden causes and use algorithms such as reversible jump Markov
chain Monte Carlo to move between solutions. In contrast, we assume that the
number of hidden causes is unbounded, but only a finite number influence
observable variables. This makes it possible to use a Gibbs sampler to
approximate the distribution over causal structures. We evaluate the
performance of both approaches in discovering hidden causes in simulated data,
and use our non-parametric approach to discover hidden causes in a real medical
dataset.



Model-based learning algorithms have been shown to use experience efficiently
when learning to solve Markov Decision Processes (MDPs) with finite state and
action spaces. However, their high computational cost due to repeatedly solving
an internal model inhibits their use in large-scale problems. We propose a
method based on real-time dynamic programming (RTDP) to speed up two
model-based algorithms, RMAX and MBIE (model-based interval estimation),
resulting in computationally much faster algorithms with little loss compared
to existing bounds. Specifically, our two new learning algorithms, RTDP-RMAX
and RTDP-IE, have considerably smaller computational demands than RMAX and
MBIE. We develop a general theoretical framework that allows us to prove that
both are efficient learners in a PAC (probably approximately correct) sense. We
also present an experimental evaluation of these new algorithms that helps
quantify the tradeoff between computational and experience demands.



We develop stochastic variational inference, a scalable algorithm for
approximating posterior distributions. We develop this technique for a large
class of probabilistic models and we demonstrate it with two probabilistic
topic models, latent Dirichlet allocation and the hierarchical Dirichlet
process topic model. Using stochastic variational inference, we analyze several
large collections of documents: 300K articles from Nature, 1.8M articles from
The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can
easily handle data sets of this size and outperforms traditional variational
inference, which can only handle a smaller subset. (We also show that the
Bayesian nonparametric topic model outperforms its parametric counterpart.)
Stochastic variational inference lets us apply complex Bayesian models to
massive data sets.



Most current sampling algorithms for high-dimensional distributions are based
on MCMC techniques and are approximate in the sense that they are valid only
asymptotically. Rejection sampling, on the other hand, produces valid samples,
but is unrealistically slow in high-dimension spaces. The OS* algorithm that we
propose is a unified approach to exact optimization and sampling, based on
incremental refinements of a functional upper bound, which combines ideas of
adaptive rejection sampling and of A* optimization search. We show that the
choice of the refinement can be done in a way that ensures tractability in
high-dimension spaces, and we present first experiments in two different
settings: inference in high-order HMMs and in large discrete graphical models.



With the growing interest on Network Analysis, Relational Data Mining is
becoming an emphasized domain of Data Mining. This paper addresses the problem
of extracting representative elements from a relational dataset. After defining
the notion of degree of representativeness, computed using the Borda
aggregation procedure, we present the extraction of exemplars which are the
representative elements of the dataset. We use these concepts to build a
network on the dataset. We expose the main properties of these notions and we
propose two typical applications of our framework. The first application
consists in resuming and structuring a set of binary images and the second in
mining co-authoring relation in a research team.



Although Breadth-First Search (BFS) has several advantages over Depth-First
Search (DFS) its prohibitive space requirements have meant that algorithm
designers often pass it over in favor of DFS. To address this shortcoming, we
introduce a theory of Efficient BFS (EBFS) along with a simple recursive
program schema for carrying out the search. The theory is based on dominance
relations, a long standing technique from the field of search algorithms. We
show how the theory can be used to systematically derive solutions to two graph
algorithms, namely the Single Source Shortest Path problem and the Minimum
Spanning Tree problem. The solutions are found by making small systematic
changes to the derivation, revealing the connections between the two problems
which are often obscured in textbook presentations of them.



We present a method for learning the parameters of a Bayesian network with
prior knowledge about the signs of influences between variables. Our method
accommodates not just the standard signs, but provides for context-specific
signs as well. We show how the various signs translate into order constraints
on the network parameters and how isotonic regression can be used to compute
order-constrained estimates from the available data. Our experimental results
show that taking prior knowledge about the signs of influences into account
leads to an improved fit of the true distribution, especially when only a small
sample of data is available. Moreover, the computed estimates are guaranteed to
be consistent with the specified signs, thereby resulting in a network that is
more likely to be accepted by experts in its domain of application.



One of the basic tasks for Bayesian networks (BNs) is that of learning a
network structure from data. The BN-learning problem is NP-hard, so the
standard solution is heuristic search. Many approaches have been proposed for
this task, but only a very small number outperform the baseline of greedy
hill-climbing with tabu lists; moreover, many of the proposed algorithms are
quite complex and hard to implement. In this paper, we propose a very simple
and easy-to-implement method for addressing this task. Our approach is based on
the well-known fact that the best network (of bounded in-degree) consistent
with a given node ordering can be found very efficiently. We therefore propose
a search not over the space of structures, but over the space of orderings,
selecting for each ordering the best network consistent with it. This search
space is much smaller, makes more global search steps, has a lower branching
factor, and avoids costly acyclicity checks. We present results for this
algorithm on both synthetic and real data sets, evaluating both the score of
the network found and in the running time. We show that ordering-based search
outperforms the standard baseline, and is competitive with recent algorithms
that are much harder to implement.



Combinatorial optimization is widely applied in a number of areas nowadays.
Unfortunately, many combinatorial optimization problems are NP-hard which
usually means that they are unsolvable in practice. However, it is often
unnecessary to have an exact solution. In this case one may use heuristic
approach to obtain a near-optimal solution in some reasonable time.
  We focus on two combinatorial optimization problems, namely the Generalized
Traveling Salesman Problem and the Multidimensional Assignment Problem. The
first problem is an important generalization of the Traveling Salesman Problem;
the second one is a generalization of the Assignment Problem for an arbitrary
number of dimensions. Both problems are NP-hard and have hosts of applications.
  In this work, we discuss different aspects of heuristics design and
evaluation. A broad spectrum of related subjects, covered in this research,
includes test bed generation and analysis, implementation and performance
issues, local search neighborhoods and efficient exploration algorithms,
metaheuristics design and population sizing in memetic algorithm.
  The most important results are obtained in the areas of local search and
memetic algorithms for the considered problems. In both cases we have
significantly advanced the existing knowledge on the local search neighborhoods
and algorithms by systematizing and improving the previous results. We have
proposed a number of efficient heuristics which dominate the existing
algorithms in a wide range of time/quality requirements.
  Several new approaches, introduced in our memetic algorithms, make them the
state-of-the-art metaheuristics for the corresponding problems. Population
sizing is one of the most promising among these approaches; it is expected to
be applicable to virtually any memetic algorithm.



In this paper, we present a formalization of an algorithm to construct
admissible discrete vector fields in the Coq theorem prover taking advantage of
the SSReflect library. Discrete vector fields are a tool which has been
welcomed in the homological analysis of digital images since it provides a
procedure to reduce the amount of information but preserving the homological
properties. In particular, thanks to discrete vector fields, we are able to
compute, inside Coq, homological properties of biomedical images which
otherwise are out of the reach of this system.



In this paper, we propose an approach for assigning an interest level to the
goals of a planetary rover. Assigning an interest level to goals, allows the
rover autonomously to transform and reallocate the goals. The interest level is
defined by data-fusing payload and navigation information. The fusion yields an
"interest map", that quantifies the level of interest of each area around the
rover. In this way the planner can choose the most interesting scientific
objectives to be analyzed, with limited human intervention, and reallocates its
goals autonomously. The Dezert-Smarandache Theory of Plausible and Paradoxical
Reasoning was used for information fusion: this theory allows dealing with
vague and conflicting data. In particular, it allows us directly to model the
behavior of the scientists that have to evaluate the relevance of a particular
set of goals. The paper shows an application of the proposed approach to the
generation of a reliable interest map.



PIDE is a general framework for document-oriented prover interaction and
integration, based on a bilingual architecture that combines ML and Scala. The
overall aim is to connect LCF-style provers like Isabelle (or Coq or HOL) with
sophisticated front-end technology on the JVM platform, overcoming command-line
interaction at last.
  The present system description specifically covers Isabelle/jEdit as part of
the official release of Isabelle2011-1 (October 2011). It is a concrete Prover
IDE implementation based on Isabelle/PIDE library modules (implemented in
Scala) on the one hand, and the well-known text editor framework of jEdit
(implemented in Java) on the other hand.
  The interaction model of our Prover IDE follows the idea of continuous proof
checking: the theory source text is annotated by semantic information by the
prover as it becomes available incrementally. This works via an asynchronous
protocol that neither blocks the editor nor stops the prover from exploiting
parallelism on multi-core hardware. The jEdit GUI provides standard metaphors
for augmented text editing (highlighting, squiggles, tooltips, hyperlinks etc.)
that we have instrumented to render the formal content from the prover context.
Further refinement of the jEdit display engine via suitable plugins and fonts
approximates mathematical rendering in the text buffer, including symbols from
the TeX repertoire, and sub-/superscripts.
  Isabelle/jEdit is presented here both as a usable interface for current
Isabelle, and as a reference application to inspire further projects based on
PIDE.



We present the Intelligent Automated Client Diagnostic (IACD) system, which
only relies on inference from Transmission Control Protocol (TCP) packet traces
for rapid diagnosis of client device problems that cause network performance
issues. Using soft-margin Support Vector Machine (SVM) classifiers, the system
(i) distinguishes link problems from client problems, and (ii) identifies
characteristics unique to client faults to report the root cause of the client
device problem. Experimental evaluation demonstrated the capability of the IACD
system to distinguish between faulty and healthy links and to diagnose the
client faults with 98% accuracy in healthy links. The system can perform fault
diagnosis independent of the client's specific TCP implementation, enabling
diagnosis capability on diverse range of client computers.



In this paper, the early design of our self-organized agent-based simulation
model for exploration of synaptic connections that faithfully generates what is
observed in natural situation is given. While we take inspiration from
neuroscience, our intent is not to create a veridical model of processes in
neurodevelopmental biology, nor to represent a real biological system. Instead,
our goal is to design a simulation model that learns acting in the same way of
human nervous system by using findings on human subjects using reflex
methodologies in order to estimate unknown connections.



Traditional network diagnosis methods of Client-Terminal Device (CTD)
problems tend to be laborintensive, time consuming, and contribute to increased
customer dissatisfaction. In this paper, we propose an automated solution for
rapidly diagnose the root causes of network performance issues in CTD. Based on
a new intelligent inference technique, we create the Intelligent Automated
Client Diagnostic (IACD) system, which only relies on collection of
Transmission Control Protocol (TCP) packet traces. Using soft-margin Support
Vector Machine (SVM) classifiers, the system (i) distinguishes link problems
from client problems and (ii) identifies characteristics unique to the specific
fault to report the root cause. The modular design of the system enables
support for new access link and fault types. Experimental evaluation
demonstrated the capability of the IACD system to distinguish between faulty
and healthy links and to diagnose the client faults with 98% accuracy. The
system can perform fault diagnosis independent of the user's specific TCP
implementation, enabling diagnosis of diverse range of client devices



In this paper, we present a Branch and Bound algorithm called QuickBB for
computing the treewidth of an undirected graph. This algorithm performs a
search in the space of perfect elimination ordering of vertices of the graph.
The algorithm uses novel pruning and propagation techniques which are derived
from the theory of graph minors and graph isomorphism. We present a new
algorithm called minor-min-width for computing a lower bound on treewidth that
is used within the branch and bound algorithm and which improves over earlier
available lower bounds. Empirical evaluation of QuickBB on randomly generated
graphs and benchmarks in Graph Coloring and Bayesian Networks shows that it is
consistently better than complete algorithms like QuickTree [Shoikhet and
Geiger, 1997] in terms of cpu time. QuickBB also has good anytime performance,
being able to generate a better upper bound on treewidth of some graphs whose
optimal treewidth could not be computed up to now.



Early, reliable detection of disease outbreaks is a critical problem today.
This paper reports an investigation of the use of causal Bayesian networks to
model spatio-temporal patterns of a non-contagious disease (respiratory anthrax
infection) in a population of people. The number of parameters in such a
network can become enormous, if not carefully managed. Also, inference needs to
be performed in real time as population data stream in. We describe techniques
we have applied to address both the modeling and inference challenges. A key
contribution of this paper is the explication of assumptions and techniques
that are sufficient to allow the scaling of Bayesian network modeling and
inference to millions of nodes for real-time surveillance applications. The
results reported here provide a proof-of-concept that Bayesian networks can
serve as the foundation of a system that effectively performs Bayesian
biosurveillance of disease outbreaks.



A key prerequisite to optimal reasoning under uncertainty in intelligent
systems is to start with good class probability estimates. This paper improves
on the current best probability estimation trees (Bagged-PETs) and also
presents a new ensemble-based algorithm (MOB-ESP). Comparisons are made using
several benchmark datasets and multiple metrics. These experiments show that
MOB-ESP outputs significantly more accurate class probabilities than either the
baseline BPETs algorithm or the enhanced version presented here (EB-PETs).
These results are based on metrics closely associated with the average accuracy
of the predictions. MOB-ESP also provides much better probability rankings than
B-PETs. The paper further suggests how these estimation techniques can be
applied in concert with a broader category of classifiers.



Pearl has provided the back door criterion, the front door criterion and the
conditional instrumental variable (IV) method as identifiability criteria for
total effects. In some situations, these three criteria can be applied to
identifying total effects simultaneously. For the purpose of increasing
estimating accuracy, this paper compares the three ways of identifying total
effects in terms of the asymptotic variance, and concludes that in some
situations the superior of them can be recognized directly from the graph
structure.



In this paper, we propose a new lower approximation scheme for POMDP with
discounted and average cost criterion. The approximating functions are
determined by their values at a finite number of belief points, and can be
computed efficiently using value iteration algorithms for finite-state MDP.
While for discounted problems several lower approximation schemes have been
proposed earlier, ours seems the first of its kind for average cost problems.
We focus primarily on the average cost case, and we show that the corresponding
approximation can be computed efficiently using multi-chain algorithms for
finite-state MDP. We give a preliminary analysis showing that regardless of the
existence of the optimal average cost J in the POMDP, the approximation
obtained is a lower bound of the liminf optimal average cost function, and can
also be used to calculate an upper bound on the limsup optimal average cost
function, as well as bounds on the cost of executing the stationary policy
associated with the approximation. Weshow the convergence of the cost
approximation, when the optimal average cost is constant and the optimal
differential cost is continuous.



The Internet has enabled the creation of a growing number of large-scale
knowledge bases in a variety of domains containing complementary information.
Tools for automatically aligning these knowledge bases would make it possible
to unify many sources of structured knowledge and answer complex queries.
However, the efficient alignment of large-scale knowledge bases still poses a
considerable challenge. Here, we present Simple Greedy Matching (SiGMa), a
simple algorithm for aligning knowledge bases with millions of entities and
facts. SiGMa is an iterative propagation algorithm which leverages both the
structural information from the relationship graph as well as flexible
similarity measures between entity properties in a greedy local search, thus
making it scalable. Despite its greedy nature, our experiments indicate that
SiGMa can efficiently match some of the world's largest knowledge bases with
high precision. We provide additional experiments on benchmark datasets which
demonstrate that SiGMa can outperform state-of-the-art approaches both in
accuracy and efficiency.



The paper presents the electronic design and motion planning of a robot based
on decision making regarding its straight motion and precise turn using
Artificial Neural Network (ANN). The ANN helps in learning of robot so that it
performs motion autonomously. The weights calculated are implemented in
microcontroller. The performance has been tested to be excellent.



The exploration/exploitation (E/E) dilemma arises naturally in many subfields
of Science. Multi-armed bandit problems formalize this dilemma in its canonical
form. Most current research in this field focuses on generic solutions that can
be applied to a wide range of problems. However, in practice, it is often the
case that a form of prior information is available about the specific class of
target problems. Prior knowledge is rarely used in current solutions due to the
lack of a systematic approach to incorporate it into the E/E strategy.
  To address a specific class of E/E problems, we propose to proceed in three
steps: (i) model prior knowledge in the form of a probability distribution over
the target class of E/E problems; (ii) choose a large hypothesis space of
candidate E/E strategies; and (iii), solve an optimization problem to find a
candidate E/E strategy of maximal average performance over a sample of problems
drawn from the prior distribution.
  We illustrate this meta-learning approach with two different hypothesis
spaces: one where E/E strategies are numerically parameterized and another
where E/E strategies are represented as small symbolic formulas. We propose
appropriate optimization algorithms for both cases. Our experiments, with
two-armed Bernoulli bandit problems and various playing budgets, show that the
meta-learnt E/E strategies outperform generic strategies of the literature
(UCB1, UCB1-Tuned, UCB-v, KL-UCB and epsilon greedy); they also evaluate the
robustness of the learnt E/E strategies, by tests carried out on arms whose
rewards follow a truncated Gaussian distribution.



Musical counterpoint, a musical technique in which two or more independent
melodies are played simultaneously with the goal of creating harmony, has been
around since the baroque era. However, to our knowledge computational
generation of aesthetically pleasing linear counterpoint based on subjective
fitness assessment has not been explored by the evolutionary computation
community (although generation using objective fitness has been attempted in
quite a few cases). The independence of contrapuntal melodies and the
subjective nature of musical aesthetics provide an excellent platform for the
application of genetic algorithms. In this paper, a genetic algorithm approach
to generating contrapuntal melodies is explained, with a description of the
various musical heuristics used and of how variable-length chromosome strings
are used to avoid generating "jerky" rhythms and melodic phrases, as well as
how subjectivity is incorporated into the algorithm's fitness measures. Next,
results from empirical testing of the algorithm are presented, with a focus on
how a user's musical sophistication influences their experience. Lastly,
further musical and compositional applications of the algorithm are discussed
along with planned future work on the algorithm.



The paper describes combinatorial synthesis approach with interval multset
estimates of system elements for modeling, analysis, design, and improvement of
a modular telemetry system. Morphological (modular) system design and
improvement are considered as composition of the telemetry system elements
(components) configuration. The solving process is based on Hierarchical
Morphological Multicriteria Design (HMMD): (i) multicriteria selection of
alternatives for system components, (ii) synthesis of the selected alternatives
into a resultant combination (while taking into account quality of the
alternatives above and their compatibility). Interval multiset estimates are
used for assessment of design alternatives for telemetry system elements. Two
additional systems problems are examined: (a) improvement of the obtained
solutions, (b) aggregation of the obtained solutions into a resultant system
configuration. The improvement and aggregation processes are based on multiple
choice problem with interval multiset estimates. Numerical examples for an
on-board telemetry subsystem illustrate the design and improvement processes.



A new stream of research was born in the last decade with the goal of mining
itemsets of interest using Constraint Programming (CP). This has promoted a
natural way to combine complex constraints in a highly flexible manner.
Although CP state-of-the-art solutions formulate the task using Boolean
variables, the few attempts to adopt propositional Satisfiability (SAT)
provided an unsatisfactory performance. This work deepens the study on when and
how to use SAT for the frequent itemset mining (FIM) problem by defining
different encodings with multiple task-driven enumeration options and search
strategies. Although for the majority of the scenarios SAT-based solutions
appear to be non-competitive with CP peers, results show a variety of
interesting cases where SAT encodings are the best option.



In this paper, we consider the problem of diversity in ranking of the nodes
in a graph. The task is to pick the top-k nodes in the graph which are both
'central' and 'diverse'. Many graph-based models of NLP like text
summarization, opinion summarization involve the concept of diversity in
generating the summaries. We develop a novel method which works in an iterative
fashion based on random walks to achieve diversity. Specifically, we use
negative reinforcement as a main tool to introduce diversity in the
Personalized PageRank framework. Experiments on two benchmark datasets show
that our algorithm is competitive to the existing methods.



Optimizing the cost of evaluating a polynomial is a classic problem in
computer science. For polynomials in one variable, Horner's method provides a
scheme for producing a computationally efficient form. For multivariate
polynomials it is possible to generalize Horner's method, but this leaves
freedom in the order of the variables. Traditionally, greedy schemes like
most-occurring variable first are used. This simple textbook algorithm has
given remarkably efficient results. Finding better algorithms has proved
difficult. In trying to improve upon the greedy scheme we have implemented
Monte Carlo tree search, a recent search method from the field of artificial
intelligence. This results in better Horner schemes and reduces the cost of
evaluating polynomials, sometimes by factors up to two.



The Distributed Ontology Language (DOL) is currently being standardized
within the OntoIOp (Ontology Integration and Interoperability) activity of
ISO/TC 37/SC 3. It aims at providing a unified framework for (1) ontologies
formalized in heterogeneous logics, (2) modular ontologies, (3) links between
ontologies, and (4) annotation of ontologies. This paper presents the current
state of DOL's standardization. It focuses on use cases where distributed
ontologies enable interoperability and reusability. We demonstrate relevant
features of the DOL syntax and semantics and explain how these integrate into
existing knowledge engineering environments.



Inferring probabilistic networks from data is a notoriously difficult task.
Under various goodness-of-fit measures, finding an optimal network is NP-hard,
even if restricted to polytrees of bounded in-degree. Polynomial-time
algorithms are known only for rare special cases, perhaps most notably for
branchings, that is, polytrees in which the in-degree of every node is at most
one. Here, we study the complexity of finding an optimal polytree that can be
turned into a branching by deleting some number of arcs or nodes, treated as a
parameter.
  We show that the problem can be solved via a matroid intersection formulation
in polynomial time if the number of deleted arcs is bounded by a constant. The
order of the polynomial time bound depends on this constant, hence the
algorithm does not establish fixed-parameter tractability when parameterized by
the number of deleted arcs. We show that a restricted version of the problem
allows fixed-parameter tractability and hence scales well with the parameter.
We contrast this positive result by showing that if we parameterize by the
number of deleted nodes, a somewhat more powerful parameter, the problem is not
fixed-parameter tractable, subject to a complexity-theoretic assumption.



This paper presents a predictive control strategy based on neural network
model of the plant is applied to Continuous Stirred Tank Reactor (CSTR). This
system is a highly nonlinear process; therefore, a nonlinear predictive method,
e.g., neural network predictive control, can be a better match to govern the
system dynamics. In the paper, the NN model and the way in which it can be used
to predict the behavior of the CSTR process over a certain prediction horizon
are described, and some comments about the optimization procedure are made.
Predictive control algorithm is applied to control the concentration in a
continuous stirred tank reactor (CSTR), whose parameters are optimally
determined by solving quadratic performance index using the optimization
algorithm. An efficient control of the product concentration in cstr can be
achieved only through accurate model. Here an attempt is made to alleviate the
modeling difficulties using Artificial Intelligent technique such as Neural
Network. Simulation results demonstrate the feasibility and effectiveness of
the NNMPC technique.



Direct policy search (DPS) and look-ahead tree (LT) policies are two widely
used classes of techniques to produce high performance policies for sequential
decision-making problems. To make DPS approaches work well, one crucial issue
is to select an appropriate space of parameterized policies with respect to the
targeted problem. A fundamental issue in LT approaches is that, to take good
decisions, such policies must develop very large look-ahead trees which may
require excessive online computational resources. In this paper, we propose a
new hybrid policy learning scheme that lies at the intersection of DPS and LT,
in which the policy is an algorithm that develops a small look-ahead tree in a
directed way, guided by a node scoring function that is learned through DPS.
The LT-based representation is shown to be a versatile way of representing
policies in a DPS scheme, while at the same time, DPS enables to significantly
reduce the size of the look-ahead trees that are required to take high-quality
decisions.
  We experimentally compare our method with two other state-of-the-art DPS
techniques and four common LT policies on four benchmark domains and show that
it combines the advantages of the two techniques from which it originates. In
particular, we show that our method: (1) produces overall better performing
policies than both pure DPS and pure LT policies, (2) requires a substantially
smaller number of policy evaluations than other DPS techniques, (3) is easy to
tune and (4) results in policies that are quite robust with respect to
perturbations of the initial conditions.



We consider the problem of answering queries about formulas of propositional
logic based on background knowledge partially represented explicitly as other
formulas, and partially represented as partially obscured examples
independently drawn from a fixed probability distribution, where the queries
are answered with respect to a weaker semantics than usual -- PAC-Semantics,
introduced by Valiant (2000) -- that is defined using the distribution of
examples. We describe a fairly general, efficient reduction to limited versions
of the decision problem for a proof system (e.g., bounded space treelike
resolution, bounded degree polynomial calculus, etc.) from corresponding
versions of the reasoning problem where some of the background knowledge is not
explicitly given as formulas, only learnable from the examples. Crucially, we
do not generate an explicit representation of the knowledge extracted from the
examples, and so the "learning" of the background knowledge is only done
implicitly. As a consequence, this approach can utilize formulas as background
knowledge that are not perfectly valid over the distribution---essentially the
analogue of agnostic learning here.



As an important tool for information filtering in the era of socialized web,
recommender systems have witnessed rapid development in the last decade. As
benefited from the better interpretability, neighborhood-based collaborative
filtering techniques, such as item-based collaborative filtering adopted by
Amazon, have gained a great success in many practical recommender systems.
However, the neighborhood-based collaborative filtering method suffers from the
rating bound problem, i.e., the rating on a target item that this method
estimates is bounded by the observed ratings of its all neighboring items.
Therefore, it cannot accurately estimate the unobserved rating on a target
item, if its ground truth rating is actually higher (lower) than the highest
(lowest) rating over all items in its neighborhood. In this paper, we address
this problem by formalizing rating estimation as a task of recovering a scalar
rating function. With a linearity assumption, we infer all the ratings by
optimizing the low-order norm, e.g., the $l_1/2$-norm, of the second derivative
of the target scalar function, while remaining its observed ratings unchanged.
Experimental results on three real datasets, namely Douban, Goodreads and
MovieLens, demonstrate that the proposed approach can well overcome the rating
bound problem. Particularly, it can significantly improve the accuracy of
rating estimation by 37% than the conventional neighborhood-based methods.



We propose parametric constructive Kripke-semantics for multi-agent
KD45-belief and S5-knowledge in terms of elementary set-theoretic constructions
of two basic functional building blocks, namely bias (or viewpoint) and
visibility, functioning also as the parameters of the doxastic and epistemic
accessibility relation. The doxastic accessibility relates two possible worlds
whenever the application of the composition of bias with visibility to the
first world is equal to the application of visibility to the second world. The
epistemic accessibility is the transitive closure of the union of our doxastic
accessibility and its converse. Therefrom, accessibility relations for common
and distributed belief and knowledge can be constructed in a standard way. As a
result, we obtain a general definition of knowledge in terms of belief that
enables us to view S5-knowledge as accurate (unbiased and thus true)
KD45-belief, negation-complete belief and knowledge as exact KD45-belief and
S5-knowledge, respectively, and perfect S5-knowledge as precise (exact and
accurate) KD45-belief, and all this generically for arbitrary functions of bias
and visibility. Our results can be seen as a semantic complement to previous
foundational results by Halpern et al. about the (un)definability and
(non-)reducibility of knowledge in terms of and to belief, respectively.



In this article, we address the question of how non-knowledge about future
events that influence economic agents' decisions in choice settings has been
formally represented in economic theory up to date. To position our discussion
within the ongoing debate on uncertainty, we provide a brief review of
historical developments in economic theory and decision theory on the
description of economic agents' choice behaviour under conditions of
uncertainty, understood as either (i) ambiguity, or (ii) unawareness.
Accordingly, we identify and discuss two approaches to the formalisation of
non-knowledge: one based on decision-making in the context of a state space
representing the exogenous world, as in Savage's axiomatisation and some
successor concepts (ambiguity as situations with unknown probabilities), and
one based on decision-making over a set of menus of potential future
opportunities, providing the possibility of derivation of agents' subjective
state spaces (unawareness as situation with imperfect subjective knowledge of
all future events possible). We also discuss impeding challenges of the
formalisation of non-knowledge.



We present a framework for a large-scale distributed eScience Artificial
Intelligence search. Our approach is generic and can be used for many different
problems. Unlike many other approaches, we do not require dedicated machines,
homogeneous infrastructure or the ability to communicate between nodes. We give
special consideration to the robustness of the framework, minimising the loss
of effort even after total loss of infrastructure, and allowing easy
verification of every step of the distribution process. In contrast to most
eScience applications, the input data and specification of the problem is very
small, being easily given in a paragraph of text. The unique challenges our
framework tackles are related to the combinatorial explosion of the space that
contains the possible solutions and the robustness of long-running
computations. Not only is the time required to finish the computations unknown,
but also the resource requirements may change during the course of the
computation. We demonstrate the applicability of our framework by using it to
solve a challenging and hitherto open problem in computational mathematics. The
results demonstrate that our approach easily scales to computations of a size
that would have been impossible to tackle in practice just a decade ago.



Many real-world datasets can be represented in the form of a graph whose edge
weights designate similarities between instances. A discrete Gaussian random
field (GRF) model is a finite-dimensional Gaussian process (GP) whose prior
covariance is the inverse of a graph Laplacian. Minimizing the trace of the
predictive covariance Sigma (V-optimality) on GRFs has proven successful in
batch active learning classification problems with budget constraints. However,
its worst-case bound has been missing. We show that the V-optimality on GRFs as
a function of the batch query set is submodular and hence its greedy selection
algorithm guarantees an (1-1/e) approximation ratio. Moreover, GRF models have
the absence-of-suppressor (AofS) condition. For active survey problems, we
propose a similar survey criterion which minimizes 1'(Sigma)1. In practice,
V-optimality criterion performs better than GPs with mutual information gain
criteria and allows nonuniform costs for different nodes.



Qualitative modelling is a technique integrating the fields of theoretical
computer science, artificial intelligence and the physical and biological
sciences. The aim is to be able to model the behaviour of systems without
estimating parameter values and fixing the exact quantitative dynamics.
Traditional applications are the study of the dynamics of physical and
biological systems at a higher level of abstraction than that obtained by
estimation of numerical parameter values for a fixed quantitative model.
Qualitative modelling has been studied and implemented to varying degrees of
sophistication in Petri nets, process calculi and constraint programming. In
this paper we reflect on the strengths and weaknesses of existing frameworks,
we demonstrate how recent advances in constraint programming can be leveraged
to produce high quality qualitative models, and we describe the advances in
theory and technology that would be needed to make constraint programming the
best option for scientific investigation in the broadest sense.



A central problem of surveillance is to monitor multiple targets moving in a
large-scale, obstacle-ridden environment with occlusions. This paper presents a
novel principled Partially Observable Markov Decision Process-based approach to
coordinating and controlling a network of active cameras for tracking and
observing multiple mobile targets at high resolution in such surveillance
environments. Our proposed approach is capable of (a) maintaining a belief over
the targets' states (i.e., locations, directions, and velocities) to track
them, even when they may not be observed directly by the cameras at all times,
(b) coordinating the cameras' actions to simultaneously improve the belief over
the targets' states and maximize the expected number of targets observed with a
guaranteed resolution, and (c) exploiting the inherent structure of our
surveillance problem to improve its scalability (i.e., linear time) in the
number of targets to be observed. Quantitative comparisons with
state-of-the-art multi-camera coordination and control techniques show that our
approach can achieve higher surveillance quality in real time. The practical
feasibility of our approach is also demonstrated using real AXIS 214 PTZ
cameras



Synaptic plasticity seems to be a capital aspect of the dynamics of neural
networks. It is about the physiological modifications of the synapse, which
have like consequence a variation of the value of the synaptic weight. The
information encoding is based on the precise timing of single spike events that
is based on the relative timing of the pre- and post-synaptic spikes, local
synapse competitions within a single neuron and global competition via lateral
connections. In order to classify temporal sequences, we present in this paper
how to use a local hebbian learning, spike-timing dependent plasticity for
unsupervised competitive learning, preserving self-organizing maps of spiking
neurons. In fact we present three variants of self-organizing maps (SOM) with
spike-timing dependent Hebbian learning rule, the Leaky Integrators Neurons
(LIN), the Spiking_SOM and the recurrent Spiking_SOM (RSSOM) models. The case
study of the proposed SOM variants is phoneme classification and word
recognition in continuous speech and speaker independent.



A significant theoretical advantage of search-and-score methods for learning
Bayesian Networks is that they can accept informative prior beliefs for each
possible network, thus complementing the data. In this paper, a method is
presented for assigning priors based on beliefs on the presence or absence of
certain paths in the true network. Such beliefs correspond to knowledge about
the possible causal and associative relations between pairs of variables. This
type of knowledge naturally arises from prior experimental and observational
data, among others. In addition, a novel search-operator is proposed to take
advantage of such prior knowledge. Experiments show that, using path beliefs
improves the learning of the skeleton, as well as the edge directions in the
network.



We propose an artificial immune model for intrusion detection in distributed
systems based on a relatively recent theory in immunology called Danger theory.
Based on Danger theory, immune response in natural systems is a result of
sensing corruption as well as sensing unknown substances. In contrast,
traditional self-nonself discrimination theory states that immune response is
only initiated by sensing nonself (unknown) patterns. Danger theory solves many
problems that could only be partially explained by the traditional model.
Although the traditional model is simpler, such problems result in high false
positive rates in immune-inspired intrusion detection systems. We believe using
danger theory in a multi-agent environment that computationally emulates the
behavior of natural immune systems is effective in reducing false positive
rates. We first describe a simplified scenario of immune response in natural
systems based on danger theory and then, convert it to a computational model as
a network protocol. In our protocol, we define several immune signals and model
cell signaling via message passing between agents that emulate cells. Most
messages include application-specific patterns that must be meaningfully
extracted from various system properties. We show how to model these messages
in practice by performing a case study on the problem of detecting distributed
denial-of-service attacks in wireless sensor networks. We conduct a set of
systematic experiments to find a set of performance metrics that can accurately
distinguish malicious patterns. The results indicate that the system can be
efficiently used to detect malicious patterns with a high level of accuracy.



Multi-step-ahead time series prediction is one of the most challenging
research topics in the field of time series modeling and prediction, and is
continually under research. Recently, the multiple-input several
multiple-outputs (MISMO) modeling strategy has been proposed as a promising
alternative for multi-step-ahead time series prediction, exhibiting advantages
compared with the two currently dominating strategies, the iterated and the
direct strategies. Built on the established MISMO strategy, this study proposes
a particle swarm optimization (PSO)-based MISMO modeling strategy, which is
capable of determining the number of sub-models in a self-adaptive mode, with
varying prediction horizons. Rather than deriving crisp divides with equal-size
s prediction horizons from the established MISMO, the proposed PSO-MISMO
strategy, implemented with neural networks, employs a heuristic to create
flexible divides with varying sizes of prediction horizons and to generate
corresponding sub-models, providing considerable flexibility in model
construction, which has been validated with simulated and real datasets.



Medical image fusion is the process of registering and combining multiple
images from single or multiple imaging modalities to improve the imaging
quality and reduce randomness and redundancy in order to increase the clinical
applicability of medical images for diagnosis and assessment of medical
problems. Multi-modal medical image fusion algorithms and devices have shown
notable achievements in improving clinical accuracy of decisions based on
medical images. This review article provides a factual listing of methods and
summarizes the broad scientific challenges faced in the field of medical image
fusion. We characterize the medical image fusion research based on (1) the
widely used image fusion methods, (2) imaging modalities, and (3) imaging of
organs that are under study. This review concludes that even though there
exists several open ended technological and scientific challenges, the fusion
of medical images has proved to be useful for advancing the clinical
reliability of using medical imaging for medical diagnostics and analysis, and
is a scientific discipline that has the potential to significantly grow in the
coming years.



Demand response (DR) for residential and small commercial buildings is
estimated to account for as much as 65% of the total energy savings potential
of DR, and previous work shows that a fully automated Energy Management System
(EMS) is a necessary prerequisite to DR in these areas. In this paper, we
propose a novel EMS formulation for DR problems in these sectors. Specifically,
we formulate a fully automated EMS's rescheduling problem as a reinforcement
learning (RL) problem, and argue that this RL problem can be approximately
solved by decomposing it over device clusters. Compared with existing
formulations, our new formulation (1) does not require explicitly modeling the
user's dissatisfaction on job rescheduling, (2) enables the EMS to
self-initiate jobs, (3) allows the user to initiate more flexible requests and
(4) has a computational complexity linear in the number of devices. We also
demonstrate the simulation results of applying Q-learning, one of the most
popular and classical RL algorithms, to a representative example.



Many computer programs have graphical user interfaces (GUIs), which need good
layout to make efficient use of the available screen real estate. Most GUIs do
not have a fixed layout, but are resizable and able to adapt themselves.
Constraints are a powerful tool for specifying adaptable GUI layouts: they are
used to specify a layout in a general form, and a constraint solver is used to
find a satisfying concrete layout, e.g.\ for a specific GUI size. The
constraint solver has to calculate a new layout every time a GUI is resized or
changed, so it needs to be efficient to ensure a good user experience. One
approach for constraint solvers is based on the Gauss-Seidel algorithm and
successive over-relaxation (SOR).
  Our observation is that a solution after resizing or changing is similar in
structure to a previous solution. Thus, our hypothesis is that we can increase
the computational performance of an SOR-based constraint solver if we reuse the
solution of a previous layout to warm-start the solving of a new layout. In
this paper we report on experiments to test this hypothesis experimentally for
three common use cases: big-step resizing, small-step resizing and constraint
change. In our experiments, we measured the solving time for randomly generated
GUI layout specifications of various sizes. For all three cases we found that
the performance is improved if an existing solution is used as a starting
solution for a new layout.



Addressing the issue of SVMs parameters optimization, this study proposes an
efficient memetic algorithm based on Particle Swarm Optimization algorithm
(PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is
responsible for exploration of the search space and the detection of the
potential regions with optimum solutions, while pattern search (PS) is used to
produce an effective exploitation on the potential regions obtained by PSO.
Moreover, a novel probabilistic selection strategy is proposed to select the
appropriate individuals among the current population to undergo local
refinement, keeping a well balance between exploration and exploitation.
Experimental results confirm that the local refinement with PS and our proposed
selection strategy are effective, and finally demonstrate effectiveness and
robustness of the proposed PSO-PS based MA for SVMs parameters optimization.



Standard models of multi-agent modal logic do not capture the fact that
information is often \emph{ambiguous}, and may be interpreted in different ways
by different agents. We propose a framework that can model this, and consider
different semantics that capture different assumptions about the agents'
beliefs regarding whether or not there is ambiguity. We examine the expressive
power of logics of ambiguity compared to logics that cannot model ambiguity,
with respect to the different semantics that we propose.



Graph vertex coloring with a given number of colors is a well-known and
much-studied NP-complete problem.The most effective methods to solve this
problem are proved to be hybrid algorithms such as memetic algorithms or
quantum annealing. Those hybrid algorithms use a powerful local search inside a
population-based algorithm.This paper presents a new memetic algorithm based on
one of the most effective algorithms: the Hybrid Evolutionary Algorithm HEA
from Galinier and Hao (1999).The proposed algorithm, denoted HEAD - for HEA in
Duet - works with a population of only two individuals.Moreover, a new way of
managing diversity is brought by HEAD.These two main differences greatly
improve the results, both in terms of solution quality and computational
time.HEAD has produced several good results for the popular DIMACS benchmark
graphs, such as 222-colorings for \textless{}dsjc1000.9\textgreater{},
81-colorings for \textless{}flat1000\_76\_0\textgreater{} and even 47-colorings
for \textless{}dsjc500.5\textgreater{} and 82-colorings for
\textless{}dsjc1000.5\textgreater{}.



We develop a technique for deriving data-dependent error bounds for
transductive learning algorithms based on transductive Rademacher complexity.
Our technique is based on a novel general error bound for transduction in terms
of transductive Rademacher complexity, together with a novel bounding technique
for Rademacher averages for particular algorithms, in terms of their
"unlabeled-labeled" representation. This technique is relevant to many advanced
graph-based transductive algorithms and we demonstrate its effectiveness by
deriving error bounds to three well known algorithms. Finally, we present a new
PAC-Bayesian bound for mixtures of transductive algorithms based on our
Rademacher bounds.



Bayesian networks are a useful tool in the representation of uncertain
knowledge. This paper proposes a new algorithm called ACO-E, to learn the
structure of a Bayesian network. It does this by conducting a search through
the space of equivalence classes of Bayesian networks using Ant Colony
Optimization (ACO). To this end, two novel extensions of traditional ACO
techniques are proposed and implemented. Firstly, multiple types of moves are
allowed. Secondly, moves can be given in terms of indices that are not based on
construction graph nodes. The results of testing show that ACO-E performs
better than a greedy search and other state-of-the-art and metaheuristic
algorithms whilst searching in the space of equivalence classes.



We present two algorithms for learning the structure of a Markov network from
data: GSMN* and GSIMN. Both algorithms use statistical independence tests to
infer the structure by successively constraining the set of structures
consistent with the results of these tests. Until very recently, algorithms for
structure learning were based on maximum likelihood estimation, which has been
proved to be NP-hard for Markov networks due to the difficulty of estimating
the parameters of the network, needed for the computation of the data
likelihood. The independence-based approach does not require the computation of
the likelihood, and thus both GSMN* and GSIMN can compute the structure
efficiently (as shown in our experiments). GSMN* is an adaptation of the
Grow-Shrink algorithm of Margaritis and Thrun for learning the structure of
Bayesian networks. GSIMN extends GSMN* by additionally exploiting Pearls
well-known properties of the conditional independence relation to infer novel
independences from known ones, thus avoiding the performance of statistical
tests to estimate them. To accomplish this efficiently GSIMN uses the Triangle
theorem, also introduced in this work, which is a simplified version of the set
of Markov axioms. Experimental comparisons on artificial and real-world data
sets show GSIMN can yield significant savings with respect to GSMN*, while
generating a Markov network with comparable or in some cases improved quality.
We also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH,
that produces all possible conditional independences resulting from repeatedly
applying Pearls theorems on the known conditional independence tests. The
results of this comparison show that GSIMN, by the sole use of the Triangle
theorem, is nearly optimal in terms of the set of independences tests that it
infers.



This paper presents a multilayered architecture that enhances the
capabilities of current QA systems and allows different types of complex
questions or queries to be processed. The answers to these questions need to be
gathered from factual information scattered throughout different documents.
Specifically, we designed a specialized layer to process the different types of
temporal questions. Complex temporal questions are first decomposed into simple
questions, according to the temporal relations expressed in the original
question. In the same way, the answers to the resulting simple questions are
recomposed, fulfilling the temporal restrictions of the original complex
question. A novel aspect of this approach resides in the decomposition which
uses a minimal quantity of resources, with the final aim of obtaining a
portable platform that is easily extensible to other languages. In this paper
we also present a methodology for evaluation of the decomposition of the
questions as well as the ability of the implemented temporal layer to perform
at a multilingual level. The temporal layer was first performed for English,
then evaluated and compared with: a) a general purpose QA system (F-measure
65.47% for QA plus English temporal layer vs. 38.01% for the general QA
system), and b) a well-known QA system. Much better results were obtained for
temporal questions with the multilayered system. This system was therefore
extended to Spanish and very good results were again obtained in the evaluation
(F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general
QA system).



This paper aims to find an algorithmic structure that affords to predict and
explain economical choice behaviour particularly under uncertainty(random
policies) by manipulating the prevalent Actor-Critic learning method to comply
with the requirements we have been entrusted ever since the field of
neuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics that
seem relevant to our discussion, we will try to outline some of the important
works which have so far been done to simulate choice making processes.
Concerning neurological findings that suggest the existence of two specific
functions that are executed through Basal Ganglia all the way up to sub-
cortical areas, namely 'rewards' and 'beliefs', we will offer a modified
version of actor/critic algorithm to shed a light on the relation between these
functions and most importantly resolve what is referred to as a challenge for
actor-critic algorithms, that is, the lack of inheritance or hierarchy which
avoids the system being evolved in continuous time tasks whence the convergence
might not be emerged.



Online offerings such as web search, news portals, and e-commerce
applications face the challenge of providing high-quality service to a large,
heterogeneous user base. Recent efforts have highlighted the potential to
improve performance by introducing methods to personalize services based on
special knowledge about users and their context. For example, a users
demographics, location, and past search and browsing may be useful in enhancing
the results offered in response to web search queries. However, reasonable
concerns about privacy by both users, providers, and government agencies acting
on behalf of citizens, may limit access by services to such information. We
introduce and explore an economics of privacy in personalization, where people
can opt to share personal information, in a standing or on-demand manner, in
return for expected enhancements in the quality of an online service. We focus
on the example of web search and formulate realistic objective functions for
search efficacy and privacy. We demonstrate how we can find a provably
near-optimal optimization of the utility-privacy tradeoff in an efficient
manner. We evaluate our methodology on data drawn from a log of the search
activity of volunteer participants. We separately assess users' preferences
about privacy and utility via a large-scale survey, aimed at eliciting
preferences about peoples' willingness to trade the sharing of personal data in
returns for gains in search efficiency. We show that a significant level of
personalization can be achieved using a relatively small amount of information
about users.



We present a method for using standard techniques from satisfiability
checking to automatically verify and discover theorems in an area of economic
theory known as ranking sets of objects. The key question in this area, which
has important applications in social choice theory and decision making under
uncertainty, is how to extend an agents preferences over a number of objects to
a preference relation over nonempty sets of such objects. Certain combinations
of seemingly natural principles for this kind of preference extension can
result in logical inconsistencies, which has led to a number of important
impossibility theorems. We first prove a general result that shows that for a
wide range of such principles, characterised by their syntactic form when
expressed in a many-sorted first-order logic, any impossibility exhibited at a
fixed (small) domain size will necessarily extend to the general case. We then
show how to formulate candidates for impossibility theorems at a fixed domain
size in propositional logic, which in turn enables us to automatically search
for (general) impossibility theorems using a SAT solver. When applied to a
space of 20 principles for preference extension familiar from the literature,
this method yields a total of 84 impossibility theorems, including both known
and nontrivial new results.



When faced with the problem of learning a model of a high-dimensional
environment, a common approach is to limit the model to make only a restricted
set of predictions, thereby simplifying the learning problem. These partial
models may be directly useful for making decisions or may be combined together
to form a more complete, structured model. However, in partially observable
(non-Markov) environments, standard model-learning methods learn generative
models, i.e. models that provide a probability distribution over all possible
futures (such as POMDPs). It is not straightforward to restrict such models to
make only certain predictions, and doing so does not always simplify the
learning problem. In this paper we present prediction profile models:
non-generative partial models for partially observable systems that make only a
given set of predictions, and are therefore far simpler than generative models
in some cases. We formalize the problem of learning a prediction profile model
as a transformation of the original model-learning problem, and show
empirically that one can learn prediction profile models that make a small set
of important predictions even in systems that are too complex for standard
generative models.



We address the problem of computing approximate marginals in Gaussian
probabilistic models by using mean field and fractional Bethe approximations.
We define the Gaussian fractional Bethe free energy in terms of the moment
parameters of the approximate marginals, derive a lower and an upper bound on
the fractional Bethe free energy and establish a necessary condition for the
lower bound to be bounded from below. It turns out that the condition is
identical to the pairwise normalizability condition, which is known to be a
sufficient condition for the convergence of the message passing algorithm. We
show that stable fixed points of the Gaussian message passing algorithm are
local minima of the Gaussian Bethe free energy. By a counterexample, we
disprove the conjecture stating that the unboundedness of the free energy
implies the divergence of the message passing algorithm.



Local search algorithms applied to optimization problems often suffer from
getting trapped in a local optimum. The common solution for this deficiency is
to restart the algorithm when no progress is observed. Alternatively, one can
start multiple instances of a local search algorithm, and allocate
computational resources (in particular, processing time) to the instances
depending on their behavior. Hence, a multi-start strategy has to decide
(dynamically) when to allocate additional resources to a particular instance
and when to start new instances. In this paper we propose multi-start
strategies motivated by works on multi-armed bandit problems and Lipschitz
optimization with an unknown constant. The strategies continuously estimate the
potential performance of each algorithm instance by supposing a convergence
rate of the local search algorithm up to an unknown constant, and in every
phase allocate resources to those instances that could converge to the optimum
for a particular range of the constant. Asymptotic bounds are given on the
performance of the strategies. In particular, we prove that at most a quadratic
increase in the number of times the target function is evaluated is needed to
achieve the performance of a local search algorithm started from the attraction
region of the optimum. Experiments are provided using SPSA (Simultaneous
Perturbation Stochastic Approximation) and k-means as local search algorithms,
and the results indicate that the proposed strategies work well in practice,
and, in all cases studied, need only logarithmically more evaluations of the
target function as opposed to the theoretically suggested quadratic increase.



This survey presents in some detail the main advances that have been recently
taking place in Computational Linguistics towards the unification of the two
prominent semantic paradigms: the compositional formal semantics view and the
distributional models of meaning based on vector spaces. After an introduction
to these two approaches, I review the most important models that aim to provide
compositionality in distributional semantics. Then I proceed and present in
more detail a particular framework by Coecke, Sadrzadeh and Clark (2010) based
on the abstract mathematical setting of category theory, as a more complete
example capable to demonstrate the diversity of techniques and scientific
disciplines that this kind of research can draw from. This paper concludes with
a discussion about important open issues that need to be addressed by the
researchers in the future.



Domain knowledge is crucial for effective performance in autonomous control
systems. Typically, human effort is required to encode this knowledge into a
control algorithm. In this paper, we present an approach to language grounding
which automatically interprets text in the context of a complex control
application, such as a game, and uses domain knowledge extracted from the text
to improve control performance. Both text analysis and control strategies are
learned jointly using only a feedback signal inherent to the application. To
effectively leverage textual information, our method automatically extracts the
text segment most relevant to the current game state, and labels it with a
task-centric predicate structure. This labeled text is then used to bias an
action selection policy for the game, guiding it towards promising regions of
the action space. We encode our model for text analysis and game playing in a
multi-layer neural network, representing linguistic decisions via latent
variables in the hidden layers, and game action quality via the output layer.
Operating within the Monte-Carlo Search framework, we estimate model parameters
using feedback from simulated games. We apply our approach to the complex
strategy game Civilization II using the official game manual as the text guide.
Our results show that a linguistically-informed game-playing agent
significantly outperforms its language-unaware counterpart, yielding a 34%
absolute improvement and winning over 65% of games when playing against the
built-in AI of Civilization.



We investigate a class of first-order temporal-epistemic logics for reasoning
about multi-agent systems. We encode typical properties of systems including
perfect recall, synchronicity, no learning, and having a unique initial state
in terms of variants of quantified interpreted systems, a first-order extension
of interpreted systems. We identify several monodic fragments of first-order
temporal-epistemic logic and show their completeness with respect to their
corresponding classes of quantified interpreted systems.



Compact closed categories have found applications in modeling quantum
information protocols by Abramsky-Coecke. They also provide semantics for
Lambek's pregroup algebras, applied to formalizing the grammatical structure of
natural language, and are implicit in a distributional model of word meaning
based on vector spaces. Specifically, in previous work Coecke-Clark-Sadrzadeh
used the product category of pregroups with vector spaces and provided a
distributional model of meaning for sentences. We recast this theory in terms
of strongly monoidal functors and advance it via Frobenius algebras over vector
spaces. The former are used to formalize topological quantum field theories by
Atiyah and Baez-Dolan, and the latter are used to model classical data in
quantum protocols by Coecke-Pavlovic-Vicary. The Frobenius algebras enable us
to work in a single space in which meanings of words, phrases, and sentences of
any structure live. Hence we can compare meanings of different language
constructs and enhance the applicability of the theory. We report on
experimental results on a number of language tasks and verify the theoretical
predictions.



We introduce an efficient message passing scheme for solving Constraint
Satisfaction Problems (CSPs), which uses stochastic perturbation of Belief
Propagation (BP) and Survey Propagation (SP) messages to bypass decimation and
directly produce a single satisfying assignment. Our first CSP solver, called
Perturbed Blief Propagation, smoothly interpolates two well-known inference
procedures; it starts as BP and ends as a Gibbs sampler, which produces a
single sample from the set of solutions. Moreover we apply a similar
perturbation scheme to SP to produce another CSP solver, Perturbed Survey
Propagation. Experimental results on random and real-world CSPs show that
Perturbed BP is often more successful and at the same time tens to hundreds of
times more efficient than standard BP guided decimation. Perturbed BP also
compares favorably with state-of-the-art SP-guided decimation, which has a
computational complexity that generally scales exponentially worse than our
method (wrt the cardinality of variable domains and constraints). Furthermore,
our experiments with random satisfiability and coloring problems demonstrate
that Perturbed SP can outperform SP-guided decimation, making it the best
incomplete random CSP-solver in difficult regimes.



Inference in natural language often involves recognizing lexical entailment
(RLE); that is, identifying whether one word entails another. For example,
"buy" entails "own". Two general strategies for RLE have been proposed: One
strategy is to manually construct an asymmetric similarity measure for context
vectors (directional similarity) and another is to treat RLE as a problem of
learning to recognize semantic relations using supervised machine learning
techniques (relation classification). In this paper, we experiment with two
recent state-of-the-art representatives of the two general strategies. The
first approach is an asymmetric similarity measure (an instance of the
directional similarity strategy), designed to capture the degree to which the
contexts of a word, a, form a subset of the contexts of another word, b. The
second approach (an instance of the relation classification strategy)
represents a word pair, a:b, with a feature vector that is the concatenation of
the context vectors of a and b, and then applies supervised learning to a
training set of labeled feature vectors. Additionally, we introduce a third
approach that is a new instance of the relation classification strategy. The
third approach represents a word pair, a:b, with a feature vector in which the
features are the differences in the similarities of a and b to a set of
reference words. All three approaches use vector space models (VSMs) of
semantics, based on word-context matrices. We perform an extensive evaluation
of the three approaches using three different datasets. The proposed new
approach (similarity differences) performs significantly better than the other
two approaches on some datasets and there is no dataset for which it is
significantly worse. Our results suggest it is beneficial to make connections
between the research in lexical entailment and the research in semantic
relation classification.



Given a current news event, we tackle the problem of generating plausible
predictions of future events it might cause. We present a new methodology for
modeling and predicting such future news events using machine learning and data
mining techniques. Our Pundit algorithm generalizes examples of causality pairs
to infer a causality predictor. To obtain precisely labeled causality examples,
we mine 150 years of news articles and apply semantic natural language modeling
techniques to headlines containing certain predefined causality patterns. For
generalization, the model uses a vast number of world knowledge ontologies.
Empirical evaluation on real news articles shows that our Pundit algorithm
performs as well as non-expert humans.



As large-scale theft of data from corporate servers is becoming increasingly
common, it becomes interesting to examine alternatives to the paradigm of
centralizing sensitive data into large databases. Instead, one could use
cryptography and distributed computation so that sensitive data can be supplied
and processed in encrypted form, and only the final result is made known. In
this paper, we examine how such a paradigm can be used to implement constraint
satisfaction, a technique that can solve a broad class of AI problems such as
resource allocation, planning, scheduling, and diagnosis. Most previous work on
privacy in constraint satisfaction only attempted to protect specific types of
information, in particular the feasibility of particular combinations of
decisions. We formalize and extend these restricted notions of privacy by
introducing four types of private information, including the feasibility of
decisions and the final decisions made, but also the identities of the
participants and the topology of the problem. We present distributed algorithms
that allow computing solutions to constraint satisfaction problems while
maintaining these four types of privacy. We formally prove the privacy
properties of these algorithms, and show experiments that compare their
respective performance on benchmark problems.



We propose randomized least-squares value iteration (RLSVI) -- a new
reinforcement learning algorithm designed to explore and generalize efficiently
via linearly parameterized value functions. We explain why versions of
least-squares value iteration that use Boltzmann or epsilon-greedy exploration
can be highly inefficient, and we present computational results that
demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish
an upper bound on the expected regret of RLSVI that demonstrates
near-optimality in a tabula rasa learning context. More broadly, our results
suggest that randomized value functions offer a promising approach to tackling
a critical challenge in reinforcement learning: synthesizing efficient
exploration and effective generalization.



We propose a novel hybrid loss for multiclass and structured prediction
problems that is a convex combination of a log loss for Conditional Random
Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs).
We provide a sufficient condition for when the hybrid loss is Fisher consistent
for classification. This condition depends on a measure of dominance between
labels--specifically, the gap between the probabilities of the best label and
the second best label. We also prove Fisher consistency is necessary for
parametric consistency when learning models such as CRFs. We demonstrate
empirically that the hybrid loss typically performs least as well as--and often
better than--both of its constituent losses on a variety of tasks, such as
human action recognition. In doing so we also provide an empirical comparison
of the efficacy of probabilistic and margin based approaches to multiclass and
structured prediction.



The computational costs of inference and planning have confined Bayesian
model-based reinforcement learning to one of two dismal fates: powerful
Bayes-adaptive planning but only for simplistic models, or powerful, Bayesian
non-parametric models but using simple, myopic planning strategies such as
Thompson sampling. We ask whether it is feasible and truly beneficial to
combine rich probabilistic models with a closer approximation to fully Bayesian
planning. First, we use a collection of counterexamples to show formal problems
with the over-optimism inherent in Thompson sampling. Then we leverage
state-of-the-art techniques in efficient Bayes-adaptive planning and
non-parametric Bayesian methods to perform qualitatively better than both
existing conventional algorithms and Thompson sampling on two contextual
bandit-like problems.



The amount of information in the form of features and variables avail- able
to machine learning algorithms is ever increasing. This can lead to classifiers
that are prone to overfitting in high dimensions, high di- mensional models do
not lend themselves to interpretable results, and the CPU and memory resources
necessary to run on high-dimensional datasets severly limit the applications of
the approaches. Variable and feature selection aim to remedy this by finding a
subset of features that in some way captures the information provided best. In
this paper we present the general methodology and highlight some specific
approaches.



Machine Learner for Automated Reasoning (MaLARea) is a learning and reasoning
system for proving in large formal libraries where thousands of theorems are
available when attacking a new conjecture, and a large number of related
problems and proofs can be used to learn specific theorem-proving knowledge.
The last version of the system has by a large margin won the 2013 CASC LTB
competition. This paper describes the motivation behind the methods used in
MaLARea, discusses the general approach and the issues arising in evaluation of
such system, and describes the Mizar@Turing100 and CASC'24 versions of MaLARea.



We describe a probabilistic framework for synthesizing control policies for
general multi-robot systems, given environment and sensor models and a cost
function. Decentralized, partially observable Markov decision processes
(Dec-POMDPs) are a general model of decision processes where a team of agents
must cooperate to optimize some objective (specified by a shared reward or cost
function) in the presence of uncertainty, but where communication limitations
mean that the agents cannot share their state, so execution must proceed in a
decentralized fashion. While Dec-POMDPs are typically intractable to solve for
real-world problems, recent research on the use of macro-actions in Dec-POMDPs
has significantly increased the size of problem that can be practically solved
as a Dec-POMDP. We describe this general model, and show how, in contrast to
most existing methods that are specialized to a particular problem class, it
can synthesize control policies that use whatever opportunities for
coordination are present in the problem, while balancing off uncertainty in
outcomes, sensor information, and information about other agents. We use three
variations on a warehouse task to show that a single planner of this type can
generate cooperative behavior using task allocation, direct communication, and
signaling, as appropriate.



We consider the following problem: There is a set of items (e.g., movies) and
a group of agents (e.g., passengers on a plane); each agent has some intrinsic
utility for each of the items. Our goal is to pick a set of $K$ items that
maximize the total derived utility of all the agents (i.e., in our example we
are to pick $K$ movies that we put on the plane's entertainment system).
However, the actual utility that an agent derives from a given item is only a
fraction of its intrinsic one, and this fraction depends on how the agent ranks
the item among the chosen, available, ones. We provide a formal specification
of the model and provide concrete examples and settings where it is applicable.
We show that the problem is hard in general, but we show a number of
tractability results for its natural special cases.



Large formal mathematical libraries consist of millions of atomic inference
steps that give rise to a corresponding number of proved statements (lemmas).
Analogously to the informal mathematical practice, only a tiny fraction of such
statements is named and re-used in later proofs by formal mathematicians. In
this work, we suggest and implement criteria defining the estimated usefulness
of the HOL Light lemmas for proving further theorems. We use these criteria to
mine the large inference graph of the lemmas in the HOL Light and Flyspeck
libraries, adding up to millions of the best lemmas to the pool of statements
that can be re-used in later proofs. We show that in combination with
learning-based relevance filtering, such methods significantly strengthen
automated theorem proving of new conjectures over large formal mathematical
libraries such as Flyspeck.



Existing work in multi-agent collision prediction and avoidance typically
assumes discrete-time trajectories with Gaussian uncertainty or that are
completely deterministic. We propose an approach that allows detection of
collisions even between continuous, stochastic trajectories with the only
restriction that means and variances can be computed. To this end, we employ
probabilistic bounds to derive criterion functions whose negative sign provably
is indicative of probable collisions. For criterion functions that are
Lipschitz, an algorithm is provided to rapidly find negative values or prove
their absence. We propose an iterative policy-search approach that avoids prior
discretisations and yields collision-free trajectories with adjustably high
certainty. We test our method with both fixed-priority and auction-based
protocols for coordinating the iterative planning process. Results are provided
in collision-avoidance simulations of feedback controlled plants.



Condorcet winning sets are a set-valued generalization of the well-known
concept of a Condorcet winner. As supersets of Condorcet winning sets are
always Condorcet winning sets themselves, an interesting property of preference
profiles is the size of the smallest Condorcet winning set they admit. This
smallest size is called the Condorcet dimension of a preference profile. Since
little is known about profiles that have a certain Condorcet dimension, we show
in this paper how the problem of finding a preference profile that has a given
Condorcet dimension can be encoded as a satisfiability problem and solved by a
SAT solver. Initial results include a minimal example of a preference profile
of Condorcet dimension 3, improving previously known examples both in terms of
the number of agents as well as alternatives. Due to the high complexity of
such problems it remains open whether a preference profile of Condorcet
dimension 4 exists.



We investigate the Student-t process as an alternative to the Gaussian
process as a nonparametric prior over functions. We derive closed form
expressions for the marginal likelihood and predictive distribution of a
Student-t process, by integrating away an inverse Wishart process prior over
the covariance kernel of a Gaussian process model. We show surprising
equivalences between different hierarchical Gaussian process models leading to
Student-t processes, and derive a new sampling scheme for the inverse Wishart
process, which helps elucidate these equivalences. Overall, we show that a
Student-t process can retain the attractive properties of a Gaussian process --
a nonparametric representation, analytic marginal and predictive distributions,
and easy model selection through covariance kernels -- but has enhanced
flexibility, and predictive covariances that, unlike a Gaussian process,
explicitly depend on the values of training observations. We verify empirically
that a Student-t process is especially useful in situations where there are
changes in covariance structure, or in applications like Bayesian optimization,
where accurate predictive covariances are critical for good performance. These
advantages come at no additional computational cost over Gaussian processes.



Unsupervised ranking faces one critical challenge in evaluation applications,
that is, no ground truth is available. When PageRank and its variants show a
good solution in related subjects, they are applicable only for ranking from
link-structure data. In this work, we focus on unsupervised ranking from
multi-attribute data which is also common in evaluation tasks. To overcome the
challenge, we propose five essential meta-rules for the design and assessment
of unsupervised ranking approaches: scale and translation invariance, strict
monotonicity, linear/nonlinear capacities, smoothness, and explicitness of
parameter size. These meta-rules are regarded as high level knowledge for
unsupervised ranking tasks. Inspired by the works in [8] and [14], we propose a
ranking principal curve (RPC) model, which learns a one-dimensional manifold
function to perform unsupervised ranking tasks on multi-attribute observations.
Furthermore, the RPC is modeled to be a cubic B\'ezier curve with control
points restricted in the interior of a hypercube, thereby complying with all
the five meta-rules to infer a reasonable ranking list. With control points as
the model parameters, one is able to understand the learned manifold and to
interpret the ranking list semantically. Numerical experiments of the presented
RPC model are conducted on two open datasets of different ranking applications.
In comparison with the state-of-the-art approaches, the new model is able to
show more reasonable ranking lists.



The brain interprets ambiguous sensory information faster and more reliably
than modern computers, using neurons that are slower and less reliable than
logic gates. But Bayesian inference, which underpins many computational models
of perception and cognition, appears computationally challenging even given
modern transistor speeds and energy budgets. The computational principles and
structures needed to narrow this gap are unknown. Here we show how to build
fast Bayesian computing machines using intentionally stochastic, digital parts,
narrowing this efficiency gap by multiple orders of magnitude. We find that by
connecting stochastic digital components according to simple mathematical
rules, one can build massively parallel, low precision circuits that solve
Bayesian inference problems and are compatible with the Poisson firing
statistics of cortical neurons. We evaluate circuits for depth and motion
perception, perceptual learning and causal reasoning, each performing inference
over 10,000+ latent variables in real time - a 1,000x speed advantage over
commodity microprocessors. These results suggest a new role for randomness in
the engineering and reverse-engineering of intelligent computation.



The use of virtual agents in social coaching has increased rapidly in the
last decade. In order to train the user in different situations than can occur
in real life, the virtual agent should be able to express different social
attitudes. In this paper, we propose a model of social attitudes that enables a
virtual agent to reason on the appropriate social attitude to express during
the interaction with a user given the course of the interaction, but also the
emotions, mood and personality of the agent. Moreover, the model enables the
virtual agent to display its social attitude through its non-verbal behaviour.
The proposed model has been developed in the context of job interview
simulation. The methodology used to develop such a model combined a theoretical
and an empirical approach. Indeed, the model is based both on the literature in
Human and Social Sciences on social attitudes but also on the analysis of an
audiovisual corpus of job interviews and on post-hoc interviews with the
recruiters on their expressed attitudes during the job interview.



We consider the design of prediction market mechanisms known as automated
market makers. We show that we can design these mechanisms via the mold of
\emph{exponential family distributions}, a popular and well-studied probability
distribution template used in statistics. We give a full development of this
relationship and explore a range of benefits. We draw connections between the
information aggregation of market prices and the belief aggregation of learning
agents that rely on exponential family distributions. We develop a very natural
analysis of the market behavior as well as the price equilibrium under the
assumption that the traders exhibit risk aversion according to exponential
utility. We also consider similar aspects under alternative models, such as
when traders are budget constrained.



We propose a statistical learning model for classifying cognitive processes
based on distributed patterns of neural activation in the brain, acquired via
functional magnetic resonance imaging (fMRI). In the proposed learning method,
local meshes are formed around each voxel. The distance between voxels in the
mesh is determined by using a functional neighbourhood concept. In order to
define the functional neighbourhood, the similarities between the time series
recorded for voxels are measured and functional connectivity matrices are
constructed. Then, the local mesh for each voxel is formed by including the
functionally closest neighbouring voxels in the mesh. The relationship between
the voxels within a mesh is estimated by using a linear regression model. These
relationship vectors, called Functional Connectivity aware Local Relational
Features (FC-LRF) are then used to train a statistical learning machine. The
proposed method was tested on a recognition memory experiment, including data
pertaining to encoding and retrieval of words belonging to ten different
semantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn)
and Support Vector Machine (SVM), are trained in order to predict the semantic
category of the item being retrieved, based on activation patterns during
encoding. The classification performance of the Functional Mesh Learning model,
which range in 62%-71% is superior to the classical multi-voxel pattern
analysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.



In many technical fields, single-objective optimization procedures in
continuous domains involve expensive numerical simulations. In this context, an
improvement of the Artificial Bee Colony (ABC) algorithm, called the Artificial
super-Bee enhanced Colony (AsBeC), is presented. AsBeC is designed to provide
fast convergence speed, high solution accuracy and robust performance over a
wide range of problems. It implements enhancements of the ABC structure and
hybridizations with interpolation strategies. The latter are inspired by the
quadratic trust region approach for local investigation and by an efficient
global optimizer for separable problems. Each modification and their combined
effects are studied with appropriate metrics on a numerical benchmark, which is
also used for comparing AsBeC with some effective ABC variants and other
derivative-free algorithms. In addition, the presented algorithm is validated
on two recent benchmarks adopted for competitions in international conferences.
Results show remarkable competitiveness and robustness for AsBeC.



Intelligent systems for the annotation of media content are increasingly
being used for the automation of parts of social science research. In this
domain the problem of integrating various Artificial Intelligence (AI)
algorithms into a single intelligent system arises spontaneously. As part of
our ongoing effort in automating media content analysis for the social
sciences, we have built a modular system by combining multiple AI modules into
a flexible framework in which they can cooperate in complex tasks. Our system
combines data gathering, machine translation, topic classification, extraction
and annotation of entities and social networks, as well as many other tasks
that have been perfected over the past years of AI research. Over the last few
years, it has allowed us to realise a series of scientific studies over a vast
range of applications including comparative studies between news outlets and
media content in different countries, modelling of user preferences, and
monitoring public mood. The framework is flexible and allows the design and
implementation of modular agents, where simple modules cooperate in the
annotation of a large dataset without central coordination.



In this paper we propose a structural parameter of CNF formulas and use it to
identify instances of weighted MaxSAT and #SAT that can be solved in polynomial
time. Given a CNF formula we say that a set of clauses is precisely satisfiable
if there is some complete assignment satisfying these clauses only. Let the
ps-value of the formula be the number of precisely satisfiable sets of clauses.
Applying the notion of branch decompositions to CNF formulas and using ps-value
as cut function, we define the ps-width of a formula. For a formula given with
a decomposition of polynomial ps-width we show dynamic programming algorithms
solving weighted MaxSAT and #SAT in polynomial time. Combining with results of
'Belmonte and Vatshelle, Graph classes with structured neighborhoods and
algorithmic applications, Theor. Comput. Sci. 511: 54-65 (2013)' we get
polynomial-time algorithms solving weighted MaxSAT and #SAT for some classes of
structured CNF formulas. For example, we get $O(m^2(m + n)s)$ algorithms for
formulas $F$ of $m$ clauses and $n$ variables and size $s$, if $F$ has a linear
ordering of the variables and clauses such that for any variable $x$ occurring
in clause $C$, if $x$ appears before $C$ then any variable between them also
occurs in $C$, and if $C$ appears before $x$ then $x$ occurs also in any clause
between them. Note that the class of incidence graphs of such formulas do not
have bounded clique-width.



We consider the problem of controlling a Markov decision process (MDP) with a
large state space, so as to minimize average cost. Since it is intractable to
compete with the optimal policy for large scale problems, we pursue the more
modest goal of competing with a low-dimensional family of policies. We use the
dual linear programming formulation of the MDP average cost problem, in which
the variable is a stationary distribution over state-action pairs, and we
consider a neighborhood of a low-dimensional subset of the set of stationary
distributions (defined in terms of state-action features) as the comparison
class. We propose two techniques, one based on stochastic convex optimization,
and one based on constraint sampling. In both cases, we give bounds that show
that the performance of our algorithms approaches the best achievable by any
policy in the comparison class. Most importantly, these results depend on the
size of the comparison class, but not on the size of the state space.
Preliminary experiments show the effectiveness of the proposed algorithms in a
queuing application.



Formal methods apply algorithms based on mathematical principles to enhance
the reliability of systems. It would only be natural to try to progress from
verification, model checking or testing a system against its formal
specification into constructing it automatically. Classical algorithmic
synthesis theory provides interesting algorithms but also alarming high
complexity and undecidability results. The use of genetic programming, in
combination with model checking and testing, provides a powerful heuristic to
synthesize programs. The method is not completely automatic, as it is fine
tuned by a user that sets up the specification and parameters. It also does not
guarantee to always succeed and converge towards a solution that satisfies all
the required properties. However, we applied it successfully on quite
nontrivial examples and managed to find solutions to hard programming
challenges, as well as to improve and to correct code. We describe here several
versions of our method for synthesizing sequential and concurrent systems.



Two-way regular path queries (2RPQs) have received increased attention
recently due to their ability to relate pairs of objects by flexibly navigating
graph-structured data. They are present in property paths in SPARQL 1.1, the
new standard RDF query language, and in the XML query language XPath. In line
with XPath, we consider the extension of 2RPQs with nesting, which allows one
to require that objects along a path satisfy complex conditions, in turn
expressed through (nested) 2RPQs. We study the computational complexity of
answering nested 2RPQs and conjunctions thereof (CN2RPQs) in the presence of
domain knowledge expressed in description logics (DLs). We establish tight
complexity bounds in data and combined complexity for a variety of DLs, ranging
from lightweight DLs (DL-Lite, EL) up to highly expressive ones. Interestingly,
we are able to show that adding nesting to (C)2RPQs does not affect worst-case
data complexity of query answering for any of the considered DLs. However, in
the case of lightweight DLs, adding nesting to 2RPQs leads to a surprising jump
in combined complexity, from P-complete to Exp-complete.



We introduce a method to learn a hierarchy of successively more abstract
representations of complex data based on optimizing an information-theoretic
objective. Intuitively, the optimization searches for a set of latent factors
that best explain the correlations in the data as measured by multivariate
mutual information. The method is unsupervised, requires no model assumptions,
and scales linearly with the number of variables which makes it an attractive
approach for very high dimensional systems. We demonstrate that Correlation
Explanation (CorEx) automatically discovers meaningful structure for data from
diverse sources including personality tests, DNA, and human language.



This work presents novel algorithms for learning Bayesian network structures
with bounded treewidth. Both exact and approximate methods are developed. The
exact method combines mixed-integer linear programming formulations for
structure learning and treewidth computation. The approximate method consists
in uniformly sampling $k$-trees (maximal graphs of treewidth $k$), and
subsequently selecting, exactly or approximately, the best structure whose
moral graph is a subgraph of that $k$-tree. Some properties of these methods
are discussed and proven. The approaches are empirically compared to each other
and to a state-of-the-art method for learning bounded treewidth structures on a
collection of public data sets with up to 100 variables. The experiments show
that our exact algorithm outperforms the state of the art, and that the
approximate approach is fairly accurate.



N-tuple networks have been successfully used as position evaluation functions
for board games such as Othello or Connect Four. The effectiveness of such
networks depends on their architecture, which is determined by the placement of
constituent n-tuples, sequences of board locations, providing input to the
network. The most popular method of placing n-tuples consists in randomly
generating a small number of long, snake-shaped board location sequences. In
comparison, we show that learning n-tuple networks is significantly more
effective if they involve a large number of systematically placed, short,
straight n-tuples. Moreover, we demonstrate that in order to obtain the best
performance and the steepest learning curve for Othello it is enough to use
n-tuples of size just 2, yielding a network consisting of only 288 weights. The
best such network evolved in this study has been evaluated in the online
Othello League, obtaining the performance of nearly 96% --- more than any other
player to date.



We report on improvements to ACL2 made since the 2013 ACL2 Workshop.



A lot of information on the web is geographically referenced. Discovering and
retrieving this geographic information to satisfy various users needs across
both open and distributed Spatial Data Infrastructures (SDI) poses eminent
research challenges. However, this is mostly caused by semantic heterogeneity
in users query and lack of semantic referencing of the Geographic Information
(GI) metadata. To addressing these challenges, this paper discusses ontology
based semantic enhanced model, which explicitly represents GI metadata, and
provides linked RDF instances of each entity. The system focuses on semantic
search, ontology, and efficient spatial information retrieval. In particular,
an integrated model that uses specific domain information extraction to improve
the searching and retrieval of ranked spatial search results.



Bayesian network structures are usually built using only the data and
starting from an empty network or from a naive Bayes structure. Very often, in
some domains, like medicine, a prior structure knowledge is already known. This
structure can be automatically or manually refined in search for better
performance models. In this work, we take Bayesian networks built by
specialists and show that minor perturbations to this original network can
yield better classifiers with a very small computational cost, while
maintaining most of the intended meaning of the original model.



The paper presents a FrameNet-based information extraction and knowledge
representation framework, called FrameNet-CNL. The framework is used on natural
language documents and represents the extracted knowledge in a tailor-made
Frame-ontology from which unambiguous FrameNet-CNL paraphrase text can be
generated automatically in multiple languages. This approach brings together
the fields of information extraction and CNL, because a source text can be
considered belonging to FrameNet-CNL, if information extraction parser produces
the correct knowledge representation as a result. We describe a
state-of-the-art information extraction parser used by a national news agency
and speculate that FrameNet-CNL eventually could shape the natural language
subset used for writing the newswire articles.



We consider the problem of learning from a similarity matrix (such as
spectral clustering and lowd imensional embedding), when computing pairwise
similarities are costly, and only a limited number of entries can be observed.
We provide a theoretical analysis using standard notions of graph
approximation, significantly generalizing previous results (which focused on
spectral clustering with two clusters). We also propose a new algorithmic
approach based on adaptive sampling, which experimentally matches or improves
on previous methods, while being considerably more general and computationally
cheaper.



We consider the problem of learning user preferences over robot trajectories
for environments rich in objects and humans. This is challenging because the
criterion defining a good trajectory varies with users, tasks and interactions
in the environment. We represent trajectory preferences using a cost function
that the robot learns and uses it to generate good trajectories in new
environments. We design a crowdsourcing system - PlanIt, where non-expert users
label segments of the robot's trajectory. PlanIt allows us to collect a large
amount of user feedback, and using the weak and noisy labels from PlanIt we
learn the parameters of our model. We test our approach on 122 different
environments for robotic navigation and manipulation tasks. Our extensive
experiments show that the learned cost function generates preferred
trajectories in human environments. Our crowdsourcing system is publicly
available for the visualization of the learned costs and for providing
preference feedback: \url{http://planit.cs.cornell.edu}



This paper investigates the impact of query topology on the difficulty of
answering conjunctive queries in the presence of OWL 2 QL ontologies. Our first
contribution is to clarify the worst-case size of positive existential (PE),
non-recursive Datalog (NDL), and first-order (FO) rewritings for various
classes of tree-like conjunctive queries, ranging from linear queries to
bounded treewidth queries. Perhaps our most surprising result is a
superpolynomial lower bound on the size of PE-rewritings that holds already for
linear queries and ontologies of depth 2. More positively, we show that
polynomial-size NDL-rewritings always exist for tree-shaped queries with a
bounded number of leaves (and arbitrary ontologies), and for bounded treewidth
queries paired with bounded depth ontologies. For FO-rewritings, we equate the
existence of polysize rewritings with well-known problems in Boolean circuit
complexity. As our second contribution, we analyze the computational complexity
of query answering and establish tractability results (either NL- or
LOGCFL-completeness) for a range of query-ontology pairs. Combining our new
results with those from the literature yields a complete picture of the
succinctness and complexity landscapes for the considered classes of queries
and ontologies.



We present a first theoretical analysis of the power of polynomial-time
preprocessing for important combinatorial problems from various areas in AI. We
consider problems from Constraint Satisfaction, Global Constraints,
Satisfiability, Nonmonotonic and Bayesian Reasoning under structural
restrictions. All these problems involve two tasks: (i) identifying the
structure in the input as required by the restriction, and (ii) using the
identified structure to solve the reasoning task efficiently. We show that for
most of the considered problems, task (i) admits a polynomial-time
preprocessing to a problem kernel whose size is polynomial in a structural
problem parameter of the input, in contrast to task (ii) which does not admit
such a reduction to a problem kernel of polynomial size, subject to a
complexity theoretic assumption. As a notable exception we show that the
consistency problem for the AtMost-NValue constraint admits a polynomial kernel
consisting of a quadratic number of variables and domain values. Our results
provide a firm worst-case guarantees and theoretical boundaries for the
performance of polynomial-time preprocessing algorithms for the considered
problems.



Encoding temporal information from the recent past as spatially distributed
activations is essential in order for the entire recent past to be
simultaneously accessible. Any biological or synthetic agent that relies on the
past to predict/plan the future, would be endowed with such a spatially
distributed temporal memory. Simplistically, we would expect that resource
limitations would demand the memory system to store only the most useful
information for future prediction. For natural signals in real world which show
scale free temporal fluctuations, the predictive information encoded in memory
is maximal if the past information is scale invariantly coarse grained. Here we
examine the general mechanism to construct a scale invariantly coarse grained
memory system. Remarkably, the generic construction is equivalent to encoding
the linear combinations of Laplace transform of the past information and their
approximated inverses. This reveals a fundamental construction constraint on
memory networks that attempt to maximize predictive information storage
relevant to the natural world.



Syndromic surveillance systems continuously monitor multiple pre-diagnostic
daily streams of indicators from different regions with the aim of early
detection of disease outbreaks. The main objective of these systems is to
detect outbreaks hours or days before the clinical and laboratory confirmation.
The type of data that is being generated via these systems is usually
multivariate and seasonal with spatial and temporal dimensions. The algorithm
What's Strange About Recent Events (WSARE) is the state-of-the-art method for
such problems. It exhaustively searches for contrast sets in the multivariate
data and signals an alarm when find statistically significant rules. This
bottom-up approach presents a much lower detection delay comparing the existing
top-down approaches. However, WSARE is very sensitive to the small-scale
changes and subsequently comes with a relatively high rate of false alarms. We
propose a new approach called EigenEvent that is neither fully top-down nor
bottom-up. In this method, we instead of top-down or bottom-up search, track
changes in data correlation structure via eigenspace techniques. This new
methodology enables us to detect both overall changes (via eigenvalue) and
dimension-level changes (via eigenvectors). Experimental results on hundred
sets of benchmark data reveals that EigenEvent presents a better overall
performance comparing state-of-the-art, in particular in terms of the false
alarm rate.



Faces are a class of visual stimuli with unique significance, for a variety
of reasons. They are ubiquitous throughout the course of a person's life, and
face recognition is crucial for daily social interaction. Faces are also unlike
any other stimulus class in terms of certain physical stimulus characteristics.
Furthermore, faces have been empirically found to elicit certain characteristic
behavioral phenomena, which are widely held to be evidence of "holistic"
processing of faces. However, little is known about the neural mechanisms
underlying such holistic face processing. In other words, for the processing of
faces by the primate visual system, the input and output characteristics are
relatively well known, but the internal neural computations are not. The main
aim of this work is to further the fundamental understanding of what causes the
visual processing of faces to be different from that of objects. In this
computational modeling work, we show that a single factor - "neural tuning
size" - is able to account for three key phenomena that are characteristic of
face processing, namely the Composite Face Effect (CFE), Face Inversion Effect
(FIE) and Whole-Part Effect (WPE). Our computational proof-of-principle
provides specific neural tuning properties that correspond to the
poorly-understood notion of holistic face processing, and connects these neural
properties to psychophysical behavior. Overall, our work provides a unified and
parsimonious theoretical account for the disparate empirical data on
face-specific processing, deepening the fundamental understanding of face
processing.



Sampling from hierarchical Bayesian models is often difficult for MCMC
methods, because of the strong correlations between the model parameters and
the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC)
methods have significant potential advantages in this setting, but are
computationally expensive. We introduce a new RMHMC method, which we call
semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass
matrix that allows the joint Hamiltonian over model parameters and
hyperparameters to decompose into two simpler Hamiltonians. This structure is
exploited by a new integrator which we call the alternating blockwise leapfrog
algorithm. The resulting method can mix faster than simpler Gibbs sampling
while being simpler and more efficient than previous instances of RMHMC.



Several real problems ranging from text classification to computational
biology are characterized by hierarchical multi-label classification tasks.
Most of the methods presented in literature focused on tree-structured
taxonomies, but only few on taxonomies structured according to a Directed
Acyclic Graph (DAG). In this contribution novel classification ensemble
algorithms for DAG-structured taxonomies are introduced. In particular
Hierarchical Top-Down (HTD-DAG) and True Path Rule (TPR-DAG) for DAGs are
presented and discussed.



Latent variable conditional models, including the latent conditional random
fields as a special case, are popular models for many natural language
processing and vision processing tasks. The computational complexity of the
exact decoding/inference in latent conditional random fields is unclear. In
this paper, we try to clarify the computational complexity of the exact
decoding. We analyze the complexity and demonstrate that it is an NP-hard
problem even on a sequential labeling setting. Furthermore, we propose the
latent-dynamic inference (LDI-Naive) method and its bounded version
(LDI-Bounded), which are able to perform exact-inference or
almost-exact-inference by using top-$n$ search and dynamic programming.



The semantics of determiner phrases, be they definite de- scriptions,
indefinite descriptions or quantified noun phrases, is often as- sumed to be a
fully solved question: common nouns are properties, and determiners are
generalised quantifiers that apply to two predicates: the property
corresponding to the common noun and the one corresponding to the verb phrase.
We first present a criticism of this standard view. Firstly, the semantics of
determiners does not follow the syntactical structure of the sentence. Secondly
the standard interpretation of the indefinite article cannot ac- count for
nominal sentences. Thirdly, the standard view misses the linguis- tic asymmetry
between the two properties of a generalised quantifier. In the sequel, we
propose a treatment of determiners and quantifiers as Hilbert terms in a richly
typed system that we initially developed for lexical semantics, using a many
sorted logic for semantical representations. We present this semantical
framework called the Montagovian generative lexicon and show how these terms
better match the syntactical structure and avoid the aforementioned problems of
the standard approach. Hilbert terms rather differ from choice functions in
that there is one polymorphic operator and not one operator per formula. They
also open an intriguing connection between the logic for meaning assembly, the
typed lambda calculus handling compositionality and the many-sorted logic for
semantical representations. Furthermore epsilon terms naturally introduce
type-judgements and confirm the claim that type judgment are a form of
presupposition.



We describe a seriation algorithm for ranking a set of items given pairwise
comparisons between these items. Intuitively, the algorithm assigns similar
rankings to items that compare similarly with all others. It does so by
constructing a similarity matrix from pairwise comparisons, using seriation
methods to reorder this matrix and construct a ranking. We first show that this
spectral seriation algorithm recovers the true ranking when all pairwise
comparisons are observed and consistent with a total order. We then show that
ranking reconstruction is still exact when some pairwise comparisons are
corrupted or missing, and that seriation based spectral ranking is more robust
to noise than classical scoring methods. Finally, we bound the ranking error
when only a random subset of the comparions are observed. An additional benefit
of the seriation formulation is that it allows us to solve semi-supervised
ranking problems. Experiments on both synthetic and real datasets demonstrate
that seriation based spectral ranking achieves competitive and in some cases
superior performance compared to classical ranking methods.



This paper presents eight PAC-Bayes bounds to analyze the generalization
performance of multi-view classifiers. These bounds adopt data dependent
Gaussian priors which emphasize classifiers with high view agreements. The
center of the prior for the first two bounds is the origin, while the center of
the prior for the third and fourth bounds is given by a data dependent vector.
An important technique to obtain these bounds is two derived logarithmic
determinant inequalities whose difference lies in whether the dimensionality of
data is involved. The centers of the fifth and sixth bounds are calculated on a
separate subset of the training set. The last two bounds use unlabeled data to
represent view agreements and are thus applicable to semi-supervised multi-view
learning. We evaluate all the presented multi-view PAC-Bayes bounds on
benchmark data and compare them with previous single-view PAC-Bayes bounds. The
usefulness and performance of the multi-view bounds are discussed.



We present a physics inspired heuristic method for solving combinatorial
optimization problems. Our approach is specifically motivated by the desire to
avoid trapping in metastable local minima- a common occurrence in hard problems
with multiple extrema. Our method involves (i) coupling otherwise independent
simulations of a system ("replicas") via geometrical distances as well as (ii)
probabilistic inference applied to the solutions found by individual replicas.
The {\it ensemble} of replicas evolves as to maximize the inter-replica
correlation while simultaneously minimize the local intra-replica cost function
(e.g., the total path length in the Traveling Salesman Problem within each
replica). We demonstrate how our method improves the performance of rudimentary
local optimization schemes long applied to the NP hard Traveling Salesman
Problem. In particular, we apply our method to the well-known "$k$-opt"
algorithm and examine two particular cases- $k=2$ and $k=3$. With the aid of
geometrical coupling alone, we are able to determine for the optimum tour
length on systems up to $280$ cities (an order of magnitude larger than the
largest systems typically solved by the bare $k=3$ opt). The probabilistic
replica-based inference approach improves $k-opt$ even further and determines
the optimal solution of a problem with $318$ cities and find tours whose total
length is close to that of the optimal solutions for other systems with a
larger number of cities.



A stochastic combinatorial semi-bandit is an online learning problem where at
each step a learning agent chooses a subset of ground items subject to
combinatorial constraints, and then observes stochastic weights of these items
and receives their sum as a payoff. In this paper, we consider efficient
learning in large-scale combinatorial semi-bandits with linear generalization,
and as a solution, propose two learning algorithms called Combinatorial Linear
Thompson Sampling (CombLinTS) and Combinatorial Linear UCB (CombLinUCB). Both
algorithms are computationally efficient as long as the offline version of the
combinatorial problem can be solved efficiently. We establish that CombLinTS
and CombLinUCB are also provably statistically efficient under reasonable
assumptions, by developing regret bounds that are independent of the problem
scale (number of items) and sublinear in time. We also evaluate CombLinTS on a
variety of problems with thousands of items. Our experiment results demonstrate
that CombLinTS is scalable, robust to the choice of algorithm parameters, and
significantly outperforms the best of our baselines.



It is well known in the literature that the problem of learning the structure
of Bayesian networks is very hard to tackle: its computational complexity is
super-exponential in the number of nodes in the worst case and polynomial in
most real-world scenarios.
  Efficient implementations of score-based structure learning benefit from past
and current research in optimisation theory, which can be adapted to the task
by using the network score as the objective function to maximise. This is not
true for approaches based on conditional independence tests, called
constraint-based learning algorithms. The only optimisation in widespread use,
backtracking, leverages the symmetries implied by the definitions of
neighbourhood and Markov blanket.
  In this paper we illustrate how backtracking is implemented in recent
versions of the bnlearn R package, and how it degrades the stability of
Bayesian network structure learning for little gain in terms of speed. As an
alternative, we describe a software architecture and framework that can be used
to parallelise constraint-based structure learning algorithms (also implemented
in bnlearn) and we demonstrate its performance using four reference networks
and two real-world data sets from genetics and systems biology. We show that on
modern multi-core or multiprocessor hardware parallel implementations are
preferable over backtracking, which was developed when single-processor
machines were the norm.



Coalition formation is a key topic in multi-agent systems. Coalitions enable
agents to achieve goals that they may not have been able to achieve on their
own. Previous work has shown problems in coalitional games to be
computationally hard. Wooldridge and Dunne (Artificial Intelligence 2006)
studied the classical computational complexity of several natural decision
problems in Coalitional Resource Games (CRG) - games in which each agent is
endowed with a set of resources and coalitions can bring about a set of goals
if they are collectively endowed with the necessary amount of resources. The
input of coalitional resource games bundles together several elements, e.g.,
the agent set Ag, the goal set G, the resource set R, etc. Shrot, Aumann and
Kraus (AAMAS 2009) examine coalition formation problems in the CRG model using
the theory of Parameterized Complexity. Their refined analysis shows that not
all parts of input act equal - some instances of the problem are indeed
tractable while others still remain intractable.
  We answer an important question left open by Shrot, Aumann and Kraus by
showing that the SC Problem (checking whether a Coalition is Successful) is
W[1]-hard when parameterized by the size of the coalition. Then via a single
theme of reduction from SC, we are able to show that various problems related
to resources, resource bounds and resource conflicts introduced by Wooldridge
et al are 1. W[1]-hard or co-W[1]-hard when parameterized by the size of the
coalition. 2. para-NP-hard or co-para-NP-hard when parameterized by |R|. 3. FPT
when parameterized by either |G| or |Ag|+|R|.



We consider a class of sparse learning problems in high dimensional feature
space regularized by a structured sparsity-inducing norm which incorporates
prior knowledge of the group structure of the features. Such problems often
pose a considerable challenge to optimization algorithms due to the
non-smoothness and non-separability of the regularization term. In this paper,
we focus on two commonly adopted sparsity-inducing regularization terms, the
overlapping Group Lasso penalty $l_1/l_2$-norm and the $l_1/l_\infty$-norm. We
propose a unified framework based on the augmented Lagrangian method, under
which problems with both types of regularization and their variants can be
efficiently solved. As the core building-block of this framework, we develop
new algorithms using an alternating partial-linearization/splitting technique,
and we prove that the accelerated versions of these algorithms require
$O(\frac{1}{\sqrt{\epsilon}})$ iterations to obtain an $\epsilon$-optimal
solution. To demonstrate the efficiency and relevance of our algorithms, we
test them on a collection of data sets and apply them to two real-world
problems to compare the relative merits of the two norms.



We investigate unsupervised pre-training of deep architectures as feature
generators for "shallow" classifiers. Stacked Denoising Autoencoders (SdA),
when used as feature pre-processing tools for SVM classification, can lead to
significant improvements in accuracy - however, at the price of a substantial
increase in computational cost. In this paper we create a simple algorithm
which mimics the layer by layer training of SdAs. However, in contrast to SdAs,
our algorithm requires no training through gradient descent as the parameters
can be computed in closed-form. It can be implemented in less than 20 lines of
MATLABTMand reduces the computation time from several hours to mere seconds. We
show that our feature transformation reliably improves the results of SVM
classification significantly on all our data sets - often outperforming SdAs
and even deep neural networks in three out of four deep learning benchmarks.



Reinforcement Learning (RL) is a method for learning decision-making tasks
that could enable robots to learn and adapt to their situation on-line. For an
RL algorithm to be practical for robotic control tasks, it must learn in very
few actions, while continually taking those actions in real-time. Existing
model-based RL methods learn in relatively few actions, but typically take too
much time between each action for practical on-line learning. In this paper, we
present a novel parallel architecture for model-based RL that runs in real-time
by 1) taking advantage of sample-based approximate planning methods and 2)
parallelizing the acting, model learning, and planning processes such that the
acting process is sufficiently fast for typical robot control cycles. We
demonstrate that algorithms using this architecture perform nearly as well as
methods using the typical sequential architecture when both are given unlimited
time, and greatly out-perform these methods on tasks that require real-time
actions such as controlling an autonomous vehicle.



This paper develops upper and lower bounds for the probability of Boolean
expressions by treating multiple occurrences of variables as independent and
assigning them new individual probabilities. Our technique generalizes and
extends the underlying idea of a number of recent approaches which are
varyingly called node splitting, variable renaming, variable splitting, or
dissociation for probabilistic databases. We prove that the probabilities we
assign to new variables are the best possible in some sense.



Using an interactive theorem prover to reason about programs involves a
sequence of interactions where the user challenges the theorem prover with
conjectures. Invariably, many of the conjectures posed are in fact false, and
users often spend considerable effort examining the theorem prover's output
before realizing this. We present a synergistic integration of testing with
theorem proving, implemented in the ACL2 Sedan (ACL2s), for automatically
generating concrete counterexamples. Our method uses the full power of the
theorem prover and associated libraries to simplify conjectures; this
simplification can transform conjectures for which finding counterexamples is
hard into conjectures where finding counterexamples is trivial. In fact, our
approach even leads to better theorem proving, e.g. if testing shows that a
generalization step leads to a false conjecture, we force the theorem prover to
backtrack, allowing it to pursue more fruitful options that may yield a proof.
The focus of the paper is on the engineering of a synergistic integration of
testing with interactive theorem proving; this includes extending ACL2 with new
functionality that we expect to be of general interest. We also discuss our
experience in using ACL2s to teach freshman students how to reason about their
programs.



The Gibbard-Satterthwaite theorem states that every non-dictatorial election
rule among at least three alternatives can be strategically manipulated. We
prove a quantitative version of the Gibbard-Satterthwaite theorem: a random
manipulation by a single random voter will succeed with a non-negligible
probability for any election rule among three alternatives that is far from
being a dictatorship and from having only two alternatives in its range.



The paper briefly describes a basic set of special combinatorial engineering
frameworks for solving complex problems in the field of hierarchical modular
systems. The frameworks consist of combinatorial problems (and corresponding
models), which are interconnected/linked (e.g., by preference relation).
Mainly, hierarchical morphological system model is used. The list of basic
standard combinatorial engineering (technological) frameworks is the following:
(1) design of system hierarchical model, (2) combinatorial synthesis
('bottom-up' process for system design), (3) system evaluation, (4) detection
of system bottlenecks, (5) system improvement (re-design, upgrade), (6)
multi-stage design (design of system trajectory), (7) combinatorial modeling of
system evolution/development and system forecasting. The combinatorial
engineering frameworks are targeted to maintenance of some system life cycle
stages. The list of main underlaying combinatorial optimization problems
involves the following: knapsack problem, multiple-choice problem, assignment
problem, spanning trees, morphological clique problem.



There are enormous amount of examples of Computation in nature, exemplified
across multiple species in biology. One crucial aim for these computations
across all life forms their ability to learn and thereby increase the chance of
their survival. In the current paper a formal definition of autonomous learning
is proposed. From that definition we establish a Turing Machine model for
learning, where rule tables can be added or deleted, but can not be modified.
Sequential and parallel implementations of this model are discussed. It is
found that for general purpose learning based on this model, the
implementations capable of parallel execution would be evolutionarily stable.
This is proposed to be of the reasons why in Nature parallelism in computation
is found in abundance.



The interest in brain-like computation has led to the design of a plethora of
innovative neuromorphic systems. Individually, spiking neural networks (SNNs),
event-driven simulation and digital hardware neuromorphic systems get a lot of
attention. Despite the popularity of event-driven SNNs in software, very few
digital hardware architectures are found. This is because existing hardware
solutions for event management scale badly with the number of events. This
paper introduces the structured heap queue, a pipelined digital hardware data
structure, and demonstrates its suitability for event management. The
structured heap queue scales gracefully with the number of events, allowing the
efficient implementation of large scale digital hardware event-driven SNNs. The
scaling is linear for memory, logarithmic for logic resources and constant for
processing time. The use of the structured heap queue is demonstrated on
field-programmable gate array (FPGA) with an image segmentation experiment and
a SNN of 65~536 neurons and 513~184 synapses. Events can be processed at the
rate of 1 every 7 clock cycles and a 406$\times$158 pixel image is segmented in
200 ms.



Graph knowledge models and ontologies are very powerful modeling and re
asoning tools. We propose an effective approach to model network attacks and
attack prediction which plays important roles in security management. The goals
of this study are: First we model network attacks, their prerequisites and
consequences using knowledge representation methods in order to provide
description logic reasoning and inference over attack domain concepts. And
secondly, we propose an ontology-based system which predicts potential attacks
using inference and observing information which provided by sensory inputs. We
generate our ontology and evaluate corresponding methods using CAPEC, CWE, and
CVE hierarchical datasets. Results from experiments show significant capability
improvements comparing to traditional hierarchical and relational models.
Proposed method also reduces false alarms and improves intrusion detection
effectiveness.



Recent advances in Bayesian reinforcement learning (BRL) have shown that
Bayes-optimality is theoretically achievable by modeling the environment's
latent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In
self-interested multi-agent environments, the transition dynamics are mainly
controlled by the other agent's stochastic behavior for which FDM's
independence and modeling assumptions do not hold. As a result, FDM does not
allow the other agent's behavior to be generalized across different states nor
specified using prior domain knowledge. To overcome these practical limitations
of FDM, we propose a generalization of BRL to integrate the general class of
parametric models and model priors, thus allowing practitioners' domain
knowledge to be exploited to produce a fine-grained and compact representation
of the other agent's behavior. Empirical evaluation shows that our approach
outperforms existing multi-agent reinforcement learning algorithms.



This paper describes experiments, on two domains, to investigate the effect
of averaging over predictions of multiple decision trees, instead of using a
single tree. Other authors have pointed out theoretical and commonsense reasons
for preferring the multiple tree approach. Ideally, we would like to consider
predictions from all trees, weighted by their probability. However, there is a
vast number of different trees, and it is difficult to estimate the probability
of each tree. We sidestep the estimation problem by using a modified version of
the ID3 algorithm to build good trees, and average over only these trees. Our
results are encouraging. For each domain, we managed to produce a small number
of good trees. We find that it is best to average across sets of trees with
different structure; this usually gives better performance than any of the
constituent trees, including the ID3 tree.



Several key issues arise in implementing computer vision recognition of world
objects in terms of Bayesian networks. Computational efficiency is a driving
force. Perceptual networks are very deep, typically fifteen levels of
structure. Images are wide, e.g., an unspecified-number of edges may appear
anywhere in an image 512 x 512 pixels or larger. For efficiency, we dynamically
instantiate hypotheses of observed objects. The network is not fixed, but is
created incrementally at runtime. Generation of hypotheses of world objects and
indexing of models for recognition are important, but they are not considered
here [4,11]. This work is aimed at near-term implementation with parallel
computation in a radar surveillance system, ADRIES [5, 15], and a system for
industrial part recognition, SUCCESSOR [2]. For many applications, vision must
be faster to be practical and so efficiently controlling the machine vision
process is critical. Perceptual operators may scan megapixels and may require
minutes of computation time. It is necessary to avoid unnecessary sensor
actions and computation. Parallel computation is available at several levels of
processor capability. The potential for parallel, distributed computation for
high-level vision means distributing non-homogeneous computations. This paper
addresses the problem of task control in machine vision systems based on
Bayesian probability models. We separate control and inference to extend the
previous work [3] to maximize utility instead of probability. Maximizing
utility allows adopting perceptual strategies for efficient information
gathering with sensors and analysis of sensor data. Results of controlling
machine vision via utility to recognize military situations are presented in
this paper. Future work extends this to industrial part recognition for
SUCCESSOR.



Roborobo! is a multi-platform, highly portable, robot simulator for
large-scale collective robotics experiments. Roborobo! is coded in C++, and
follows the KISS guideline ("Keep it simple"). Therefore, its external
dependency is solely limited to the widely available SDL library for fast 2D
Graphics. Roborobo! is based on a Khepera/ePuck model. It is targeted for fast
single and multi-robots simulation, and has already been used in more than a
dozen published research mainly concerned with evolutionary swarm robotics,
including environment-driven self-adaptation and distributed evolutionary
optimization, as well as online onboard embodied evolution and embodied
morphogenesis.



This paper presents a methodology for research and development of the
inferencing and knowledge representation aspects of an Expert System approach
for performing reasoning under uncertainty in support of a real time vehicle
guidance and navigation system. Such a system could be of major benefit for
non-terrain following low altitude flight systems operating in foreign hostile
environments such as might be experienced by NOE helicopter or similar mission
craft. An innovative extension of the evidential reasoning methodology, termed
the Sum-and-Lattice-Points Method, has been developed. The research and
development effort presented in this paper consists of a formal mathematical
development of the Sum-and-Lattice-Points Method, its formulation and
representation in a parallel environment, prototype software development of the
method within an expert system, and initial testing of the system within the
confines of the vehicle guidance system.



The control and integration of distributed, multi-sensor perceptual systems
is a complex and challenging problem. The observations or opinions of different
sensors are often disparate incomparable and are usually only partial views.
Sensor information is inherently uncertain and in addition the individual
sensors may themselves be in error with respect to the system as a whole. The
successful operation of a multi-sensor system must account for this uncertainty
and provide for the aggregation of disparate information in an intelligent and
robust manner. We consider the sensors of a multi-sensor system to be members
or agents of a team, able to offer opinions and bargain in group decisions. We
will analyze the coordination and control of this structure using a theory of
team decision-making. We present some new analytic results on multi-sensor
aggregation and detail a simulation which we use to investigate our ideas. This
simulation provides a basis for the analysis of complex agent structures
cooperating in the presence of uncertainty. The results of this study are
discussed with reference to multi-sensor robot systems, distributed Al and
decision making under uncertainty.



This paper describes a machine induction program (WITT) that attempts to
model human categorization. Properties of categories to which human subjects
are sensitive includes best or prototypical members, relative contrasts between
putative categories, and polymorphy (neither necessary or sufficient features).
This approach represents an alternative to usual Artificial Intelligence
approaches to generalization and conceptual clustering which tend to focus on
necessary and sufficient feature rules, equivalence classes, and simple search
and match schemes. WITT is shown to be more consistent with human
categorization while potentially including results produced by more traditional
clustering schemes. Applications of this approach in the domains of expert
systems and information retrieval are also discussed.



Solving large traveling salesman problem (TSP) in an efficient way is a
challenging area for the researchers of computer science. This paper presents a
modified version of the ant colony system (ACS) algorithm called Red-Black Ant
Colony System (RB-ACS) for the solutions of TSP which is the most prominent
member of the combinatorial optimization problem. RB-ACS uses the concept of
ant colony system together with the parallel search of genetic algorithm for
obtaining the optimal solutions quickly. In this paper, it is shown that the
proposed RB-ACS algorithm yields significantly better performance than the
existing best-known algorithms.



An undirected graphical model is a joint probability distribution defined on
an undirected graph G*, where the vertices in the graph index a collection of
random variables and the edges encode conditional independence relationships
among random variables. The undirected graphical model selection (UGMS) problem
is to estimate the graph G* given observations drawn from the undirected
graphical model. This paper proposes a framework for decomposing the UGMS
problem into multiple subproblems over clusters and subsets of the separators
in a junction tree. The junction tree is constructed using a graph that
contains a superset of the edges in G*. We highlight three main properties of
using junction trees for UGMS. First, different regularization parameters or
different UGMS algorithms can be used to learn different parts of the graph.
This is possible since the subproblems we identify can be solved independently
of each other. Second, under certain conditions, a junction tree based UGMS
algorithm can produce consistent results with fewer observations than the usual
requirements of existing algorithms. Third, both our theoretical and
experimental results show that the junction tree framework does a significantly
better job at finding the weakest edges in a graph than existing methods. This
property is a consequence of both the first and second properties. Finally, we
note that our framework is independent of the choice of the UGMS algorithm and
can be used as a wrapper around standard UGMS algorithms for more accurate
graph estimation.



In this study we show that by the current state-of-the-art synthetically
generated fingerprints can easily be discriminated from real fingerprints. We
propose a method based on second order extended minutiae histograms (MHs) which
can distinguish between real and synthetic prints with very high accuracy. MHs
provide a fixed-length feature vector for a fingerprint which are invariant
under rotation and translation. This 'test of realness' can be applied to
synthetic fingerprints produced by any method. In this work, tests are
conducted on the 12 publicly available databases of FVC2000, FVC2002 and
FVC2004 which are well established benchmarks for evaluating the performance of
fingerprint recognition algorithms; 3 of these 12 databases consist of
artificial fingerprints generated by the SFinGe software. Additionally, we
evaluate the discriminative performance on a database of synthetic fingerprints
generated by the software of Bicz versus real fingerprint images. We conclude
with suggestions for the improvement of synthetic fingerprint generation.



In this paper we consider the problem of minimizing a convex function using a
randomized block coordinate descent method. One of the key steps at each
iteration of the algorithm is determining the update to a block of variables.
Existing algorithms assume that in order to compute the update, a particular
subproblem is solved exactly. In his work we relax this requirement, and allow
for the subproblem to be solved inexactly, leading to an inexact block
coordinate descent method. Our approach incorporates the best known results for
exact updates as a special case. Moreover, these theoretical guarantees are
complemented by practical considerations: the use of iterative techniques to
determine the update as well as the use of preconditioning for further
acceleration.



We consider a simple sequential allocation procedure for sharing indivisible
items between agents in which agents take turns to pick items. Supposing
additive utilities and independence between the agents, we show that the
expected utility of each agent is computable in polynomial time. Using this
result, we prove that the expected utilitarian social welfare is maximized when
agents take alternate turns. We also argue that this mechanism remains optimal
when agents behave strategically



Abductive reasoning (or Abduction, for short) is among the most fundamental
AI reasoning methods, with a broad range of applications, including fault
diagnosis, belief revision, and automated planning. Unfortunately, Abduction is
of high computational complexity; even propositional Abduction is
\Sigma_2^P-complete and thus harder than NP and coNP. This complexity barrier
rules out the existence of a polynomial transformation to propositional
satisfiability (SAT). In this work we use structural properties of the
Abduction instance to break this complexity barrier. We utilize the problem
structure in terms of small backdoor sets. We present fixed-parameter tractable
transformations from Abduction to SAT, which make the power of today's SAT
solvers available to Abduction.



We study the computational complexity of controlling the result of an
election by breaking ties strategically. This problem is equivalent to the
problem of deciding the winner of an election under parallel universes
tie-breaking. When the chair of the election is only asked to break ties to
choose between one of the co-winners, the problem is trivially easy. However,
in multi-round elections, we prove that it can be NP-hard for the chair to
compute how to break ties to ensure a given result. Additionally, we show that
the form of the tie-breaking function can increase the opportunities for
control. Indeed, we prove that it can be NP-hard to control an election by
breaking ties even with a two-stage voting rule.



Probabilistic logic programs are logic programs in which some of the facts
are annotated with probabilities. This paper investigates how classical
inference and learning tasks known from the graphical model community can be
tackled for probabilistic logic programs. Several such tasks such as computing
the marginals given evidence and learning from (partial) interpretations have
not really been addressed for probabilistic logic programs before.
  The first contribution of this paper is a suite of efficient algorithms for
various inference tasks. It is based on a conversion of the program and the
queries and evidence to a weighted Boolean formula. This allows us to reduce
the inference tasks to well-studied tasks such as weighted model counting,
which can be solved using state-of-the-art methods known from the graphical
model and knowledge compilation literature. The second contribution is an
algorithm for parameter estimation in the learning from interpretations
setting. The algorithm employs Expectation Maximization, and is built on top of
the developed inference algorithms.
  The proposed approach is experimentally evaluated. The results show that the
inference algorithms improve upon the state-of-the-art in probabilistic logic
programming and that it is indeed possible to learn the parameters of a
probabilistic logic program from interpretations.



We consider deep neural networks, in which the output of each node is a
quadratic function of its inputs. Similar to other deep architectures, these
networks can compactly represent any function on a finite training set. The
main goal of this paper is the derivation of an efficient layer-by-layer
algorithm for training such networks, which we denote as the \emph{Basis
Learner}. The algorithm is a universal learner in the sense that the training
error is guaranteed to decrease at every iteration, and can eventually reach
zero under mild conditions. We present practical implementations of this
algorithm, as well as preliminary experimental results. We also compare our
deep architecture to other shallow architectures for learning polynomials, in
particular kernel learning.



Generalized traveling salesman problem (GTSP) is an extension of classical
traveling salesman problem (TSP), which is a combinatorial optimization problem
and an NP-hard problem. In this paper, an efficient discrete state transition
algorithm (DSTA) for GTSP is proposed, where a new local search operator named
\textit{K-circle}, directed by neighborhood information in space, has been
introduced to DSTA to shrink search space and strengthen search ability. A
novel robust update mechanism, restore in probability and risk in probability
(Double R-Probability), is used in our work to escape from local minima. The
proposed algorithm is tested on a set of GTSP instances. Compared with other
heuristics, experimental results have demonstrated the effectiveness and strong
adaptability of DSTA and also show that DSTA has better search ability than its
competitors.



With the emerging of new networks, such as wireless sensor networks, vehicle
networks, P2P networks, cloud computing, mobile Internet, or social networks,
the network dynamics and complexity expands from system design, hardware,
software, protocols, structures, integration, evolution, application, even to
business goals. Thus the dynamics and uncertainty are unavoidable
characteristics, which come from the regular network evolution and unexpected
hardware defects, unavoidable software errors, incomplete management
information and dependency relationship between the entities among the emerging
complex networks. Due to the complexity of emerging networks, it is not always
possible to build precise models in modeling and optimization (local and
global) for networks. This paper presents a survey on probabilistic modeling
for evolving networks and identifies the new challenges which emerge on the
probabilistic models and optimization strategies in the potential application
areas of network performance, network management and network security for
evolving networks.



We report on highlights of the ACL2 enhancements introduced in ACL2 releases
since the 2011 ACL2 Workshop. Although many enhancements are critical for
soundness or robustness, we focus in this paper on those improvements that
could benefit users who are aware of them, but that might not be discovered in
everyday practice.



In traditional assembly lines, it is reasonable to assume that task execution
times are the same for each worker. However, in sheltered work centres for
disabled this assumption is not valid: some workers may execute some tasks
considerably slower or even be incapable of executing them. Worker
heterogeneity leads to a problem called the assembly line worker assignment and
balancing problem (ALWABP). For a fixed number of workers the problem is to
maximize the production rate of an assembly line by assigning workers to
stations and tasks to workers, while satisfying precedence constraints between
the tasks. This paper introduces new heuristic and exact methods to solve this
problem. We present a new MIP model, propose a novel heuristic algorithm based
on beam search, as well as a task-oriented branch-and-bound procedure which
uses new reduction rules and lower bounds for solving the problem. Extensive
computational tests on a large set of instances show that these methods are
effective and improve over existing ones.



The Bayesian approach to machine learning amounts to computing posterior
distributions of random variables from a probabilistic model of how the
variables are related (that is, a prior distribution) and a set of observations
of variables. There is a trend in machine learning towards expressing Bayesian
models as probabilistic programs. As a foundation for this kind of programming,
we propose a core functional calculus with primitives for sampling prior
distributions and observing variables. We define measure-transformer
combinators inspired by theorems in measure theory, and use these to give a
rigorous semantics to our core calculus. The original features of our semantics
include its support for discrete, continuous, and hybrid measures, and, in
particular, for observations of zero-probability events. We compile our core
language to a small imperative language that is processed by an existing
inference engine for factor graphs, which are data structures that enable many
efficient inference algorithms. This allows efficient approximate inference of
posterior marginal distributions, treating thousands of observations per second
for large instances of realistic models.



This paper proposes a method to estimate the total time required to solve SAT
in distributed environments via partitioning approach. It is based on the
observation that for some simple forms of problem partitioning one can use the
Monte Carlo approach to estimate the time required to solve an original
problem. The method proposed is based on an algorithm for searching for
partitioning with an optimal solving time estimation. We applied this method to
estimate the time required to perform logical cryptanalysis of the widely known
stream ciphers A5/1 and Bivium. The paper also describes a volunteer computing
project SAT@home aimed at solving hard combinatorial problems reduced to SAT.
In this project during several months there were solved 10 problems of logical
cryptanalysis of the A5/1 cipher thatcould not be solved using known rainbow
tables.



This is a preliminary theoretical discussion on the computational
requirements of the state of the art smoothed particle hydrodynamics (SPH) from
the optics of pattern recognition and artificial intelligence. It is pointed
out in the present paper that, when including anisotropy detection to improve
resolution on shock layer, SPH is a very peculiar case of unsupervised machine
learning. On the other hand, the free particle nature of SPH opens an
opportunity for artificial intelligence to study particles as agents acting in
a collaborative framework in which the timed outcomes of a fluid simulation
forms a large knowledge base, which might be very attractive in computational
astrophysics phenomenological problems like self-propagating star formation.



It will be shown that according to theorems of K. Menger, every neuron grid
if identified with a curve is able to preserve the adopted qualitative
structure of a data space. Furthermore, if this identification is made, the
neuron grid structure can always be mapped to a subset of a universal neuron
grid which is constructable in three space dimensions. Conclusions will be
drawn for established neuron grid types as well as neural fields.



This paper advances a framework for modeling the component interactions
between cognitive and social aspects of scientific creativity and technological
innovation. Specifically, it aims to characterize Innovation Networks; those
networks that involve the interplay of people, ideas and organizations to
create new, technologically feasible, commercially-realizable products,
processes and organizational structures. The tri-partite framework captures
networks of ideas (Concept Level), people (Individual Level) and social
structures (Social-Organizational Level) and the interactions between these
levels. At the concept level, new ideas are the nodes that are created and
linked, kept open for further investigation or closed if solved by actors at
the individual or organizational levels. At the individual level, the nodes are
actors linked by shared worldviews (based on shared professional, educational,
experiential backgrounds) who are the builders of the concept level. At the
social-organizational level, the nodes are organizations linked by common
efforts on a given project (e.g., a company-university collaboration) that by
virtue of their intellectual property or rules of governance constrain the
actions of individuals (at the Individual Level) or ideas (at the Concept
Level). After describing this framework and its implications we paint a number
of scenarios to flesh out how it can be applied.



This paper investigates the control of an ML component within the Covariance
Matrix Adaptation Evolution Strategy (CMA-ES) devoted to black-box
optimization. The known CMA-ES weakness is its sample complexity, the number of
evaluations of the objective function needed to approximate the global optimum.
This weakness is commonly addressed through surrogate optimization, learning an
estimate of the objective function a.k.a. surrogate model, and replacing most
evaluations of the true objective function with the (inexpensive) evaluation of
the surrogate model. This paper presents a principled control of the learning
schedule (when to relearn the surrogate model), based on the Kullback-Leibler
divergence of the current search distribution and the training distribution of
the former surrogate model. The experimental validation of the proposed
approach shows significant performance gains on a comprehensive set of
ill-conditioned benchmark problems, compared to the best state of the art
including the quasi-Newton high-precision BFGS method.



The production of renewable and sustainable energy is one of the most
important challenges currently facing mankind. Wind has made an increasing
contribution to the world's energy supply mix, but still remains a long way
from reaching its full potential. In this paper, we investigate the use of
artificial evolution to design vertical-axis wind turbine prototypes that are
physically instantiated and evaluated under fan generated wind conditions.
Initially a conventional evolutionary algorithm is used to explore the design
space of a single wind turbine and later a cooperative coevolutionary algorithm
is used to explore the design space of an array of wind turbines. Artificial
neural networks are used throughout as surrogate models to assist learning and
found to reduce the number of fabrications required to reach a higher
aerodynamic efficiency. Unlike in other approaches, such as computational fluid
dynamics simulations, no mathematical formulations are used and no model
assumptions are made.



We present a system that demonstrates how the compositional structure of
events, in concert with the compositional structure of language, can interplay
with the underlying focusing mechanisms in video action recognition, thereby
providing a medium, not only for top-down and bottom-up integration, but also
for multi-modal integration between vision and language. We show how the roles
played by participants (nouns), their characteristics (adjectives), the actions
performed (verbs), the manner of such actions (adverbs), and changing spatial
relations between participants (prepositions) in the form of whole sentential
descriptions mediated by a grammar, guides the activity-recognition process.
Further, the utility and expressiveness of our framework is demonstrated by
performing three separate tasks in the domain of multi-activity videos:
sentence-guided focus of attention, generation of sentential descriptions of
video, and query-based video search, simply by leveraging the framework in
different manners.



G\"odel's ontological proof has been analysed for the first-time with an
unprecedent degree of detail and formality with the help of higher-order
theorem provers. The following has been done (and in this order): A detailed
natural deduction proof. A formalization of the axioms, definitions and
theorems in the TPTP THF syntax. Automatic verification of the consistency of
the axioms and definitions with Nitpick. Automatic demonstration of the
theorems with the provers LEO-II and Satallax. A step-by-step formalization
using the Coq proof assistant. A formalization using the Isabelle proof
assistant, where the theorems (and some additional lemmata) have been automated
with Sledgehammer and Metis.



This paper summarizes efforts to computationally model two transitions in the
evolution of human creativity: its origins about two million years ago, and the
'big bang' of creativity about 50,000 years ago. Using a computational model of
cultural evolution in which neural network based agents evolve ideas for
actions through invention and imitation, we tested the hypothesis that human
creativity began with onset of the capacity for recursive recall. We compared
runs in which agents were limited to single-step actions to runs in which they
used recursive recall to chain simple actions into complex ones. Chaining
resulted in higher diversity, open-ended novelty, no ceiling on the mean
fitness of actions, and greater ability to make use of learning. Using a
computational model of portrait painting, we tested the hypothesis that the
explosion of creativity in the Middle/Upper Paleolithic was due to onset of
con-textual focus: the capacity to shift between associative and analytic
thought. This resulted in faster convergence on portraits that resembled the
sitter, employed painterly techniques, and were rated as preferable. We
conclude that recursive recall and contextual focus provide a computationally
plausible explanation of how humans evolved the means to transform this planet.



Complex systems are naturally hybrid: their dynamic behavior is both
continuous and discrete. For these systems, maintenance and repair are an
increasing part of the total cost of final product. Efficient diagnosis and
prognosis techniques have to be adopted to detect, isolate and anticipate
faults. This paper presents an original integrated theoretical framework for
diagnosis and prognosis of hybrid systems. The formalism used for hybrid
diagnosis is enriched in order to be able to follow the evolution of an aging
law for each fault of the system. The paper presents a methodology for
interleaving diagnosis and prognosis in a hybrid framework.



Procedural content generation (PCG) has recently become one of the hottest
topics in computational intelligence and AI game researches. Among a variety of
PCG techniques, search-based approaches overwhelmingly dominate PCG development
at present. While SBPCG leads to promising results and successful applications,
it poses a number of challenges ranging from representation to evaluation of
the content being generated. In this paper, we present an alternative yet
generic PCG framework, named learning-based procedure content generation
(LBPCG), to provide potential solutions to several challenging problems in
existing PCG techniques. By exploring and exploiting information gained in game
development and public beta test via data-driven learning, our framework can
generate robust content adaptable to end-user or target players on-line with
minimal interruption to their experience. Furthermore, we develop enabling
techniques to implement the various models required in our framework. For a
proof of concept, we have developed a prototype based on the classic open
source first-person shooter game, Quake. Simulation results suggest that our
framework is promising in generating quality content.



Privacy-preserving machine learning algorithms are crucial for the
increasingly common setting in which personal data, such as medical or
financial records, are analyzed. We provide general techniques to produce
privacy-preserving approximations of classifiers learned via (regularized)
empirical risk minimization (ERM). These algorithms are private under the
$\epsilon$-differential privacy definition due to Dwork et al. (2006). First we
apply the output perturbation ideas of Dwork et al. (2006), to ERM
classification. Then we propose a new method, objective perturbation, for
privacy-preserving machine learning algorithm design. This method entails
perturbing the objective function before optimizing over classifiers. If the
loss and regularizer satisfy certain convexity and differentiability criteria,
we prove theoretical results showing that our algorithms preserve privacy, and
provide generalization bounds for linear and nonlinear kernels. We further
present a privacy-preserving technique for tuning the parameters in general
machine learning algorithms, thereby providing end-to-end privacy guarantees
for the training process. We apply these results to produce privacy-preserving
analogues of regularized logistic regression and support vector machines. We
obtain encouraging results from evaluating their performance on real
demographic and benchmark data sets. Our results show that both theoretically
and empirically, objective perturbation is superior to the previous
state-of-the-art, output perturbation, in managing the inherent tradeoff
between privacy and learning performance.



Understanding predator-prey relationships among insects is a challenging task
in the domain of insect-colony research. This is due to several factors
involved, such as determining whether a particular behavior is the result of a
predator-prey interaction, a friend-foe interaction or another kind of
interaction. In this paper, we analyze a series of predator-prey and friend-foe
interactions in two colonies of carpenter ants to better understand and predict
such behavior. Using the data gathered, we have also come up with a preliminary
model for predicting such behavior under the specific conditions the experiment
was conducted in. In this paper, we present the results of our data analysis as
well as an overview of the processes involved.



Action description languages, such as A and B, are expressive instruments
introduced for formalizing planning domains and planning problem instances. The
paper starts by proposing a methodology to encode an action language (with
conditional effects and static causal laws), a slight variation of B, using
Constraint Logic Programming over Finite Domains. The approach is then
generalized to raise the use of constraints to the level of the action language
itself. A prototype implementation has been developed, and the preliminary
results are presented and discussed.
  To appear in Theory and Practice of Logic Programming (TPLP)



Abduction is a fundamental and important form of non-monotonic reasoning.
Given a knowledge base explaining how the world behaves it aims at finding an
explanation for some observed manifestation. In this paper we focus on
propositional abduction, where the knowledge base and the manifestation are
represented by propositional formulae. The problem of deciding whether there
exists an explanation has been shown to be SigmaP2-complete in general. We
consider variants obtained by restricting the allowed connectives in the
formulae to certain sets of Boolean functions. We give a complete
classification of the complexity for all considerable sets of Boolean
functions. In this way, we identify easier cases, namely NP-complete and
polynomial cases; and we highlight sources of intractability. Further, we
address the problem of counting the explanations and draw a complete picture
for the counting complexity.



In this review we integrate results of long term experimental study on ant
"language" and intelligence which were fully based on fundamental ideas of
Information Theory, such as the Shannon entropy, the Kolmogorov complexity, and
the Shannon's equation connecting the length of a message ($l$) and its
frequency $(p)$, i.e. $l = - \log p$ for rational communication systems. This
approach, new for studying biological communication systems, enabled us to
obtain the following important results on ants' communication and intelligence:
i) to reveal "distant homing" in ants, that is, their ability to transfer
information about remote events; ii) to estimate the rate of information
transmission; iii) to reveal that ants are able to grasp regularities and to
use them for "compression" of information; iv) to reveal that ants are able to
transfer to each other the information about the number of objects; v) to
discover that ants can add and subtract small numbers. The obtained results
show that Information Theory is not only wonderful mathematical theory, but
many its results may be considered as Nature laws.



With the growing popularity of Social Web applications, more and more user
data is published on the Web everyday. Our research focuses on investigating
ways of mining data from such platforms that can be used for modeling users and
for semantically augmenting user profiles. This process can enhance adaptation
and personalization in various adaptive Web-based systems. In this paper, we
present the U-Sem people modeling service, a framework for the semantic
enrichment and mining of people's profiles from usage data on the Social Web.
We explain the architecture of our people modeling service and describe its
application in an adult e-learning context as an example. Versions: Mar 21,
10:10, Mar 25, 09:37



Users who need several queries before finding what they need can benefit from
an automatic search assistant that provides feedback on their query
modification strategies. We present a method to learn from a search log which
types of query modifications have and have not been effective in the past. The
method analyses query modifications along two dimensions: a traditional
term-based dimension and a semantic dimension, for which queries are enriches
with linked data entities. Applying the method to the search logs of two search
engines, we identify six opportunities for a query modification assistant to
improve search: modification strategies that are commonly used, but that often
do not lead to satisfactory results.



Many fundamental problems in artificial intelligence, knowledge
representation, and verification involve reasoning about sets and relations
between sets and can be modeled as set constraint satisfaction problems (set
CSPs). Such problems are frequently intractable, but there are several
important set CSPs that are known to be polynomial-time tractable. We introduce
a large class of set CSPs that can be solved in quadratic time. Our class,
which we call EI, contains all previously known tractable set CSPs, but also
some new ones that are of crucial importance for example in description logics.
The class of EI set constraints has an elegant universal-algebraic
characterization, which we use to show that every set constraint language that
properly contains all EI set constraints already has a finite sublanguage with
an NP-hard constraint satisfaction problem.



Enhancement of technology-based system support for knowledge workers is an
issue of great importance. The "Knowledge work Support System (KwSS)" framework
analyzes this issue from a holistic perspective. KwSS proposes a set of design
principles for building a comprehensive IT-based support system, which enhances
the capability of a human agent for performing a set of complex and
interrelated knowledge-works relevant to one or more target task-types within a
domain of professional activities. In this paper, we propose a high-level,
software-agent based architecture for realizing a KwSS system that incorporates
these design principles. Here we focus on developing a number of crucial
enabling components of the architecture, including (1) an Activity Theory-based
novel modeling technique for knowledgeintensive activities; (2) a graph
theoretic formalism for representing these models in a knowledge base in
conjunction with relevant entity taxonomies/ontologies; and (3) an algorithm
for reasoning, using the knowledge base, about various aspects of possible
supports for activities at performance-time.



Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide
powerful generalizations of linear regression, where the target variable is
assumed to be a (possibly unknown) 1-dimensional function of a linear
predictor. In general, these problems entail non-convex estimation procedures,
and, in practice, iterative local search heuristics are often used. Kalai and
Sastry (2009) recently provided the first provably efficient method for
learning SIMs and GLMs, under the assumptions that the data are in fact
generated under a GLM and under certain monotonicity and Lipschitz constraints.
However, to obtain provable performance, the method requires a fresh sample
every iteration. In this paper, we provide algorithms for learning GLMs and
SIMs, which are both computationally and statistically efficient. We also
provide an empirical study, demonstrating their feasibility in practice.



Summary: Traffic light coordination is a complex problem. In this paper, we
extend previous work on an abstract model of city traffic to allow for multiple
street intersections. We test a self-organizing method in our model, showing
that it is close to theoretical optima and superior to a traditional method of
traffic light coordination.
  Abstract: The elementary cellular automaton following rule 184 can mimic
particles flowing in one direction at a constant speed. This automaton can
therefore model highway traffic. In a recent paper, we have incorporated
intersections regulated by traffic lights to this model using exclusively
elementary cellular automata. In such a paper, however, we only explored a
rectangular grid. We now extend our model to more complex scenarios employing
an hexagonal grid. This extension shows first that our model can readily
incorporate multiple-way intersections and hence simulate complex scenarios. In
addition, the current extension allows us to study and evaluate the behavior of
two different kinds of traffic light controller for a grid of six-way streets
allowing for either two or three street intersections: a traffic light that
tries to adapt to the amount of traffic (which results in self-organizing
traffic lights) and a system of synchronized traffic lights with coordinated
rigid periods (sometimes called the "green wave" method). We observe a tradeoff
between system capacity and topological complexity. The green wave method is
unable to cope with the complexity of a higher-capacity scenario, while the
self-organizing method is scalable, adapting to the complexity of a scenario
and exploiting its maximum capacity. Additionally, in this paper we propose a
benchmark, independent of methods and models, to measure the performance of a
traffic light controller comparing it against a theoretical optimum.



We explore self-organizing strategies for role assignment in a foraging task
carried out by a colony of artificial agents. Our strategies are inspired by
various mechanisms of division of labor (polyethism) observed in eusocial
insects like ants, termites, or bees. Specifically we instantiate models of
caste polyethism and age or temporal polyethism to evaluated the benefits to
foraging in a dynamic environment. Our experiment is directly related to the
exploration/exploitation trade of in machine learning.



Experiments in cognitive science and decision theory show that the ways in
which people combine concepts and make decisions cannot be described by
classical logic and probability theory. This has serious implications for
applied disciplines such as information retrieval, artificial intelligence and
robotics. Inspired by a mathematical formalism that generalizes quantum
mechanics the authors have constructed a contextual framework for both concept
representation and decision making, together with quantum models that are in
strong alignment with experimental data. The results can be interpreted by
assuming the existence in human thought of a double-layered structure, a
'classical logical thought' and a 'quantum conceptual thought', the latter
being responsible of the above paradoxes and nonclassical effects. The presence
of a quantum structure in cognition is relevant, for it shows that quantum
mechanics provides not only a useful modeling tool for experimental data but
also supplies a structural model for human and artificial thought processes.
This approach has strong connections with theories formalizing meaning, such as
semantic analysis, and has also a deep impact on computer science, information
retrieval and artificial intelligence. More specifically, the links with
information retrieval are discussed in this paper.



The mathematical formalism of quantum mechanics has been successfully
employed in the last years to model situations in which the use of classical
structures gives rise to problematical situations, and where typically quantum
effects, such as 'contextuality' and 'entanglement', have been recognized. This
'Quantum Interaction Approach' is briefly reviewed in this paper focusing, in
particular, on the quantum models that have been elaborated to describe how
concepts combine in cognitive science, and on the ensuing identification of a
quantum structure in human thought. We point out that these results provide
interesting insights toward the development of a unified theory for meaning and
knowledge formalization and representation. Then, we analyze the technological
aspects and implications of our approach, and a particular attention is devoted
to the connections with symbolic artificial intelligence, quantum computation
and robotics.



The article proposes an expert system for detection, and subsequent
investigation, of groups of collaborating automobile insurance fraudsters. The
system is described and examined in great detail, several technical
difficulties in detecting fraud are also considered, for it to be applicable in
practice. Opposed to many other approaches, the system uses networks for
representation of data. Networks are the most natural representation of such a
relational domain, allowing formulation and analysis of complex relations
between entities. Fraudulent entities are found by employing a novel assessment
algorithm, \textit{Iterative Assessment Algorithm} (\textit{IAA}), also
presented in the article. Besides intrinsic attributes of entities, the
algorithm explores also the relations between entities. The prototype was
evaluated and rigorously analyzed on real world data. Results show that
automobile insurance fraud can be efficiently detected with the proposed system
and that appropriate data representation is vital.



We consider a variation of the prototype combinatorial-optimisation problem
known as graph-colouring. Our optimisation goal is to colour the vertices of a
graph with a fixed number of colours, in a way to maximise the number of
different colours present in the set of nearest neighbours of each given
vertex. This problem, which we pictorially call "palette-colouring", has been
recently addressed as a basic example of problem arising in the context of
distributed data storage. Even though it has not been proved to be NP complete,
random search algorithms find the problem hard to solve. Heuristics based on a
naive belief propagation algorithm are observed to work quite well in certain
conditions. In this paper, we build upon the mentioned result, working out the
correct belief propagation algorithm, which needs to take into account the
many-body nature of the constraints present in this problem. This method
improves the naive belief propagation approach, at the cost of increased
computational effort. We also investigate the emergence of a satisfiable to
unsatisfiable "phase transition" as a function of the vertex mean degree, for
different ensembles of sparse random graphs in the large size ("thermodynamic")
limit.



We present an approach to propagation based solving, Boolean
equi-propagation, where constraints are modelled as propagators of information
about equalities between Boolean literals. Propagation based solving applies
this information as a form of partial evaluation resulting in optimized SAT
encodings. We demonstrate for a variety of benchmarks that our approach results
in smaller CNF encodings and leads to speed-ups in solving times.



A natural and established way to restrict the constraint satisfaction problem
is to fix the relations that can be used to pose constraints; such a family of
relations is called a constraint language. In this article, we study arc
consistency, a heavily investigated inference method, and three extensions
thereof from the perspective of constraint languages. We conduct a comparison
of the studied methods on the basis of which constraint languages they solve,
and we present new polynomial-time tractability results for singleton arc
consistency, the most powerful method studied.



Research on agent communication languages has typically taken the speech acts
paradigm as its starting point. Despite their manifest attractions, speech-act
models of communication have several serious disadvantages as a foundation for
communication in artificial agent systems. In particular, it has proved to be
extremely difficult to give a satisfactory semantics to speech-act based agent
communication languages. In part, the problem is that speech-act semantics
typically make reference to the "mental states" of agents (their beliefs,
desires, and intentions), and there is in general no way to attribute such
attitudes to arbitrary computational agents. In addition, agent programming
languages have only had their semantics formalised for abstract, stand-alone
versions, neglecting aspects such as communication primitives. With respect to
communication, implemented agent programming languages have tended to be rather
ad hoc. This paper addresses both of these problems, by giving semantics to
speech-act based messages received by an AgentSpeak agent. AgentSpeak is a
logic-based agent programming language which incorporates the main features of
the PRS model of reactive planning systems. The paper builds upon a structural
operational semantics to AgentSpeak that we developed in previous work. The
main contributions of this paper are as follows: an extension of our earlier
work on the theoretical foundations of AgentSpeak interpreters; a
computationally grounded semantics for (the core) performatives used in
speech-act based agent communication languages; and a well-defined extension of
AgentSpeak that supports agent communication.



In this paper we address the problem of pool based active learning, and
provide an algorithm, called UPAL, that works by minimizing the unbiased
estimator of the risk of a hypothesis in a given hypothesis space. For the
space of linear classifiers and the squared loss we show that UPAL is
equivalent to an exponentially weighted average forecaster. Exploiting some
recent results regarding the spectra of random matrices allows us to establish
consistency of UPAL when the true hypothesis is a linear hypothesis. Empirical
comparison with an active learner implementation in Vowpal Wabbit, and a
previously proposed pool based active learner implementation show good
empirical performance and better scalability.



Since Estimation of Distribution Algorithms (EDA) were proposed, many
attempts have been made to improve EDAs' performance in the context of global
optimization. So far, the studies or applications of multivariate probabilistic
model based continuous EDAs are still restricted to rather low dimensional
problems (smaller than 100D). Traditional EDAs have difficulties in solving
higher dimensional problems because of the curse of dimensionality and their
rapidly increasing computational cost. However, scaling up continuous EDAs for
higher dimensional optimization is still necessary, which is supported by the
distinctive feature of EDAs: Because a probabilistic model is explicitly
estimated, from the learnt model one can discover useful properties or features
of the problem. Besides obtaining a good solution, understanding of the problem
structure can be of great benefit, especially for black box optimization. We
propose a novel EDA framework with Model Complexity Control (EDA-MCC) to scale
up EDAs. By using Weakly dependent variable Identification (WI) and Subspace
Modeling (SM), EDA-MCC shows significantly better performance than traditional
EDAs on high dimensional problems. Moreover, the computational cost and the
requirement of large population sizes can be reduced in EDA-MCC. In addition to
being able to find a good solution, EDA-MCC can also produce a useful problem
structure characterization. EDA-MCC is the first successful instance of
multivariate model based EDAs that can be effectively applied a general class
of up to 500D problems. It also outperforms some newly developed algorithms
designed specifically for large scale optimization. In order to understand the
strength and weakness of EDA-MCC, we have carried out extensive computational
studies of EDA-MCC. Our results have revealed when EDA-MCC is likely to
outperform others on what kind of benchmark functions.



Biclustering numerical data became a popular data-mining task in the
beginning of 2000's, especially for analysing gene expression data. A bicluster
reflects a strong association between a subset of objects and a subset of
attributes in a numerical object/attribute data-table. So called biclusters of
similar values can be thought as maximal sub-tables with close values. Only few
methods address a complete, correct and non redundant enumeration of such
patterns, which is a well-known intractable problem, while no formal framework
exists. In this paper, we introduce important links between biclustering and
formal concept analysis. More specifically, we originally show that Triadic
Concept Analysis (TCA), provides a nice mathematical framework for
biclustering. Interestingly, existing algorithms of TCA, that usually apply on
binary data, can be used (directly or with slight modifications) after a
preprocessing step for extracting maximal biclusters of similar values.



Temporal networks are ubiquitous and evolve over time by the addition,
deletion, and changing of links, nodes, and attributes. Although many
relational datasets contain temporal information, the majority of existing
techniques in relational learning focus on static snapshots and ignore the
temporal dynamics. We propose a framework for discovering temporal
representations of relational data to increase the accuracy of statistical
relational learning algorithms. The temporal relational representations serve
as a basis for classification, ensembles, and pattern mining in evolving
domains. The framework includes (1) selecting the time-varying relational
components (links, attributes, nodes), (2) selecting the temporal granularity,
(3) predicting the temporal influence of each time-varying relational
component, and (4) choosing the weighted relational classifier. Additionally,
we propose temporal ensemble methods that exploit the temporal-dimension of
relational data. These ensembles outperform traditional and more sophisticated
relational ensembles while avoiding the issue of learning the most optimal
representation. Finally, the space of temporal-relational models are evaluated
using a sample of classifiers. In all cases, the proposed temporal-relational
classifiers outperform competing models that ignore the temporal information.
The results demonstrate the capability and necessity of the temporal-relational
representations for classification, ensembles, and for mining temporal
datasets.



RGB-D cameras, which give an RGB image to- gether with depths, are becoming
increasingly popular for robotic perception. In this paper, we address the task
of detecting commonly found objects in the 3D point cloud of indoor scenes
obtained from such cameras. Our method uses a graphical model that captures
various features and contextual relations, including the local visual
appearance and shape cues, object co-occurence relationships and geometric
relationships. With a large number of object classes and relations, the model's
parsimony becomes important and we address that by using multiple types of edge
potentials. We train the model using a maximum-margin learning approach. In our
experiments over a total of 52 3D scenes of homes and offices (composed from
about 550 views), we get a performance of 84.06% and 73.38% in labeling office
and home scenes respectively for 17 object classes each. We also present a
method for a robot to search for an object using the learned model and the
contextual information available from the current labelings of the scene. We
applied this algorithm successfully on a mobile robot for the task of finding
12 object classes in 10 different offices and achieved a precision of 97.56%
with 78.43% recall.



The size of 3D models used on the web or stored in databases is becoming
increasingly high. Then, an efficient method that allows users to find similar
3D objects for a given 3D model query has become necessary. Keywords and the
geometry of a 3D model cannot meet the needs of users' retrieval because they
do not include the semantic information. In this paper, a new method has been
proposed to 3D models retrieval using semantic concepts combined with shape
indexes. To obtain these concepts, we use the machine learning methods to label
3D models by k-means algorithm in measures and shape indexes space. Moreover,
semantic concepts have been organized and represented by ontology language OWL
and spatial relationships are used to disambiguate among models of similar
appearance. The SPARQL query language has been used to question the information
displayed in this language and to compute the similarity between two 3D models.
We interpret our results using the Princeton Shape Benchmark Database and the
results show the performance of the proposed new approach to retrieval 3D
models. Keywords: 3D Model, 3D retrieval, measures, shape indexes, semantic,
ontology



The paper addresses aggregation issues for composite (modular) solutions. A
systemic view point is suggested for various aggregation problems. Several
solution structures are considered: sets, set morphologies, trees, etc. Mainly,
the aggregation approach is targeted to set morphologies. The aggregation
problems are based on basic structures as substructure, superstructure,
median/consensus, and extended median/consensus. In the last case, preliminary
structure is built (e.g., substructure, median/consensus) and addition of
solution elements is considered while taking into account profit of the
additional elements and total resource constraint. Four aggregation strategies
are examined: (i) extension strategy (designing a substructure of initial
solutions as "system kernel" and extension of the substructure by additional
elements); (ii) compression strategy (designing a superstructure of initial
solutions and deletion of some its elements); (iii) combined strategy; and (iv)
new design strategy to build a new solution over an extended domain of solution
elements. Numerical real-world examples (e.g., telemetry system, communication
protocol, student plan, security system, Web-based information system,
investment, educational courses) illustrate the suggested aggregation approach.



There is a growing trend towards the convergence of cyber-physical systems
(CPS) and social computing, which will lead to the emergence of smart
communities composed of various objects (including both human individuals and
physical things) that interact and cooperate with each other. These smart
communities promise to enable a number of innovative applications and services
that will improve the quality of life. This position paper addresses some
opportunities and challenges of building smart communities characterized by
cyber-physical and social intelligence.



Even with impressive advances in automated formal methods, certain problems
in system verification and synthesis remain challenging. Examples include the
verification of quantitative properties of software involving constraints on
timing and energy consumption, and the automatic synthesis of systems from
specifications. The major challenges include environment modeling,
incompleteness in specifications, and the complexity of underlying decision
problems.
  This position paper proposes sciduction, an approach to tackle these
challenges by integrating inductive inference, deductive reasoning, and
structure hypotheses. Deductive reasoning, which leads from general rules or
concepts to conclusions about specific problem instances, includes techniques
such as logical inference and constraint solving. Inductive inference, which
generalizes from specific instances to yield a concept, includes algorithmic
learning from examples. Structure hypotheses are used to define the class of
artifacts, such as invariants or program fragments, generated during
verification or synthesis. Sciduction constrains inductive and deductive
reasoning using structure hypotheses, and actively combines inductive and
deductive reasoning: for instance, deductive techniques generate examples for
learning, and inductive reasoning is used to guide the deductive engines.
  We illustrate this approach with three applications: (i) timing analysis of
software; (ii) synthesis of loop-free programs, and (iii) controller synthesis
for hybrid systems. Some future applications are also discussed.



Text mining is becoming vital as Web 2.0 offers collaborative content
creation and sharing. Now Researchers have growing interest in text mining
methods for discovering knowledge. Text mining researchers come from variety of
areas like: Natural Language Processing, Computational Linguistic, Machine
Learning, and Statistics. A typical text mining application involves
preprocessing of text, stemming and lemmatization, tagging and annotation,
deriving knowledge patterns, evaluating and interpreting the results. There are
numerous approaches for performing text mining tasks, like: clustering,
categorization, sentimental analysis, and summarization. There is a growing
need to standardize the evaluation of these tasks. One major component of
establishing standardization is to provide standard datasets for these tasks.
Although there are various standard datasets available for traditional text
mining tasks, but there are very few and expensive datasets for blog-mining
task. Blogs, a new genre in web 2.0 is a digital diary of web user, which has
chronological entries and contains a lot of useful knowledge, thus offers a lot
of challenges and opportunities for text mining. In this paper, we report a new
indigenous dataset for Pakistani Political Blogosphere. The paper describes the
process of data collection, organization, and standardization. We have used
this dataset for carrying out various text mining tasks for blogosphere, like:
blog-search, political sentiments analysis and tracking, identification of
influential blogger, and clustering of the blog-posts. We wish to offer this
dataset free for others who aspire to pursue further in this domain.



Combining models in appropriate ways to achieve high performance is commonly
seen in machine learning fields today. Although a large amount of combinatorial
models have been created, little attention is drawn to the commons in different
models and their connections. A general modelling technique is thus worth
studying to understand model combination deeply and shed light on creating new
models. Prediction markets show a promise of becoming such a generic, flexible
combinatorial model. By reviewing on several popular combinatorial models and
prediction market models, this paper aims to show how the market models can
generalise different combinatorial stuctures and how they implement these
popular combinatorial models in specific conditions. Besides, we will see among
different market models, Storkey's \emph{Machine Learning Markets} provide more
fundamental, generic modelling mechanisms than the others, and it has a
significant appeal for both theoretical study and application.



A complex system is made up of many components with many interactions. So the
design of systems such as simulation systems, cooperative systems or assistance
systems includes a very accurate modelling of interactional and communicational
levels. The agent-based approach provides an adapted abstraction level for this
problem. After having studied the organizational context and communicative
capacities of agentbased systems, to simulate the reorganization of a flexible
manufacturing, to regulate an urban transport system, and to simulate an
epidemic detection system, our thoughts on the interactional level were
inspired by human-machine interface models, especially those in "cognitive
engineering". To provide a general framework for agent-based complex systems
modelling, we then proposed a scale of four behaviours that agents may adopt in
their complex systems (reactive, routine, cognitive, and collective). To
complete the description of multi-level agent models, which is the focus of
this paper, we illustrate our modelling and discuss our ongoing work on each
level.



In this paper, we propose a dynamic shared context processing method based on
DSC (Dynamic Shared Context) model, applied in an e-collaborative learning
environment. Firstly, we present the model. This is a way to measure the
relevance between events and roles in collaborative environments. With this
method, we can share the most appropriate event information for each role
instead of sharing all information to all roles in a collaborative work
environment. Then, we apply and verify this method in our project with Google
App supported e-learning collaborative environment. During this experiment, we
compared DSC method measured relevance of events and roles to manual measured
relevance. And we describe the favorable points from this comparison and our
finding. Finally, we discuss our future research of a hybrid DSC method to make
dynamical information shared more effective in a collaborative work
environment.



This paper has been withdrawn by the authors. We present a framework for
sequential decision making in problems described by graphical models. The
setting is given by dependent discrete random variables with associated costs
or revenues. In our examples, the dependent variables are the potential
outcomes (oil, gas or dry) when drilling a petroleum well. The goal is to
develop an optimal selection strategy that incorporates a chosen utility
function within an approximated dynamic programming scheme. We propose and
compare different approximations, from simple heuristics to more complex
iterative schemes, and we discuss their computational properties. We apply our
strategies to oil exploration over multiple prospects modeled by a directed
acyclic graph, and to a reservoir drilling decision problem modeled by a Markov
random field. The results show that the suggested strategies clearly improve
the simpler intuitive constructions, and this is useful when selecting
exploration policies.



The bi-objective winner determination problem (2WDP-SC) of a combinatorial
procurement auction for transport contracts is characterized by a set B of
bundle bids, with each bundle bid b in B consisting of a bidding carrier c_b, a
bid price p_b, and a set tau_b transport contracts which is a subset of the set
T of tendered transport contracts. Additionally, the transport quality
q_{t,c_b} is given which is expected to be realized when a transport contract t
is executed by a carrier c_b. The task of the auctioneer is to find a set X of
winning bids (X subset B), such that each transport contract is part of at
least one winning bid, the total procurement costs are minimized, and the total
transport quality is maximized. This article presents a metaheuristic approach
for the 2WDP-SC which integrates the greedy randomized adaptive search
procedure with a two-stage candidate component selection procedure, large
neighborhood search, and self-adaptive parameter setting in order to find a
competitive set of non-dominated solutions. The heuristic outperforms all
existing approaches. For seven small benchmark instances, the heuristic is the
sole approach that finds all Pareto-optimal solutions. For 28 out of 30 large
instances, none of the existing approaches is able to compute a solution that
dominates a solution found by the proposed heuristic.



We develop a conceptually clear, intuitive, and feasible decision procedure
for testing satisfiability in the full multi-agent epistemic logic CMAEL(CD)
with operators for common and distributed knowledge for all coalitions of
agents mentioned in the language. To that end, we introduce Hintikka structures
for CMAEL(CD) and prove that satisfiability in such structures is equivalent to
satisfiability in standard models. Using that result, we design an incremental
tableau-building procedure that eventually constructs a satisfying Hintikka
structure for every satisfiable input set of formulae of CMAEL(CD) and closes
for every unsatisfiable input set of formulae.



A resistive memory network that has no crossover wiring is proposed to
overcome the hardware limitations to size and functional complexity that is
associated with conventional analogue neural networks. The proposed memory
network is based on simple network cells that are arranged in a hierarchical
modular architecture. Cognitive functionality of this network is demonstrated
by an example of character recognition. The network is trained by an
evolutionary process to completely recognise characters deformed by random
noise, rotation, scaling and shifting



We apply kernel-based methods to solve the difficult reinforcement learning
problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in
keepaway are the high-dimensionality of the state space (rendering conventional
discretization-based function approximation like tilecoding infeasible), the
stochasticity due to noise and multiple learning agents needing to cooperate
(meaning that the exact dynamics of the environment are unknown) and real-time
learning (meaning that an efficient online implementation is required). We
employ the general framework of approximate policy iteration with
least-squares-based policy evaluation. As underlying function approximator we
consider the family of regularization networks with subset of regressors
approximation. The core of our proposed solution is an efficient recursive
implementation with automatic supervised selection of relevant basis functions.
Simulation results indicate that the behavior learned through our approach
clearly outperforms the best results obtained earlier with tilecoding by Stone
et al. (2005).



We analyse the philosopher Davidson's semantics of actions, using a strongly
typed logic with contexts given by sets of partial equations between the
outcomes of actions. This provides a perspicuous and elegant treatment of
reasoning about action, analogous to Reiter's work on artificial intelligence.
We define a sequent calculus for this logic, prove cut elimination, and give a
semantics based on fibrations over partial cartesian categories: we give a
structure theory for such fibrations. The existence of lax comma objects is
necessary for the proof of cut elimination, and we give conditions on the
domain fibration of a partial cartesian category for such comma objects to
exist.



The goal of supervised feature selection is to find a subset of input
features that are responsible for predicting output values. The least absolute
shrinkage and selection operator (Lasso) allows computationally efficient
feature selection based on linear dependency between input features and output
values. In this paper, we consider a feature-wise kernelized Lasso for
capturing non-linear input-output dependency. We first show that, with
particular choices of kernel functions, non-redundant features with strong
statistical dependence on output values can be found in terms of kernel-based
independence measures. We then show that the globally optimal solution can be
efficiently computed; this makes the approach scalable to high-dimensional
problems. The effectiveness of the proposed method is demonstrated through
feature selection experiments with thousands of features.



We propose a novel, type-elimination-based method for reasoning in the
description logic SHIQbs including DL-safe rules. To this end, we first
establish a knowledge compilation method converting the terminological part of
an ALCIb knowledge base into an ordered binary decision diagram (OBDD) which
represents a canonical model. This OBDD can in turn be transformed into
disjunctive Datalog and merged with the assertional part of the knowledge base
in order to perform combined reasoning. In order to leverage our technique for
full SHIQbs, we provide a stepwise reduction from SHIQbs to ALCIb that
preserves satisfiability and entailment of positive and negative ground facts.
The proposed technique is shown to be worst case optimal w.r.t. combined and
data complexity and easily admits extensions with ground conjunctive queries.



Sequence optimization, where the items in a list are ordered to maximize some
reward has many applications such as web advertisement placement, search, and
control libraries in robotics. Previous work in sequence optimization produces
a static ordering that does not take any features of the item or context of the
problem into account. In this work, we propose a general approach to order the
items within the sequence based on the context (e.g., perceptual information,
environment description, and goals). We take a simple, efficient,
reduction-based approach where the choice and order of the items is established
by repeatedly learning simple classifiers or regressors for each "slot" in the
sequence. Our approach leverages recent work on submodular function
maximization to provide a formal regret reduction from submodular sequence
optimization to simple cost-sensitive prediction. We apply our contextual
sequence prediction algorithm to optimize control libraries and demonstrate
results on two robotics problems: manipulator trajectory prediction and mobile
robot path planning.



We discuss the frequent pattern mining problem in a general setting. From an
analysis of abstract representations, summarization and frequent pattern
mining, we arrive at a generalization of the problem. Then, we show how the
problem can be cast into the powerful language of algorithmic information
theory. This allows us to formulate a simple algorithm to mine for all frequent
patterns.



In general Evolutionary Computation (EC) includes a number of optimization
methods inspired by biological mechanisms of evolution. The methods catalogued
in this area use the Darwinian principles of life evolution to produce
algorithms that returns high quality solutions to hard-to-solve optimization
problems. The main strength of EC is precisely that they provide good solutions
even if the computational resources (e.g., running time) are limited. Astronomy
and Astrophysics are two fields that often require optimizing problems of high
complexity or analyzing a huge amount of data and the so-called complete
optimization methods are inherently limited by the size of the problem/data.
For instance, reliable analysis of large amounts of data is central to modern
astrophysics and astronomical sciences in general. EC techniques perform well
where other optimization methods are inherently limited (as complete methods
applied to NP-hard problems), and in the last ten years, numerous proposals
have come up that apply with greater or lesser success methodologies of
evolutional computation to common engineering problems. Some of these problems,
such as the estimation of non-lineal parameters, the development of automatic
learning techniques, the implementation of control systems, or the resolution
of multi-objective optimization problems, have had (and have) a special
repercussion in the fields. For these reasons EC emerges as a feasible
alternative for traditional methods. In this paper, we discuss some promising
applications in this direction and a number of recent works in this area; the
paper also includes a general description of EC to provide a global perspective
to the reader and gives some guidelines of application of EC techniques for
future research



The problem of neural network association is to retrieve a previously
memorized pattern from its noisy version using a network of neurons. An ideal
neural network should include three components simultaneously: a learning
algorithm, a large pattern retrieval capacity and resilience against noise.
Prior works in this area usually improve one or two aspects at the cost of the
third.
  Our work takes a step forward in closing this gap. More specifically, we show
that by forcing natural constraints on the set of learning patterns, we can
drastically improve the retrieval capacity of our neural network. Moreover, we
devise a learning algorithm whose role is to learn those patterns satisfying
the above mentioned constraints. Finally we show that our neural network can
cope with a fair amount of noise.



In this paper we propose two new algorithms based on biclustering analysis,
which can be used at the basis of a recommender system for educational
orientation of Russian School graduates. The first algorithm was designed to
help students make a choice between different university faculties when some of
their preferences are known. The second algorithm was developed for the special
situation when nothing is known about their preferences. The final version of
this recommender system will be used by Higher School of Economics.



Concept Relation Discovery and Innovation Enabling Technology (CORDIET), is a
toolbox for gaining new knowledge from unstructured text data. At the core of
CORDIET is the C-K theory which captures the essential elements of innovation.
The tool uses Formal Concept Analysis (FCA), Emergent Self Organizing Maps
(ESOM) and Hidden Markov Models (HMM) as main artifacts in the analysis
process. The user can define temporal, text mining and compound attributes. The
text mining attributes are used to analyze the unstructured text in documents,
the temporal attributes use these document's timestamps for analysis. The
compound attributes are XML rules based on text mining and temporal attributes.
The user can cluster objects with object-cluster rules and can chop the data in
pieces with segmentation rules. The artifacts are optimized for efficient data
analysis; object labels in the FCA lattice and ESOM map contain an URL on which
the user can click to open the selected document.



It is a high-quality algorithm for hierarchical clustering of large software
source code. This effectively allows to break the complexity of tens of
millions lines of source code, so that a human software engineer can comprehend
a software system at high level by means of looking at its architectural
diagram that is reconstructed automatically from the source code of the
software system. The architectural diagram shows a tree of subsystems having
OOP classes in its leaves (in the other words, a nested software
decomposition). The tool reconstructs the missing
(inconsistent/incomplete/inexistent) architectural documentation for a software
system from its source code. This facilitates software maintenance: change
requests can be performed substantially faster. Simply speaking, this unique
tool allows to lift the comprehensible grain of object-oriented software
systems from OOP class-level to subsystem-level. It is estimated that a
commercial tool, developed on the basis of this work, will reduce software
maintenance expenses 10 times on the current needs, and will allow to implement
next-generation software systems which are currently too complex to be within
the range of human comprehension, therefore can't yet be designed or
implemented. Implemented prototype in Open Source:
http://sourceforge.net/p/insoar/code-0/1/tree/



The problem of active diagnosis arises in several applications such as
disease diagnosis, and fault diagnosis in computer networks, where the goal is
to rapidly identify the binary states of a set of objects (e.g., faulty or
working) by sequentially selecting, and observing, (noisy) responses to binary
valued queries. Current algorithms in this area rely on loopy belief
propagation for active query selection. These algorithms have an exponential
time complexity, making them slow and even intractable in large networks. We
propose a rank-based greedy algorithm that sequentially chooses queries such
that the area under the ROC curve of the rank-based output is maximized. The
AUC criterion allows us to make a simplifying assumption that significantly
reduces the complexity of active query selection (from exponential to near
quadratic), with little or no compromise on the performance quality.



Affinity propagation is an exemplar-based clustering algorithm that finds a
set of data-points that best exemplify the data, and associates each datapoint
with one exemplar. We extend affinity propagation in a principled way to solve
the hierarchical clustering problem, which arises in a variety of domains
including biology, sensor networks and decision making in operational research.
We derive an inference algorithm that operates by propagating information up
and down the hierarchy, and is efficient despite the high-order potentials
required for the graphical model formulation. We demonstrate that our method
outperforms greedy techniques that cluster one layer at a time. We show that on
an artificial dataset designed to mimic the HIV-strain mutation dynamics, our
method outperforms related methods. For real HIV sequences, where the ground
truth is not available, we show our method achieves better results, in terms of
the underlying objective function, and show the results correspond meaningfully
to geographical location and strain subtypes. Finally we report results on
using the method for the analysis of mass spectra, showing it performs
favorably compared to state-of-the-art methods.



The key limiting factor in graphical model inference and learning is the
complexity of the partition function. We thus ask the question: what are
general conditions under which the partition function is tractable? The answer
leads to a new kind of deep architecture, which we call sum-product networks
(SPNs). SPNs are directed acyclic graphs with variables as leaves, sums and
products as internal nodes, and weighted edges. We show that if an SPN is
complete and consistent it represents the partition function and all marginals
of some graphical model, and give semantics to its nodes. Essentially all
tractable graphical models can be cast as SPNs, but SPNs are also strictly more
general. We then propose learning algorithms for SPNs, based on backpropagation
and EM. Experiments show that inference and learning with SPNs can be both
faster and more accurate than with standard deep networks. For example, SPNs
perform image completion better than state-of-the-art deep networks for this
task. SPNs also have intriguing potential connections to the architecture of
the cortex.



Distributions over rankings are used to model data in various settings such
as preference analysis and political elections. The factorial size of the space
of rankings, however, typically forces one to make structural assumptions, such
as smoothness, sparsity, or probabilistic independence about these underlying
distributions. We approach the modeling problem from the computational
principle that one should make structural assumptions which allow for efficient
calculation of typical probabilistic queries. For ranking models, "typical"
queries predominantly take the form of partial ranking queries (e.g., given a
user's top-k favorite movies, what are his preferences over remaining movies?).
In this paper, we argue that riffled independence factorizations proposed in
recent literature [7, 8] are a natural structural assumption for ranking
distributions, allowing for particularly efficient processing of partial
ranking queries.



Determinantal point processes (DPPs), which arise in random matrix theory and
quantum physics, are natural models for subset selection problems where
diversity is preferred. Among many remarkable properties, DPPs offer tractable
algorithms for exact inference, including computing marginal probabilities and
sampling; however, an important open question has been how to learn a DPP from
labeled training data. In this paper we propose a natural feature-based
parameterization of conditional DPPs, and show how it leads to a convex and
efficient learning formulation. We analyze the relationship between our model
and binary Markov random fields with repulsive potentials, which are
qualitatively similar but computationally intractable. Finally, we apply our
approach to the task of extractive summarization, where the goal is to choose a
small subset of sentences conveying the most important information from a set
of documents. In this task there is a fundamental tradeoff between sentences
that are highly relevant to the collection as a whole, and sentences that are
diverse and not repetitive. Our parameterization allows us to naturally balance
these two characteristics. We evaluate our system on data from the DUC 2003/04
multi-document summarization task, achieving state-of-the-art results.



Computing maximum a posteriori (MAP) estimation in graphical models is an
important inference problem with many applications. We present message-passing
algorithms for quadratic programming (QP) formulations of MAP estimation for
pairwise Markov random fields. In particular, we use the concave-convex
procedure (CCCP) to obtain a locally optimal algorithm for the non-convex QP
formulation. A similar technique is used to derive a globally convergent
algorithm for the convex QP relaxation of MAP. We also show that a recently
developed expectation-maximization (EM) algorithm for the QP formulation of MAP
can be derived from the CCCP perspective. Experiments on synthetic and
real-world problems confirm that our new approach is competitive with
max-product and its variations. Compared with CPLEX, we achieve more than an
order-of-magnitude speedup in solving optimally the convex QP relaxation.



We demonstrate a limitation of discounted expected utility, a standard
approach for representing the preference to risk when future cost is
discounted. Specifically, we provide an example of the preference of a decision
maker that appears to be rational but cannot be represented with any discounted
expected utility. A straightforward modification to discounted expected utility
leads to inconsistent decision making over time. We will show that an iterated
risk measure can represent the preference that cannot be represented by any
discounted expected utility and that the decisions based on the iterated risk
measure are consistent over time.



We introduce a rich class of graphical models for multi-armed bandit problems
that permit both the state or context space and the action space to be very
large, yet succinctly specify the payoffs for any context-action pair. Our main
result is an algorithm for such models whose regret is bounded by the number of
parameters and whose running time depends only on the treewidth of the graph
substructure induced by the action space.



Knuth (1990) introduced the class of nested formulas and showed that their
satisfiability can be decided in polynomial time. We show that, parameterized
by the size of a smallest strong backdoor set to the target class of nested
formulas, checking the satisfiability of any CNF formula is fixed-parameter
tractable. Thus, for any k>0, the satisfiability problem can be solved in
polynomial time for any formula F for which there exists a variable set B of
size at most k such that for every truth assignment t to B, the formula F[t] is
nested; moreover, the degree of the polynomial is independent of k.
  Our algorithm uses the grid-minor theorem of Robertson and Seymour (1986) to
either find that the incidence graph of the formula has bounded treewidth - a
case that is solved using model checking for monadic second order logic - or to
find many vertex-disjoint obstructions in the incidence graph. For the latter
case, new combinatorial arguments are used to find a small backdoor set.
Combining both cases leads to an approximation algorithm producing a strong
backdoor set whose size is upper bounded by a function of the optimum. Going
through all assignments to this set of variables and using Knuth's algorithm,
the satisfiability of the input formula is decided.



We show that the existence of a computationally efficient calibration
algorithm, with a low weak calibration rate, would imply the existence of an
efficient algorithm for computing approximate Nash equilibria - thus implying
the unlikely conclusion that every problem in PPAD is solvable in polynomial
time.



Computer-supported learning is an increasingly important form of study since
it allows for independent learning and individualized instruction. In this
paper, we discuss a novel approach to developing an intelligent tutoring system
for teaching textbook-style mathematical proofs. We characterize the
particularities of the domain and discuss common ITS design models. Our
approach is motivated by phenomena found in a corpus of tutorial dialogs that
were collected in a Wizard-of-Oz experiment. We show how an intelligent tutor
for textbook-style mathematical proofs can be built on top of an adapted
assertion-level proof assistant by reusing representations and proof search
strategies originally developed for automated and interactive theorem proving.
The resulting prototype was successfully evaluated on a corpus of tutorial
dialogs and yields good results.



The Isabelle/PIDE platform addresses the question whether proof assistants of
the LCF family are suitable as technological basis for educational tools. The
traditionally strong logical foundations of systems like HOL, Coq, or Isabelle
have so far been counter-balanced by somewhat inaccessible interaction via the
TTY (or minor variations like the well-known Proof General / Emacs interface).
Thus the fundamental question of math education tools with fully-formal
background theories has often been answered negatively due to accidental
weaknesses of existing proof engines.
  The idea of "PIDE" (which means "Prover IDE") is to integrate existing
provers like Isabelle into a larger environment, that facilitates access by
end-users and other tools. We use Scala to expose the proof engine in ML to the
JVM world, where many user-interfaces, editor frameworks, and educational tools
already exist. This shall ultimately lead to combined mathematical assistants,
where the logical engine is in the background, without obstructing the view on
applications of formal methods, formalized mathematics, and math education in
particular.



In recent years, diagrammatic languages have been shown to be a powerful and
expressive tool for reasoning about physical, logical, and semantic processes
represented as morphisms in a monoidal category. In particular, categorical
quantum mechanics, or "Quantum Picturalism", aims to turn concrete features of
quantum theory into abstract structural properties, expressed in the form of
diagrammatic identities. One way we search for these properties is to start
with a concrete model (e.g. a set of linear maps or finite relations) and start
composing generators into diagrams and looking for graphical identities.
  Naively, we could automate this procedure by enumerating all diagrams up to a
given size and check for equalities, but this is intractable in practice
because it produces far too many equations. Luckily, many of these identities
are not primitive, but rather derivable from simpler ones. In 2010, Johansson,
Dixon, and Bundy developed a technique called conjecture synthesis for
automatically generating conjectured term equations to feed into an inductive
theorem prover. In this extended abstract, we adapt this technique to
diagrammatic theories, expressed as graph rewrite systems, and demonstrate its
application by synthesising a graphical theory for studying entangled quantum
states.



In this paper, we address the problem of global transmit power minimization
in a self-congiguring network where radio devices are subject to operate at a
minimum signal to interference plus noise ratio (SINR) level. We model the
network as a parallel Gaussian interference channel and we introduce a fully
decentralized algorithm (based on trial and error) able to statistically
achieve a congiguration where the performance demands are met. Contrary to
existing solutions, our algorithm requires only local information and can learn
stable and efficient working points by using only one bit feedback. We model
the network under two different game theoretical frameworks: normal form and
satisfaction form. We show that the converging points correspond to equilibrium
points, namely Nash and satisfaction equilibrium. Similarly, we provide
sufficient conditions for the algorithm to converge in both formulations.
Moreover, we provide analytical results to estimate the algorithm's
performance, as a function of the network parameters. Finally, numerical
results are provided to validate our theoretical conclusions. Keywords:
Learning, power control, trial and error, Nash equilibrium, spectrum sharing.



3D city models - which represent in 3 dimensions the geometric elements of a
city - are increasingly used for an intended wide range of applications. Such
uses are made possible by using semantically enriched 3D city models and by
presenting such enriched 3D city models in a way that allows decision-making
processes to be carried out from the best choices among sets of objectives, and
across issues and scales. In order to help in such a decision-making process we
have defined a framework to find the best visualization technique(s) for a set
of potentially heterogeneous data that have to be visualized within the same 3D
city model, in order to perform a given task in a specific context. We have
chosen an ontology-based approach. This approach and the specification and use
of the resulting ontology of 3D visualization techniques are described in this
paper.



Relational data representations have become an increasingly important topic
due to the recent proliferation of network datasets (e.g., social, biological,
information networks) and a corresponding increase in the application of
statistical relational learning (SRL) algorithms to these domains. In this
article, we examine a range of representation issues for graph-based relational
data. Since the choice of relational data representation for the nodes, links,
and features can dramatically affect the capabilities of SRL algorithms, we
survey approaches and opportunities for relational representation
transformation designed to improve the performance of these algorithms. This
leads us to introduce an intuitive taxonomy for data representation
transformations in relational domains that incorporates link transformation and
node transformation as symmetric representation tasks. In particular, the
transformation tasks for both nodes and links include (i) predicting their
existence, (ii) predicting their label or type, (iii) estimating their weight
or importance, and (iv) systematically constructing their relevant features. We
motivate our taxonomy through detailed examples and use it to survey and
compare competing approaches for each of these tasks. We also discuss general
conditions for transforming links, nodes, and features. Finally, we highlight
challenges that remain to be addressed.



Linear principal component analysis (PCA) can be extended to a nonlinear PCA
by using artificial neural networks. But the benefit of curved components
requires a careful control of the model complexity. Moreover, standard
techniques for model selection, including cross-validation and more generally
the use of an independent test set, fail when applied to nonlinear PCA because
of its inherent unsupervised characteristics. This paper presents a new
approach for validating the complexity of nonlinear PCA models by using the
error in missing data estimation as a criterion for model selection. It is
motivated by the idea that only the model of optimal complexity is able to
predict missing values with the highest accuracy. While standard test set
validation usually favours over-fitted nonlinear PCA models, the proposed model
validation approach correctly selects the optimal model complexity.



Bayesian networks (BN) are used in a big range of applications but they have
one issue concerning parameter learning. In real application, training data are
always incomplete or some nodes are hidden. To deal with this problem many
learning parameter algorithms are suggested foreground EM, Gibbs sampling and
RBE algorithms. In order to limit the search space and escape from local maxima
produced by executing EM algorithm, this paper presents a learning parameter
algorithm that is a fusion of EM and RBE algorithms. This algorithm
incorporates the range of a parameter into the EM algorithm. This range is
calculated by the first step of RBE algorithm allowing a regularization of each
parameter in bayesian network after the maximization step of the EM algorithm.
The threshold EM algorithm is applied in brain tumor diagnosis and show some
advantages and disadvantages over the EM algorithm.



We propose a graphical model for representing networks of stochastic
processes, the minimal generative model graph. It is based on reduced
factorizations of the joint distribution over time. We show that under
appropriate conditions, it is unique and consistent with another type of
graphical model, the directed information graph, which is based on a
generalization of Granger causality. We demonstrate how directed information
quantifies Granger causality in a particular sequential prediction setting. We
also develop efficient methods to estimate the topological structure from data
that obviate estimating the joint statistics. One algorithm assumes
upper-bounds on the degrees and uses the minimal dimension statistics
necessary. In the event that the upper-bounds are not valid, the resulting
graph is nonetheless an optimal approximation. Another algorithm uses
near-minimal dimension statistics when no bounds are known but the distribution
satisfies a certain criterion. Analogous to how structure learning algorithms
for undirected graphical models use mutual information estimates, these
algorithms use directed information estimates. We characterize the
sample-complexity of two plug-in directed information estimators and obtain
confidence intervals. For the setting when point estimates are unreliable, we
propose an algorithm that uses confidence intervals to identify the best
approximation that is robust to estimation error. Lastly, we demonstrate the
effectiveness of the proposed algorithms through analysis of both synthetic
data and real data from the Twitter network. In the latter case, we identify
which news sources influence users in the network by merely analyzing tweet
times.



We argue for the value of publishing the exact code, configuration and data
processing scripts used to produce empirical work in robotics. In particular,
we recommend publishing a unique identifier for the code package in the paper
itself, as a promise to the reader that this is the relavant code. We review
some recent discussion of best practice for reproducibility in various
professional organisations and journals, and discuss the current reward
structure for publishing code in robotics, along with some ideas for
improvement.



The bioinformatical methods to detect lateral gene transfer events are mainly
based on functional coding DNA characteristics. In this paper, we propose the
use of DNA traits not depending on protein coding requirements. We introduce
several semilocal variables that depend on DNA primary sequence and that
reflect thermodynamic as well as physico-chemical magnitudes that are able to
tell apart the genome of different organisms. After combining these variables
in a neural classificator, we obtain results whose power of resolution go as
far as to detect the exchange of genomic material between bacteria that are
phylogenetically close.



Logs of the interactions with a search engine show that users often
reformulate their queries. Examining these reformulations shows that
recommendations that precise the focus of a query are helpful, like those based
on expansions of the original queries. But it also shows that queries that
express some topical shift with respect to the original query can help user
access more rapidly the information they need. We propose a method to identify
from the query logs of past users queries that either focus or shift the
initial query topic. This method combines various click-based, topic-based and
session based ranking strategies and uses supervised learning in order to
maximize the semantic similarities between the query and the recommendations,
while at the same diversifying them. We evaluate our method using the
query/click logs of a Japanese web search engine and we show that the
combination of the three methods proposed is significantly better than any of
them taken individually.



A useful step towards better interpretation and analysis of the usage
patterns is to formalize the semantics of the resources that users are
accessing in the Web. We focus on this problem and present an approach for the
semantic formalization of usage logs, which lays the basis for eective
techniques of querying expressive usage patterns. We also present a query
answering approach, which is useful to nd in the logs expressive patterns of
usage behavior via formulation of semantic and temporal-based constraints. We
have processed over 30 thousand user browsing sessions extracted from usage
logs of DBPedia and Semantic Web Dog Food. All these events are formalized
semantically using respective domain ontologies and RDF representations of the
Web resources being accessed. We show the eectiveness of our approach through
experimental results, providing in this way an exploratory analysis of the way
users browse theWeb of Data.



Novel research in the field of Linked Data focuses on the problem of entity
summarization. This field addresses the problem of ranking features according
to their importance for the task of identifying a particular entity. Next to a
more human friendly presentation, these summarizations can play a central role
for semantic search engines and semantic recommender systems. In current
approaches, it has been tried to apply entity summarization based on patterns
that are inherent to the regarded data.
  The proposed approach of this paper focuses on the movie domain. It utilizes
usage data in order to support measuring the similarity between movie entities.
Using this similarity it is possible to determine the k-nearest neighbors of an
entity. This leads to the idea that features that entities share with their
nearest neighbors can be considered as significant or important for these
entities. Additionally, we introduce a downgrading factor (similar to TF-IDF)
in order to overcome the high number of commonly occurring features. We
exemplify the approach based on a movie-ratings dataset that has been linked to
Freebase entities.



We present a framework that allows an observer to determine occluded portions
of a structure by finding the maximum-likelihood estimate of those occluded
portions consistent with visible image evidence and a consistency model. Doing
this requires determining which portions of the structure are occluded in the
first place. Since each process relies on the other, we determine a solution to
both problems in tandem. We extend our framework to determine confidence of
one's assessment of which portions of an observed structure are occluded, and
the estimate of that occluded structure, by determining the sensitivity of
one's assessment to potential new observations. We further extend our framework
to determine a robotic action whose execution would allow a new observation
that would maximally increase one's confidence.



Cardinality constraints or, more generally, weight constraints are well
recognized as an important extension of answer-set programming. Clearly, all
common algorithmic tasks related to programs with cardinality or weight
constraints - like checking the consistency of a program - are intractable.
Many intractable problems in the area of knowledge representation and reasoning
have been shown to become linear time tractable if the treewidth of the
programs or formulas under consideration is bounded by some constant. The goal
of this paper is to apply the notion of treewidth to programs with cardinality
or weight constraints and to identify tractable fragments. It will turn out
that the straightforward application of treewidth to such class of programs
does not suffice to obtain tractability. However, by imposing further
restrictions, tractability can be achieved.



Animals behave adaptively in the environment with multiply competing goals.
Understanding of the mechanisms underlying such goal-directed behavior remains
a challenge for neuroscience as well for adaptive system research. To address
this problem we developed an evolutionary model of adaptive behavior in the
multigoal stochastic environment. Proposed neuroevolutionary algorithm is based
on neuron's duplication as a basic mechanism of agent's recurrent neural
network development. Results of simulation demonstrate that in the course of
evolution agents acquire the ability to store the short-term memory and,
therefore, use it in behavioral strategies with alternative actions. We found
that evolution discovered two mechanisms for short-term memory. The first
mechanism is integration of sensory signals and ongoing internal neural
activity, resulting in emergence of cell groups specialized on alternative
actions. And the second mechanism is slow neurodynamical processes that makes
possible to code the previous behavioral choice.



This paper presents the results of computational experiments on the effects
of social influence on individual and systemic behavior of situated cognitive
agents in a product-consumer environment. Paired experiments were performed
with identical initial conditions to compare social agents with non- social
agents. Experiment results show that social agents are more productive in
consuming available products, both in terms of aggregate unit consumption and
aggregate utility. But this comes at a cost of individual average utility per
unit consumed. In effect, social interaction achieved higher productivity by
'lowering the standards' of individual consumers. While still at an early stage
of development, such an agent-based model laboratory is shown to be an
effective research tool to investigate rich collective behavior in the context
of demanding cognitive tasks.



For the task of moving a set of indistinguishable agents on a connected graph
with unit edge distance to an arbitrary set of goal vertices, free of
collisions, we propose a fast distance optimal control algorithm that guides
the agents into the desired formation. Moreover, we show that the algorithm
also provides a tight convergence time guarantee (time optimality and distance
optimality cannot be simultaneously satisfied). Our generic graph formulation
allows the algorithm to be applied to scenarios such as grids with holes
(modeling obstacles) in arbitrary dimensions. Simulations, available online,
confirm our theoretical developments.



In this paper, we study the problem of optimal multi-robot path planning
(MPP) on graphs. We propose two multiflow based integer linear programming
(ILP) models that computes minimum last arrival time and minimum total distance
solutions for our MPP formulation, respectively. The resulting algorithms from
these ILP models are complete and guaranteed to yield true optimal solutions.
In addition, our flexible framework can easily accommodate other variants of
the MPP problem. Focusing on the time optimal algorithm, we evaluate its
performance, both as a stand alone algorithm and as a generic heuristic for
quickly solving large problem instances. Computational results confirm the
effectiveness of our method.



Many biological processes involve synchronization between nonequivalent
systems, i.e, systems where the difference is limited to a rather small
parameter mismatch. The maintenance of the synchronized regime in this cases is
energetically costly \cite{1}. This work studies the energy implications of
synchronization phenomena in a pair of structurally flexible coupled neurons
that interact through electrical coupling. We show that the forced
synchronization between two nonidentical neurons creates appropriate conditions
for an efficient actuation of adaptive laws able to make the neurons
structurally approach their behaviours in order to decrease the flow of energy
required to maintain the synchronization regime.



Successive elimination of candidates is often a route to making manipulation
intractable to compute. We prove that eliminating candidates does not
necessarily increase the computational complexity of manipulation. However, for
many voting rules used in practice, the computational complexity increases. For
example, it is already known that it is NP-hard to compute how a single voter
can manipulate the result of single transferable voting (the elimination
version of plurality voting). We show here that it is NP-hard to compute how a
single voter can manipulate the result of the elimination version of veto
voting, of the closely related Coombs' rule, and of the elimination versions of
a general class of scoring rules.



People sometimes worry about the Singularity [Vinge, 1993; Kurzweil, 2005],
or about the world being taken over by artificially intelligent robots. I
believe the risks of these are very small. However, few people recognize that
we already share our world with artificial creatures that participate as
intelligent agents in our society: corporations. Our planet is inhabited by two
distinct kinds of intelligent beings --- individual humans and corporate
entities --- whose natures and interests are intimately linked. To co-exist
well, we need to find ways to define the rights and responsibilities of both
individual humans and corporate entities, and to find ways to ensure that
corporate entities behave as responsible members of society.



A number of representation schemes have been presented for use within
Learning Classifier Systems, ranging from binary encodings to neural networks.
This paper presents results from an investigation into using a discrete
dynamical system representation within the XCS Learning Classifier System. In
particular, asynchronous random Boolean networks are used to represent the
traditional condition-action production system rules. It is shown possible to
use self-adaptive, open-ended evolution to design an ensemble of such discrete
dynamical systems within XCS to solve a number of well-known test problems.



A number of representation schemes have been presented for use within
Learning Classifier Systems, ranging from binary encodings to Neural Networks,
and more recently Dynamical Genetic Programming (DGP). This paper presents
results from an investigation into using a fuzzy DGP representation within the
XCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic
Networks are used to represent the traditional condition-action production
system rules. It is shown possible to use self-adaptive, open-ended evolution
to design an ensemble of such fuzzy dynamical systems within XCSF to solve
several well-known continuous-valued test problems.



Learning in Riemannian orbifolds is motivated by existing machine learning
algorithms that directly operate on finite combinatorial structures such as
point patterns, trees, and graphs. These methods, however, lack statistical
justification. This contribution derives consistency results for learning
problems in structured domains and thereby generalizes learning in vector
spaces and manifolds.



We identify the presence of typically quantum effects, namely 'superposition'
and 'interference', in what happens when human concepts are combined, and
provide a quantum model in complex Hilbert space that represents faithfully
experimental data measuring the situation of combining concepts. Our model
shows how 'interference of concepts' explains the effects of underextension and
overextension when two concepts combine to the disjunction of these two
concepts. This result supports our earlier hypothesis that human thought has a
superposed two-layered structure, one layer consisting of 'classical logical
thought' and a superposed layer consisting of 'quantum conceptual thought'.
Possible connections with recent findings of a 'grid-structure' for the brain
are analyzed, and influences on the mind/brain relation, and consequences on
applied disciplines, such as artificial intelligence and quantum computation,
are considered.



Objectives: Electronic health records (EHRs) are only a first step in
capturing and utilizing health-related data - the challenge is turning that
data into useful information. Furthermore, EHRs are increasingly likely to
include data relating to patient outcomes, functionality such as clinical
decision support, and genetic information as well, and, as such, can be seen as
repositories of increasingly valuable information about patients' health
conditions and responses to treatment over time. Methods: We describe a case
study of 423 patients treated by Centerstone within Tennessee and Indiana in
which we utilized electronic health record data to generate predictive
algorithms of individual patient treatment response. Multiple models were
constructed using predictor variables derived from clinical, financial and
geographic data. Results: For the 423 patients, 101 deteriorated, 223 improved
and in 99 there was no change in clinical condition. Based on modeling of
various clinical indicators at baseline, the highest accuracy in predicting
individual patient response ranged from 70-72% within the models tested. In
terms of individual predictors, the Centerstone Assessment of Recovery Level -
Adult (CARLA) baseline score was most significant in predicting outcome over
time (odds ratio 4.1 + 2.27). Other variables with consistently significant
impact on outcome included payer, diagnostic category, location and provision
of case management services. Conclusions: This approach represents a promising
avenue toward reducing the current gap between research and practice across
healthcare, developing data-driven clinical decision support based on
real-world populations, and serving as a component of embedded clinical
artificial intelligences that "learn" over time.



Many real world problems can be defined as optimisation problems in which the
aim is to maximise an objective function. The quality of obtained solution is
directly linked to the pertinence of the used objective function. However,
designing such function, which has to translate the user needs, is usually
fastidious. In this paper, a method to help user objective functions designing
is proposed. Our approach, which is highly interactive, is based on man machine
dialogue and more particularly on the comparison of problem instance solutions
by the user. We propose an experiment in the domain of cartographic
generalisation that shows promising results.



We study the inverse power index problem for weighted voting games: the
problem of finding a weighted voting game in which the power of the players is
as close as possible to a certain target distribution. Our goal is to find
algorithms that solve this problem exactly. Thereto, we study various
subclasses of simple games, and their associated representation methods. We
survey algorithms and impossibility results for the synthesis problem, i.e.,
converting a representation of a simple game into another representation.
  We contribute to the synthesis problem by showing that it is impossible to
compute in polynomial time the list of ceiling coalitions (also known as
shift-maximal losing coalitions) of a game from its list of roof coalitions
(also known as shift-minimal winning coalitions), and vice versa.
  Then, we proceed by studying the problem of enumerating the set of weighted
voting games. We present first a naive algorithm for this, running in doubly
exponential time. Using our knowledge of the synthesis problem, we then improve
on this naive algorithm, and we obtain an enumeration algorithm that runs in
quadratic exponential time (that is, O(2^(n^2) p(n)) for a polynomial p).
Moreover, we show that this algorithm runs in output-polynomial time, making it
the best possible enumeration algorithm up to a polynomial factor.
  Finally, we propose an exact anytime algorithm for the inverse power index
problem that runs in exponential time. This algorithm is straightforward and
general: it computes the error for each game enumerated, and outputs the game
that minimizes this error. By the genericity of our approach, our algorithm can
be used to find a weighted voting game that optimizes any exponential time
computable function. We implement our algorithm for the case of the normalized
Banzhaf index, and we perform experiments in order to study performance and
error convergence.



This paper deals with chain graphs under the alternative
Andersson-Madigan-Perlman (AMP) interpretation. In particular, we present a
constraint based algorithm for learning an AMP chain graph a given probability
distribution is faithful to. We also show that the extension of Meek's
conjecture to AMP chain graphs does not hold, which compromises the development
of efficient and correct score+search learning algorithms under assumptions
weaker than faithfulness.



We propose an analysis of the codified Law of France as a structured system.
Fifty two legal codes are selected on the basis of explicit legal criteria and
considered as vertices with their mutual quotations forming the edges in a
network which properties are analyzed relying on graph theory. We find that a
group of 10 codes are simultaneously the most citing and the most cited by
other codes, and are also strongly connected together so forming a "rich club"
sub-graph. Three other code communities are also found that somewhat partition
the legal field is distinct thematic sub-domains. The legal interpretation of
this partition is opening new untraditional lines of research. We also
conjecture that many legal systems are forming such new kind of networks that
share some properties in common with small worlds but are far denser. We
propose to call "concentrated world".



In this paper we propose a game-theoretic model to analyze events similar to
the 2009 \emph{DARPA Network Challenge}, which was organized by the Defense
Advanced Research Projects Agency (DARPA) for exploring the roles that the
Internet and social networks play in incentivizing wide-area collaborations.
The challenge was to form a group that would be the first to find the locations
of ten moored weather balloons across the United States. We consider a model in
which $N$ people (who can form groups) are located in some topology with a
fixed coverage volume around each person's geographical location. We consider
various topologies where the players can be located such as the Euclidean
$d$-dimension space and the vertices of a graph. A balloon is placed in the
space and a group wins if it is the first one to report the location of the
balloon. A larger team has a higher probability of finding the balloon, but we
assume that the prize money is divided equally among the team members. Hence
there is a competing tension to keep teams as small as possible.
  \emph{Risk aversion} is the reluctance of a person to accept a bargain with
an uncertain payoff rather than another bargain with a more certain, but
possibly lower, expected payoff. In our model we consider the \emph{isoelastic}
utility function derived from the Arrow-Pratt measure of relative risk
aversion. The main aim is to analyze the structures of the groups in Nash
equilibria for our model. For the $d$-dimensional Euclidean space ($d\geq 1$)
and the class of bounded degree regular graphs we show that in any Nash
Equilibrium the \emph{richest} group (having maximum expected utility per
person) covers a constant fraction of the total volume.



A fundamental question in systems biology is the construction and training to
data of mathematical models. Logic formalisms have become very popular to model
signaling networks because their simplicity allows us to model large systems
encompassing hundreds of proteins. An approach to train (Boolean) logic models
to high-throughput phospho-proteomics data was recently introduced and solved
using optimization heuristics based on stochastic methods. Here we demonstrate
how this problem can be solved using Answer Set Programming (ASP), a
declarative problem solving paradigm, in which a problem is encoded as a
logical program such that its answer sets represent solutions to the problem.
ASP has significant improvements over heuristic methods in terms of efficiency
and scalability, it guarantees global optimality of solutions as well as
provides a complete set of solutions. We illustrate the application of ASP with
in silico cases based on realistic networks and data.



Understanding human activities and object affordances are two very important
skills, especially for personal robots which operate in human environments. In
this work, we consider the problem of extracting a descriptive labeling of the
sequence of sub-activities being performed by a human, and more importantly, of
their interactions with the objects in the form of associated affordances.
Given a RGB-D video, we jointly model the human activities and object
affordances as a Markov random field where the nodes represent objects and
sub-activities, and the edges represent the relationships between object
affordances, their relations with sub-activities, and their evolution over
time. We formulate the learning problem using a structural support vector
machine (SSVM) approach, where labelings over various alternate temporal
segmentations are considered as latent variables. We tested our method on a
challenging dataset comprising 120 activity videos collected from 4 subjects,
and obtained an accuracy of 79.4% for affordance, 63.4% for sub-activity and
75.0% for high-level activity labeling. We then demonstrate the use of such
descriptive labeling in performing assistive tasks by a PR2 robot.



Existing Bayesian models, especially nonparametric Bayesian methods, rely on
specially conceived priors to incorporate domain knowledge for discovering
improved latent representations. While priors can affect posterior
distributions through Bayes' rule, imposing posterior regularization is
arguably more direct and in some cases more natural and general. In this paper,
we present regularized Bayesian inference (RegBayes), a novel computational
framework that performs posterior inference with a regularization term on the
desired post-data posterior distribution under an information theoretical
formulation. RegBayes is more flexible than the procedure that elicits expert
knowledge via priors, and it covers both directed Bayesian networks and
undirected Markov networks whose Bayesian formulation results in hybrid chain
graph models. When the regularization is induced from a linear operator on the
posterior distributions, such as the expectation operator, we present a general
convex-analysis theorem to characterize the solution of RegBayes. Furthermore,
we present two concrete examples of RegBayes, infinite latent support vector
machines (iLSVM) and multi-task infinite latent support vector machines
(MT-iLSVM), which explore the large-margin idea in combination with a
nonparametric Bayesian model for discovering predictive latent features for
classification and multi-task learning, respectively. We present efficient
inference methods and report empirical studies on several benchmark datasets,
which appear to demonstrate the merits inherited from both large-margin
learning and Bayesian nonparametrics. Such results were not available until
now, and contribute to push forward the interface between these two important
subfields, which have been largely treated as isolated in the community.



This paper evaluates heterogeneous information fusion using multi-task
Gaussian processes in the context of geological resource modeling.
Specifically, it empirically demonstrates that information integration across
heterogeneous information sources leads to superior estimates of all the
quantities being modeled, compared to modeling them individually. Multi-task
Gaussian processes provide a powerful approach for simultaneous modeling of
multiple quantities of interest while taking correlations between these
quantities into consideration. Experiments are performed on large scale real
sensor data.



Social media channels such as Twitter have emerged as popular platforms for
crowds to respond to public events such as speeches, sports and debates. While
this promises tremendous opportunities to understand and make sense of the
reception of an event from the social media, the promises come entwined with
significant technical challenges. In particular, given an event and an
associated large scale collection of tweets, we need approaches to effectively
align tweets and the parts of the event they refer to. This in turn raises
questions about how to segment the event into smaller yet meaningful parts, and
how to figure out whether a tweet is a general one about the entire event or
specific one aimed at a particular segment of the event. In this work, we
present ET-LDA, an effective method for aligning an event and its tweets
through joint statistical modeling of topical influences from the events and
their associated tweets. The model enables the automatic segmentation of the
events and the characterization of tweets into two categories: (1) episodic
tweets that respond specifically to the content in the segments of the events,
and (2) steady tweets that respond generally about the events. We present an
efficient inference method for this model, and a comprehensive evaluation of
its effectiveness over existing methods. In particular, through a user study,
we demonstrate that users find the topics, the segments, the alignment, and the
episodic tweets discovered by ET-LDA to be of higher quality and more
interesting as compared to the state-of-the-art, with improvements in the range
of 18-41%.



Android and Facebook provide third-party applications with access to users'
private data and the ability to perform potentially sensitive operations (e.g.,
post to a user's wall or place phone calls). As a security measure, these
platforms restrict applications' privileges with permission systems: users must
approve the permissions requested by applications before the applications can
make privacy- or security-relevant API calls. However, recent studies have
shown that users often do not understand permission requests and lack a notion
of typicality of requests. As a first step towards simplifying permission
systems, we cluster a corpus of 188,389 Android applications and 27,029
Facebook applications to find patterns in permission requests. Using a method
for Boolean matrix factorization for finding overlapping clusters, we find that
Facebook permission requests follow a clear structure that exhibits high
stability when fitted with only five clusters, whereas Android applications
demonstrate more complex permission requests. We also find that low-reputation
applications often deviate from the permission request patterns that we
identified for high-reputation applications suggesting that permission request
patterns are indicative for user satisfaction or application quality.



In this paper a general methodology is introduced for the determination of
potential prototype curves used for the drawing of prehistoric wall-paintings.
The approach includes a) preprocessing of the wall-paintings contours to
properly partition them, according to their curvature, b) choice of prototype
curves families, c) analysis and optimization in 4-manifold for a first
estimation of the form of these prototypes, d) clustering of the contour parts
and the prototypes, to determine a minimal number of potential guides, e)
further optimization in 4-manifold, applied to each cluster separately, in
order to determine the exact functional form of the potential guides, together
with the corresponding drawn contour parts. The introduced methodology
simultaneously deals with two problems: a) the arbitrariness in data-points
orientation and b) the determination of one proper form for a prototype curve
that optimally fits the corresponding contour data. Arbitrariness in
orientation has been dealt with a novel curvature based error, while the proper
forms of curve prototypes have been exhaustively determined by embedding
curvature deformations of the prototypes into 4-manifolds. Application of this
methodology to celebrated wall-paintings excavated at Tyrins, Greece and the
Greek island of Thera, manifests it is highly probable that these
wall-paintings had been drawn by means of geometric guides that correspond to
linear spirals and hyperbolae. These geometric forms fit the drawings' lines
with an exceptionally low average error, less than 0.39mm. Hence, the approach
suggests the existence of accurate realizations of complicated geometric
entities, more than 1000 years before their axiomatic formulation in Classical
Ages.



Rules complement and extend ontologies on the Semantic Web. We refer to these
rules as onto-relational since they combine DL-based ontology languages and
Knowledge Representation formalisms supporting the relational data model within
the tradition of Logic Programming and Deductive Databases. Rule authoring is a
very demanding Knowledge Engineering task which can be automated though
partially by applying Machine Learning algorithms. In this chapter we show how
Inductive Logic Programming (ILP), born at the intersection of Machine Learning
and Logic Programming and considered as a major approach to Relational
Learning, can be adapted to Onto-Relational Learning. For the sake of
illustration, we provide details of a specific Onto-Relational Learning
solution to the problem of learning rule-based definitions of DL concepts and
roles with ILP.



We present the new multi-threaded version of the state-of-the-art answer set
solver clasp. We detail its component and communication architecture and
illustrate how they support the principal functionalities of clasp. Also, we
provide some insights into the data representation used for different
constraint types handled by clasp. All this is accompanied by an extensive
experimental analysis of the major features related to multi-threading in
clasp.



This paper describes Artex, another algorithm for Automatic Text
Summarization. In order to rank sentences, a simple inner product is calculated
between each sentence, a document vector (text topic) and a lexical vector
(vocabulary used by a sentence). Summaries are then generated by assembling the
highest ranked sentences. No ruled-based linguistic post-processing is
necessary in order to obtain summaries. Tests over several datasets (coming
from Document Understanding Conferences (DUC), Text Analysis Conferences (TAC),
evaluation campaigns, etc.) in French, English and Spanish have shown that
summarizer achieves interesting results.



In social networks, information and influence diffuse among users as
cascades. While the importance of studying cascades has been recognized in
various applications, it is difficult to observe the complete structure of
cascades in practice. Moreover, much less is known on how to infer cascades
based on partial observations. In this paper we study the cascade inference
problem following the independent cascade model, and provide a full treatment
from complexity to algorithms: (a) We propose the idea of consistent trees as
the inferred structures for cascades; these trees connect source nodes and
observed nodes with paths satisfying the constraints from the observed temporal
information. (b) We introduce metrics to measure the likelihood of consistent
trees as inferred cascades, as well as several optimization problems for
finding them. (c) We show that the decision problems for consistent trees are
in general NP-complete, and that the optimization problems are hard to
approximate. (d) We provide approximation algorithms with performance
guarantees on the quality of the inferred cascades, as well as heuristics. We
experimentally verify the efficiency and effectiveness of our inference
algorithms, using real and synthetic data.



Financial statements contain quantitative information and manager's
subjective evaluation of firm's financial status. Using information released in
U.S. 10-K filings. Both qualitative and quantitative appraisals are crucial for
quality financial decisions. To extract such opinioned statements from the
reports, we built tagging models based on the conditional random field (CRF)
techniques, considering a variety of combinations of linguistic factors
including morphology, orthography, predicate-argument structure, syntax, and
simple semantics. Our results show that the CRF models are reasonably effective
to find opinion holders in experiments when we adopted the popular MPQA corpus
for training and testing. The contribution of our paper is to identify opinion
patterns in multiword expressions (MWEs) forms rather than in single word
forms.
  We find that the managers of corporations attempt to use more optimistic
words to obfuscate negative financial performance and to accentuate the
positive financial performance. Our results also show that decreasing earnings
were often accompanied by ambiguous and mild statements in the reporting year
and that increasing earnings were stated in assertive and positive way.



Generalized relational theories with null values in the sense of Reiter are
first-order theories that provide a semantics for relational databases with
incomplete information. In this paper we show that any such theory can be
turned into an equivalent logic program, so that models of the theory can be
generated using computational methods of answer set programming. As a step
towards this goal, we develop a general method for calculating stable models
under the domain closure assumption but without the unique name assumption.



In this work, we propose the kernel Pitman-Yor process (KPYP) for
nonparametric clustering of data with general spatial or temporal
interdependencies. The KPYP is constructed by first introducing an infinite
sequence of random locations. Then, based on the stick-breaking construction of
the Pitman-Yor process, we define a predictor-dependent random probability
measure by considering that the discount hyperparameters of the
Beta-distributed random weights (stick variables) of the process are not
uniform among the weights, but controlled by a kernel function expressing the
proximity between the location assigned to each weight and the given
predictors.



Much effort has been directed at algorithms for obtaining the highest
probability configuration in a probabilistic random field model known as the
maximum a posteriori (MAP) inference problem. In many situations, one could
benefit from having not just a single solution, but the top M most probable
solutions known as the M-Best MAP problem. In this paper, we propose an
efficient message-passing based algorithm for solving the M-Best MAP problem.
Specifically, our algorithm solves the recently proposed Linear Programming
(LP) formulation of M-Best MAP [7], while being orders of magnitude faster than
a generic LP-solver. Our approach relies on studying a particular partial
Lagrangian relaxation of the M-Best MAP LP which exposes a natural
combinatorial structure of the problem that we exploit.



Empty taxi cruising represents a wastage of resources in the context of urban
taxi services. In this work, we seek to minimize such wastage. An analysis of a
large trace of taxi operations reveals that the services' inefficiency is
caused by drivers' greedy cruising behavior. We model the existing system as a
continuous time Markov chain. To address the problem, we propose that each taxi
be equipped with an intelligent agent that will guide the driver when cruising
for passengers. Then, drawing from AI literature on multiagent planning, we
explore two possible ways to compute such guidance. The first formulation
assumes fully cooperative drivers. This allows us, in principle, to compute
systemwide optimal cruising policy. This is modeled as a Markov decision
process. The second formulation assumes rational drivers, seeking to maximize
their own profit. This is modeled as a stochastic congestion game, a
specialization of stochastic games. Nash equilibrium policy is proposed as the
solution to the game, where no driver has the incentive to singly deviate from
it. Empirical result shows that both formulations improve the efficiency of the
service significantly.



We consider a setting where an agent's uncertainty is represented by a set of
probability measures, rather than a single measure. Measure-bymeasure updating
of such a set of measures upon acquiring new information is well-known to
suffer from problems; agents are not always able to learn appropriately. To
deal with these problems, we propose using weighted sets of probabilities: a
representation where each measure is associated with a weight, which denotes
its significance. We describe a natural approach to updating in such a
situation and a natural approach to determining the weights. We then show how
this representation can be used in decision-making, by modifying a standard
approach to decision making-minimizing expected regret-to obtain minimax
weighted expected regret (MWER).We provide an axiomatization that characterizes
preferences induced by MWER both in the static and dynamic case.



Much of scientific data is collected as randomized experiments intervening on
some and observing other variables of interest. Quite often, a given phenomenon
is investigated in several studies, and different sets of variables are
involved in each study. In this article we consider the problem of integrating
such knowledge, inferring as much as possible concerning the underlying causal
structure with respect to the union of observed variables from such
experimental or passive observational overlapping data sets. We do not assume
acyclicity or joint causal sufficiency of the underlying data generating model,
but we do restrict the causal relationships to be linear and use only second
order statistics of the data. We derive conditions for full model
identifiability in the most generic case, and provide novel techniques for
incorporating an assumption of faithfulness to aid in inference. In each case
we seek to establish what is and what is not determined by the data at hand.



In typical real-time strategy (RTS) games, enemy units are visible only when
they are within sight range of a friendly unit. Knowledge of an opponent's
disposition is limited to what can be observed through scouting. Information is
costly, since units dedicated to scouting are unavailable for other purposes,
and the enemy will resist scouting attempts. It is important to infer as much
as possible about the opponent's current and future strategy from the available
observations. We present a dynamic Bayes net model of strategies in the RTS
game Starcraft that combines a generative model of how strategies relate to
observable quantities with a principled framework for incorporating evidence
gained via scouting. We demonstrate the model's ability to infer unobserved
aspects of the game from realistic observations.



A nonparametric approach for policy learning for POMDPs is proposed. The
approach represents distributions over the states, observations, and actions as
embeddings in feature spaces, which are reproducing kernel Hilbert spaces.
Distributions over states given the observations are obtained by applying the
kernel Bayes' rule to these distribution embeddings. Policies and value
functions are defined on the feature space over states, which leads to a
feature space expression for the Bellman equation. Value iteration may then be
used to estimate the optimal value function and associated policy. Experimental
results confirm that the correct policy is learned using the feature space
representation.



Learning a Bayesian network structure from data is an NP-hard problem and
thus exact algorithms are feasible only for small data sets. Therefore, network
structures for larger networks are usually learned with various heuristics.
Another approach to scaling up the structure learning is local learning. In
local learning, the modeler has one or more target variables that are of
special interest; he wants to learn the structure near the target variables and
is not interested in the rest of the variables. In this paper, we present a
score-based local learning algorithm called SLL. We conjecture that our
algorithm is theoretically sound in the sense that it is optimal in the limit
of large sample size. Empirical results suggest that SLL is competitive when
compared to the constraint-based HITON algorithm. We also study the prospects
of constructing the network structure for the whole node set based on local
results by presenting two algorithms and comparing them to several heuristics.



Agents learning to act autonomously in real-world domains must acquire a
model of the dynamics of the domain in which they operate. Learning domain
dynamics can be challenging, especially where an agent only has partial access
to the world state, and/or noisy external sensors. Even in standard STRIPS
domains, existing approaches cannot learn from noisy, incomplete observations
typical of real-world domains. We propose a method which learns STRIPS action
models in such domains, by decomposing the problem into first learning a
transition function between states in the form of a set of classifiers, and
then deriving explicit STRIPS rules from the classifiers' parameters. We
evaluate our approach on simulated standard planning domains from the
International Planning Competition, and show that it learns useful domain
descriptions from noisy, incomplete observations.



Markov networks (MNs) are a powerful way to compactly represent a joint
probability distribution, but most MN structure learning methods are very slow,
due to the high cost of evaluating candidates structures. Dependency networks
(DNs) represent a probability distribution as a set of conditional probability
distributions. DNs are very fast to learn, but the conditional distributions
may be inconsistent with each other and few inference algorithms support DNs.
In this paper, we present a closed-form method for converting a DN into an MN,
allowing us to enjoy both the efficiency of DN learning and the convenience of
the MN representation. When the DN is consistent, this conversion is exact. For
inconsistent DNs, we present averaging methods that significantly improve the
approximation. In experiments on 12 standard datasets, our methods are orders
of magnitude faster than and often more accurate than combining conditional
distributions using weight learning.



Stochastic domains often involve risk-averse decision makers. While recent
work has focused on how to model risk in Markov decision processes using risk
measures, it has not addressed the problem of solving large risk-averse
formulations. In this paper, we propose and analyze a new method for solving
large risk-averse MDPs with hybrid continuous-discrete state spaces and
continuous action spaces. The proposed method iteratively improves a bound on
the value function using a linearity structure of the MDP. We demonstrate the
utility and properties of the method on a portfolio optimization problem.



EDML is a recently proposed algorithm for learning MAP parameters in Bayesian
networks. In this paper, we present a number of new advances and insights on
the EDML algorithm. First, we provide the multivalued extension of EDML,
originally proposed for Bayesian networks over binary variables. Next, we
identify a simplified characterization of EDML that further implies a simple
fixed-point algorithm for the convex optimization problem that underlies it.
This characterization further reveals a connection between EDML and EM: a fixed
point of EDML is a fixed point of EM, and vice versa. We thus identify also a
new characterization of EM fixed points, but in the semantics of EDML. Finally,
we propose a hybrid EDML/EM algorithm that takes advantage of the improved
empirical convergence behavior of EDML, while maintaining the monotonic
improvement property of EM.



Recently two search algorithms, A* and breadth-first branch and bound
(BFBnB), were developed based on a simple admissible heuristic for learning
Bayesian network structures that optimize a scoring function. The heuristic
represents a relaxation of the learning problem such that each variable chooses
optimal parents independently. As a result, the heuristic may contain many
directed cycles and result in a loose bound. This paper introduces an improved
admissible heuristic that tries to avoid directed cycles within small groups of
variables. A sparse representation is also introduced to store only the unique
optimal parent choices. Empirical results show that the new techniques
significantly improved the efficiency and scalability of A* and BFBnB on most
of datasets tested in this paper.



We describe theoretical bounds and a practical algorithm for teaching a model
by demonstration in a sequential decision making environment. Unlike previous
efforts that have optimized learners that watch a teacher demonstrate a static
policy, we focus on the teacher as a decision maker who can dynamically choose
different policies to teach different parts of the environment. We develop
several teaching frameworks based on previously defined supervised protocols,
such as Teaching Dimension, extending them to handle noise and sequences of
inputs encountered in an MDP.We provide theoretical bounds on the learnability
of several important model classes in this setting and suggest a practical
algorithm for dynamic teaching.



Most state-of-the-art techniques for multi-class image segmentation and
labeling use conditional random fields defined over pixels or image regions.
While region-level models often feature dense pairwise connectivity,
pixel-level models are considerably larger and have only permitted sparse graph
structures. In this paper, we consider fully connected CRF models defined on
the complete set of pixels in an image. The resulting graphs have billions of
edges, making traditional inference algorithms impractical. Our main
contribution is a highly efficient approximate inference algorithm for fully
connected CRF models in which the pairwise edge potentials are defined by a
linear combination of Gaussian kernels. Our experiments demonstrate that dense
connectivity at the pixel level substantially improves segmentation and
labeling accuracy.



Our broader goal is to automatically translate English sentences into
formulas in appropriate knowledge representation languages as a step towards
understanding and thus answering questions with respect to English text. Our
focus in this paper is on the language of Answer Set Programming (ASP). Our
approach to translate sentences to ASP rules is inspired by Montague's use of
lambda calculus formulas as meaning of words and phrases. With ASP as the
target language the meaning of words and phrases are ASP-lambda formulas. In an
earlier work we illustrated our approach by manually developing a dictionary of
words and their ASP-lambda formulas. However such an approach is not scalable.
In this paper our focus is on two algorithms that allow one to construct
ASP-lambda formulas in an inverse manner. In particular the two algorithms take
as input two lambda-calculus expressions G and H and compute a lambda-calculus
expression F such that F with input as G, denoted by F@G, is equal to H; and
similarly G@F = H. We present correctness and complexity results about these
algorithms. To do that we develop the notion of typed ASP-lambda calculus
theories and their orders and use it in developing the completeness results.
(To appear in Theory and Practice of Logic Programming.)



Cooperative pathfinding is a multi-agent path planning problem where a group
of vehicles searches for a corresponding set of non-conflicting space-time
trajectories. Many of the practical methods for centralized solving of
cooperative pathfinding problems are based on the prioritized planning
strategy. However, in some domains (e.g., multi-robot teams of unmanned aerial
vehicles, autonomous underwater vehicles, or unmanned ground vehicles) a
decentralized approach may be more desirable than a centralized one due to
communication limitations imposed by the domain and/or privacy concerns.
  In this paper we present an asynchronous decentralized variant of prioritized
planning ADPP and its interruptible version IADPP. The algorithm exploits the
inherent parallelism of distributed systems and allows for a speed up of the
computation process. Unlike the synchronized planning approaches, the algorithm
allows an agent to react to updates about other agents' paths immediately and
invoke its local spatio-temporal path planner to find the best trajectory, as
response to the other agents' choices. We provide a proof of correctness of the
algorithms and experimentally evaluate them on synthetic domains.



Inference is an integral part of probabilistic topic models, but is often
non-trivial to derive an efficient algorithm for a specific model. It is even
much more challenging when we want to find a fast inference algorithm which
always yields sparse latent representations of documents. In this article, we
introduce a simple framework for inference in probabilistic topic models,
denoted by FW. This framework is general and flexible enough to be easily
adapted to mixture models. It has a linear convergence rate, offers an easy way
to incorporate prior knowledge, and provides us an easy way to directly trade
off sparsity against quality and time. We demonstrate the goodness and
flexibility of FW over existing inference methods by a number of tasks.
Finally, we show how inference in topic models with nonconjugate priors can be
done efficiently.



Concept map is a graphical tool for representing knowledge. They have been
used in many different areas, including education, knowledge management,
business and intelligence. Constructing of concept maps manually can be a
complex task; an unskilled person may encounter difficulties in determining and
positioning concepts relevant to the problem area. An application that
recommends concept candidates and their position in a concept map can
significantly help the user in that situation. This paper gives an overview of
different approaches to automatic and semi-automatic creation of concept maps
from textual and non-textual sources. The concept map mining process is
defined, and one method suitable for the creation of concept maps from
unstructured textual sources in highly inflected languages such as the Croatian
language is described in detail. Proposed method uses statistical and data
mining techniques enriched with linguistic tools. With minor adjustments, that
method can also be used for concept map mining from textual sources in other
morphologically rich languages.



Heuristic approaches often do so well that they seem to pretty much always
give the right answer. How close can heuristic algorithms get to always giving
the right answer, without inducing seismic complexity-theoretic consequences?
This article first discusses how a series of results by Berman, Buhrman,
Hartmanis, Homer, Longpr\'{e}, Ogiwara, Sch\"{o}ening, and Watanabe, from the
early 1970s through the early 1990s, explicitly or implicitly limited how well
heuristic algorithms can do on NP-hard problems. In particular, many desirable
levels of heuristic success cannot be obtained unless severe, highly unlikely
complexity class collapses occur. Second, we survey work initiated by Goldreich
and Wigderson, who showed how under plausible assumptions deterministic
heuristics for randomized computation can achieve a very high frequency of
correctness. Finally, we consider formal ways in which theory can help explain
the effectiveness of heuristics that solve NP-hard problems in practice.



Dynamical systems driven by strong external signals are ubiquituous in nature
and engineering. Here we study "echo state networks", networks of a large
number of randomly connected nodes, which represent a simple model of a neural
network, and have important applications in machine learning. We develop a mean
field theory of echo state networks. The dynamics of the network is captured by
the evolution law, similar to a logistic map, for a single collective variable.
When the network is driven by many independent external signals, this
collective variable reaches a steady state. But when the network is driven by a
single external signal, the collective variable is nonstationnary but can be
characterised by its time averaged distribution. The predictions of the mean
field theory, including the value of the largest Lyaponuov exponent, are
compared with the numerical integration of the equations of motion.



Much work has been done refining and characterizing the receptive fields
learned by deep learning algorithms. A lot of this work has focused on the
development of Gabor-like filters learned when enforcing sparsity constraints
on a natural image dataset. Little work however has investigated how these
filters might expand to the temporal domain, namely through training on natural
movies. Here we investigate exactly this problem in established temporal deep
learning algorithms as well as a new learning paradigm suggested here, the
Temporal Autoencoding Restricted Boltzmann Machine (TARBM).



One conjecture in both deep learning and classical connectionist viewpoint is
that the biological brain implements certain kinds of deep networks as its
back-end. However, to our knowledge, a detailed correspondence has not yet been
set up, which is important if we want to bridge between neuroscience and
machine learning. Recent researches emphasized the biological plausibility of
Linear-Nonlinear-Poisson (LNP) neuron model. We show that with neurally
plausible settings, the whole network is capable of representing any Boltzmann
machine and performing a semi-stochastic Bayesian inference algorithm lying
between Gibbs sampling and variational inference.



The systematic biases seen in people's probability judgments are typically
taken as evidence that people do not reason about probability using the rules
of probability theory, but instead use heuristics which sometimes yield
reasonable judgments and sometimes systematic biases. This view has had a major
impact in economics, law, medicine, and other fields; indeed, the idea that
people cannot reason with probabilities has become a widespread truism. We
present a simple alternative to this view, where people reason about
probability according to probability theory but are subject to random variation
or noise in the reasoning process. In this account the effect of noise is
cancelled for some probabilistic expressions: analysing data from two
experiments we find that, for these expressions, people's probability judgments
are strikingly close to those required by probability theory. For other
expressions this account produces systematic deviations in probability
estimates. These deviations explain four reliable biases in human probabilistic
reasoning (conservatism, subadditivity, conjunction and disjunction fallacies).
These results suggest that people's probability judgments embody the rules of
probability theory, and that biases in those judgments are due to the effects
of random noise.



Perhaps surprisingly, it is possible to predict how long an algorithm will
take to run on a previously unseen input, using machine learning techniques to
build a model of the algorithm's runtime as a function of problem-specific
instance features. Such models have important applications to algorithm
analysis, portfolio-based algorithm selection, and the automatic configuration
of parameterized algorithms. Over the past decade, a wide variety of techniques
have been studied for building such models. Here, we describe extensions and
improvements of existing models, new families of models, and -- perhaps most
importantly -- a much more thorough treatment of algorithm parameters as model
inputs. We also comprehensively describe new and existing features for
predicting algorithm runtime for propositional satisfiability (SAT), travelling
salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate
these innovations through the largest empirical analysis of its kind, comparing
to a wide range of runtime modelling techniques from the literature. Our
experiments consider 11 algorithms and 35 instance distributions; they also
span a very wide range of SAT, MIP, and TSP instances, with the least
structured having been generated uniformly at random and the most structured
having emerged from real industrial applications. Overall, we demonstrate that
our new models yield substantially better runtime predictions than previous
approaches in terms of their generalization to new problem instances, to new
algorithms from a parameterized space, and to both simultaneously.



The paper addresses the modular design of composite solving strategies for
multicriteria ranking (sorting). Here a 'scale of creativity' that is close to
creative levels proposed by Altshuller is used as the reference viewpoint: (i)
a basic object, (ii) a selected object, (iii) a modified object, and (iv) a
designed object (e.g., composition of object components). These levels maybe
used in various parts of decision support systems (DSS) (e.g., information,
operations, user). The paper focuses on the more creative above-mentioned level
(i.e., composition or combinatorial synthesis) for the operational part (i.e.,
composite solving strategy). This is important for a search/exploration mode of
decision making process with usage of various procedures and techniques and
analysis/integration of obtained results. The paper describes methodological
issues of decision technology and synthesis of composite strategy for
multicriteria ranking. The synthesis of composite strategies is based on
'hierarchical morphological multicriteria design' (HMMD) which is based on
selection and combination of design alternatives (DAs) (here: local procedures
or techniques) while taking into account their quality and quality of their
interconnections (IC). A new version of HMMD with interval multiset estimates
for DAs is used. The operational environment of DSS COMBI for multicriteria
ranking, consisting of a morphology of local procedures or techniques (as
design alternatives DAs), is examined as a basic one.



During broadcast events such as the Superbowl, the U.S. Presidential and
Primary debates, etc., Twitter has become the de facto platform for crowds to
share perspectives and commentaries about them. Given an event and an
associated large-scale collection of tweets, there are two fundamental research
problems that have been receiving increasing attention in recent years. One is
to extract the topics covered by the event and the tweets; the other is to
segment the event. So far these problems have been viewed separately and
studied in isolation. In this work, we argue that these problems are in fact
inter-dependent and should be addressed together. We develop a joint Bayesian
model that performs topic modeling and event segmentation in one unified
framework. We evaluate the proposed model both quantitatively and qualitatively
on two large-scale tweet datasets associated with two events from different
domains to show that it improves significantly over baseline models.



Disease Intelligence (DI) is based on the acquisition and aggregation of
fragmented knowledge of diseases at multiple sources all over the world to
provide valuable information to doctors, researchers and information seeking
community. Some diseases have their own characteristics changed rapidly at
different places of the world and are reported on documents as unrelated and
heterogeneous information which may be going unnoticed and may not be quickly
available. This research presents an Ontology based theoretical framework in
the context of medical intelligence and country/region. Ontology is designed
for storing information about rapidly spreading and changing diseases with
incorporating existing disease taxonomies to genetic information of both humans
and infectious organisms. It further maps disease symptoms to diseases and drug
effects to disease symptoms. The machine understandable disease ontology
represented as a website thus allows the drug effects to be evaluated on
disease symptoms and exposes genetic involvements in the human diseases.
Infectious agents which have no known place in an existing classification but
have data on genetics would still be identified as organisms through the
intelligence of this system. It will further facilitate researchers on the
subject to try out different solutions for curing diseases.



Information-Geometric Optimization (IGO) is a unified framework of stochastic
algorithms for optimization problems. Given a family of probability
distributions, IGO turns the original optimization problem into a new
maximization problem on the parameter space of the probability distributions.
IGO updates the parameter of the probability distribution along the natural
gradient, taken with respect to the Fisher metric on the parameter manifold,
aiming at maximizing an adaptive transform of the objective function. IGO
recovers several known algorithms as particular instances: for the family of
Bernoulli distributions IGO recovers PBIL, for the family of Gaussian
distributions the pure rank-mu CMA-ES update is recovered, and for exponential
families in expectation parametrization the cross-entropy/ML method is
recovered. This article provides a theoretical justification for the IGO
framework, by proving that any step size not greater than 1 guarantees monotone
improvement over the course of optimization, in terms of q-quantile values of
the objective function f. The range of admissible step sizes is independent of
f and its domain. We extend the result to cover the case of different step
sizes for blocks of the parameters in the IGO algorithm. Moreover, we prove
that expected fitness improves over time when fitness-proportional selection is
applied, in which case the RPP algorithm is recovered.



The RoboCup 2D Simulation League incorporates several challenging features,
setting a benchmark for Artificial Intelligence (AI). In this paper we describe
some of the ideas and tools around the development of our team, Gliders2012. In
our description, we focus on the evaluation function as one of our central
mechanisms for action selection. We also point to a new framework for watching
log files in a web browser that we release for use and further development by
the RoboCup community. Finally, we also summarize results of the group and
final matches we played during RoboCup 2012, with Gliders2012 finishing 4th out
of 19 teams.



This paper introduces TwitterPaul, a system designed to make use of Social
Media data to help to predict game outcomes for the 2010 FIFA World Cup
tournament. To this end, we extracted over 538K mentions to football games from
a large sample of tweets that occurred during the World Cup, and we classified
into different types with a precision of up to 88%. The different mentions were
aggregated in order to make predictions about the outcomes of the actual games.
We attempt to learn which Twitter users are accurate predictors and explore
several techniques in order to exploit this information to make more accurate
predictions. We compare our results to strong baselines and against the betting
line (prediction market) and found that the quality of extractions is more
important than the quantity, suggesting that high precision methods working on
a medium-sized dataset are preferable over low precision methods that use a
larger amount of data. Finally, by aggregating some classes of predictions, the
system performance is close to the one of the betting line. Furthermore, we
believe that this domain independent framework can help to predict other
sports, elections, product release dates and other future events that people
talk about in social media.



Recently, much of the existing work in manifold learning has been done under
the assumption that the data is sampled from a manifold without boundaries and
singularities or that the functions of interest are evaluated away from such
points. At the same time, it can be argued that singularities and boundaries
are an important aspect of the geometry of realistic data.
  In this paper we consider the behavior of graph Laplacians at points at or
near boundaries and two main types of other singularities: intersections, where
different manifolds come together and sharp "edges", where a manifold sharply
changes direction. We show that the behavior of graph Laplacian near these
singularities is quite different from that in the interior of the manifolds. In
fact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis of
Fourier series, can be observed in the behavior of graph Laplacian near such
points. Unlike in the interior of the domain, where graph Laplacian converges
to the Laplace-Beltrami operator, near singularities graph Laplacian tends to a
first-order differential operator, which exhibits different scaling behavior as
a function of the kernel width. One important implication is that while points
near the singularities occupy only a small part of the total volume, the
difference in scaling results in a disproportionately large contribution to the
total behavior. Another significant finding is that while the scaling behavior
of the operator is the same near different types of singularities, they are
very distinct at a more refined level of analysis.
  We believe that a comprehensive understanding of these structures in addition
to the standard case of a smooth manifold can take us a long way toward better
methods for analysis of complex non-linear data and can lead to significant
progress in algorithm design.



The considerable mathematical knowledge encoded by the Flyspeck project is
combined with external automated theorem provers (ATPs) and machine-learning
premise selection methods trained on the proofs, producing an AI system capable
of answering a wide range of mathematical queries automatically. The
performance of this architecture is evaluated in a bootstrapping scenario
emulating the development of Flyspeck from axioms to the last theorem, each
time using only the previous theorems and proofs. It is shown that 39% of the
14185 theorems could be proved in a push-button mode (without any high-level
advice and user interaction) in 30 seconds of real time on a fourteen-CPU
workstation. The necessary work involves: (i) an implementation of sound
translations of the HOL Light logic to ATP formalisms: untyped first-order,
polymorphic typed first-order, and typed higher-order, (ii) export of the
dependency information from HOL Light and ATP proofs for the machine learners,
and (iii) choice of suitable representations and methods for learning from
previous proofs, and their integration as advisors with HOL Light. This work is
described and discussed here, and an initial analysis of the body of proofs
that were found fully automatically is provided.



In game theory, an Evolutionarily Stable Set (ES set) is a set of Nash
Equilibrium (NE) strategies that give the same payoffs. Similar to an
Evolutionarily Stable Strategy (ES strategy), an ES set is also a strict NE.
This work investigates the evolutionary stability of classical and quantum
strategies in the quantum penny flip games. In particular, we developed an
evolutionary game theory model to conduct a series of simulations where a
population of mixed classical strategies from the ES set of the game were
invaded by quantum strategies. We found that when only one of the two players'
mixed classical strategies were invaded, the results were different. In one
case, due to the interference phenomenon of superposition, quantum strategies
provided more payoff, hence successfully replaced the mixed classical
strategies in the ES set. In the other case, the mixed classical strategies
were able to sustain the invasion of quantum strategies and remained in the ES
set. Moreover, when both players' mixed classical strategies were invaded by
quantum strategies, a new quantum ES set emerged. The strategies in the quantum
ES set give both players payoff 0, which is the same as the payoff of the
strategies in the mixed classical ES set of this game.



In this work we show that randomized (block) coordinate descent methods can
be accelerated by parallelization when applied to the problem of minimizing the
sum of a partially separable smooth convex function and a simple separable
convex function. The theoretical speedup, as compared to the serial method, and
referring to the number of iterations needed to approximately solve the problem
with high probability, is a simple expression depending on the number of
parallel processors and a natural and easily computable measure of separability
of the smooth component of the objective function. In the worst case, when no
degree of separability is present, there may be no speedup; in the best case,
when the problem is separable, the speedup is equal to the number of
processors. Our analysis also works in the mode when the number of blocks being
updated at each iteration is random, which allows for modeling situations with
busy or unreliable processors. We show that our algorithm is able to solve a
LASSO problem involving a matrix with 20 billion nonzeros in 2 hours on a large
memory node with 24 cores.



Instead of requiring a domain expert to specify the probabilistic
dependencies of the data, in this work we present an approach that uses the
relational DB schema to automatically construct a Bayesian graphical model for
a database. This resulting model contains customized distributions for columns,
latent variables that cluster the data, and factors that reflect and represent
the foreign key links. Experiments demonstrate the accuracy of the model and
the scalability of inference on synthetic and real-world data.



The accuracy of machine learning systems is a widely studied research topic.
Established techniques such as cross-validation predict the accuracy on unseen
data of the classifier produced by applying a given learning method to a given
training data set. However, they do not predict whether incurring the cost of
obtaining more data and undergoing further training will lead to higher
accuracy. In this paper we investigate techniques for making such early
predictions. We note that when a machine learning algorithm is presented with a
training set the classifier produced, and hence its error, will depend on the
characteristics of the algorithm, on training set's size, and also on its
specific composition. In particular we hypothesise that if a number of
classifiers are produced, and their observed error is decomposed into bias and
variance terms, then although these components may behave differently, their
behaviour may be predictable.
  We test our hypothesis by building models that, given a measurement taken
from the classifier created from a limited number of samples, predict the
values that would be measured from the classifier produced when the full data
set is presented. We create separate models for bias, variance and total error.
Our models are built from the results of applying ten different machine
learning algorithms to a range of data sets, and tested with "unseen"
algorithms and datasets. We analyse the results for various numbers of initial
training samples, and total dataset sizes. Results show that our predictions
are very highly correlated with the values observed after undertaking the extra
training. Finally we consider the more complex case where an ensemble of
heterogeneous classifiers is trained, and show how we can accurately estimate
an upper bound on the accuracy achievable after further training.



AdaBoost is one of the most popular machine-learning algorithms. It is simple
to implement and often found very effective by practitioners, while still being
mathematically elegant and theoretically sound. AdaBoost's behavior in
practice, and in particular the test-error behavior, has puzzled many eminent
researchers for over a decade: It seems to defy our general intuition in
machine learning regarding the fundamental trade-off between model complexity
and generalization performance. In this paper, we establish the convergence of
"Optimal AdaBoost," a term coined by Rudin, Daubechies, and Schapire in 2004.
We prove the convergence, with the number of rounds, of the classifier itself,
its generalization error, and its resulting margins for fixed data sets, under
certain reasonable conditions. More generally, we prove that the time/per-round
average of almost any function of the example weights converges. Our approach
is to frame AdaBoost as a dynamical system, to provide sufficient conditions
for the existence of an invariant measure, and to employ tools from ergodic
theory. Unlike previous work, we do not assume AdaBoost cycles; actually, we
present empirical evidence against it on real-world datasets. Our main
theoretical results hold under a weaker condition. We show sufficient empirical
evidence that Optimal AdaBoost always met the condition on every real-world
dataset we tried. Our results formally ground future convergence-rate analyses,
and may even provide opportunities for slight algorithmic modifications to
optimize the generalization ability of AdaBoost classifiers, thus reducing a
practitioner's burden of deciding how long to run the algorithm.



Many problems in sequential decision making and stochastic control often have
natural multiscale structure: sub-tasks are assembled together to accomplish
complex goals. Systematically inferring and leveraging hierarchical structure,
particularly beyond a single level of abstraction, has remained a longstanding
challenge. We describe a fast multiscale procedure for repeatedly compressing,
or homogenizing, Markov decision processes (MDPs), wherein a hierarchy of
sub-problems at different scales is automatically determined. Coarsened MDPs
are themselves independent, deterministic MDPs, and may be solved using
existing algorithms. The multiscale representation delivered by this procedure
decouples sub-tasks from each other and can lead to substantial improvements in
convergence rates both locally within sub-problems and globally across
sub-problems, yielding significant computational savings. A second fundamental
aspect of this work is that these multiscale decompositions yield new transfer
opportunities across different problems, where solutions of sub-tasks at
different levels of the hierarchy may be amenable to transfer to new problems.
Localized transfer of policies and potential operators at arbitrary scales is
emphasized. Finally, we demonstrate compression and transfer in a collection of
illustrative domains, including examples involving discrete and continuous
statespaces.



The paper addresses design/building frameworks for some kinds of tree-like
and hierarchical structures of systems. The following approaches are examined:
(1) expert-based procedures, (2) hierarchical clustering; (3) spanning problems
(e.g., minimum spanning tree, minimum Steiner tree, maximum leaf spanning tree
problem; (4) design of organizational 'optimal' hierarchies; (5) design of
multi-layer (e.g., three-layer) k-connected network; (6) modification of
hierarchies or networks: (i) modification of tree via condensing of neighbor
nodes, (ii) hotlink assignment, (iii) transformation of tree into Steiner tree,
(iv) restructuring as modification of an initial structural solution into a
solution that is the most close to a goal solution while taking into account a
cost of the modification. Combinatorial optimization problems are considered as
basic ones (e.g., classification, knapsack problem, multiple choice problem,
assignment problem). Some numerical examples illustrate the suggested problems
and solving frameworks.



The monotone duality problem is defined as follows: Given two monotone
formulas f and g in iredundant DNF, decide whether f and g are dual. This
problem is the same as duality testing for hypergraphs, that is, checking
whether a hypergraph H consists of precisely all minimal transversals of a
simple hypergraph G. By exploiting a recent problem-decomposition method by
Boros and Makino (ICALP 2009), we show that duality testing for hypergraphs,
and thus for monotone DNFs, is feasible in DSPACE[log^2 n], i.e., in quadratic
logspace. As the monotone duality problem is equivalent to a number of problems
in the areas of databases, data mining, and knowledge discovery, the results
presented here yield new complexity results for those problems, too. For
example, it follows from our results that whenever for a Boolean-valued
relation (whose attributes represent items), a number of maximal frequent
itemsets and a number of minimal infrequent itemsets are known, then it can be
decided in quadratic logspace whether there exist additional frequent or
infrequent itemsets.



We study and solve some variations of the random K-satisfiability problem -
balanced K-SAT and biased random K-SAT - on a regular tree, using techniques we
have developed earlier(arXiv:1110.2065). In both these problems, as well as
variations of these that we have looked at, we find that the SAT-UNSAT
transition obtained on the Bethe lattice matches the exact threshold for the
same model on a random graph for K=2 and is very close to the numerical value
obtained for K=3. For higher K it deviates from the numerical estimates of the
solvability threshold on random graphs, but is very close to the dynamical
1-RSB threshold as obtained from the first non-trivial fixed point of the
survey propagation algorithm.



Online portfolio selection is a fundamental problem in computational finance,
which has been extensively studied across several research communities,
including finance, statistics, artificial intelligence, machine learning, and
data mining, etc. This article aims to provide a comprehensive survey and a
structural understanding of published online portfolio selection techniques.
From an online machine learning perspective, we first formulate online
portfolio selection as a sequential decision problem, and then survey a variety
of state-of-the-art approaches, which are grouped into several major
categories, including benchmarks, "Follow-the-Winner" approaches,
"Follow-the-Loser" approaches, "Pattern-Matching" based approaches, and
"Meta-Learning Algorithms". In addition to the problem formulation and related
algorithms, we also discuss the relationship of these algorithms with the
Capital Growth theory in order to better understand the similarities and
differences of their underlying trading ideas. This article aims to provide a
timely and comprehensive survey for both machine learning and data mining
researchers in academia and quantitative portfolio managers in the financial
industry to help them understand the state-of-the-art and facilitate their
research and practical applications. We also discuss some open issues and
evaluate some emerging new trends for future research directions.



Constraint-based (CB) learning is a formalism for learning a causal network
with a database D by performing a series of conditional-independence tests to
infer structural information. This paper considers a new test of independence
that combines ideas from Bayesian learning, Bayesian network inference, and
classical hypothesis testing to produce a more reliable and robust test. The
new test can be calculated in the same asymptotic time and space required for
the standard tests such as the chi-squared test, but it allows the
specification of a prior distribution over parameters and can be used when the
database is incomplete. We prove that the test is correct, and we demonstrate
empirically that, when used with a CB causal discovery algorithm with
noninformative priors, it recovers structural features more reliably and it
produces networks with smaller KL-Divergence, especially as the number of nodes
increases or the number of records decreases. Another benefit is the dramatic
reduction in the probability that a CB algorithm will stall during the search,
providing a remedy for an annoying problem plaguing CB learning when the
database is small.



In this paper, we provide new complexity results for algorithms that learn
discrete-variable Bayesian networks from data. Our results apply whenever the
learning algorithm uses a scoring criterion that favors the simplest model able
to represent the generative distribution exactly. Our results therefore hold
whenever the learning algorithm uses a consistent scoring criterion and is
applied to a sufficiently large dataset. We show that identifying high-scoring
structures is hard, even when we are given an independence oracle, an inference
oracle, and/or an information oracle. Our negative results also apply to the
learning of discrete-variable Bayesian networks in which each node has at most
k parents, for all k > 3.



Bayesian network classifiers are used in many fields, and one common class of
classifiers are naive Bayes classifiers. In this paper, we introduce an
approach for reasoning about Bayesian network classifiers in which we
explicitly convert them into Ordered Decision Diagrams (ODDs), which are then
used to reason about the properties of these classifiers. Specifically, we
present an algorithm for converting any naive Bayes classifier into an ODD, and
we show theoretically and experimentally that this algorithm can give us an ODD
that is tractable in size even given an intractable number of instances. Since
ODDs are tractable representations of classifiers, our algorithm allows us to
efficiently test the equivalence of two naive Bayes classifiers and
characterize discrepancies between them. We also show a number of additional
results including a count of distinct classifiers that can be induced by
changing some CPT in a naive Bayes classifier, and the range of allowable
changes to a CPT which keeps the current classifier unchanged.



In 1950, Forsythe and Leibler (1950) introduced a statistical technique for
finding the inverse of a matrix by characterizing the elements of the matrix
inverse as expected values of a sequence of random walks. Barto and Duff (1994)
subsequently showed relations between this technique and standard dynamic
programming and temporal differencing methods. The advantage of the Monte Carlo
matrix inversion (MCMI) approach is that it scales better with respect to
state-space size than alternative techniques. In this paper, we introduce an
algorithm for performing reinforcement learning policy evaluation using MCMI.
We demonstrate that MCMI improves on runtime over a maximum likelihood
model-based policy evaluation approach and on both runtime and accuracy over
the temporal differencing (TD) policy evaluation approach. We further improve
on MCMI policy evaluation by adding an importance sampling technique to our
algorithm to reduce the variance of our estimator. Lastly, we illustrate
techniques for scaling up MCMI to large state spaces in order to perform policy
improvement.



We analyze a new property of directed acyclic graphs (DAGs), called
layerwidth, arising from a class of DAGs proposed by Eiter and Lukasiewicz.
This class of DAGs permits certain problems of structural model-based causality
and explanation to be tractably solved. In this paper, we first address an open
question raised by Eiter and Lukasiewicz - the computational complexity of
deciding whether a given graph has a bounded layerwidth. After proving that
this problem is NP-complete, we proceed by proving numerous important
properties of layerwidth that are helpful in efficiently computing the optimal
layerwidth. Finally, we compare this new DAG property to two other important
DAG properties: treewidth and bandwidth.



Loopy and generalized belief propagation are popular algorithms for
approximate inference in Markov random fields and Bayesian networks. Fixed
points of these algorithms correspond to extrema of the Bethe and Kikuchi free
energy. However, belief propagation does not always converge, which explains
the need for approaches that explicitly minimize the Kikuchi/Bethe free energy,
such as CCCP and UPS. Here we describe a class of algorithms that solves this
typically nonconvex constrained minimization of the Kikuchi free energy through
a sequence of convex constrained minimizations of upper bounds on the Kikuchi
free energy. Intuitively one would expect tighter bounds to lead to faster
algorithms, which is indeed convincingly demonstrated in our simulations.
Several ideas are applied to obtain tight convex bounds that yield dramatic
speed-ups over CCCP.



This paper presents a scalable control algorithm that enables a deployed
mobile robot system to make high-level decisions under full consideration of
its probabilistic belief. Our approach is based on insights from the rich
literature of hierarchical controllers and hierarchical MDPs. The resulting
controller has been successfully deployed in a nursing facility near
Pittsburgh, PA. To the best of our knowledge, this work is a unique instance of
applying POMDPs to high-level robotic control problems.



This paper proposes and evaluates the k-greedy equivalence search algorithm
(KES) for learning Bayesian networks (BNs) from complete data. The main
characteristic of KES is that it allows a trade-off between greediness and
randomness, thus exploring different good local optima. When greediness is set
at maximum, KES corresponds to the greedy equivalence search algorithm (GES).
When greediness is kept at minimum, we prove that under mild assumptions KES
asymptotically returns any inclusion optimal BN with nonzero probability.
Experimental results for both synthetic and real data are reported showing that
KES often finds a better local optima than GES. Moreover, we use KES to
experimentally confirm that the number of different local optima is often huge.



Bio-Inspired Algorithms on Road Traffic Congestion and safety is a very
promising research problem. Searching for an efficient optimization method to
increase the degree of speed optimization and thereby increasing the traffic
Flow in an unplanned zone is a widely concerning issue. However, there has been
a limited research effort on the optimization of the lane usage with speed
optimization. The main objective of this article is to find avenues or
techniques in a novel way to solve the problem optimally using the knowledge
from analysis of speeds of vehicles, which, in turn will act as a guide for
design of lanes optimally to provide better optimized traffic. The accident
factors adjust the base model estimates for individual geometric design element
dimensions and for traffic control features. The application of these
algorithms in partially modified form in accordance of this novel Speed
Optimization Technique in an Unplanned Traffic analysis technique is applied to
the proposed design and speed optimization plan. The experimental results based
on real life data are quite encouraging.



We introduce Dimple, a fully open-source API for probabilistic modeling.
Dimple allows the user to specify probabilistic models in the form of graphical
models, Bayesian networks, or factor graphs, and performs inference (by
automatically deriving an inference engine from a variety of algorithms) on the
model. Dimple also serves as a compiler for GP5, a hardware accelerator for
inference.



We present ML4PG - a machine learning extension for Proof General. It allows
users to gather proof statistics related to shapes of goals, sequences of
applied tactics, and proof tree structures from the libraries of interactive
higher-order proofs written in Coq and SSReflect. The gathered data is
clustered using the state-of-the-art machine learning algorithms available in
MATLAB and Weka. ML4PG provides automated interfacing between Proof General and
MATLAB/Weka. The results of clustering are used by ML4PG to provide proof hints
in the process of interactive proof development.



The problem of replicating the flexibility of human common-sense reasoning
has captured the imagination of computer scientists since the early days of
Alan Turing's foundational work on computation and the philosophy of artificial
intelligence. In the intervening years, the idea of cognition as computation
has emerged as a fundamental tenet of Artificial Intelligence (AI) and
cognitive science. But what kind of computation is cognition?
  We describe a computational formalism centered around a probabilistic Turing
machine called QUERY, which captures the operation of probabilistic
conditioning via conditional simulation. Through several examples and analyses,
we demonstrate how the QUERY abstraction can be used to cast common-sense
reasoning as probabilistic inference in a statistical model of our observations
and the uncertain structure of the world that generated that experience. This
formulation is a recent synthesis of several research programs in AI and
cognitive science, but it also represents a surprising convergence of several
of Turing's pioneering insights in AI, the foundations of computation, and
statistics.



For robots to be accommodated in human spaces and in humans daily activities,
robots should be able to understand messages from the human conversation
partner. In the same light, humans must also understand the messages that are
being communicated by robots, including the non-verbal ones. We conducted a
web-based video study wherein participants gave interpretations on the iconic
gestures and emblems that were produced by an anthropomorphic robot. Out of the
15 gestures presented, we found 6 robotic gestures that can be accurately
recognized by the human observer. These were nodding, clapping, hugging,
expressing anger, walking, and flying. We reviewed these gestures for their
meaning from literatures in human and animal behavior. We conclude by
discussing the possible implications of these gestures for the design of social
robots that are aimed to have engaging interactions with humans.



In the last decade, a new computational paradigm was introduced in the field
of Machine Learning, under the name of Reservoir Computing (RC). RC models are
neural networks which a recurrent part (the reservoir) that does not
participate in the learning process, and the rest of the system where no
recurrence (no neural circuit) occurs. This approach has grown rapidly due to
its success in solving learning tasks and other computational applications.
Some success was also observed with another recently proposed neural network
designed using Queueing Theory, the Random Neural Network (RandNN). Both
approaches have good properties and identified drawbacks. In this paper, we
propose a new RC model called Echo State Queueing Network (ESQN), where we use
ideas coming from RandNNs for the design of the reservoir. ESQNs consist in
ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The
paper positions ESQNs in the global Machine Learning area, and provides
examples of their use and performances. We show on largely used benchmarks that
ESQNs are very accurate tools, and we illustrate how they compare with standard
ESNs.



We present a method to stop the evaluation of a prediction process when the
result of the full evaluation is obvious. This trait is highly desirable in
prediction tasks where a predictor evaluates all its features for every example
in large datasets. We observe that some examples are easier to classify than
others, a phenomenon which is characterized by the event when most of the
features agree on the class of an example. By stopping the feature evaluation
when encountering an easy- to-classify example, the predictor can achieve
substantial gains in computation. Our method provides a natural attention
mechanism for linear predictors where the predictor concentrates most of its
computation on hard-to-classify examples and quickly discards easy-to-classify
ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to
include our attentive method we prove that the average number of features
computed is O(sqrt(n log 1/sqrt(delta))) where n is the original number of
features, and delta is the error rate incurred due to early stopping. We
demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim,
Gisette, and synthetic datasets.



Visual features can help predict if a manipulation behavior will succeed at a
given location. For example, the success of a behavior that flips light
switches depends on the location of the switch. Within this paper, we present
methods that enable a mobile manipulator to autonomously learn a function that
takes an RGB image and a registered 3D point cloud as input and returns a 3D
location at which a manipulation behavior is likely to succeed. Given a pair of
manipulation behaviors that can change the state of the world between two sets
(e.g., light switch up and light switch down), classifiers that detect when
each behavior has been successful, and an initial hint as to where one of the
behaviors will be successful, the robot autonomously trains a pair of support
vector machine (SVM) classifiers by trying out the behaviors at locations in
the world and observing the results. When an image feature vector associated
with a 3D location is provided as input to one of the SVMs, the SVM predicts if
the associated manipulation behavior will be successful at the 3D location. To
evaluate our approach, we performed experiments with a PR2 robot from Willow
Garage in a simulated home using behaviors that flip a light switch, push a
rocker-type light switch, and operate a drawer. By using active learning, the
robot efficiently learned SVMs that enabled it to consistently succeed at these
tasks. After training, the robot also continued to learn in order to adapt in
the event of failure.



Joint distributions over many variables are frequently modeled by decomposing
them into products of simpler, lower-dimensional conditional distributions,
such as in sparsely connected Bayesian networks. However, automatically
learning such models can be very computationally expensive when there are many
datapoints and many continuous variables with complex nonlinear relationships,
particularly when no good ways of decomposing the joint distribution are known
a priori. In such situations, previous research has generally focused on the
use of discretization techniques in which each continuous variable has a single
discretization that is used throughout the entire network. \ In this paper, we
present and compare a wide variety of tree-based algorithms for learning and
evaluating conditional density estimates over continuous variables. These trees
can be thought of as discretizations that vary according to the particular
interactions being modeled; however, the density within a given leaf of the
tree need not be assumed constant, and we show that such nonuniform leaf
densities lead to more accurate density estimation. We have developed Bayesian
network structure-learning algorithms that employ these tree-based conditional
density representations, and we show that they can be used to practically learn
complex joint probability models over dozens of continuous variables from
thousands of datapoints. We focus on finding models that are simultaneously
accurate, fast to learn, and fast to evaluate once they are learned.



Filtering---estimating the state of a partially observable Markov process
from a sequence of observations---is one of the most widely studied problems in
control theory, AI, and computational statistics. Exact computation of the
posterior distribution is generally intractable for large discrete systems and
for nonlinear continuous systems, so a good deal of effort has gone into
developing robust approximation algorithms. This paper describes a simple
stochastic approximation algorithm for filtering called {em decayed MCMC}. The
algorithm applies Markov chain Monte Carlo sampling to the space of state
trajectories using a proposal distribution that favours flips of more recent
state variables. The formal analysis of the algorithm involves a generalization
of standard coupling arguments for MCMC convergence. We prove that for any
ergodic underlying Markov process, the convergence time of decayed MCMC with
inverse-polynomial decay remains bounded as the length of the observation
sequence grows. We show experimentally that decayed MCMC is at least
competitive with other approximation algorithms such as particle filtering.



Typical Recommender systems adopt a static view of the recommendation process
and treat it as a prediction problem. We argue that it is more appropriate to
view the problem of generating recommendations as a sequential decision problem
and, consequently, that Markov decision processes (MDP) provide a more
appropriate model for Recommender systems. MDPs introduce two benefits: they
take into account the long-term effects of each recommendation, and they take
into account the expected value of each recommendation. To succeed in practice,
an MDP-based Recommender system must employ a strong initial model; and the
bulk of this paper is concerned with the generation of such a model. In
particular, we suggest the use of an n-gram predictive model for generating the
initial MDP. Our n-gram model induces a Markov-chain model of user behavior
whose predictive accuracy is greater than that of existing predictive models.
We describe our predictive model in detail and evaluate its performance on real
data. In addition, we show how the model can be used in an MDP-based
Recommender system.



In many supervised learning tasks, the entities to be labeled are related to
each other in complex ways and their labels are not independent. For example,
in hypertext classification, the labels of linked pages are highly correlated.
A standard approach is to classify each entity independently, ignoring the
correlations between them. Recently, Probabilistic Relational Models, a
relational version of Bayesian networks, were used to define a joint
probabilistic model for a collection of related entities. In this paper, we
present an alternative framework that builds on (conditional) Markov networks
and addresses two limitations of the previous approach. First, undirected
models do not impose the acyclicity constraint that hinders representation of
many important relational dependencies in directed models. Second, undirected
models are well suited for discriminative training, where we optimize the
conditional likelihood of the labels given the features, which generally
improves classification accuracy. We show how to train these models
effectively, and how to use approximate probabilistic inference over the
learned model for collective classification of multiple related entities. We
provide experimental results on a webpage classification task, showing that
accuracy can be significantly improved by modeling relational dependencies.



Iterative Proportional Fitting (IPF), combined with EM, is commonly used as
an algorithm for likelihood maximization in undirected graphical models. In
this paper, we present two iterative algorithms that generalize upon IPF. The
first one is for likelihood maximization in discrete chain factor graphs, which
we define as a wide class of discrete variable models including undirected
graphical models and Bayesian networks, but also chain graphs and sigmoid
belief networks. The second one is for conditional likelihood maximization in
standard undirected models and Bayesian networks. In both algorithms, the
iteration steps are expressed in closed form. Numerical simulations show that
the algorithms are competitive with state of the art methods.



In this paper, by adopting a coherence-based probabilistic approach to
default reasoning, we focus the study on the logical operation of quasi
conjunction and the Goodman-Nguyen inclusion relation for conditional events.
We recall that quasi conjunction is a basic notion for defining consistency of
conditional knowledge bases. By deepening some results given in a previous
paper we show that, given any finite family of conditional events F and any
nonempty subset S of F, the family F p-entails the quasi conjunction C(S);
then, given any conditional event E|H, we analyze the equivalence between
p-entailment of E|H from F and p-entailment of E|H from C(S), where S is some
nonempty subset of F. We also illustrate some alternative theorems related with
p-consistency and p-entailment. Finally, we deepen the study of the connections
between the notions of p-entailment and inclusion relation by introducing for a
pair (F,E|H) the (possibly empty) class K of the subsets S of F such that C(S)
implies E|H. We show that the class K satisfies many properties; in particular
K is additive and has a greatest element which can be determined by applying a
suitable algorithm.



We present a new algorithm for approximate inference in probabilistic
programs, based on a stochastic gradient for variational programs. This method
is efficient without restrictions on the probabilistic program; it is
particularly practical for distributions which are not analytically tractable,
including highly structured distributions that arise in probabilistic programs.
We show how to automatically derive mean-field probabilistic programs and
optimize them, and demonstrate that our perspective improves inference
efficiency over other algorithms.



Over the last two decades, propositional satisfiability (SAT) has become one
of the most successful and widely applied techniques for the solution of
NP-complete problems. The aim of this paper is to investigate theoretically how
Sat can be utilized for the efficient solution of problems that are harder than
NP or co-NP. In particular, we consider the fundamental reasoning problems in
propositional disjunctive answer set programming (ASP), Brave Reasoning and
Skeptical Reasoning, which ask whether a given atom is contained in at least
one or in all answer sets, respectively. Both problems are located at the
second level of the Polynomial Hierarchy and thus assumed to be harder than NP
or co-NP. One cannot transform these two reasoning problems into SAT in
polynomial time, unless the Polynomial Hierarchy collapses. We show that
certain structural aspects of disjunctive logic programs can be utilized to
break through this complexity barrier, using new techniques from Parameterized
Complexity. In particular, we exhibit transformations from Brave and Skeptical
Reasoning to SAT that run in time O(2^k n^2) where k is a structural parameter
of the instance and n the input size. In other words, the reduction is
fixed-parameter tractable for parameter k. As the parameter k we take the size
of a smallest backdoor with respect to the class of normal (i.e.,
disjunction-free) programs. Such a backdoor is a set of atoms that when deleted
makes the program normal. In consequence, the combinatorial explosion, which is
expected when transforming a problem from the second level of the Polynomial
Hierarchy to the first level, can now be confined to the parameter k, while the
running time of the reduction is polynomial in the input size n, where the
order of the polynomial is independent of k.



In a causal graphical model, an instrument for a variable X and its effect Y
is a random variable that is a cause of X and independent of all the causes of
Y except X. (Pearl (1995), Spirtes et al (2000)). Instrumental variables can be
used to estimate how the distribution of an effect will respond to a
manipulation of its causes, even in the presence of unmeasured common causes
(confounders). In typical instrumental variable estimation, instruments are
chosen based on domain knowledge. There is currently no statistical test for
validating a variable as an instrument. In this paper, we introduce the concept
of semi-instrument, which generalizes the concept of instrument. We show that
in the framework of additive models, under certain conditions, we can test
whether a variable is semi-instrumental. Moreover, adding some distribution
assumptions, we can test whether two semi-instruments are instrumental. We give
algorithms to estimate the p-value that a random variable is semi-instrumental,
and the p-value that two semi-instruments are both instrumental. These
algorithms can be used to test the experts' choice of instruments, or to
identify instruments automatically.



It is often stated in papers tackling the task of inferring Bayesian network
structures from data that there are these two distinct approaches: (i) Apply
conditional independence tests when testing for the presence or otherwise of
edges; (ii) Search the model space using a scoring metric. Here I argue that
for complete data and a given node ordering this division is a myth, by showing
that cross entropy methods for checking conditional independence are
mathematically identical to methods based upon discriminating between models by
their overall goodness-of-fit logarithmic scores.



A serious problem in learning probabilistic models is the presence of hidden
variables. These variables are not observed, yet interact with several of the
observed variables. Detecting hidden variables poses two problems: determining
the relations to other variables in the model and determining the number of
states of the hidden variable. In this paper, we address the latter problem in
the context of Bayesian networks. We describe an approach that utilizes a
score-based agglomerative state-clustering. As we show, this approach allows us
to efficiently evaluate models with a range of cardinalities for the hidden
variable. We show how to extend this procedure to deal with multiple
interacting hidden variables. We demonstrate the effectiveness of this approach
by evaluating it on synthetic and real-life data. We show that our approach
learns models with hidden variables that generalize better and have better
structure than previous approaches.



The Information bottleneck method is an unsupervised non-parametric data
organization technique. Given a joint distribution P(A,B), this method
constructs a new variable T that extracts partitions, or clusters, over the
values of A that are informative about B. The information bottleneck has
already been applied to document classification, gene expression, neural code,
and spectral analysis. In this paper, we introduce a general principled
framework for multivariate extensions of the information bottleneck method.
This allows us to consider multiple systems of data partitions that are
inter-related. Our approach utilizes Bayesian networks for specifying the
systems of clusters and what information each captures. We show that this
construction provides insight about bottleneck variations and enables us to
characterize solutions of these variations. We also present a general framework
for iterative algorithms for constructing solutions, and apply it to several
examples.



A novel method for estimating Bayesian network (BN) parameters from data is
presented which provides improved performance on test data. Previous research
has shown the value of representing conditional probability distributions
(CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and
decision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network
(BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local
distributions,each having a different set of parents.This increases the space
of possible structures which can be considered,enabling the CPDs to have
finer-grained dependencies.The resulting estimation procedure induces a
modelthat is better able to emulate the underlying interactions occurring in
the data than conventional conditional Bernoulli network models.The results for
artificially generated data indicate that overfitting is best reduced by
restricting the complexity of candidate mixture substructures local to each
node. Furthermore, mixtures of very simple substructures can perform almost as
well as more complex ones.The BMN is also applied to data collected from an
online adventure game with an application to keyhole plan recognition. The
results show that the BMN-based model brings a dramatic improvement in
performance over a conventional BN model.



The search space of Bayesian Network structures is usually defined as Acyclic
Directed Graphs (DAGs) and the search is done by local transformations of DAGs.
But the space of Bayesian Networks is ordered by DAG Markov model inclusion and
it is natural to consider that a good search policy should take this into
account. First attempt to do this (Chickering 1996) was using equivalence
classes of DAGs instead of DAGs itself. This approach produces better results
but it is significantly slower. We present a compromise between these two
approaches. It uses DAGs to search the space in such a way that the ordering by
inclusion is taken into account. This is achieved by repetitive usage of local
moves within the equivalence class of DAGs. We show that this new approach
produces better results than the original DAGs approach without substantial
change in time complexity. We present empirical results, within the framework
of heuristic search and Markov Chain Monte Carlo, provided through the Alarm
dataset.



Chow and Liu (1968) studied the problem of learning a maximumlikelihood
Markov tree. We generalize their work to more complexMarkov networks by
considering the problem of learning a maximumlikelihood Markov network of
bounded complexity. We discuss howtree-width is in many ways the appropriate
measure of complexity andthus analyze the problem of learning a maximum
likelihood Markovnetwork of bounded tree-width.Similar to the work of Chow and
Liu, we are able to formalize thelearning problem as a combinatorial
optimization problem on graphs. Weshow that learning a maximum likelihood
Markov network of boundedtree-width is equivalent to finding a maximum weight
hypertree. Thisequivalence gives rise to global, integer-programming
based,approximation algorithms with provable performance guarantees, for
thelearning problem. This contrasts with heuristic local-searchalgorithms which
were previously suggested (e.g. by Malvestuto 1991).The equivalence also allows
us to study the computational hardness ofthe learning problem. We show that
learning a maximum likelihoodMarkov network of bounded tree-width is NP-hard,
and discuss thehardness of approximation.



There exist a number of reinforcement learning algorithms which learnby
climbing the gradient of expected reward. Their long-runconvergence has been
proved, even in partially observableenvironments with non-deterministic
actions, and without the need fora system model. However, the variance of the
gradient estimator hasbeen found to be a significant practical problem. Recent
approacheshave discounted future rewards, introducing a bias-variance
trade-offinto the gradient estimate. We incorporate a reward baseline into
thelearning system, and show that it affects variance without
introducingfurther bias. In particular, as we approach the
zero-bias,high-variance parameterization, the optimal (or variance
minimizing)constant reward baseline is equal to the long-term average
expectedreward. Modified policy-gradient algorithms are presented, and anumber
of experiments demonstrate their improvement over previous work.



Automatic continuous speech recognition (CSR) is sufficiently mature that a
variety of real world applications are now possible including large vocabulary
transcription and interactive spoken dialogues. This paper reviews the
evolution of the statistical modelling techniques which underlie current-day
systems, specifically hidden Markov models (HMMs) and N-grams. Starting from a
description of the speech signal and its parameterisation, the various
modelling assumptions and their consequences are discussed. It then describes
various techniques by which the effects of these assumptions can be mitigated.
Despite the progress that has been made, the limitations of current modelling
techniques are still evident. The paper therefore concludes with a brief review
of some of the more fundamental modelling work now in progress.



We treat collaborative filtering as a univariate time series estimation
problem: given a user's previous votes, predict the next vote. We describe two
families of methods for transforming data to encode time order in ways amenable
to off-the-shelf classification and density estimation tools, and examine the
results of using these approaches on several real-world data sets. The
improvements in predictive accuracy we realize recommend the use of other
predictive algorithms that exploit the temporal order of data.



Artifact systems are a novel paradigm for specifying and implementing
business processes described in terms of interacting modules called artifacts.
Artifacts consist of data and lifecycles, accounting respectively for the
relational structure of the artifacts' states and their possible evolutions
over time. In this paper we put forward artifact-centric multi-agent systems, a
novel formalisation of artifact systems in the context of multi-agent systems
operating on them. Differently from the usual process-based models of services,
the semantics we give explicitly accounts for the data structures on which
artifact systems are defined. We study the model checking problem for
artifact-centric multi-agent systems against specifications written in a
quantified version of temporal-epistemic logic expressing the knowledge of the
agents in the exchange. We begin by noting that the problem is undecidable in
general. We then identify two noteworthy restrictions, one syntactical and one
semantical, that enable us to find bisimilar finite abstractions and therefore
reduce the model checking problem to the instance on finite models. Under these
assumptions we show that the model checking problem for these systems is
EXPSPACE-complete. We then introduce artifact-centric programs, compact and
declarative representations of the programs governing both the artifact system
and the agents. We show that, while these in principle generate infinite-state
systems, under natural conditions their verification problem can be solved on
finite abstractions that can be effectively computed from the programs. Finally
we exemplify the theoretical results of the paper through a mainstream
procurement scenario from the artifact systems literature.



BliStr is a system that automatically develops strategies for E prover on a
large set of problems. The main idea is to interleave (i) iterated
low-timelimit local search for new strategies on small sets of similar easy
problems with (ii) higher-timelimit evaluation of the new strategies on all
problems. The accumulated results of the global higher-timelimit runs are used
to define and evolve the notion of "similar easy problems", and to control the
selection of the next strategy to be improved. The technique was used to
significantly strengthen the set of E strategies used by the MaLARea, PS-E,
E-MaLeS, and E systems in the CASC@Turing 2012 competition, particularly in the
Mizar division. Similar improvement was obtained on the problems created from
the Flyspeck corpus.



Recent work has established an empirically successful framework for adapting
learning rates for stochastic gradient descent (SGD). This effectively removes
all needs for tuning, while automatically reducing learning rates over time on
stationary problems, and permitting learning rates to grow appropriately in
non-stationary tasks. Here, we extend the idea in three directions, addressing
proper minibatch parallelization, including reweighted updates for sparse or
orthogonal gradients, improving robustness on non-smooth loss functions, in the
process replacing the diagonal Hessian estimation procedure that may not always
be available by a robust finite-difference approximation. The final algorithm
integrates all these components, has linear complexity and is hyper-parameter
free.



In this work, dynamic Bayesian multinets are introduced where a Markov chain
state at time t determines conditional independence patterns between random
variables lying within a local time window surrounding t. It is shown how
information-theoretic criterion functions can be used to induce sparse,
discriminative, and class-conditional network structures that yield an optimal
approximation to the class posterior probability, and therefore are useful for
the classification task. Using a new structure learning heuristic, the
resulting models are tested on a medium-vocabulary isolated-word speech
recognition task. It is demonstrated that these discriminatively structured
dynamic Bayesian multinets, when trained in a maximum likelihood setting using
EM, can outperform both HMMs and other dynamic Bayesian networks with a similar
number of parameters.



Recently developed techniques have made it possible to quickly learn accurate
probability density functions from data in low-dimensional continuous space. In
particular, mixtures of Gaussians can be fitted to data very quickly using an
accelerated EM algorithm that employs multiresolution kd-trees (Moore, 1999).
In this paper, we propose a kind of Bayesian networks in which low-dimensional
mixtures of Gaussians over different subsets of the domain's variables are
combined into a coherent joint probability model over the entire domain. The
network is also capable of modeling complex dependencies between discrete
variables and continuous variables without requiring discretization of the
continuous variables. We present efficient heuristic algorithms for
automatically learning these networks from data, and perform comparative
experiments illustrated how well these networks model real scientific data and
synthetic data. We also briefly discuss some possible improvements to the
networks, as well as possible applications.



Particle filters (PFs) are powerful sampling-based inference/learning
algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a
principled way, any type of probability distribution, nonlinearity and
non-stationarity. They have appeared in several fields under such names as
"condensation", "sequential Monte Carlo" and "survival of the fittest". In this
paper, we show how we can exploit the structure of the DBN to increase the
efficiency of particle filtering, using a technique known as
Rao-Blackwellisation. Essentially, this samples some of the variables, and
marginalizes out the rest exactly, using the Kalman filter, HMM filter,
junction tree algorithm, or any other finite dimensional optimal filter. We
show that Rao-Blackwellised particle filters (RBPFs) lead to more accurate
estimates than standard PFs. We demonstrate RBPFs on two problems, namely
non-stationary online regression with radial basis function networks and robot
localization and map building. We also discuss other potential application
areas and provide references to some finite dimensional optimal filters.



In many domains, we are interested in analyzing the structure of the
underlying distribution, e.g., whether one variable is a direct parent of the
other. Bayesian model-selection attempts to find the MAP model and use its
structure to answer these questions. However, when the amount of available data
is modest, there might be many models that have non-negligible posterior. Thus,
we want compute the Bayesian posterior of a feature, i.e., the total posterior
probability of all models that contain it. In this paper, we propose a new
approach for this task. We first show how to efficiently compute a sum over the
exponential number of networks that are consistent with a fixed ordering over
network variables. This allows us to compute, for a given ordering, both the
marginal probability of the data and the posterior of a feature. We then use
this result as the basis for an algorithm that approximates the Bayesian
posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC)
method, but over orderings rather than over network structures. The space of
orderings is much smaller and more regular than the space of structures, and
has a smoother posterior `landscape'. We present empirical results on synthetic
and real-life datasets that compare our approach to full model averaging (when
possible), to MCMC over network structures, and to a non-Bayesian bootstrap
approach.



In this paper we address the problem of learning the structure of a Bayesian
network in domains with continuous variables. This task requires a procedure
for comparing different candidate structures. In the Bayesian framework, this
is done by evaluating the {em marginal likelihood/} of the data given a
candidate structure. This term can be computed in closed-form for standard
parametric families (e.g., Gaussians), and can be approximated, at some
computational cost, for some semi-parametric families (e.g., mixtures of
Gaussians).
  We present a new family of continuous variable probabilistic networks that
are based on {em Gaussian Process/} priors. These priors are semi-parametric in
nature and can learn almost arbitrary noisy functional relations. Using these
priors, we can directly compute marginal likelihoods for structure learning.
The resulting method can discover a wide range of functional dependencies in
multivariate data. We develop the Bayesian score of Gaussian Process Networks
and describe how to learn them from data. We present empirical results on
artificial data as well as on real-life domains with non-linear dependencies.



We describe a graphical model for probabilistic relationships---an
alternative to the Bayesian network---called a dependency network. The graph of
a dependency network, unlike a Bayesian network, is potentially cyclic. The
probability component of a dependency network, like a Bayesian network, is a
set of conditional distributions, one for each node given its parents. We
identify several basic properties of this representation and describe a
computationally efficient procedure for learning the graph and probability
components from data. We describe the application of this representation to
probabilistic inference, collaborative filtering (the task of predicting
preferences), and the visualization of acausal predictive relationships.



There are two main objectives of this paper. The first is to present a
statistical framework for models with context specific independence structures,
i.e., conditional independences holding only for sepcific values of the
conditioning variables. This framework is constituted by the class of split
models. Split models are extension of graphical models for contigency tables
and allow for a more sophisticiated modelling than graphical models. The
treatment of split models include estimation, representation and a Markov
property for reading off those independencies holding in a specific context.
The second objective is to present a software package named YGGDRASIL which is
designed for statistical inference in split models, i.e., for learning such
models on the basis of data.



In this paper we present decomposable priors, a family of priors over
structure and parameters of tree belief nets for which Bayesian learning with
complete observations is tractable, in the sense that the posterior is also
decomposable and can be completely determined analytically in polynomial time.
This follows from two main results: First, we show that factored distributions
over spanning trees in a graph can be integrated in closed form. Second, we
examine priors over tree parameters and show that a set of assumptions similar
to (Heckerman and al. 1995) constrain the tree parameter priors to be a
compactly parameterized product of Dirichlet distributions. Beside allowing for
exact Bayesian learning, these results permit us to formulate a new class of
tractable latent variable models in which the likelihood of a data point is
computed through an ensemble average over tree structures.



Sampling is an important tool for estimating large, complex sums and
integrals over high dimensional spaces. For instance, important sampling has
been used as an alternative to exact methods for inference in belief networks.
Ideally, we want to have a sampling distribution that provides optimal-variance
estimators. In this paper, we present methods that improve the sampling
distribution by systematically adapting it as we obtain information from the
samples. We present a stochastic-gradient-descent method for sequentially
updating the sampling distribution based on the direct minization of the
variance. We also present other stochastic-gradient-descent methods based on
the minimizationof typical notions of distance between the current sampling
distribution and approximations of the target, optimal distribution. We finally
validate and compare the different methods empirically by applying them to the
problem of action evaluation in influence diagrams.



Dynamic trees are mixtures of tree structured belief networks. They solve
some of the problems of fixed tree networks at the cost of making exact
inference intractable. For this reason approximate methods such as sampling or
mean field approaches have been used. However, mean field approximations assume
a factorized distribution over node states. Such a distribution seems unlickely
in the posterior, as nodes are highly correlated in the prior. Here a
structured variational approach is used, where the posterior distribution over
the non-evidential nodes is itself approximated by a dynamic tree. It turns out
that this form can be used tractably and efficiently. The result is a set of
update rules which can propagate information through the network to obtain both
a full variational approximation, and the relevant marginals. The progagtion
rules are more efficient than the mean field approach and give noticeable
quantitative and qualitative improvement in the inference. The marginals
calculated give better approximations to the posterior than loopy propagation
on a small toy problem.



This paper extends the work in [Suzuki, 1996] and presents an efficient
depth-first branch-and-bound algorithm for learning Bayesian network
structures, based on the minimum description length (MDL) principle, for a
given (consistent) variable ordering. The algorithm exhaustively searches
through all network structures and guarantees to find the network with the best
MDL score. Preliminary experiments show that the algorithm is efficient, and
that the time complexity grows slowly with the sample size. The algorithm is
useful for empirically studying both the performance of suboptimal heuristic
search algorithms and the adequacy of the MDL principle in learning Bayesian
networks.



We present an approach to model-based hierarchical clustering by formulating
an objective function based on a Bayesian analysis. This model organizes the
data into a cluster hierarchy while specifying a complex feature-set
partitioning that is a key component of our model. Features can have either a
unique distribution in every cluster or a common distribution over some (or
even all) of the clusters. The cluster subsets over which these features have
such a common distribution correspond to the nodes (clusters) of the tree
representing the hierarchy. We apply this general model to the problem of
document clustering for which we use a multinomial likelihood function and
Dirichlet priors. Our algorithm consists of a two-stage process wherein we
first perform a flat clustering followed by a modified hierarchical
agglomerative merging process that includes determining the features that will
have common distributions over the merged clusters. The regularization induced
by using the marginal likelihood automatically determines the optimal model
structure including number of clusters, the depth of the tree and the subset of
features to be modeled as having a common distribution at each node. We present
experimental results on both synthetic data and a real document collection.



Recently, variational approximations such as the mean field approximation
have received much interest. We extend the standard mean field method by using
an approximating distribution that factorises into cluster potentials. This
includes undirected graphs, directed acyclic graphs and junction trees. We
derive generalized mean field equations to optimize the cluster potentials. We
show that the method bridges the gap between the standard mean field
approximation and the exact junction tree algorithm. In addition, we address
the problem of how to choose the graphical structure of the approximating
distribution. From the generalised mean field equations we derive rules to
simplify the structure of the approximating distribution in advance without
affecting the quality of the approximation. We also show how the method fits
into some other variational approximations that are currently popular.



The application of Bayesian networks (BNs) to cognitive assessment and
intelligent tutoring systems poses new challenges for model construction. When
cognitive task analyses suggest constructing a BN with several latent
variables, empirical model criticism of the latent structure becomes both
critical and complex. This paper introduces a methodology for criticizing
models both globally (a BN in its entirety) and locally (observable nodes), and
explores its value in identifying several kinds of misfit: node errors, edge
errors, state errors, and prior probability errors in the latent structure. The
results suggest the indices have potential for detecting model misfit and
assisting in locating problematic components of the model.



In this paper, we study CPU utilization time patterns of several MapReduce
applications. After extracting running patterns of several applications, they
are saved in a reference database to be later used to tweak system parameters
to efficiently execute unknown applications in future. To achieve this goal,
CPU utilization patterns of new applications are compared with the already
known ones in the reference database to find/predict their most probable
execution patterns. Because of different patterns lengths, the Dynamic Time
Warping (DTW) is utilized for such comparison; a correlation analysis is then
applied to DTWs outcomes to produce feasible similarity patterns. Three real
applications (WordCount, Exim Mainlog parsing and Terasort) are used to
evaluate our hypothesis in tweaking system parameters in executing similar
applications. Results were very promising and showed effectiveness of our
approach on pseudo-distributed MapReduce platforms.



Re-identification algorithms are used in data privacy to measure disclosure
risk. They model the situation in which an adversary attacks a published
database by means of linking the information of this adversary with the
database.
  In this paper we formalize this type of algorithm in terms of true
probabilities and compatible belief functions. The purpose of this work is to
leave aside as re-identification algorithms those algorithms that do not
satisfy a minimum requirement.



The dynamics of belief and knowledge is one of the major components of any
autonomous system that should be able to incorporate new pieces of information.
In this paper, we argue that to apply rationality result of belief dynamics
theory to various practical problems, it should be generalized in two respects:
first of all, it should allow a certain part of belief to be declared as
immutable; and second, the belief state need not be deductively closed. Such a
generalization of belief dynamics, referred to as base dynamics, is presented,
along with the concept of a generalized revision algorithm for Horn knowledge
bases. We show that Horn knowledge base dynamics has interesting connection
with kernel change and abduction. Finally, we also show that both variants are
rational in the sense that they satisfy certain rationality postulates stemming
from philosophical works on belief dynamics.



This paper addresses the problem of learning a task from demonstration. We
adopt the framework of inverse reinforcement learning, where tasks are
represented in the form of a reward function. Our contribution is a novel
active learning algorithm that enables the learning agent to query the expert
for more informative demonstrations, thus leading to more sample-efficient
learning. For this novel algorithm (Generalized Binary Search for Inverse
Reinforcement Learning, or GBS-IRL), we provide a theoretical bound on sample
complexity and illustrate its applicability on several different tasks. To our
knowledge, GBS-IRL is the first active IRL algorithm with provable sample
complexity bounds. We also discuss our method in light of other existing
methods in the literature and its general applicability in multi-class
classification problems. Finally, motivated by recent work on learning from
demonstration in robots, we also discuss how different forms of human feedback
can be integrated in a transparent manner in our learning framework.



Development of Interactive Theorem Provers has led to the creation of big
libraries and varied infrastructures for formal proofs. However, despite (or
perhaps due to) their sophistication, the re-use of libraries by non-experts or
across domains is a challenge. In this paper, we provide detailed case studies
and evaluate the machine-learning tool ML4PG built to interactively data-mine
the electronic libraries of proofs, and to provide user guidance on the basis
of proof patterns found in the existing libraries.



In this paper, we empirically evaluate algorithms for learning four types of
Bayesian network (BN) classifiers - Naive-Bayes, tree augmented Naive-Bayes, BN
augmented Naive-Bayes and general BNs, where the latter two are learned using
two variants of a conditional-independence (CI) based BN-learning algorithm.
Experimental results show the obtained classifiers, learned using the CI based
algorithms, are competitive with (or superior to) the best known classifiers,
based on both Bayesian networks and other formalisms; and that the
computational time for learning and using these classifiers is relatively
small. Moreover, these results also suggest a way to learn yet more effective
classifiers; we demonstrate empirically that this new algorithm does work as
expected. Collectively, these results argue that BN classifiers deserve more
attention in machine learning and data mining communities.



In recent years there has been significant progress in algorithms and methods
for inducing Bayesian networks from data. However, in complex data analysis
problems, we need to go beyond being satisfied with inducing networks with high
scores. We need to provide confidence measures on features of these networks:
Is the existence of an edge between two nodes warranted? Is the Markov blanket
of a given node robust? Can we say something about the ordering of the
variables? We should be able to address these questions, even when the amount
of data is not enough to induce a high scoring network. In this paper we
propose Efron's Bootstrap as a computationally efficient approach for answering
these questions. In addition, we propose to use these confidence measures to
induce better structures from the data, and to detect the presence of latent
variables.



Learning Bayesian networks is often cast as an optimization problem, where
the computational task is to find a structure that maximizes a statistically
motivated score. By and large, existing learning tools address this
optimization problem using standard heuristic search techniques. Since the
search space is extremely large, such search procedures can spend most of the
time examining candidates that are extremely unreasonable. This problem becomes
critical when we deal with data sets that are large either in the number of
instances, or the number of attributes. In this paper, we introduce an
algorithm that achieves faster learning by restricting the search space. This
iterative algorithm restricts the parents of each variable to belong to a small
subset of candidates. We then search for a network that satisfies these
constraints. The learned network is then used for selecting better candidates
for the next iteration. We evaluate this algorithm both on synthetic and
real-life data. Our results show that it is significantly faster than
alternative search procedures without loss of quality in the learned
structures.



As observations and student models become complex, educational assessments
that exploit advances in technology and cognitive psychology can outstrip
familiar testing models and analytic methods. Within the Portal conceptual
framework for assessment design, Bayesian inference networks (BINs) record
beliefs about students' knowledge and skills, in light of what they say and do.
Joining evidence model BIN fragments- which contain observable variables and
pointers to student model variables - to the student model allows one to update
belief about knowledge and skills as observations arrive. Markov Chain Monte
Carlo (MCMC) techniques can estimate the required conditional probabilities
from empirical data, supplemented by expert judgment or substantive theory.
Details for the special cases of item response theory (IRT) and multivariate
latent class modeling are given, with a numerical example of the latter.



In this paper we present a new Bayesian network model for classification that
combines the naive-Bayes (NB) classifier and the finite-mixture (FM)
classifier. The resulting classifier aims at relaxing the strong assumptions on
which the two component models are based, in an attempt to improve on their
classification performance, both in terms of accuracy and in terms of
calibration of the estimated probabilities. The proposed classifier is obtained
by superimposing a finite mixture model on the set of feature variables of a
naive Bayes model. We present experimental results that compare the predictive
performance on real datasets of the new classifier with the predictive
performance of the NB classifier and the FM classifier.



We show how to use a variational approximation to the logistic function to
perform approximate inference in Bayesian networks containing discrete nodes
with continuous parents. Essentially, we convert the logistic function to a
Gaussian, which facilitates exact inference, and then iteratively adjust the
variational parameters to improve the quality of the approximation. We
demonstrate experimentally that this approximation is faster and potentially
more accurate than sampling. We also introduce a simple new technique for
handling evidence, which allows us to handle arbitrary distributions on
observed nodes, as well as achieving a significant speedup in networks with
discrete variables of large cardinality.



A major problem for the learning of Bayesian networks (BNs) is the
exponential number of parameters needed for conditional probability tables.
Recent research reduces this complexity by modeling local structure in the
probability tables. We examine the use of log-linear local models. While
log-linear models in this context are not new (Whittaker, 1990; Buntine, 1991;
Neal, 1992; Heckerman and Meek, 1997), for structure learning they are
generally subsumed under a naive Bayes model. We describe an alternative
interpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metric
for structure learning of networks exhibiting causal independence, which we
term first-order networks (FONs). We also investigate local model selection on
a node-by-node basis.



In this paper we propose a logic-based, framework inspired by artificial
intelligence, but scaled down for practical database and programming
applications. Computation in the framework is viewed as the task of generating
a sequence of state transitions, with the purpose of making an agent's goals
all true. States are represented by sets of atomic sentences (or facts),
representing the values of program variables, tuples in a coordination
language, facts in relational databases, or Herbrand models.
  In the model-theoretic semantics, the entire sequence of states and events
are combined into a single model-theoretic structure, by associating timestamps
with facts and events. But in the operational semantics, facts are updated
destructively, without timestamps. We show that the model generated by
destructive updates is identical to the model generated by reasoning with facts
containing timestamps. We also extend the model with intentional predicates and
composite event predicates defined by logic programs containing conditions in
first-order logic, which query the current state.



The field of embodied intelligence emphasises the importance of the
morphology and environment with respect to the behaviour of a cognitive system.
The contribution of the morphology to the behaviour, commonly known as
morphological computation, is well-recognised in this community. We believe
that the field would benefit from a formalisation of this concept as we would
like to ask how much the morphology and the environment contribute to an
embodied agent's behaviour, or how an embodied agent can maximise the
exploitation of its morphology within its environment. In this work we derive
two concepts of measuring morphological computation, and we discuss their
relation to the Information Bottleneck Method. The first concepts asks how much
the world contributes to the overall behaviour and the second concept asks how
much the agent's action contributes to a behaviour. Various measures are
derived from the concepts and validated in two experiments which highlight
their strengths and weaknesses.



In recent years there has been a flurry of works on learning Bayesian
networks from data. One of the hard problems in this area is how to effectively
learn the structure of a belief network from incomplete data- that is, in the
presence of missing values or hidden variables. In a recent paper, I introduced
an algorithm called Structural EM that combines the standard Expectation
Maximization (EM) algorithm, which optimizes parameters, with structure search
for model selection. That algorithm learns networks based on penalized
likelihood scores, which include the BIC/MDL score and various approximations
to the Bayesian score. In this paper, I extend Structural EM to deal directly
with Bayesian model selection. I prove the convergence of the resulting
algorithm and show how to apply it for learning a large class of probabilistic
models, including Bayesian networks and some variants thereof.



People using consumer software applications typically do not use technical
jargon when querying an online database of help topics. Rather, they attempt to
communicate their goals with common words and phrases that describe software
functionality in terms of structure and objects they understand. We describe a
Bayesian approach to modeling the relationship between words in a user's query
for assistance and the informational goals of the user. After reviewing the
general method, we describe several extensions that center on integrating
additional distinctions and structure about language usage and user goals into
the Bayesian models.



We describe computationally efficient methods for learning mixtures in which
each component is a directed acyclic graphical model (mixtures of DAGs or
MDAGs). We argue that simple search-and-score algorithms are infeasible for a
variety of problems, and introduce a feasible approach in which parameter and
structure search is interleaved and expected data is treated as real data. Our
approach can be viewed as a combination of (1) the Cheeseman--Stutz asymptotic
approximation for model posterior probability and (2) the
Expectation--Maximization algorithm. We evaluate our procedure for selecting
among MDAGs on synthetic and real examples.



Damage recovery is critical for autonomous robots that need to operate for a
long time without assistance. Most current methods are complex and costly
because they require anticipating each potential damage in order to have a
contingency plan ready. As an alternative, we introduce the T-resilience
algorithm, a new algorithm that allows robots to quickly and autonomously
discover compensatory behaviors in unanticipated situations. This algorithm
equips the robot with a self-model and discovers new behaviors by learning to
avoid those that perform differently in the self-model and in reality. Our
algorithm thus does not identify the damaged parts but it implicitly searches
for efficient behaviors that do not use them. We evaluate the T-Resilience
algorithm on a hexapod robot that needs to adapt to leg removal, broken legs
and motor failures; we compare it to stochastic local search, policy gradient
and the self-modeling algorithm proposed by Bongard et al. The behavior of the
robot is assessed on-board thanks to a RGB-D sensor and a SLAM algorithm. Using
only 25 tests on the robot and an overall running time of 20 minutes,
T-Resilience consistently leads to substantially better results than the other
approaches.



A key problem of robotic environmental sensing and monitoring is that of
active sensing: How can a team of robots plan the most informative observation
paths to minimize the uncertainty in modeling and predicting an environmental
phenomenon? This paper presents two principled approaches to efficient
information-theoretic path planning based on entropy and mutual information
criteria for in situ active sensing of an important broad class of
widely-occurring environmental phenomena called anisotropic fields. Our
proposed algorithms are novel in addressing a trade-off between active sensing
performance and time efficiency. An important practical consequence is that our
algorithms can exploit the spatial correlation structure of Gaussian
process-based anisotropic fields to improve time efficiency while preserving
near-optimal active sensing performance. We analyze the time complexity of our
algorithms and prove analytically that they scale better than state-of-the-art
algorithms with increasing planning horizon length. We provide theoretical
guarantees on the active sensing performance of our algorithms for a class of
exploration tasks called transect sampling, which, in particular, can be
improved with longer planning time and/or lower spatial correlation along the
transect. Empirical evaluation on real-world anisotropic field data shows that
our algorithms can perform better or at least as well as the state-of-the-art
algorithms while often incurring a few orders of magnitude less computational
time, even when the field conditions are less favorable.



We undertook a study of the use of a memristor network for music generation,
making use of the memristor's memory to go beyond the Markov hypothesis. Seed
transition matrices are created and populated using memristor equations, and
which are shown to generate musical melodies and change in style over time as a
result of feedback into the transition matrix. The spiking properties of simple
memristor networks are demonstrated and discussed with reference to
applications of music making. The limitations of simulating composing memristor
networks in von Neumann hardware is discussed and a hardware solution based on
physical memristor properties is presented.



Recently several researchers have investigated techniques for using data to
learn Bayesian networks containing compact representations for the conditional
probability distributions (CPDs) stored at each node. The majority of this work
has concentrated on using decision-tree representations for the CPDs. In
addition, researchers typically apply non-Bayesian (or asymptotically Bayesian)
scoring functions such as MDL to evaluate the goodness-of-fit of networks to
the data. In this paper we investigate a Bayesian approach to learning Bayesian
networks that contain the more general decision-graph representations of the
CPDs. First, we describe how to evaluate the posterior probability that is, the
Bayesian score of such a network, given a database of observed cases. Second,
we describe various search spaces that can be used, in conjunction with a
scoring function and a search procedure, to identify one or more high-scoring
networks. Finally, we present an experimental evaluation of the search spaces,
using a greedy algorithm and a Bayesian scoring function.



We consider the problem of belief aggregation: given a group of individual
agents with probabilistic beliefs over a set of uncertain events, formulate a
sensible consensus or aggregate probability distribution over these events.
Researchers have proposed many aggregation methods, although on the question of
which is best the general consensus is that there is no consensus. We develop a
market-based approach to this problem, where agents bet on uncertain events by
buying or selling securities contingent on their outcomes. Each agent acts in
the market so as to maximize expected utility at given securities prices,
limited in its activity only by its own risk aversion. The equilibrium prices
of goods in this market represent aggregate beliefs. For agents with constant
risk aversion, we demonstrate that the aggregate probability exhibits several
desirable properties, and is related to independently motivated techniques. We
argue that the market-based approach provides a plausible mechanism for belief
aggregation in multiagent systems, as it directly addresses self-motivated
agent incentives for participation and for truthfulness, and can provide a
decision-theoretic foundation for the "expert weights" often employed in
centralized pooling techniques.



Social networks are increasingly being used to conduct polls. We introduce a
simple model of such social polling. We suppose agents vote sequentially, but
the order in which agents choose to vote is not necessarily fixed. We also
suppose that an agent's vote is influenced by the votes of their friends who
have already voted. Despite its simplicity, this model provides useful insights
into a number of areas including social polling, sequential voting, and
manipulation. We prove that the number of candidates and the network structure
affect the computational complexity of computing which candidate necessarily or
possibly can win in such a social poll. For social networks with bounded
treewidth and a bounded number of candidates, we provide polynomial algorithms
for both problems. In other cases, we prove that computing which candidates
necessarily or possibly win are computationally intractable.



Ever growing number of image documents available on the Internet continuously
motivates research in better annotation models and more efficient retrieval
methods. Formal knowledge representation of objects and events in pictures,
their interaction as well as context complexity becomes no longer an option for
a quality image repository, but a necessity. We present an ontology-based
online image annotation tool WNtags and demonstrate its usefulness in several
typical multimedia retrieval tasks using International Affective Picture System
emotionally annotated image database. WNtags is built around WordNet lexical
ontology but considers Suggested Upper Merged Ontology as the preferred
labeling formalism. WNtags uses sets of weighted WordNet synsets as high-level
image semantic descriptors and query matching is performed with word stemming
and node distance metrics. We also elaborate our near future plans to expand
image content description with induced affect as in stimuli for research of
human emotion and attention.



Cooperative pathfinding is a problem of finding a set of non-conflicting
trajectories for a number of mobile agents. Its applications include planning
for teams of mobile robots, such as autonomous aircrafts, cars, or underwater
vehicles. The state-of-the-art algorithms for cooperative pathfinding typically
rely on some heuristic forward-search pathfinding technique, where A* is often
the algorithm of choice. Here, we propose MA-RRT*, a novel algorithm for
multi-agent path planning that builds upon a recently proposed
asymptotically-optimal sampling-based algorithm for finding single-agent
shortest path called RRT*. We experimentally evaluate the performance of the
algorithm and show that the sampling-based approach offers better scalability
than the classical forward-search approach in relatively large, but sparse
environments, which are typical in real-world applications such as
multi-aircraft collision avoidance.



Approaches to learning Bayesian networks from data typically combine a
scoring function with a heuristic search procedure. Given a Bayesian network
structure, many of the scoring functions derived in the literature return a
score for the entire equivalence class to which the structure belongs. When
using such a scoring function, it is appropriate for the heuristic search
algorithm to search over equivalence classes of Bayesian networks as opposed to
individual structures. We present the general formulation of a search space for
which the states of the search correspond to equivalence classes of structures.
Using this space, any one of a number of heuristic search algorithms can easily
be applied. We compare greedy search performance in the proposed search space
to greedy search performance in a search space for which the states correspond
to individual Bayesian network structures.



We discuss Bayesian methods for learning Bayesian networks when data sets are
incomplete. In particular, we examine asymptotic approximations for the
marginal likelihood of incomplete data given a Bayesian network. We consider
the Laplace approximation and the less accurate but more efficient BIC/MDL
approximation. We also consider approximations proposed by Draper (1993) and
Cheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL,
but their accuracy has not been studied in any depth. We compare the accuracy
of these approximations under the assumption that the Laplace approximation is
the most accurate. In experiments using synthetic data generated from discrete
naive-Bayes models having a hidden root node, we find that the CS measure is
the most accurate.



In this paper we examine a novel addition to the known methods for learning
Bayesian networks from data that improves the quality of the learned networks.
Our approach explicitly represents and learns the local structure in the
conditional probability tables (CPTs), that quantify these networks. This
increases the space of possible models, enabling the representation of CPTs
with a variable number of parameters that depends on the learned local
structures. The resulting learning procedure is capable of inducing models that
better emulate the real complexity of the interactions present in the data. We
describe the theoretical foundations and practical aspects of learning local
structures, as well as an empirical evaluation of the proposed method. This
evaluation indicates that learning curves characterizing the procedure that
exploits the local structure converge faster than these of the standard
procedure. Our results also show that networks learned with local structure
tend to be more complex (in terms of arcs), yet require less parameters.



We extend the Bayesian Information Criterion (BIC), an asymptotic
approximation for the marginal likelihood, to Bayesian networks with hidden
variables. This approximation can be used to select models given large samples
of data. The standard BIC as well as our extension punishes the complexity of a
model according to the dimension of its parameters. We argue that the dimension
of a Bayesian network with hidden variables is the rank of the Jacobian matrix
of the transformation between the parameters of the network and the parameters
of the observable variables. We compute the dimensions of several networks
including the naive Bayes model with a hidden root node.



Research in the application of quantum structures to cognitive science
confirms that these structures quite systematically appear in the dynamics of
concepts and their combinations and quantum-based models faithfully represent
experimental data of situations where classical approaches are problematical.
In this paper, we analyze the data we collected in an experiment on a specific
conceptual combination, showing that Bell's inequalities are violated in the
experiment. We present a new refined entanglement scheme to model these data
within standard quantum theory rules, where 'entangled measurements and
entangled evolutions' occur, in addition to the expected 'entangled states',
and present a full quantum representation in complex Hilbert space of the data.
This stronger form of entanglement in measurements and evolutions might have
relevant applications in the foundations of quantum theory, as well as in the
interpretation of nonlocality tests. It could indeed explain some
non-negligible 'anomalies' identified in EPR-Bell experiments.



Gaussian processes are rich distributions over functions, which provide a
Bayesian nonparametric approach to smoothing and interpolation. We introduce
simple closed form kernels that can be used with Gaussian processes to discover
patterns and enable extrapolation. These kernels are derived by modelling a
spectral density -- the Fourier transform of a kernel -- with a Gaussian
mixture. The proposed kernels support a broad class of stationary covariances,
but Gaussian process inference remains simple and analytic. We demonstrate the
proposed kernels by discovering patterns and performing long range
extrapolation on synthetic examples, as well as atmospheric CO2 trends and
airline passenger data. We also show that we can reconstruct standard
covariances within our framework.



Morris (1996, 1997) introduced preference-based definitions of knowledge and
belief in standard state-space structures. This paper extends this
preference-based approach to unawareness structures (Heifetz, Meier, and
Schipper, 2006, 2008). By defining unawareness and knowledge in terms of
preferences over acts in unawareness structures and showing their equivalence
to the epistemic notions of unawareness and knowledge, we try to build a bridge
between decision theory and epistemic logic. Unawareness of an event is
characterized behaviorally as the event being null and its negation being null.



When modeling a probability distribution with a Bayesian network, we are
faced with the problem of how to handle continuous variables. Most previous
work has either solved the problem by discretizing, or assumed that the data
are generated by a single Gaussian. In this paper we abandon the normality
assumption and instead use statistical methods for nonparametric density
estimation. For a naive Bayesian classifier, we present experimental results on
a variety of natural and artificial domains, comparing two methods of density
estimation: assuming normality and modeling each conditional distribution with
a single Gaussian; and using nonparametric kernel density estimation. We
observe large reductions in error on several natural and artificial data sets,
which suggests that kernel estimation is a useful tool for learning Bayesian
models.



In our work we define a new algebra of operators as a substitute for fuzzy
logic. Its primary purpose is for construction of binary discriminators for
phonemes based on spectral content. It is optimized for design of
non-parametric computational circuits, and makes uses of 4 operations: $\min$,
$\max$, the difference and generalized additively homogenuous means.



Integration is affected by the curse of dimensionality and quickly becomes
intractable as the dimensionality of the problem grows. We propose a randomized
algorithm that, with high probability, gives a constant-factor approximation of
a general discrete integral defined over an exponentially large set. This
algorithm relies on solving only a small number of instances of a discrete
combinatorial optimization problem subject to randomly generated parity
constraints used as a hash function. As an application, we demonstrate that
with a small number of MAP queries we can efficiently approximate the partition
function of discrete graphical models, which can in turn be used, for instance,
for marginal computation or model selection.



We describe algorithms for learning Bayesian networks from a combination of
user knowledge and statistical data. The algorithms have two components: a
scoring metric and a search procedure. The scoring metric takes a network
structure, statistical data, and a user's prior knowledge, and returns a score
proportional to the posterior probability of the network structure given the
data. The search procedure generates networks for evaluation by the scoring
metric. Previous work has concentrated on metrics for domains containing only
discrete variables, under the assumption that data represents a multinomial
sample. In this paper, we extend this work, developing scoring metrics for
domains containing all continuous variables or a mixture of discrete and
continuous variables, under the assumption that continuous data is sampled from
a multivariate normal distribution. Our work extends traditional statistical
approaches for identifying vanishing regression coefficients in that we
identify two important assumptions, called event equivalence and parameter
modularity, that when combined allow the construction of prior distributions
for multivariate normal parameters from a single prior Bayesian network
specified by a user.



We describe our language-independent unsupervised word sense induction
system. This system only uses topic features to cluster different word senses
in their global context topic space. Using unlabeled data, this system trains a
latent Dirichlet allocation (LDA) topic model then uses it to infer the topics
distribution of the test instances. By clustering these topics distributions in
their topic space we cluster them into different senses. Our hypothesis is that
closeness in topic space reflects similarity between different word senses.
This system participated in SemEval-2 word sense induction and disambiguation
task and achieved the second highest V-measure score among all other systems.



We investigate the accuracy of the two most common estimators for the maximum
expected value of a general set of random variables: a generalization of the
maximum sample average, and cross validation. No unbiased estimator exists and
we show that it is non-trivial to select a good estimator without knowledge
about the distributions of the random variables. We investigate and bound the
bias and variance of the aforementioned estimators and prove consistency. The
variance of cross validation can be significantly reduced, but not without
risking a large bias. The bias and variance of different variants of cross
validation are shown to be very problem-dependent, and a wrong choice can lead
to very inaccurate estimates.



This paper deals with chain graphs under the Andersson-Madigan-Perlman (AMP)
interpretation. In particular, we present a constraint based algorithm for
learning an AMP chain graph a given probability distribution is faithful to.
Moreover, we show that the extension of Meek's conjecture to AMP chain graphs
does not hold, which compromises the development of efficient and correct
score+search learning algorithms under assumptions weaker than faithfulness.
  We also introduce a new family of graphical models that consists of
undirected and bidirected edges. We name this new family maximal
covariance-concentration graphs (MCCGs) because it includes both covariance and
concentration graphs as subfamilies. However, every MCCG can be seen as the
result of marginalizing out some nodes in an AMP CG. We describe global, local
and pairwise Markov properties for MCCGs and prove their equivalence. We
characterize when two MCCGs are Markov equivalent, and show that every Markov
equivalence class of MCCGs has a distinguished member. We present a constraint
based algorithm for learning a MCCG a given probability distribution is
faithful to.
  Finally, we present a graphical criterion for reading dependencies from a
MCCG of a probability distribution that satisfies the graphoid properties, weak
transitivity and composition. We prove that the criterion is sound and complete
in certain sense.



We propose a validity preserving translation from a subset of epistemic
Alternating-time Temporal Logic (ATL) to epistemic Computation Tree Logic
(CTL). The considered subset of epistemic ATL is known to have the finite model
property and decidable model-checking. This entails the decidability of
validity but the implied algorithm is unfeasible. Reducing the validity problem
to that in a corresponding system of CTL makes the techniques for automated
deduction for that logic available for the handling of the apparently more
complex system of ATL.



We present GURLS, a least squares, modular, easy-to-extend software library
for efficient supervised learning. GURLS is targeted to machine learning
practitioners, as well as non-specialists. It offers a number state-of-the-art
training strategies for medium and large-scale learning, and routines for
efficient model selection. The library is particularly well suited for
multi-output problems (multi-category/multi-label). GURLS is currently
available in two independent implementations: Matlab and C++. It takes
advantage of the favorable properties of regularized least squares algorithm to
exploit advanced tools in linear algebra. Routines to handle computations with
very large matrices by means of memory-mapped storage and distributed task
execution are available. The package is distributed under the BSD licence and
is available for download at https://github.com/CBCL/GURLS.



We analyze the meaning of the violation of the marginal probability law for
situations of correlation measurements where entanglement is identified. We
show that for quantum theory applied to the cognitive realm such a violation
does not lead to the type of problems commonly believed to occur in situations
of quantum theory applied to the physical realm. We briefly situate our quantum
approach for modeling concepts and their combinations with respect to the
notions of 'extension' and 'intension' in theories of meaning, and in existing
concept theories.



We introduce GP-FNARX: a new model for nonlinear system identification based
on a nonlinear autoregressive exogenous model (NARX) with filtered regressors
(F) where the nonlinear regression problem is tackled using sparse Gaussian
processes (GP). We integrate data pre-processing with system identification
into a fully automated procedure that goes from raw data to an identified
model. Both pre-processing parameters and GP hyper-parameters are tuned by
maximizing the marginal likelihood of the probabilistic model. We obtain a
Bayesian model of the system's dynamics which is able to report its uncertainty
in regions where the data is scarce. The automated approach, the modeling of
uncertainty and its relatively low computational cost make of GP-FNARX a good
candidate for applications in robotics and adaptive control.



Bayesian Reinforcement Learning (RL) is capable of not only incorporating
domain knowledge, but also solving the exploration-exploitation dilemma in a
natural way. As Bayesian RL is intractable except for special cases, previous
work has proposed several approximation methods. However, these methods are
usually too sensitive to parameter values, and finding an acceptable parameter
setting is practically impossible in many applications. In this paper, we
propose a new algorithm that greedily approximates Bayesian RL to achieve
robustness in parameter space. We show that for a desired learning behavior,
our proposed algorithm has a polynomial sample complexity that is lower than
those of existing algorithms. We also demonstrate that the proposed algorithm
naturally outperforms other existing algorithms when the prior distributions
are not significantly misleading. On the other hand, the proposed algorithm
cannot handle greatly misspecified priors as well as the other algorithms can.
This is a natural consequence of the fact that the proposed algorithm is
greedier than the other algorithms. Accordingly, we discuss a way to select an
appropriate algorithm for different tasks based on the algorithms' greediness.
We also introduce a new way of simplifying Bayesian planning, based on which
future work would be able to derive new algorithms.



Recent improvements of the LEO-II theorem prover are presented. These
improvements include a revised ATP interface, new translations into first-order
logic, rule support for the axiom of choice, detection of defined equality, and
more flexible strategy scheduling.



This article offers a modification of Chow and Liu's learning algorithm in
the context of handwritten digit recognition. The modified algorithm directs
the user to group digits into several classes consisting of digits that are
hard to distinguish and then constructing an optimal conditional tree
representation for each class of digits instead of for each single digit as
done by Chow and Liu (1968). Advantages and extensions of the new method are
discussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) which
offer a different entropy-based learning algorithm are shown to rest on
inappropriate assumptions.



What does it mean to claim that a physical or natural system computes? One
answer, endorsed here, is that computing is about programming a system to
behave in different ways. This paper offers an account of what it means for a
physical system to compute based on this notion. It proposes a behavioural
characterisation of computing in terms of a measure of programmability, which
reflects a system's ability to react to external stimuli. The proposed measure
of programmability is useful for classifying computers in terms of the apparent
algorithmic complexity of their evolution in time. I make some specific
proposals in this connection and discuss this approach in the context of other
behavioural approaches, notably Turing's test of machine intelligence. I also
anticipate possible objections and consider the applicability of these
proposals to the task of relating abstract computation to nature-like
computation.



Associative memories store content in such a way that the content can be
later retrieved by presenting the memory with a small portion of the content,
rather than presenting the memory with an address as in more traditional
memories. Associative memories are used as building blocks for algorithms
within database engines, anomaly detection systems, compression algorithms, and
face recognition systems. A classical example of an associative memory is the
Hopfield neural network. Recently, Gripon and Berrou have introduced an
alternative construction which builds on ideas from the theory of error
correcting codes and which greatly outperforms the Hopfield network in
capacity, diversity, and efficiency. In this paper we implement a variation of
the Gripon-Berrou associative memory on a general purpose graphical processing
unit (GPU). The work of Gripon and Berrou proposes two retrieval rules,
sum-of-sum and sum-of-max. The sum-of-sum rule uses only matrix-vector
multiplication and is easily implemented on the GPU. The sum-of-max rule is
much less straightforward to implement because it involves non-linear
operations. However, the sum-of-max rule gives significantly better retrieval
error rates. We propose a hybrid rule tailored for implementation on a GPU
which achieves a 880-fold speedup without sacrificing any accuracy.



Orthogonality is a discipline of programming that in a syntactic manner
guarantees determinism of functional specifications. Essentially, orthogonality
avoids, on the one side, the inherent ambiguity of non determinism, prohibiting
the existence of different rules that specify the same function and that may
apply simultaneously (non-ambiguity), and, on the other side, it eliminates the
possibility of occurrence of repetitions of variables in the left-hand side of
these rules (left linearity). In the theory of term rewriting systems (TRSs)
determinism is captured by the well-known property of confluence, that
basically states that whenever different computations or simplifications from a
term are possible, the computed answers should coincide. Although the proofs
are technically elaborated, confluence is well-known to be a consequence of
orthogonality. Thus, orthogonality is an important mathematical discipline
intrinsic to the specification of recursive functions that is naturally applied
in functional programming and specification. Starting from a formalization of
the theory of TRSs in the proof assistant PVS, this work describes how
confluence of orthogonal TRSs has been formalized, based on axiomatizations of
properties of rules, positions and substitutions involved in parallel steps of
reduction, in this proof assistant. Proofs for some similar but restricted
properties such as the property of confluence of non-ambiguous and (left and
right) linear TRSs have been fully formalized.



So-called combined approaches answer a conjunctive query over a description
logic ontology in three steps: first, they materialise certain consequences of
the ontology and the data; second, they evaluate the query over the data; and
third, they filter the result of the second phase to eliminate unsound answers.
Such approaches were developed for various members of the DL-Lite and the EL
families of languages, but none of them can handle ontologies containing
nominals. In our work, we bridge this gap and present a combined query
answering approach for ELHO---a logic that contains all features of the OWL 2
EL standard apart from transitive roles and complex role inclusions. This
extension is nontrivial because nominals require equality reasoning, which
introduces complexity into the first and the third step. Our empirical
evaluation suggests that our technique is suitable for practical application,
and so it provides a practical basis for conjunctive query answering in a large
fragment of OWL 2 EL.



In addition to their limpid interface with semantics, categorial grammars
enjoy another important property: learnability. This was first noticed by
Buskowsky and Penn and further studied by Kanazawa, for Bar-Hillel categorial
grammars.
  What about Lambek categorial grammars? In a previous paper we showed that
product free Lambek grammars where learnable from structured sentences, the
structures being incomplete natural deductions. These grammars were shown to be
unlearnable from strings by Foret and Le Nir. In the present paper we show that
Lambek grammars, possibly with product, are learnable from proof frames that
are incomplete proof nets.
  After a short reminder on grammatical inference \`a la Gold, we provide an
algorithm that learns Lambek grammars with product from proof frames and we
prove its convergence. We do so for 1-valued also known as rigid Lambek
grammars with product, since standard techniques can extend our result to
$k$-valued grammars. Because of the correspondence between cut-free proof nets
and normal natural deductions, our initial result on product free Lambek
grammars can be recovered.
  We are sad to dedicate the present paper to Philippe Darondeau, with whom we
started to study such questions in Rennes at the beginning of the millennium,
and who passed away prematurely.
  We are glad to dedicate the present paper to Jim Lambek for his 90 birthday:
he is the living proof that research is an eternal learning process.



We introduce a novel class of labeled directed acyclic graph (LDAG) models
for finite sets of discrete variables. LDAGs generalize earlier proposals for
allowing local structures in the conditional probability distribution of a
node, such that unrestricted label sets determine which edges can be deleted
from the underlying directed acyclic graph (DAG) for a given context. Several
properties of these models are derived, including a generalization of the
concept of Markov equivalence classes. Efficient Bayesian learning of LDAGs is
enabled by introducing an LDAG-based factorization of the Dirichlet prior for
the model parameters, such that the marginal likelihood can be calculated
analytically. In addition, we develop a novel prior distribution for the model
structures that can appropriately penalize a model for its labeling complexity.
A non-reversible Markov chain Monte Carlo algorithm combined with a greedy hill
climbing approach is used for illustrating the useful properties of LDAG models
for both real and synthetic data sets.



Computational intensity and sequential nature of estimation techniques for
Bayesian methods in statistics and machine learning, combined with their
increasing applications for big data analytics, necessitate both the
identification of potential opportunities to parallelize techniques such as
MCMC sampling, and the development of general strategies for mapping such
parallel algorithms to modern CPUs in order to elicit the performance up the
compute-based and/or memory-based hardware limits. Two opportunities for
Single-Instruction Multiple-Data (SIMD) parallelization of MCMC sampling for
probabilistic graphical models are presented. In exchangeable models with many
observations such as Bayesian Generalized Linear Models, child-node
contributions to the conditional posterior of each node can be calculated
concurrently. In undirected graphs with discrete nodes, concurrent sampling of
conditionally-independent nodes can be transformed into a SIMD form.
High-performance libraries with multi-threading and vectorization capabilities
can be readily applied to such SIMD opportunities to gain decent speedup, while
a series of high-level source-code and runtime modifications provide further
performance boost by reducing parallelization overhead and increasing data
locality for NUMA architectures. For big-data Bayesian GLM graphs, the
end-result is a routine for evaluating the conditional posterior and its
gradient vector that is 5 times faster than a naive implementation using
(built-in) multi-threaded Intel MKL BLAS, and reaches within the striking
distance of the memory-bandwidth-induced hardware limit. The proposed
optimization strategies improve the scaling of performance with number of cores
and width of vector units (applicable to many-core SIMD processors such as
Intel Xeon Phi and GPUs), resulting in cost-effectiveness, energy efficiency,
and higher speed on multi-core x86 processors.



This book chapter is an introduction to and an overview of the
information-theoretic, task independent utility function "Empowerment", which
is defined as the channel capacity between an agent's actions and an agent's
sensors. It quantifies how much influence and control an agent has over the
world it can perceive. This book chapter discusses the general idea behind
empowerment as an intrinsic motivation and showcases several previous
applications of empowerment to demonstrate how empowerment can be applied to
different sensor-motor configuration, and how the same formalism can lead to
different observed behaviors. Furthermore, we also present a fast approximation
for empowerment in the continuous domain.



Bayesian optimization (BO) aims to minimize a given blackbox function using a
model that is updated whenever new evidence about the function becomes
available. Here, we address the problem of BO under partially right-censored
response data, where in some evaluations we only obtain a lower bound on the
function value. The ability to handle such response data allows us to
adaptively censor costly function evaluations in minimization problems where
the cost of a function evaluation corresponds to the function value. One
important application giving rise to such censored data is the
runtime-minimizing variant of the algorithm configuration problem: finding
settings of a given parametric algorithm that minimize the runtime required for
solving problem instances from a given distribution. We demonstrate that
terminating slow algorithm runs prematurely and handling the resulting
right-censored observations can substantially improve the state of the art in
model-based algorithm configuration.



In this note, we argue that the axiomatic requirement of range to the measure
of aggregated total uncertainty (ATU) in Dempster-Shafer theory is not
reasonable.



We consider the scenario where the parameters of a probabilistic model are
expected to vary over time. We construct a novel prior distribution that
promotes sparsity and adapts the strength of correlation between parameters at
successive timesteps, based on the data. We derive approximate variational
inference procedures for learning and prediction with this prior. We test the
approach on two tasks: forecasting financial quantities from relevant text, and
modeling language contingent on time-varying financial measurements.



Large formal mathematical libraries consist of millions of atomic inference
steps that give rise to a corresponding number of proved statements (lemmas).
Analogously to the informal mathematical practice, only a tiny fraction of such
statements is named and re-used in later proofs by formal mathematicians. In
this work, we suggest and implement criteria defining the estimated usefulness
of the HOL Light lemmas for proving further theorems. We use these criteria to
mine the large inference graph of all lemmas in the core HOL Light library,
adding thousands of the best lemmas to the pool of named statements that can be
re-used in later proofs. The usefulness of the new lemmas is then evaluated by
comparing the performance of automated proving of the core HOL Light theorems
with and without such added lemmas.



Before Alan Turing made his crucial contributions to the theory of
computation, he studied the question of whether quantum mechanics could throw
light on the nature of free will. This article investigates the roles of
quantum mechanics and computation in free will. Although quantum mechanics
implies that events are intrinsically unpredictable, the `pure stochasticity'
of quantum mechanics adds only randomness to decision making processes, not
freedom. By contrast, the theory of computation implies that even when our
decisions arise from a completely deterministic decision-making process, the
outcomes of that process can be intrinsically unpredictable, even to --
especially to -- ourselves. I argue that this intrinsic computational
unpredictability of the decision making process is what give rise to our
impression that we possess free will. Finally, I propose a `Turing test' for
free will: a decision maker who passes this test will tend to believe that he,
she, or it possesses free will, whether the world is deterministic or not.



One of the remarkable feats of intelligent life is that it restructures the
world it lives in for its own benefit. This extended abstract outlines how the
information-theoretic principle of empowerment, as an intrinsic motivation, can
be used to restructure the environment an agent lives in. We present a first
qualitative evaluation of how an agent in a 3d-gridworld builds a
staircase-like structure, which reflects the agent's embodiment.



We present the architecture and the evaluation of a new system for
recognizing textual entailment (RTE). In RTE we want to identify automatically
the type of a logical relation between two input texts. In particular, we are
interested in proving the existence of an entailment between them. We conceive
our system as a modular environment allowing for a high-coverage syntactic and
semantic text analysis combined with logical inference. For the syntactic and
semantic analysis we combine a deep semantic analysis with a shallow one
supported by statistical models in order to increase the quality and the
accuracy of results. For RTE we use logical inference of first-order employing
model-theoretic techniques and automated reasoning tools. The inference is
supported with problem-relevant background knowledge extracted automatically
and on demand from external sources like, e.g., WordNet, YAGO, and OpenCyc, or
other, more experimental sources with, e.g., manually defined presupposition
resolutions, or with axiomatized general and common sense knowledge. The
results show that fine-grained and consistent knowledge coming from diverse
sources is a necessary condition determining the correctness and traceability
of results.



There have been several efforts to extend distributional semantics beyond
individual words, to measure the similarity of word pairs, phrases, and
sentences (briefly, tuples; ordered sets of words, contiguous or
noncontiguous). One way to extend beyond words is to compare two tuples using a
function that combines pairwise similarities between the component words in the
tuples. A strength of this approach is that it works with both relational
similarity (analogy) and compositional similarity (paraphrase). However, past
work required hand-coding the combination function for different tasks. The
main contribution of this paper is that combination functions are generated by
supervised learning. We achieve state-of-the-art results in measuring
relational similarity between word pairs (SAT analogies and SemEval~2012 Task
2) and measuring compositional similarity between noun-modifier phrases and
unigrams (multiple-choice paraphrase questions).



Gaussian processes are typically used for smoothing and interpolation on
small datasets. We introduce a new Bayesian nonparametric framework -- GPatt --
enabling automatic pattern extrapolation with Gaussian processes on large
multidimensional datasets. GPatt unifies and extends highly expressive kernels
and fast exact inference techniques. Without human intervention -- no hand
crafting of kernel features, and no sophisticated initialisation procedures --
we show that GPatt can solve large scale pattern extrapolation, inpainting, and
kernel discovery problems, including a problem with 383400 training points. We
find that GPatt significantly outperforms popular alternative scalable Gaussian
process methods in speed and accuracy. Moreover, we discover profound
differences between each of these methods, suggesting expressive kernels,
nonparametric representations, and exact inference are useful for modelling
large scale multidimensional patterns.



A crowdsourced stream processing system (CSP) is a system that incorporates
crowdsourced tasks in the processing of a data stream. This can be seen as
enabling crowdsourcing work to be applied on a sample of large-scale data at
high speed, or equivalently, enabling stream processing to employ human
intelligence. It also leads to a substantial expansion of the capabilities of
data processing systems. Engineering a CSP system requires the combination of
human and machine computation elements. From a general systems theory
perspective, this means taking into account inherited as well as emerging
properties from both these elements. In this paper, we position CSP systems
within a broader taxonomy, outline a series of design principles and evaluation
metrics, present an extensible framework for their design, and describe several
design patterns. We showcase the capabilities of CSP systems by performing a
case study that applies our proposed framework to the design and analysis of a
real system (AIDR) that classifies social media messages during time-critical
crisis events. Results show that compared to a pure stream processing system,
AIDR can achieve a higher data classification accuracy, while compared to a
pure crowdsourcing solution, the system makes better use of human workers by
requiring much less manual work effort.



Even though modern service-oriented and data-oriented architectures promise
to deliver loosely coupled control systems, they are inherently brittle as they
commonly depend on a priori agreed interfaces and data models. At the same
time, the Semantic Web and a whole set of accompanying standards and tools are
emerging, advocating ontologies as the basis for knowledge exchange. In this
paper we aim to identify a number of key ideas from the myriad of
knowledge-based practices that can readily be implemented by control systems
today. We demonstrate with a practical example (a three-channel imager for the
Mercator Telescope) how ontologies developed in the Web Ontology Language (OWL)
can serve as a meta-model for our instrument, covering as many engineering
aspects of the project as needed. We show how a concrete system model can be
built on top of this meta-model via a set of Domain Specific Languages (DSLs),
supporting both formal verification and the generation of software and
documentation artifacts. Finally we reason how the available semantics can be
exposed at run-time by adding a "semantic layer" that can be browsed, queried,
monitored etc. by any OPC UA-enabled client.



The ability for an autonomous agent to self-localise is directly proportional
to the accuracy and precision with which it can perceive salient features
within its local environment. The identification of such features by
recognising geometric profile allows robustness against lighting variations,
which is necessary in most industrial robotics applications. This paper details
a framework by which the random sample consensus (RANSAC) algorithm, often
applied to parameter fitting in linear models, can be extended to identify
higher-order geometric features. Goalpost identification within humanoid robot
soccer is investigated as an application, with the developed system yielding an
order-of-magnitude improvement in classification performance relative to a
traditional histogramming methodology.



Current studies about motor imagery based rehabilitation training systems for
stroke subjects lack an appropriate analytic method, which can achieve a
considerable classification accuracy, at the same time detects gradual changes
of imagery patterns during rehabilitation process and disinters potential
mechanisms about motor function recovery. In this study, we propose an adaptive
boosting algorithm based on the cortex plasticity and spectral band shifts.
This approach models the usually predetermined spatial-spectral configurations
in EEG study into variable preconditions, and introduces a new heuristic of
stochastic gradient boost for training base learners under these preconditions.
We compare our proposed algorithm with commonly used methods on datasets
collected from 2 months' clinical experiments. The simulation results
demonstrate the effectiveness of the method in detecting the variations of
stroke patients' EEG patterns. By chronologically reorganizing the weight
parameters of the learned additive model, we verify the spatial compensatory
mechanism on impaired cortex and detect the changes of accentuation bands in
spectral domain, which may contribute important prior knowledge for
rehabilitation practice.



This file summarizes the plenary talk on laboratory experiments on logic at
the TARK 2013 - 14th Conference on Theoretical Aspects of Rationality and
Knowledge.



We give algorithms with provable guarantees that learn a class of deep nets
in the generative model view popularized by Hinton and others. Our generative
model is an $n$ node multilayer neural net that has degree at most $n^{\gamma}$
for some $\gamma <1$ and each edge has a random edge weight in $[-1,1]$. Our
algorithm learns {\em almost all} networks in this class with polynomial
running time. The sample complexity is quadratic or cubic depending upon the
details of the model.
  The algorithm uses layerwise learning. It is based upon a novel idea of
observing correlations among features and using these to infer the underlying
edge structure via a global graph recovery procedure. The analysis of the
algorithm reveals interesting structure of neural networks with random edge
weights.



This report describes the suicidality prediction models created under the
DARPA DCAPS program in association with the Durkheim Project
[http://durkheimproject.org/]. The models were built primarily from
unstructured text (free-format clinician notes) for several hundred patient
records obtained from the Veterans Health Administration (VHA). The models were
constructed using a genetic programming algorithm applied to bag-of-words and
bag-of-phrases datasets. The influence of additional structured data was
explored but was found to be minor. Given the small dataset size,
classification between cohorts was high fidelity (98%). Cross-validation
suggests these models are reasonably predictive, with an accuracy of 50% to 69%
on five rotating folds, with ensemble averages of 58% to 67%. One particularly
noteworthy result is that word-pairs can dramatically improve classification
accuracy; but this is the case only when one of the words in the pair is
already known to have a high predictive value. By contrast, the set of all
possible word-pairs does not improve on a simple bag-of-words model.



Thompson Sampling, one of the oldest heuristics for solving multi-armed
bandits, has recently been shown to demonstrate state-of-the-art performance.
The empirical success has led to great interests in theoretical understanding
of this heuristic. In this paper, we approach this problem in a way very
different from existing efforts. In particular, motivated by the connection
between Thompson Sampling and exponentiated updates, we propose a new family of
algorithms called Generalized Thompson Sampling in the expert-learning
framework, which includes Thompson Sampling as a special case. Similar to most
expert-learning algorithms, Generalized Thompson Sampling uses a loss function
to adjust the experts' weights. General regret bounds are derived, which are
also instantiated to two important loss functions: square loss and logarithmic
loss. In contrast to existing bounds, our results apply to quite general
contextual bandits. More importantly, they quantify the effect of the "prior"
distribution on the regret bounds.



We consider the classical TD(0) algorithm implemented on a network of agents
wherein the agents also incorporate the updates received from neighboring
agents using a gossip-like mechanism. The combined scheme is shown to converge
for both discounted and average cost problems.



We outline the rationale and preliminary results of using the state context
property (SCOP) formalism, originally developed as a generalization of quantum
mechanics, to describe the contextual manner in which concepts are evoked, used
and combined to generate meaning. The quantum formalism was developed to cope
with problems arising in the description of (i) the measurement process, and
(ii) the generation of new states with new properties when particles become
entangled. Similar problems arising with concepts motivated the formal
treatment introduced here. Concepts are viewed not as fixed representations,
but entities existing in states of potentiality that require interaction with a
context-a stimulus or another concept-to 'collapse' to an instantiated form
(e.g. exemplar, prototype, or other possibly imaginary instance). The stimulus
situation plays the role of the measurement in physics, acting as context that
induces a change of the cognitive state from superposition state to collapsed
state. The collapsed state is more likely to consist of a conjunction of
concepts for associative than analytic thought because more stimulus or concept
properties take part in the collapse. We provide two contextual measures of
conceptual distance-one using collapse probabilities and the other weighted
properties-and show how they can be applied to conjunctions using the pet fish
problem.



We present a new temporal logic called Distribution Temporal Logic (DTL)
defined over predicates of belief states and hidden states of partially
observable systems. DTL can express properties involving uncertainty and
likelihood that cannot be described by existing logics. A co-safe formulation
of DTL is defined and algorithmic procedures are given for monitoring
executions of a partially observable Markov decision process with respect to
such formulae. A simulation case study of a rescue robotics application
outlines our approach.



Arrays are ubiquitous in the context of software verification. However,
effective reasoning over arrays is still rare in CP, as local reasoning is
dramatically ill-conditioned for constraints over arrays. In this paper, we
propose an approach combining both global symbolic reasoning and local
consistency filtering in order to solve constraint systems involving arrays
(with accesses, updates and size constraints) and finite-domain constraints
over their elements and indexes. Our approach, named FDCC, is based on a
combination of a congruence closure algorithm for the standard theory of arrays
and a CP solver over finite domains. The tricky part of the work lies in the
bi-directional communication mechanism between both solvers. We identify the
significant information to share, and design ways to master the communication
overhead. Experiments on random instances show that FDCC solves more formulas
than any portfolio combination of the two solvers taken in isolation, while
overhead is kept reasonable.



In this paper, we consider active information acquisition when the prediction
model is meant to be applied on a targeted subset of the population. The goal
is to label a pre-specified fraction of customers in the target or test set by
iteratively querying for information from the non-target or training set. The
number of queries is limited by an overall budget. Arising in the context of
two rather disparate applications- banking and medical diagnosis, we pose the
active information acquisition problem as a constrained optimization problem.
We propose two greedy iterative algorithms for solving the above problem. We
conduct experiments with synthetic data and compare results of our proposed
algorithms with few other baseline approaches. The experimental results show
that our proposed approaches perform better than the baseline schemes.



Many large datasets exhibit power-law statistics: The web graph, social
networks, text data, click through data etc. Their adjacency graphs are termed
natural graphs, and are known to be difficult to partition. As a consequence
most distributed algorithms on these graphs are communication intensive. Many
algorithms on natural graphs involve an Allreduce: a sum or average of
partitioned data which is then shared back to the cluster nodes. Examples
include PageRank, spectral partitioning, and many machine learning algorithms
including regression, factor (topic) models, and clustering. In this paper we
describe an efficient and scalable Allreduce primitive for power-law data. We
point out scaling problems with existing butterfly and round-robin networks for
Sparse Allreduce, and show that a hybrid approach improves on both.
Furthermore, we show that Sparse Allreduce stages should be nested instead of
cascaded (as in the dense case). And that the optimum throughput Allreduce
network should be a butterfly of heterogeneous degree where degree decreases
with depth into the network. Finally, a simple replication scheme is introduced
to deal with node failures. We present experiments showing significant
improvements over existing systems such as PowerGraph and Hadoop.



It is time-consuming and error-prone to implement inference procedures for
each new probabilistic model. Probabilistic programming addresses this problem
by allowing a user to specify the model and having a compiler automatically
generate an inference procedure for it. For this approach to be practical, it
is important to generate inference code that has reasonable performance. In
this paper, we present a probabilistic programming language and compiler for
Bayesian networks designed to make effective use of data-parallel architectures
such as GPUs. Our language is fully integrated within the Scala programming
language and benefits from tools such as IDE support, type-checking, and code
completion. We show that the compiler can generate data-parallel inference code
scalable to thousands of GPU cores by making use of the conditional
independence relationships in the Bayesian network.



We study the complexity of (approximate) winner determination under the
Monroe and Chamberlin--Courant multiwinner voting rules, which determine the
set of representatives by optimizing the total (dis)satisfaction of the voters
with their representatives. The total (dis)satisfaction is calculated either as
the sum of individual (dis)satisfactions (the utilitarian case) or as the
(dis)satisfaction of the worst off voter (the egalitarian case). We provide
good approximation algorithms for the satisfaction-based utilitarian versions
of the Monroe and Chamberlin--Courant rules, and inapproximability results for
the dissatisfaction-based utilitarian versions of them and also for all
egalitarian cases. Our algorithms are applicable and particularly appealing
when voters submit truncated ballots. We provide experimental evaluation of the
algorithms both on real-life preference-aggregation data and on synthetic data.
These experiments show that our simple and fast algorithms can in many cases
find near-perfect solutions.



In this paper we study the complexity of strategic argumentation for dialogue
games. A dialogue game is a 2-player game where the parties play arguments. We
show how to model dialogue games in a skeptical, non-monotonic formalism, and
we show that the problem of deciding what move (set of rules) to play at each
turn is an NP-complete problem.



A distinctive property of human and animal intelligence is the ability to
form abstractions by neglecting irrelevant information which allows to separate
structure from noise. From an information theoretic point of view abstractions
are desirable because they allow for very efficient information processing. In
artificial systems abstractions are often implemented through computationally
costly formations of groups or clusters. In this work we establish the relation
between the free-energy framework for decision making and rate-distortion
theory and demonstrate how the application of rate-distortion for
decision-making leads to the emergence of abstractions. We argue that
abstractions are induced due to a limit in information processing capacity.



Given that semantic Web realization is based on the critical mass of metadata
accessibility and the representation of data with formal knowledge, it needs to
generate metadata that is specific, easy to understand and well-defined.
However, semantic annotation of the web documents is the successful way to make
the Semantic Web vision a reality. This paper introduces the Semantic Web and
its vision (stack layers) with regard to some concept definitions that helps
the understanding of semantic annotation. Additionally, this paper introduces
the semantic annotation categories, tools, domains and models.



The number of malicious software (malware) is growing out of control.
Syntactic signature based detection cannot cope with such growth and manual
construction of malware signature databases needs to be replaced by computer
learning based approaches. Currently, a single modern signature capturing the
semantics of a malicious behavior can be used to replace an arbitrarily large
number of old-fashioned syntactical signatures. However teaching computers to
learn such behaviors is a challenge. Existing work relies on dynamic analysis
to extract malicious behaviors, but such technique does not guarantee the
coverage of all behaviors. To sidestep this limitation we show how to learn
malware signatures using static reachability analysis. The idea is to model
binary programs using pushdown systems (that can be used to model the stack
operations occurring during the binary code execution), use reachability
analysis to extract behaviors in the form of trees, and use subtrees that are
common among the trees extracted from a training set of malware files as
signatures. To detect malware we propose to use a tree automaton to compactly
store malicious behavior trees and check if any of the subtrees extracted from
the file under analysis is malicious. Experimental data shows that our approach
can be used to learn signatures from a training set of malware files and use
them to detect a test set of malware that is 5 times the size of the training
set.



Computational trust mechanisms aim to produce trust ratings from both direct
and indirect information about agents' behaviour. Subjective Logic (SL) has
been widely adopted as the core of such systems via its fusion and discount
operators. In recent research we revisited the semantics of these operators to
explore an alternative, geometric interpretation. In this paper we present a
principled desiderata for discounting and fusion operators in SL. Building upon
this we present operators that satisfy these desirable properties, including a
family of discount operators. We then show, through a rigorous empirical study,
that specific, geometrically interpreted operators significantly outperform
standard SL operators in estimating ground truth. These novel operators offer
real advantages for computational models of trust and reputation, in which they
may be employed without modifying other aspects of an existing system.



Induction of common sense knowledge about prototypical sequences of events
has recently received much attention. Instead of inducing this knowledge in the
form of graphs, as in much of the previous work, in our method, distributed
representations of event realizations are computed based on distributed
representations of predicates and their arguments, and then these
representations are used to predict prototypical event orderings. The
parameters of the compositional process for computing the event representations
and the ranking component of the model are jointly estimated from texts. We
show that this approach results in a substantial boost in ordering performance
with respect to previous methods.



Numerous machine learning problems require an exploration basis - a mechanism
to explore the action space. We define a novel geometric notion of exploration
basis with low variance, called volumetric spanners, and give efficient
algorithms to construct such a basis.
  We show how efficient volumetric spanners give rise to the first efficient
and optimal regret algorithm for bandit linear optimization over general convex
sets. Previously such results were known only for specific convex sets, or
under special conditions such as the existence of an efficient self-concordant
barrier for the underlying set.



We propose a sparse-coding framework for activity recognition in ubiquitous
and mobile computing that alleviates two fundamental problems of current
supervised learning approaches. (i) It automatically derives a compact, sparse
and meaningful feature representation of sensor data that does not rely on
prior expert knowledge and generalizes extremely well across domain boundaries.
(ii) It exploits unlabeled sample data for bootstrapping effective activity
recognizers, i.e., substantially reduces the amount of ground truth annotation
required for model estimation. Such unlabeled data is trivial to obtain, e.g.,
through contemporary smartphones carried by users as they go about their
everyday activities.
  Based on the self-taught learning paradigm we automatically derive an
over-complete set of basis vectors from unlabeled data that captures inherent
patterns present within activity data. Through projecting raw sensor data onto
the feature space defined by such over-complete sets of basis vectors effective
feature extraction is pursued. Given these learned feature representations,
classification backends are then trained using small amounts of labeled
training data.
  We study the new approach in detail using two datasets which differ in terms
of the recognition tasks and sensor modalities. Primarily we focus on
transportation mode analysis task, a popular task in mobile-phone based
sensing. The sparse-coding framework significantly outperforms the
state-of-the-art in supervised learning approaches. Furthermore, we demonstrate
the great practical potential of the new approach by successfully evaluating
its generalization capabilities across both domain and sensor modalities by
considering the popular Opportunity dataset. Our feature learning approach
outperforms state-of-the-art approaches to analyzing activities in daily
living.



Generalizing empirical findings to new environments, settings, or populations
is essential in most scientific explorations. This article treats a particular
problem of generalizability, called "transportability", defined as a license to
transfer information learned in experimental studies to a different population,
on which only observational studies can be conducted. Given a set of
assumptions concerning commonalities and differences between the two
populations, Pearl and Bareinboim (2011) derived sufficient conditions that
permit such transfer to take place. This article summarizes their findings and
supplements them with an effective procedure for deciding when and how
transportability is feasible. It establishes a necessary and sufficient
condition for deciding when causal effects in the target population are
estimable from both the statistical information available and the causal
information transferred from the experiments. The article further provides a
complete algorithm for computing the transport formula, that is, a way of
combining observational and experimental information to synthesize bias-free
estimate of the desired causal relation. Finally, the article examines the
differences between transportability and other variants of generalizability.



We apply diffusion strategies to develop a fully-distributed cooperative
reinforcement learning algorithm in which agents in a network communicate only
with their immediate neighbors to improve predictions about their environment.
The algorithm can also be applied to off-policy learning, meaning that the
agents can predict the response to a behavior different from the actual
policies they are following. The proposed distributed strategy is efficient,
with linear complexity in both computation time and memory footprint. We
provide a mean-square-error performance analysis and establish convergence
under constant step-size updates, which endow the network with continuous
learning capabilities. The results show a clear gain from cooperation: when the
individual agents can estimate the solution, cooperation increases stability
and reduces bias and variance of the prediction error; but, more importantly,
the network is able to approach the optimal solution even when none of the
individual agents can (e.g., when the individual behavior policies restrict
each agent to sample a small portion of the state space).



In many cases, government data is still "locked" in several "data silos",
even within the boundaries of a single (inter-)national public organization
with disparate and distributed organizational units and departments spread
across multiple sites. Opening data and enabling its unified querying from a
single site in an efficient and effective way is a semantic application
integration and open government data challenge. This paper describes how NARA
is using Semantic Web technology to implement an application integration
approach within the boundaries of its organization via opening and querying
multiple governmental data sources from a single site. The generic approach
proposed, namely S3-AI, provides support to answering unified,
ontology-mediated, federated queries to data produced and exploited by
disparate applications, while these are being located in different
organizational sites. S3-AI preserves ownership, autonomy and independency of
applications and data. The paper extensively demonstrates S3-AI, using the D2RQ
and Fuseki technologies, for addressing the needs of a governmental "IT
helpdesk support" case.



Recently, multiple formulations of vision problems as probabilistic
inversions of generative models based on computer graphics have been proposed.
However, applications to 3D perception from natural images have focused on
low-dimensional latent scenes, due to challenges in both modeling and
inference. Accounting for the enormous variability in 3D object shape and 2D
appearance via realistic generative models seems intractable, as does inverting
even simple versions of the many-to-many computations that link 3D scenes to 2D
images. This paper proposes and evaluates an approach that addresses key
aspects of both these challenges. We show that it is possible to solve
challenging, real-world 3D vision problems by approximate inference in
generative models for images based on rendering the outputs of probabilistic
CAD (PCAD) programs. Our PCAD object geometry priors generate deformable 3D
meshes corresponding to plausible objects and apply affine transformations to
place them in a scene. Image likelihoods are based on similarity in a feature
space based on standard mid-level image representations from the vision
literature. Our inference algorithm integrates single-site and locally blocked
Metropolis-Hastings proposals, Hamiltonian Monte Carlo and discriminative
data-driven proposals learned from training data generated from our models. We
apply this approach to 3D human pose estimation and object shape reconstruction
from single images, achieving quantitative and qualitative performance
improvements over state-of-the-art baselines.



Recent work introduced Generalized First Order Decision Diagrams (GFODD) as a
knowledge representation that is useful in mechanizing decision theoretic
planning in relational domains. GFODDs generalize function-free first order
logic and include numerical values and numerical generalizations of existential
and universal quantification. Previous work presented heuristic inference
algorithms for GFODDs and implemented these heuristics in systems for decision
theoretic planning. In this paper, we study the complexity of the computational
problems addressed by such implementations. In particular, we study the
evaluation problem, the satisfiability problem, and the equivalence problem for
GFODDs under the assumption that the size of the intended model is given with
the problem, a restriction that guarantees decidability. Our results provide a
complete characterization placing these problems within the polynomial
hierarchy. The same characterization applies to the corresponding restriction
of problems in first order logic, giving an interesting new avenue for
efficient inference when the number of objects is bounded. Our results show
that for $\Sigma_k$ formulas, and for corresponding GFODDs, evaluation and
satisfiability are $\Sigma_k^p$ complete, and equivalence is $\Pi_{k+1}^p$
complete. For $\Pi_k$ formulas evaluation is $\Pi_k^p$ complete, satisfiability
is one level higher and is $\Sigma_{k+1}^p$ complete, and equivalence is
$\Pi_{k+1}^p$ complete.



Biomedical taxonomies, thesauri and ontologies in the form of the
International Classification of Diseases (ICD) as a taxonomy or the National
Cancer Institute Thesaurus as an OWL-based ontology, play a critical role in
acquiring, representing and processing information about human health. With
increasing adoption and relevance, biomedical ontologies have also
significantly increased in size. For example, the 11th revision of the ICD,
which is currently under active development by the WHO contains nearly 50,000
classes representing a vast variety of different diseases and causes of death.
This evolution in terms of size was accompanied by an evolution in the way
ontologies are engineered. Because no single individual has the expertise to
develop such large-scale ontologies, ontology-engineering projects have evolved
from small-scale efforts involving just a few domain experts to large-scale
projects that require effective collaboration between dozens or even hundreds
of experts, practitioners and other stakeholders. Understanding how these
stakeholders collaborate will enable us to improve editing environments that
support such collaborations. We uncover how large ontology-engineering
projects, such as the ICD in its 11th revision, unfold by analyzing usage logs
of five different biomedical ontology-engineering projects of varying sizes and
scopes using Markov chains. We discover intriguing interaction patterns (e.g.,
which properties users subsequently change) that suggest that large
collaborative ontology-engineering projects are governed by a few general
principles that determine and drive development. From our analysis, we identify
commonalities and differences between different projects that have implications
for project managers, ontology editors, developers and contributors working on
collaborative ontology-engineering projects and tools in the biomedical domain.



Learning Markov blanket (MB) structures has proven useful in performing
feature selection, learning Bayesian networks (BNs), and discovering causal
relationships. We present a formula for efficiently determining the number of
MB structures given a target variable and a set of other variables. As
expected, the number of MB structures grows exponentially. However, we show
quantitatively that there are many fewer MB structures that contain the target
variable than there are BN structures that contain it. In particular, the ratio
of BN structures to MB structures appears to increase exponentially in the
number of variables.



A major problem in road network analysis is discovery of important
crossroads, which can provide useful information for transport planning.
However, none of existing approaches addresses the problem of identifying
network-wide important crossroads in real road network. In this paper, we
propose a novel data-driven based approach named CRRank to rank important
crossroads. Our key innovation is that we model the trip network reflecting
real travel demands with a tripartite graph, instead of solely analysis on the
topology of road network. To compute the importance scores of crossroads
accurately, we propose a HITS-like ranking algorithm, in which a procedure of
score propagation on our tripartite graph is performed. We conduct experiments
on CRRank using a real-world dataset of taxi trajectories. Experiments verify
the utility of CRRank.



We develop a technique for generalising from data in which models are
samplers represented as program text. We establish encouraging empirical
results that suggest that Markov chain Monte Carlo probabilistic programming
inference techniques coupled with higher-order probabilistic programming
languages are now sufficiently powerful to enable successful inference of this
kind in nontrivial domains. We also introduce a new notion of probabilistic
program compilation and show how the same machinery might be used in the future
to compile probabilistic programs for efficient reusable predictive inference.



Models of object vision have been of great interest in computer vision and
visual neuroscience. During the last decades, several models have been
developed to extract visual features from images for object recognition tasks.
Some of these were inspired by the hierarchical structure of primate visual
system, and some others were engineered models. The models are varied in
several aspects: models that are trained by supervision, models trained without
supervision, and models (e.g. feature extractors) that are fully hard-wired and
do not need training. Some of the models come with a deep hierarchical
structure consisting of several layers, and some others are shallow and come
with only one or two layers of processing. More recently, new models have been
developed that are not hand-tuned but trained using millions of images, through
which they learn how to extract informative task-related features. Here I will
survey all these different models and provide the reader with an intuitive, as
well as a more detailed, understanding of the underlying computations in each
of the models.



Schema Matching, i.e. the process of discovering semantic correspondences
between concepts adopted in different data source schemas, has been a key topic
in Database and Artificial Intelligence research areas for many years. In the
past, it was largely investigated especially for classical database models
(e.g., E/R schemas, relational databases, etc.). However, in the latest years,
the widespread adoption of XML in the most disparate application fields pushed
a growing number of researchers to design XML-specific Schema Matching
approaches, called XML Matchers, aiming at finding semantic matchings between
concepts defined in DTDs and XSDs. XML Matchers do not just take well-known
techniques originally designed for other data models and apply them on
DTDs/XSDs, but they exploit specific XML features (e.g., the hierarchical
structure of a DTD/XSD) to improve the performance of the Schema Matching
process. The design of XML Matchers is currently a well-established research
area. The main goal of this paper is to provide a detailed description and
classification of XML Matchers. We first describe to what extent the
specificities of DTDs/XSDs impact on the Schema Matching task. Then we
introduce a template, called XML Matcher Template, that describes the main
components of an XML Matcher, their role and behavior. We illustrate how each
of these components has been implemented in some popular XML Matchers. We
consider our XML Matcher Template as the baseline for objectively comparing
approaches that, at first glance, might appear as unrelated. The introduction
of this template can be useful in the design of future XML Matchers. Finally,
we analyze commercial tools implementing XML Matchers and introduce two
challenging issues strictly related to this topic, namely XML source clustering
and uncertainty management in XML Matchers.



We attack the problem of learning face models for public faces from
weakly-labelled images collected from web through querying a name. The data is
very noisy even after face detection, with several irrelevant faces
corresponding to other people. We propose a novel method, Face Association
through Model Evolution (FAME), that is able to prune the data in an iterative
way, for the face models associated to a name to evolve. The idea is based on
capturing discriminativeness and representativeness of each instance and
eliminating the outliers. The final models are used to classify faces on novel
datasets with possibly different characteristics. On benchmark datasets, our
results are comparable to or better than state-of-the-art studies for the task
of face identification.



How do we allocate scarcere sources? How do we fairly allocate costs? These
are two pressing challenges facing society today. I discuss two recent projects
at NICTA concerning resource and cost allocation. In the first, we have been
working with FoodBank Local, a social startup working in collaboration with
food bank charities around the world to optimise the logistics of collecting
and distributing donated food. Before we can distribute this food, we must
decide how to allocate it to different charities and food kitchens. This gives
rise to a fair division problem with several new dimensions, rarely considered
in the literature. In the second, we have been looking at cost allocation
within the distribution network of a large multinational company. This also has
several new dimensions rarely considered in the literature.



We study computational aspects of three prominent voting rules that use
approval ballots to elect multiple winners. These rules are satisfaction
approval voting, proportional approval voting, and reweighted approval voting.
We first show that computing the winner for proportional approval voting is
NP-hard, closing a long standing open problem. As none of the rules are
strategyproof, even for dichotomous preferences, we study various strategic
aspects of the rules. In particular, we examine the computational complexity of
computing a best response for both a single agent and a group of agents. In
many settings, we show that it is NP-hard for an agent or agents to compute how
best to vote given a fixed set of approval ballots from the other agents.



An originally chaotic system can be controlled into various periodic
dynamics. When it is implemented into a legged robot's locomotion control as a
central pattern generator (CPG), sophisticated gait patterns arise so that the
robot can perform various walking behaviors. However, such a single chaotic CPG
controller has difficulties dealing with leg malfunction. Specifically, in the
scenarios presented here, its movement permanently deviates from the desired
trajectory. To address this problem, we extend the single chaotic CPG to
multiple CPGs with learning. The learning mechanism is based on a simulated
annealing algorithm. In a normal situation, the CPGs synchronize and their
dynamics are identical. With leg malfunction or disability, the CPGs lose
synchronization leading to independent dynamics. In this case, the learning
mechanism is applied to automatically adjust the remaining legs' oscillation
frequencies so that the robot adapts its locomotion to deal with the
malfunction. As a consequence, the trajectory produced by the multiple chaotic
CPGs resembles the original trajectory far better than the one produced by only
a single CPG. The performance of the system is evaluated first in a physical
simulation of a quadruped as well as a hexapod robot and finally in a real
six-legged walking machine called AMOSII. The experimental results presented
here reveal that using multiple CPGs with learning is an effective approach for
adaptive locomotion generation where, for instance, different body parts have
to perform independent movements for malfunction compensation.



Bayesian probability theory is one of the most successful frameworks to model
reasoning under uncertainty. Its defining property is the interpretation of
probabilities as degrees of belief in propositions about the state of the world
relative to an inquiring subject. This essay examines the notion of
subjectivity by drawing parallels between Lacanian theory and Bayesian
probability theory, and concludes that the latter must be enriched with causal
interventions to model agency. The central contribution of this work is an
abstract model of the subject that accommodates causal interventions in a
measure-theoretic formalisation. This formalisation is obtained through a
game-theoretic Ansatz based on modelling the inside and outside of the subject
as an extensive-form game with imperfect information between two players.
Finally, I illustrate the expressiveness of this model with an example of
causal induction.



Out of thousands of names to choose from, picking the right one for your
child is a daunting task. In this work, our objective is to help parents making
an informed decision while choosing a name for their baby. We follow a
recommender system approach and combine, in an ensemble, the individual
rankings produced by simple collaborative filtering algorithms in order to
produce a personalized list of names that meets the individual parents' taste.
Our experiments were conducted using real-world data collected from the query
logs of 'nameling' (nameling.net), an online portal for searching and exploring
names, which corresponds to the dataset released in the context of the ECML
PKDD Discover Challenge 2013. Our approach is intuitive, easy to implement, and
features fast training and prediction steps.



In this paper, we present an ontology of mathematical knowledge concepts that
covers a wide range of the fields of mathematics and introduces a balanced
representation between comprehensive and sensible models. We demonstrate the
applications of this representation in information extraction, semantic search,
and education. We argue that the ontology can be a core of future integration
of math-aware data sets in the Web of Data and, therefore, provide mappings
onto relevant datasets, such as DBpedia and ScienceWISE.



Kernel-based reinforcement learning (KBRL) stands out among reinforcement
learning algorithms for its strong theoretical guarantees. By casting the
learning problem as a local kernel approximation, KBRL provides a way of
computing a decision policy which is statistically consistent and converges to
a unique solution. Unfortunately, the model constructed by KBRL grows with the
number of sample transitions, resulting in a computational cost that precludes
its application to large-scale or on-line domains. In this paper we introduce
an algorithm that turns KBRL into a practical reinforcement learning tool.
Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a
transition matrix is represented as the product of two stochastic matrices, one
can swap the factors of the multiplication to obtain another transition matrix,
potentially much smaller, which retains some fundamental properties of its
precursor. KBSF exploits such an insight to compress the information contained
in KBRL's model into an approximator of fixed size. This makes it possible to
build an approximation that takes into account both the difficulty of the
problem and the associated computational cost. KBSF's computational complexity
is linear in the number of sample transitions, which is the best one can do
without discarding data. Moreover, the algorithm's simple mechanics allow for a
fully incremental implementation that makes the amount of memory used
independent of the number of sample transitions. The result is a kernel-based
reinforcement learning algorithm that can be applied to large-scale problems in
both off-line and on-line regimes. We derive upper bounds for the distance
between the value functions computed by KBRL and KBSF using the same data. We
also illustrate the potential of our algorithm in an extensive empirical study
in which KBSF is applied to difficult tasks based on real-world data.



Counterexample-guided inductive synthesis CEGIS is used to synthesize
programs from a candidate space of programs. The technique is guaranteed to
terminate and synthesize the correct program if the space of candidate programs
is finite. But the technique may or may not terminate with the correct program
if the candidate space of programs is infinite. In this paper, we perform a
theoretical analysis of counterexample-guided inductive synthesis technique. We
investigate whether the set of candidate spaces for which the correct program
can be synthesized using CEGIS depends on the counterexamples used in inductive
synthesis, that is, whether there are good mistakes which would increase the
synthesis power. We investigate whether the use of minimal counterexamples
instead of arbitrary counterexamples expands the set of candidate spaces of
programs for which inductive synthesis can successfully synthesize a correct
program. We consider two kinds of counterexamples: minimal counterexamples and
history bounded counterexamples. The history bounded counterexample used in any
iteration of CEGIS is bounded by the examples used in previous iterations of
inductive synthesis. We examine the relative change in power of inductive
synthesis in both cases. We show that the synthesis technique using minimal
counterexamples MinCEGIS has the same synthesis power as CEGIS but the
synthesis technique using history bounded counterexamples HCEGIS has different
power than that of CEGIS, but none dominates the other.



The \emph{maximum a posteriori} (MAP) assignment for general structure Markov
random fields (MRFs) is computationally intractable. In this paper, we exploit
tree-based methods to efficiently address this problem. Our novel method, named
Tree-based Iterated Local Search (T-ILS) takes advantage of the tractability of
tree-structures embedded within MRFs to derive strong local search in an ILS
framework. The method efficiently explores exponentially large neighborhood and
does so with limited memory without any requirement on the cost functions. We
evaluate the T-ILS in a simulation of Ising model and two real-world problems
in computer vision: stereo matching, image denoising. Experimental results
demonstrate that our methods are competitive against state-of-the-art rivals
with a significant computational gain.



Particle swarm optimization is used in several combinatorial optimization
problems. In this work, particle swarms are used to solve quadratic programming
problems with quadratic constraints. The approach of particle swarms is an
example for interior point methods in optimization as an iterative technique.
This approach is novel and deals with classification problems without the use
of a traditional classifier. Our method determines the optimal hyperplane or
classification boundary for a data set. In a binary classification problem, we
constrain each class as a cluster, which is enclosed by an ellipsoid. The
estimation of the optimal hyperplane between the two clusters is posed as a
quadratically constrained quadratic problem. The optimization problem is solved
in distributed format using modified particle swarms. Our method has the
advantage of using the direction towards optimal solution rather than searching
the entire feasible region. Our results on the Iris, Pima, Wine, and Thyroid
datasets show that the proposed method works better than a neural network and
the performance is close to that of SVM.



In this paper we introduce an evolutionary algorithm for the solution of
linear integer programs. The strategy is based on the separation of the
variables into the integer subset and the continuous subset; the integer
variables are fixed by the evolutionary system, and the continuous ones are
determined in function of them, by a linear program solver.
  We report results obtained for some standard benchmark problems, and compare
them with those obtained by branch-and-bound. The performance of the
evolutionary algorithm is promising. Good feasible solutions were generally
obtained, and in some of the difficult benchmark tests it outperformed
branch-and-bound.



This paper deals with the relations among structural, topological, and
chemical properties of the E.Coli proteome from the vantage point of the
solubility/aggregation propensity of proteins. Each E.Coli protein is initially
represented according to its known folded 3D shape. This step consists in
representing the available E.Coli proteins in terms of graphs. We first analyze
those graphs by considering pure topological characterizations, i.e., by
analyzing the mass fractal dimension and the distribution underlying both
shortest paths and vertex degrees. Results confirm the general architectural
principles of proteins. Successively, we focus on the statistical properties of
a representation of such graphs in terms of vectors composed of several
numerical features, which we extracted from their structural representation. We
found that protein size is the main discriminator for the solubility, while
however there are other factors that help explaining the solubility degree. We
finally analyze such data through a novel one-class classifier, with the aim of
discriminating among very and poorly soluble proteins. Results are encouraging
and consolidate the potential of pattern recognition techniques when employed
to describe complex biological systems.



Bots are, for many Web and social media users, the source of many dangerous
attacks or the carrier of unwanted messages, such as spam. Nevertheless,
crawlers and software agents are a precious tool for analysts, and they are
continuously executed to collect data or to test distributed applications.
However, no one knows which is the real potential of a bot whose purpose is to
control a community, to manipulate consensus, or to influence user behavior. It
is commonly believed that the better an agent simulates human behavior in a
social network, the more it can succeed to generate an impact in that
community. We contribute to shed light on this issue through an online social
experiment aimed to study to what extent a bot with no trust, no profile, and
no aims to reproduce human behavior, can become popular and influential in a
social media. Results show that a basic social probing activity can be used to
acquire social relevance on the network and that the so-acquired popularity can
be effectively leveraged to drive users in their social connectivity choices.
We also register that our bot activity unveiled hidden social polarization
patterns in the community and triggered an emotional response of individuals
that brings to light subtle privacy hazards perceived by the user base.



Many large-scale machine learning problems--clustering, non-parametric
learning, kernel machines, etc.--require selecting a small yet representative
subset from a large dataset. Such problems can often be reduced to maximizing a
submodular set function subject to various constraints. Classical approaches to
submodular optimization require centralized access to the full dataset, which
is impractical for truly large-scale problems. In this paper, we consider the
problem of submodular function maximization in a distributed fashion. We
develop a simple, two-stage protocol GreeDi, that is easily implemented using
MapReduce style computations. We theoretically analyze our approach, and show
that under certain natural conditions, performance close to the centralized
approach can be achieved. We begin with monotone submodular maximization
subject to a cardinality constraint, and then extend this approach to obtain
approximation guarantees for (not necessarily monotone) submodular maximization
subject to more general constraints including matroid or knapsack constraints.
In our extensive experiments, we demonstrate the effectiveness of our approach
on several applications, including sparse Gaussian process inference and
exemplar based clustering on tens of millions of examples using Hadoop.



Recent development in developing humanoid robot poses new challenges to
human-machine interaction communication. A major challenge is to develop robots
that can behave like and interact with human in the most natural way possible.
This paper proposes a system to develop a robot that can receive command, and
talk to people in natural language. In addition, the robot can also be
"trained" to become an expert in sepcific areas to provide expert advice to
human-beings. Most important of all, the robot can display emotions through
facial expression, speech and gesture so that the interaction process will
become more comprehensive and compelling.



Generative Adversarial Nets [8] were recently introduced as a novel way to
train generative models. In this work we introduce the conditional version of
generative adversarial nets, which can be constructed by simply feeding the
data, y, we wish to condition on to both the generator and discriminator. We
show that this model can generate MNIST digits conditioned on class labels. We
also illustrate how this model could be used to learn a multi-modal model, and
provide preliminary examples of an application to image tagging in which we
demonstrate how this approach can generate descriptive tags which are not part
of training labels.



The dynamics of belief and knowledge is one of the major components of any
autonomous system that should be able to incorporate new pieces of information.
We show that knowledge base dynamics has interesting connection with kernel
change via hitting set and abduction. The approach extends and integrates
standard techniques for efficient query answering and integrity checking. The
generation of hitting set is carried out through a hyper tableaux calculus and
magic set that is focused on the goal of minimality. Many different view update
algorithms have been proposed in the literature to address this problem. The
present paper provides a comparative study of view update algorithms in
rational approach.



Answering conjunctive queries (CQs) over $\mathcal{EL}$ knowledge bases (KBs)
with complex role inclusions is PSPACE-hard and in PSPACE in certain cases;
however, if complex role inclusions are restricted to role transitivity, the
tight upper complexity bound has so far been unknown. Furthermore, the existing
algorithms cannot handle reflexive roles, and they are not practicable.
Finally, the problem is tractable for acyclic CQs and $\mathcal{ELH}$, and
NP-complete for unrestricted CQs and $\mathcal{ELHO}$ KBs. In this paper we
complete the complexity landscape of CQ answering for several important cases.
In particular, we present a practicable NP algorithm for answering CQs over
$\mathcal{ELHO}^s$ KBs---a logic containing all of OWL 2 EL, but with complex
role inclusions restricted to role transitivity. Our preliminary evaluation
suggests that the algorithm can be suitable for practical use. Moreover, we
show that, even for a restricted class of so-called arborescent acyclic
queries, CQ answering over $\mathcal{EL}$ KBs becomes NP-hard in the presence
of either transitive or reflexive roles. Finally, we show that answering
arborescent CQs over $\mathcal{ELHO}$ KBs is tractable, whereas answering
acyclic CQs is NP-hard.



Given empirical evidence for the dependence of an outcome variable on an
exposure variable, we can typically only provide bounds for the "probability of
causation" in the case of an individual who has developed the outcome after
being exposed. We show how these bounds can be adapted or improved if further
information becomes available. In addition to reviewing existing work on this
topic, we provide a new analysis for the case where a mediating variable can be
observed. In particular we show how the probability of causation can be bounded
when there is no direct effect and no confounding.
  Keywords: Causal inference, Mediation Analysis, Probability of Causation



We propose a framework for inferring the latent attitudes or preferences of
users by performing probabilistic first-order logical reasoning over the social
network graph. Our method answers questions about Twitter users like {\em Does
this user like sushi?} or {\em Is this user a New York Knicks fan?} by building
a probabilistic model that reasons over user attributes (the user's location or
gender) and the social network (the user's friends and spouse), via inferences
like homophily (I am more likely to like sushi if spouse or friends like sushi,
I am more likely to like the Knicks if I live in New York). The algorithm uses
distant supervision, semi-supervised data harvesting and vector space models to
extract user attributes (e.g. spouse, education, location) and preferences
(likes and dislikes) from text. The extracted propositions are then fed into a
probabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft
Logic). Our experiments show that probabilistic logical reasoning significantly
improves the performance on attribute and relation extraction, and also
achieves an F-score of 0.791 at predicting a users likes or dislikes,
significantly better than two strong baselines.



Rewriting is widely used to optimise owl:sameAs reasoning in materialisation
based OWL 2 RL systems. We investigate issues related to both the correctness
and efficiency of rewriting, and present an algorithm that guarantees
correctness, improves efficiency, and can be effectively parallelised. Our
evaluation shows that our approach can reduce reasoning times on practical data
sets by orders of magnitude.



We propose a scalable temporal latent space model for link prediction in
dynamic social networks, where the goal is to predict links over time based on
a sequence of previous graph snapshots. The model assumes that each user lies
in an unobserved latent space and interactions are more likely to form between
similar users in the latent space representation. In addition, the model allows
each user to gradually move its position in the latent space as the network
structure evolves over time. We present a global optimization algorithm to
effectively infer the temporal latent space, with a quadratic convergence rate.
Two alternative optimization algorithms with local and incremental updates are
also proposed, allowing the model to scale to larger networks without
compromising prediction accuracy. Empirically, we demonstrate that our model,
when evaluated on a number of real-world dynamic networks, significantly
outperforms existing approaches for temporal link prediction in terms of both
scalability and predictive power.



The paper presents an approach to verification of a multi-agent data analysis
algorithm. We base correct simulation of the multi-agent system by a finite
integer model. For verification we use model checking tool SPIN. Protocols of
agents are written in Promela language and properties of the multi-agent data
analysis system are expressed in logic LTL. We run several experiments with
SPIN and the model.



In this paper fuzzy VRPTW with an uncertain travel time is considered.
Credibility theory is used to model the problem and specifies a preference
index at which it is desired that the travel times to reach the customers fall
into their time windows. We propose the integration of fuzzy and ant colony
system based evolutionary algorithm to solve the problem while preserving the
constraints. Computational results for certain benchmark problems having short
and long time horizons are presented to show the effectiveness of the
algorithm. Comparison between different preferences indexes have been obtained
to help the user in making suitable decisions.



The automatic design of controllers for mobile robots usually requires two
stages. In the first stage,sensorial data are preprocessed or transformed into
high level and meaningful values of variables whichare usually defined from
expert knowledge. In the second stage, a machine learning technique is applied
toobtain a controller that maps these high level variables to the control
commands that are actually sent tothe robot. This paper describes an algorithm
that is able to embed the preprocessing stage into the learningstage in order
to get controllers directly starting from sensorial raw data with no expert
knowledgeinvolved. Due to the high dimensionality of the sensorial data, this
approach uses Quantified Fuzzy Rules(QFRs), that are able to transform
low-level input variables into high-level input variables, reducingthe
dimensionality through summarization. The proposed learning algorithm, called
Iterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic
programming. IQFRL is able to learn rules with differentstructures, and can
manage linguistic variables with multiple granularities. The algorithm has been
testedwith the implementation of the wall-following behavior both in several
realistic simulated environmentswith different complexity and on a Pioneer 3-AT
robot in two real environments. Results have beencompared with several
well-known learning algorithms combined with different data
preprocessingtechniques, showing that IQFRL exhibits a better and statistically
significant performance. Moreover,three real world applications for which IQFRL
plays a central role are also presented: path and objecttracking with static
and moving obstacles avoidance.



We propose and analyze estimators for statistical functionals of one or more
distributions under nonparametric assumptions. Our estimators are based on the
theory of influence functions, which appear in the semiparametric statistics
literature. We show that estimators based either on data-splitting or a
leave-one-out technique enjoy fast rates of convergence and other favorable
theoretical properties. We apply this framework to derive estimators for
several popular information theoretic quantities, and via empirical evaluation,
show the advantage of this approach over existing estimators.



Graph partitioning, a well studied problem of parallel computing has many
applications in diversified fields such as distributed computing, social
network analysis, data mining and many other domains. In this paper, we
introduce FGPGA, an efficient genetic approach for producing feasible graph
partitions. Our method takes into account the heterogeneity and capacity
constraints of the partitions to ensure balanced partitioning. Such approach
has various applications in mobile cloud computing that include feasible
deployment of software applications on the more resourceful infrastructure in
the cloud instead of mobile hand set. Our proposed approach is light weight and
hence suitable for use in cloud architecture. We ensure feasibility of the
partitions generated by not allowing over-sized partitions to be generated
during the initialization and search. Our proposed method tested on standard
benchmark datasets significantly outperforms the state-of-the-art methods in
terms of quality of partitions and feasibility of the solutions.



We study the extension of relational multiagent systems (RMASs), where agents
manipulate full-fledged relational databases, with data types and facets
equipped with domain-specific, rigid relations (such as total orders).
Specifically, we focus on design-time verification of RMASs against rich
first-order temporal properties expressed in a variant of first-order
mu-calculus with quantification across states. We build on previous
decidability results under the "state-bounded" assumption, i.e., in each single
state only a bounded number of data objects is stored in the agent databases,
while unboundedly many can be encountered over time. We recast this condition,
showing decidability in presence of dense, linear orders, and facets defined on
top of them. Our approach is based on the construction of a finite-state, sound
and complete abstraction of the original system, in which dense linear orders
are reformulated as non-rigid relations working on the active domain of the
system only. We also show undecidability when including a data type equipped
with the successor relation.



We investigate the potential of using ordinal peer grading for the evaluation
of students in massive online open courses (MOOCs). According to such grading
schemes, each student receives a few assignments (by other students) which she
has to rank. Then, a global ranking (possibly translated into numerical scores)
is produced by combining the individual ones. This is a novel application area
for social choice concepts and methods where the important problem to be solved
is as follows: how should the assignments be distributed so that the collected
individual rankings can be easily merged into a global one that is as close as
possible to the ranking that represents the relative performance of the
students in the assignment? Our main theoretical result suggests that using
very simple ways to distribute the assignments so that each student has to rank
only $k$ of them, a Borda-like aggregation method can recover a $1-O(1/k)$
fraction of the true ranking when each student correctly ranks the assignments
she receives. Experimental results strengthen our analysis further and also
demonstrate that the same method is extremely robust even when students have
imperfect capabilities as graders. We believe that our results provide strong
evidence that ordinal peer grading can be a highly effective and scalable
solution for evaluation in MOOCs.



A wide range of evidence points toward the existence of a common algorithm
underlying the processing of information throughout the cerebral cortex.
Several hypothesized features of this cortical algorithm are reviewed,
including sparse distributed representation, Bayesian inference, hierarchical
organization composed of alternating template matching and pooling layers,
temporal slowness and predictive coding. Hierarchical Temporal Memory (HTM) is
a family of learning algorithms and corresponding theories of cortical function
that embodies these principles. HTM has previously been applied mainly to
perceptual tasks typical of posterior cortex. In order to evaluate HTM as a
candidate model of cortical function, it is necessary also to investigate its
compatibility with the requirements of frontal cortical function. To this end,
a variety of models of frontal cortical function are reviewed and integrated,
to arrive at the hypothesis that frontal functions including attention, working
memory and action selection depend largely upon the same basic algorithms as do
posterior functions, with the notable additions of a mechanism for the active
maintenance of representations and of multiple cortico-striato-thalamo-cortical
loops that allow communication between regions of frontal cortex to be gated in
an adaptive manner. Computational models of this system are reviewed. Finally,
there is a discussion of how HTM can contribute to the understanding of frontal
cortical function, and of what the requirements of frontal cortical function
mean for the future development of HTM.



Finite chase, or alternatively chase termination, is an important condition
to ensure the decidability of existential rule languages. In the past few
years, a number of rule languages with finite chase have been studied. In this
work, we propose a novel approach for classifying the rule languages with
finite chase. Using this approach, a family of decidable rule languages, which
extend the existing languages with the finite chase property, are naturally
defined. We then study the complexity of these languages. Although all of them
are tractable for data complexity, we show that their combined complexity can
be arbitrarily high. Furthermore, we prove that all the rule languages with
finite chase that extend the weakly acyclic language are of the same
expressiveness as the weakly acyclic one, while rule languages with higher
combined complexity are in general more succinct than those with lower combined
complexity.



This paper describes a new information-theoretic policy evaluation technique
for reinforcement learning. This technique converts any compression or density
model into a corresponding estimate of value. Under appropriate stationarity
and ergodicity conditions, we show that the use of a sufficiently powerful
model gives rise to a consistent value function estimator. We also study the
behavior of this technique when applied to various Atari 2600 video games,
where the use of suboptimal modeling techniques is unavoidable. We consider
three fundamentally different models, all too limited to perfectly model the
dynamics of the system. Remarkably, we find that our technique provides
sufficiently accurate value estimates for effective on-policy control. We
conclude with a suggestive study highlighting the potential of our technique to
scale to large problems.



Discovering visual knowledge from weakly labeled data is crucial to scale up
computer vision recognition system, since it is expensive to obtain fully
labeled data for a large number of concept categories. In this paper, we
propose ConceptLearner, which is a scalable approach to discover visual
concepts from weakly labeled image collections. Thousands of visual concept
detectors are learned automatically, without human in the loop for additional
annotation. We show that these learned detectors could be applied to recognize
concepts at image-level and to detect concepts at image region-level
accurately. Under domain-specific supervision, we further evaluate the learned
concepts for scene recognition on SUN database and for object detection on
Pascal VOC 2007. ConceptLearner shows promising performance compared to fully
supervised and weakly supervised methods.



In this paper we explore the bi-directional mapping between images and their
sentence-based descriptions. We propose learning this mapping using a recurrent
neural network. Unlike previous approaches that map both sentences and images
to a common embedding, we enable the generation of novel sentences given an
image. Using the same model, we can also reconstruct the visual features
associated with an image given its visual description. We use a novel recurrent
visual memory that automatically learns to remember long-term visual concepts
to aid in both sentence generation and visual feature reconstruction. We
evaluate our approach on several tasks. These include sentence generation,
sentence retrieval and image retrieval. State-of-the-art results are shown for
the task of generating novel image descriptions. When compared to human
generated captions, our automatically generated captions are preferred by
humans over $19.8\%$ of the time. Results are better than or comparable to
state-of-the-art results on the image and sentence retrieval tasks for methods
using similar visual features.



Detecting and segmenting salient objects in natural scenes, often referred to
as salient object detection, has attracted a lot of interest in computer
vision. While many models have been proposed and several applications have
emerged, yet a deep understanding of achievements and issues is lacking. We aim
to provide a comprehensive review of the recent progress in salient object
detection and situate this field among other closely related areas such as
generic scene segmentation, object proposal generation, and saliency for
fixation prediction. Covering 228 publications, we survey i) roots, key
concepts, and tasks, ii) core techniques and main modeling trends, and iii)
datasets and evaluation metrics in salient object detection. We also discuss
open problems such as evaluation metrics and dataset bias in model performance
and suggest future research directions.



We present Dynamic Epistemic Temporal Logic, a framework for reasoning about
operations on multi-agent Kripke models that contain a designated temporal
relation. These operations are natural extensions of the well-known "action
models" from Dynamic Epistemic Logic. Our "temporal action models" may be used
to define a number of informational actions that can modify the "objective"
temporal structure of a model along with the agents' basic and higher-order
knowledge and beliefs about this structure, including their beliefs about the
time. In essence, this approach provides one way to extend the domain of action
model-style operations from atemporal Kripke models to temporal Kripke models
in a manner that allows actions to control the flow of time. We present a
number of examples to illustrate the subtleties involved in interpreting the
effects of our extended action models on temporal Kripke models. We also study
preservation of important epistemic-temporal properties of temporal Kripke
models under temporal action model-induced operations, provide complete
axiomatizations for two theories of temporal action models, and connect our
approach with previous work on time in Dynamic Epistemic Logic.



We propose a novel diverse feature selection method based on determinantal
point processes (DPPs). Our model enables one to flexibly define diversity
based on the covariance of features (similar to orthogonal matching pursuit) or
alternatively based on side information. We introduce our approach in the
context of Bayesian sparse regression, employing a DPP as a variational
approximation to the true spike and slab posterior distribution. We
subsequently show how this variational DPP approximation generalizes and
extends mean-field approximation, and can be learned efficiently by exploiting
the fast sampling properties of DPPs. Our motivating application comes from
bioinformatics, where we aim to identify a diverse set of genes whose
expression profiles predict a tumor type where the diversity is defined with
respect to a gene-gene interaction network. We also explore an application in
spatial statistics. In both cases, we demonstrate that the proposed method
yields significantly more diverse feature sets than classic sparse methods,
without compromising accuracy.



More users and companies make use of cloud services every day. They all
expect a perfect performance and any issue to remain transparent to them. This
last statement is very challenging to perform. A user's activities in our cloud
can affect the overall performance of our servers, having an impact on other
resources. We can consider these kind of activities as fraudulent. They can be
either illegal activities, such as launching a DDoS attack or just activities
which are undesired by the cloud provider, such as Bitcoin mining, which uses
substantial power, reduces the life of the hardware and can possibly slow down
other user's activities. This article discusses a method to detect such
activities by using non-intrusive, privacy-friendly data: billing data. We use
OpenStack as an example with data provided by Telemetry, the component in
charge of measuring resource usage for billing purposes. Results will be shown
proving the efficiency of this method and ways to improve it will be provided
as well as its advantages and disadvantages.



This paper deals with the problem of neural code solving. On the basis of the
formulated hypotheses the information model of a neuron-detector is suggested,
the detector being one of the basic elements of an artificial neural network
(ANN). The paper subjects the connectionist paradigm of ANN building to
criticism and suggests a new presentation paradigm for ANN building and
neuroelements (NE) learning. The adequacy of the suggested model is proved by
the fact that is does not contradict the modern propositions of neuropsychology
and neurophysiology.



Identifying important components or factors in large amounts of noisy data is
a key problem in machine learning and data mining. Motivated by a pattern
decomposition problem in materials discovery, aimed at discovering new
materials for renewable energy, e.g. for fuel and solar cells, we introduce
CombiFD, a framework for factor based pattern decomposition that allows the
incorporation of a-priori knowledge as constraints, including complex
combinatorial constraints. In addition, we propose a new pattern decomposition
algorithm, called AMIQO, based on solving a sequence of (mixed-integer)
quadratic programs. Our approach considerably outperforms the state of the art
on the materials discovery problem, scaling to larger datasets and recovering
more precise and physically meaningful decompositions. We also show the
effectiveness of our approach for enforcing background knowledge on other
application domains.



We study the computational complexity of candidate control in elections with
few voters, that is, we consider the parameterized complexity of candidate
control in elections with respect to the number of voters as a parameter. We
consider both the standard scenario of adding and deleting candidates, where
one asks whether a given candidate can become a winner (or, in the destructive
case, can be precluded from winning) by adding or deleting few candidates, as
well as a combinatorial scenario where adding/deleting a candidate
automatically means adding or deleting a whole group of candidates. Considering
several fundamental voting rules, our results show that the parameterized
complexity of candidate control, with the number of voters as the parameter, is
much more varied than in the setting with many voters.



Within the Kolmogorov theory of probability, Bayes' rule allows one to
perform statistical inference by relating conditional probabilities to
unconditional probabilities. As we show here, however, there is a continuous
set of alternative inference rules that yield the same results, and that may
have computational or practical advantages for certain problems. We formulate
generalized axioms for probability theory, according to which the reverse
conditional probability distribution P(B|A) is not specified by the forward
conditional probability distribution P(A|B) and the marginals P(A) and P(B).
Thus, in order to perform statistical inference, one must specify an additional
"inference axiom," which relates P(B|A) to P(A|B), P(A), and P(B). We show that
when Bayes' rule is chosen as the inference axiom, the axioms are equivalent to
the classical Kolmogorov axioms. We then derive consistency conditions on the
inference axiom, and thereby characterize the set of all possible rules for
inference. The set of "first-order" inference axioms, defined as the set of
axioms in which P(B|A) depends on the first power of P(A|B), is found to be a
1-simplex, with Bayes' rule at one of the extreme points. The other extreme
point, the "inversion rule," is studied in depth.



We propose a novel online learning method for minimizing regret in large
extensive-form games. The approach learns a function approximator online to
estimate the regret for choosing a particular action. A no-regret algorithm
uses these estimates in place of the true regrets to define a sequence of
policies.
  We prove the approach sound by providing a bound relating the quality of the
function approximation and regret of the algorithm. A corollary being that the
method is guaranteed to converge to a Nash equilibrium in self-play so long as
the regrets are ultimately realizable by the function approximator. Our
technique can be understood as a principled generalization of existing work on
abstraction in large games; in our work, both the abstraction as well as the
equilibrium are learned during self-play. We demonstrate empirically the method
achieves higher quality strategies than state-of-the-art abstraction techniques
given the same resources.



A database of fetal heart rate (FHR) time series measured from 7221 patients
during labor is analyzed with the aim of learning the types of features of
these recordings that are informative of low cord pH. Our 'highly comparative'
analysis involves extracting over 9000 time-series analysis features from each
FHR time series, including measures of autocorrelation, entropy, distribution,
and various model fits. This diverse collection of features was developed in
previous work, and is publicly available. We describe five features that most
accurately classify a balanced training set of 59 'low pH' and 59 'normal pH'
FHR recordings. We then describe five of the features with the strongest linear
correlation to cord pH across the full dataset of FHR time series. The features
identified in this work may be used as part of a system for guiding
intervention during labor in future. This work successfully demonstrates the
utility of comparing across a large, interdisciplinary literature on
time-series analysis to automatically contribute new scientific results for
specific biomedical signal processing challenges.



The FO Model Counting problem (FOMC) is the following: given a sentence
$\Phi$ in FO and a number $n$, compute the number of models of $\Phi$ over a
domain of size $n$; the Weighted variant (WFOMC) generalizes the problem by
associating a weight to each tuple and defining the weight of a model to be the
product of weights of its tuples. In this paper we study the complexity of the
symmetric WFOMC, where all tuples of a given relation have the same weight. Our
motivation comes from an important application, inference in Knowledge Bases
with soft constraints, like Markov Logic Networks, but the problem is also of
independent theoretical interest. We study both the data complexity, and the
combined complexity of FOMC and WFOMC. For the data complexity we prove the
existence of an FO$^{3}$ formula for which FOMC is #P$_1$-complete, and the
existence of a Conjunctive Query for which WFOMC is #P$_1$-complete. We also
prove that all $\gamma$-acyclic queries have polynomial time data complexity.
For the combined complexity, we prove that, for every fragment FO$^{k}$, $k\geq
2$, the combined complexity of FOMC (or WFOMC) is #P-complete.



Deep neural networks (DNNs) have recently been achieving state-of-the-art
performance on a variety of pattern-recognition tasks, most notably visual
classification problems. Given that DNNs are now able to classify objects in
images with near-human-level performance, questions naturally arise as to what
differences remain between computer and human vision. A recent study revealed
that changing an image (e.g. of a lion) in a way imperceptible to humans can
cause a DNN to label the image as something else entirely (e.g. mislabeling a
lion a library). Here we show a related result: it is easy to produce images
that are completely unrecognizable to humans, but that state-of-the-art DNNs
believe to be recognizable objects with 99.99% confidence (e.g. labeling with
certainty that white noise static is a lion). Specifically, we take
convolutional neural networks trained to perform well on either the ImageNet or
MNIST datasets and then find images with evolutionary algorithms or gradient
ascent that DNNs label with high confidence as belonging to each dataset class.
It is possible to produce images totally unrecognizable to human eyes that DNNs
believe with near certainty are familiar objects, which we call "fooling
images" (more generally, fooling examples). Our results shed light on
interesting differences between human vision and current DNNs, and raise
questions about the generality of DNN computer vision.



In this paper we present a non-invasive ambient intelligence framework for
the semi-automatic analysis of non-verbal communication applied to the
restorative justice field. In particular, we propose the use of computer vision
and social signal processing technologies in real scenarios of Victim-Offender
Mediations, applying feature extraction techniques to multi-modal
audio-RGB-depth data. We compute a set of behavioral indicators that define
communicative cues from the fields of psychology and observational methodology.
We test our methodology on data captured in real world Victim-Offender
Mediation sessions in Catalonia in collaboration with the regional government.
We define the ground truth based on expert opinions when annotating the
observed social responses. Using different state-of-the-art binary
classification approaches, our system achieves recognition accuracies of 86%
when predicting satisfaction, and 79% when predicting both agreement and
receptivity. Applying a regression strategy, we obtain a mean deviation for the
predictions between 0.5 and 0.7 in the range [1-5] for the computed social
signals.



Formalisms for specifying statistical models, such as
probabilistic-programming languages, typically consist of two components: a
specification of a stochastic process (the prior), and a specification of
observations that restrict the probability space to a conditional subspace (the
posterior). Use cases of such formalisms include the development of algorithms
in machine learning and artificial intelligence. We propose and investigate a
declarative framework for specifying statistical models on top of a database,
through an appropriate extension of Datalog. By virtue of extending Datalog,
our framework offers a natural integration with the database, and has a robust
declarative semantics. Our Datalog extension provides convenient mechanisms to
include numerical probability functions; in particular, conclusions of rules
may contain values drawn from such functions. The semantics of a program is a
probability distribution over the possible outcomes of the input database with
respect to the program; these outcomes are minimal solutions with respect to a
related program with existentially quantified variables in conclusions.
Observations are naturally incorporated by means of integrity constraints over
the extensional and intensional relations. We focus on programs that use
discrete numerical distributions, but even then the space of possible outcomes
may be uncountable (as a solution can be infinite). We define a probability
measure over possible outcomes by applying the known concept of cylinder sets
to a probabilistic chase procedure. We show that the resulting semantics is
robust under different chases. We also identify conditions guaranteeing that
all possible outcomes are finite (and then the probability space is discrete).
We argue that the framework we propose retains the purely declarative nature of
Datalog, and allows for natural specifications of statistical models.



We provide a rigorous definition of the visual cause of a behavior that is
broadly applicable to the visually driven behavior in humans, animals, neurons,
robots and other perceiving systems. Our framework generalizes standard
accounts of causal learning to settings in which the causal variables need to
be constructed from micro-variables. We prove the Causal Coarsening Theorem,
which allows us to gain causal knowledge from observational data with minimal
experimental effort. The theorem provides a connection to standard inference
techniques in machine learning that identify features of an image that
correlate with, but may not cause, the target behavior. Finally, we propose an
active learning scheme to learn a manipulator function that performs optimal
manipulations on the image to automatically identify the visual cause of a
target behavior. We illustrate our inference and learning algorithms in
experiments based on both synthetic and real data.



In recent years, adaptive learning systems rely increasingly on learning
hierarchy to customize the educational logic developed in their courses. Most
approaches do not consider that the relationships of prerequisites between the
skills are fuzzy relationships. In this article, we describe a new approach of
a practical application of fuzzy logic techniques to the construction of
learning hierarchies. For this, we use a learning hierarchy predefined by one
or more experts of a specific field. However, the relationships of
prerequisites between the skills in the learning hierarchy are not definitive
and they are fuzzy relationships. Indeed, we measure relevance degree of all
relationships existing in this learning hierarchy and we try to answer to the
following question: Is the relationships of prerequisites predefined in initial
learning hierarchy are correctly established or not?



We introduce a new approach to unsupervised estimation of feature-rich
semantic role labeling models. Our model consists of two components: (1) an
encoding component: a semantic role labeling model which predicts roles given a
rich set of syntactic and lexical features; (2) a reconstruction component: a
tensor factorization model which relies on roles to predict argument fillers.
When the components are estimated jointly to minimize errors in argument
reconstruction, the induced roles largely correspond to roles defined in
annotated resources. Our method performs on par with most accurate role
induction methods on English and German, even though, unlike these previous
approaches, we do not incorporate any prior linguistic knowledge about the
languages.



User ignorance towards the use of communication services like Instant
Messengers, emails, websites, social networks etc. is becoming the biggest
advantage for phishers. It is required to create technical awareness in users
by educating them to create a phishing detection application which would
generate phishing alerts for the user so that phishing messages are not
ignored. The lack of basic security features to detect and prevent phishing has
had a profound effect on the IM clients, as they lose their faith in e-banking
and e-commerce transactions, which will have a disastrous impact on the
corporate and banking sectors and businesses which rely heavily on the
internet. Very little research contributions were available in for phishing
detection in Instant messengers. A context based, dynamic and intelligent
phishing detection methodology in IMs is proposed, to analyze and detect
phishing in Instant Messages with relevance to domain ontology (OBIE) and
utilizes the Classification based on Association (CBA) for generating phishing
rules and alerting the victims. A PDS Monitoring system algorithm is used to
identify the phishing activity during exchange of messages in IMs, with high
ratio of precision and recall. The results have shown improvement by the
increased percentage of precision and recall when compared to the existing
methods.



We propose a practical and scalable Gaussian process model for large-scale
nonlinear probabilistic regression. Our mixture-of-experts model is
conceptually simple and hierarchically recombines computations for an overall
approximation of a full Gaussian process. Closed-form and distributed
computations allow for efficient and massive parallelisation while keeping the
memory consumption small. Given sufficient computing resources, our model can
handle arbitrarily large data sets, without explicit sparse approximations. We
provide strong experimental evidence that our model can be applied to large
data sets of sizes far beyond millions. Hence, our model has the potential to
lay the foundation for general large-scale Gaussian process research.



The computation of the global minimum energy conformation (GMEC) is an
important and challenging topic in structure-based computational protein
design. In this paper, we propose a new protein design algorithm based on the
AND/OR branch-and-bound (AOBB) search, which is a variant of the traditional
branch-and-bound search algorithm, to solve this combinatorial optimization
problem. By integrating with a powerful heuristic function, AOBB is able to
fully exploit the graph structure of the underlying residue interaction network
of a backbone template to significantly accelerate the design process. Tests on
real protein data show that our new protein design algorithm is able to solve
many prob- lems that were previously unsolvable by the traditional exact search
algorithms, and for the problems that can be solved with traditional provable
algorithms, our new method can provide a large speedup by several orders of
magnitude while still guaranteeing to find the global minimum energy
conformation (GMEC) solution.



Mastering the game of Go has remained a long standing challenge to the field
of AI. Modern computer Go systems rely on processing millions of possible
future positions to play well, but intuitively a stronger and more 'humanlike'
way to play the game would be to rely on pattern recognition abilities rather
then brute force computation. Following this sentiment, we train deep
convolutional neural networks to play Go by training them to predict the moves
made by expert Go players. To solve this problem we introduce a number of novel
techniques, including a method of tying weights in the network to 'hard code'
symmetries that are expect to exist in the target function, and demonstrate in
an ablation study they considerably improve performance. Our final networks are
able to achieve move prediction accuracies of 41.1% and 44.4% on two different
Go datasets, surpassing previous state of the art on this task by significant
margins. Additionally, while previous move prediction programs have not yielded
strong Go playing programs, we show that the networks trained in this work
acquired high levels of skill. Our convolutional neural networks can
consistently defeat the well known Go program GNU Go, indicating it is state of
the art among programs that do not use Monte Carlo Tree Search. It is also able
to win some games against state of the art Go playing program Fuego while using
a fraction of the play time. This success at playing Go indicates high level
principles of the game were learned.



We study logic for reasoning with if-then formulas describing dependencies
between attributes of objects which are observed in consecutive points in time.
We introduce semantic entailment of the formulas, show its fixed-point
characterization, investigate closure properties of model classes, present an
axiomatization and prove its completeness, and investigate alternative
axiomatizations and normalized proofs. We investigate decidability and
complexity issues of the logic and prove that the entailment problem is NP-hard
and belongs to EXPSPACE. We show that by restricting to predictive formulas,
the entailment problem is decidable in pseudo-linear time.



This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform "weight tuning" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.



The discovery of causal relationships from purely observational data is a
fundamental problem in science. The most elementary form of such a causal
discovery problem is to decide whether X causes Y or, alternatively, Y causes
X, given joint observations of two variables X, Y. An example is to decide
whether altitude causes temperature, or vice versa, given only joint
measurements of both variables. Even under the simplifying assumptions of no
confounding, no feedback loops, and no selection bias, such bivariate causal
discovery problems are challenging. Nevertheless, several approaches for
addressing those problems have been proposed in recent years. We review two
families of such methods: Additive Noise Methods (ANM) and Information
Geometric Causal Inference (IGCI). We present the benchmark CauseEffectPairs
that consists of data for 100 different cause-effect pairs selected from 37
datasets from various domains (e.g., meteorology, biology, medicine,
engineering, economy, etc.) and motivate our decisions regarding the "ground
truth" causal directions of all pairs. We evaluate the performance of several
bivariate causal discovery methods on these real-world benchmark data and in
addition on artificially simulated data. Our empirical results on real-world
data indicate that certain methods are indeed able to distinguish cause from
effect using only purely observational data, although more benchmark data would
be needed to obtain statistically significant conclusions. One of the best
performing methods overall is the additive-noise method originally proposed by
Hoyer et al. (2009), which obtains an accuracy of 63+-10 % and an AUC of
0.74+-0.05 on the real-world benchmark. As the main theoretical contribution of
this work we prove the consistency of that method.



Arriving at the complete probabilistic knowledge of a domain, i.e., learning
how all variables interact, is indeed a demanding task. In reality, settings
often arise for which an individual merely possesses partial knowledge of the
domain, and yet, is expected to give adequate answers to a variety of posed
queries. That is, although precise answers to some queries, in principle,
cannot be achieved, a range of plausible answers is attainable for each query
given the available partial knowledge. In this paper, we propose the
Multi-Context Model (MCM), a new graphical model to represent the state of
partial knowledge as to a domain. MCM is a middle ground between Probabilistic
Logic, Bayesian Logic, and Probabilistic Graphical Models. For this model we
discuss: (i) the dynamics of constructing a contradiction-free MCM, i.e., to
form partial beliefs regarding a domain in a gradual and probabilistically
consistent way, and (ii) how to perform inference, i.e., to evaluate a
probability of interest involving some variables of the domain.



A key element in transfer learning is representation learning; if
representations can be developed that expose the relevant factors underlying
the data, then new tasks and domains can be learned readily based on mappings
of these salient factors. We propose that an important aim for these
representations are to be unbiased. Different forms of representation learning
can be derived from alternative definitions of unwanted bias, e.g., bias to
particular tasks, domains, or irrelevant underlying data dimensions. One very
useful approach to estimating the amount of bias in a representation comes from
maximum mean discrepancy (MMD) [5], a measure of distance between probability
distributions. We are not the first to suggest that MMD can be a useful
criterion in developing representations that apply across multiple domains or
tasks [1]. However, in this paper we describe a number of novel applications of
this criterion that we have devised, all based on the idea of developing
unbiased representations. These formulations include: a standard domain
adaptation framework; a method of learning invariant representations; an
approach based on noise-insensitive autoencoders; and a novel form of
generative model.



We demonstrate that any physical object, as long as its volume is conserved
when coupled with suitable operations, provides a sophisticated decision-making
capability. We consider the problem of finding, as accurately and quickly as
possible, the most profitable option from a set of options that gives
stochastic rewards. These decisions are made as dictated by a physical object,
which is moved in a manner similar to the fluctuations of a rigid body in a
tug-of-war game. Our analytical calculations validate statistical reasons why
our method exhibits higher efficiency than conventional algorithms.



Majority of the existing robot navigation systems, which facilitate the use
of laser range finders, sonar sensors or artificial landmarks, has the ability
to locate itself in an unknown environment and then build a map of the
corresponding environment. Stereo vision, while still being a rapidly
developing technique in the field of autonomous mobile robots, are currently
less preferable due to its high implementation cost. This paper aims at
describing an experimental approach for the building of a stereo vision system
that helps the robots to avoid obstacles and navigate through indoor
environments and at the same time remaining very much cost effective. This
paper discusses the fusion techniques of stereo vision and ultrasound sensors
which helps in the successful navigation through different types of complex
environments. The data from the sensor enables the robot to create the two
dimensional topological map of unknown environments and stereo vision systems
models the three dimension model of the same environment.



In unsupervised learning, an unbiased uniform sampling strategy is typically
used, in order that the learned features faithfully encode the statistical
structure of the training data. In this work, we explore whether active example
selection strategies - algorithms that select which examples to use, based on
the current estimate of the features - can accelerate learning. Specifically,
we investigate effects of heuristic and saliency-inspired selection algorithms
on the dictionary learning task with sparse activations. We show that some
selection algorithms do improve the speed of learning, and we speculate on why
they might work.



The relationship between statistical dependency and causality lies at the
heart of all statistical approaches to causal inference. Recent results in the
ChaLearn cause-effect pair challenge have shown that causal directionality can
be inferred with good accuracy also in Markov indistinguishable configurations
thanks to data driven approaches. This paper proposes a supervised machine
learning approach to infer the existence of a directed causal link between two
variables in multivariate settings with $n>2$ variables. The approach relies on
the asymmetry of some conditional (in)dependence relations between the members
of the Markov blankets of two variables causally connected. Our results show
that supervised learning methods may be successfully used to extract causal
information on the basis of asymmetric statistical descriptors also for $n>2$
variate distributions.



Methods of deep machine learning enable to to reuse low-level representations
efficiently for generating more abstract high-level representations.
Originally, deep learning has been applied passively (e.g., for classification
purposes). Recently, it has been extended to estimate the value of actions for
autonomous agents within the framework of reinforcement learning (RL). Explicit
models of the environment can be learned to augment such a value function.
Although "flat" connectionist methods have already been used for model-based
RL, up to now, only model-free variants of RL have been equipped with methods
from deep learning. We propose a variant of deep model-based RL that enables an
agent to learn arbitrarily abstract hierarchical representations of its
environment. In this paper, we present research on how such hierarchical
representations can be grounded in sensorimotor interaction between an agent
and its environment.



In order to identify an object, human eyes firstly search the field of view
for points or areas which have particular properties. These properties are used
to recognise an image or an object. Then this process could be taken as a model
to develop computer algorithms for images identification. This paper proposes
the idea of applying the simplified firefly algorithm to search for key-areas
in 2D images. For a set of input test images the proposed version of firefly
algorithm has been examined. Research results are presented and discussed to
show the efficiency of this evolutionary computation method.



The KF metamodel is a comprehensive unifying metamodel covering the static
structural entities and constraints of UML Class Diagrams (v2.4.1), ER, EER,
ORM, and ORM2, and intended to boost interoperability of common conceptual data
modelling languages. It was originally designed in UML with textual
constraints, and in this report we present its formalisations in FOL and OWL,
which accompanies the paper that describes, discusses, and analyses the KF
metamodel in detail. These new formalizations contribute to give a precise
meaning to the metamodel, to understand its complexity properties and to
provide a basis for future implementations.



We present experiments demonstrating that some other form of capacity
control, different from network size, plays a central role in learning
multilayer feed-forward networks. We argue, partially through analogy to matrix
factorization, that this is an inductive bias that can help shed light on deep
learning.



In computational biology, biological entities such as genes or proteins are
usually annotated with terms extracted from Gene Ontology (GO). The functional
similarity among terms of an ontology is evaluated by using Semantic Similarity
Measures (SSM). More recently, the extensive application of SSMs yielded to the
Semantic Similarity Networks (SSNs). SSNs are edge-weighted graphs where the
nodes are concepts (e.g. proteins) and each edge has an associated weight that
represents the semantic similarity among related pairs of nodes. The analysis
of SSNs may reveal biologically meaningful knowledge. For these aims, the need
for the introduction of tool able to manage and analyze SSN arises.
Consequently we developed SSN-Analyzer a web based tool able to build and
preprocess SSN. As proof of concept we demonstrate that community detection
algorithms applied to filtered (thresholded) networks, have better performances
in terms of biological relevance of the results, with respect to the use of raw
unfiltered networks.



In this paper, we propose a generic technique to model temporal dependencies
and sequences using a combination of a recurrent neural network and a Deep
Belief Network. Our technique, RNN-DBN, is an amalgamation of the memory state
of the RNN that allows it to provide temporal information and a multi-layer DBN
that helps in high level representation of the data. This makes RNN-DBNs ideal
for sequence generation. Further, the use of a DBN in conjunction with the RNN
makes this model capable of significantly more complex data representation than
an RBM. We apply this technique to the task of polyphonic music generation.



Identification of the influential clinical symptoms and laboratory features
that help in the diagnosis of dengue fever in early phase of the illness would
aid in designing effective public health management and virological
surveillance strategies. Keeping this as our main objective we develop in this
paper, a new computational intelligence based methodology that predicts the
diagnosis in real time, minimizing the number of false positives and false
negatives. Our methodology consists of three major components (i) a novel
missing value imputation procedure that can be applied on any data set
consisting of categorical (nominal) and/or numeric (real or integer) (ii) a
wrapper based features selection method with genetic search for extracting a
subset of most influential symptoms that can diagnose the illness and (iii) an
alternating decision tree method that employs boosting for generating highly
accurate decision rules. The predictive models developed using our methodology
are found to be more accurate than the state-of-the-art methodologies used in
the diagnosis of the dengue fever.



Crowdsourcing provides a popular paradigm for data collection at scale. We
study the problem of selecting subsets of workers from a given worker pool to
maximize the accuracy under a budget constraint. One natural question is
whether we should hire as many workers as the budget allows, or restrict on a
small number of top-quality workers. By theoretically analyzing the error rate
of a typical setting in crowdsourcing, we frame the worker selection problem
into a combinatorial optimization problem and propose an algorithm to solve it
efficiently. Empirical results on both simulated and real-world datasets show
that our algorithm is able to select a small number of high-quality workers,
and performs as good as, sometimes even better than, the much larger crowds as
the budget allows.



Rectified activation units (rectifiers) are essential for state-of-the-art
neural networks. In this work, we study rectifier neural networks for image
classification from two aspects. First, we propose a Parametric Rectified
Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU
improves model fitting with nearly zero extra computational cost and little
overfitting risk. Second, we derive a robust initialization method that
particularly considers the rectifier nonlinearities. This method enables us to
train extremely deep rectified models directly from scratch and to investigate
deeper or wider network architectures. Based on our PReLU networks
(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012
classification dataset. This is a 26% relative improvement over the ILSVRC 2014
winner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass
human-level performance (5.1%, Russakovsky et al.) on this visual recognition
challenge.



Many real incidents demonstrate that users of Online Social Networks need
mechanisms that help them manage their interactions by increasing the awareness
of the different contexts that coexist in Online Social Networks and preventing
them from exchanging inappropriate information in those contexts or
disseminating sensitive information from some contexts to others. Contextual
integrity is a privacy theory that conceptualises the appropriateness of
information sharing based on the contexts in which this information is to be
shared. Computational models of Contextual Integrity assume the existence of
well-defined contexts, in which individuals enact pre-defined roles and
information sharing is governed by an explicit set of norms. However, contexts
in Online Social Networks are known to be implicit, unknown a priori and ever
changing; users relationships are constantly evolving; and the information
sharing norms are implicit. This makes current Contextual Integrity models not
suitable for Online Social Networks.
  In this paper, we propose the first computational model of Implicit
Contextual Integrity, presenting an information model and an Information
Assistant Agent that uses the information model to learn implicit contexts,
relationships and the information sharing norms to help users avoid
inappropriate information exchanges and undesired information disseminations.
Through an experimental evaluation, we validate the properties of Information
Assistant Agents, which are shown to: infer the information sharing norms even
if a small proportion of the users follow the norms and in presence of
malicious users; help reduce the exchange of inappropriate information and the
dissemination of sensitive information with only a partial view of the system
and the information received and sent by their users; and minimise the burden
to the users in terms of raising unnecessary alerts.



A wide variety of problems in machine learning, including exemplar
clustering, document summarization, and sensor placement, can be cast as
constrained submodular maximization problems. Unfortunately, the resulting
submodular optimization problems are often too large to be solved on a single
machine. We develop a simple distributed algorithm that is embarrassingly
parallel and it achieves provable, constant factor, worst-case approximation
guarantees. In our experiments, we demonstrate its efficiency in large problems
with different kinds of constraints with objective values always close to what
is achievable in the centralized setting.



We consider the problem of learning deep generative models from data. We
formulate a method that generates an independent sample via a single
feedforward pass through a multilayer perceptron, as in the recently proposed
generative adversarial networks (Goodfellow et al., 2014). Training a
generative adversarial network, however, requires careful optimization of a
difficult minimax program. Instead, we utilize a technique from statistical
hypothesis testing known as maximum mean discrepancy (MMD), which leads to a
simple objective that can be interpreted as matching all orders of statistics
between a dataset and samples from the model, and can be trained by
backpropagation. We further boost the performance of this approach by combining
our generative network with an auto-encoder network, using MMD to learn to
generate codes that can then be decoded to produce samples. We show that the
combination of these techniques yields excellent generative models compared to
baseline approaches as measured on MNIST and the Toronto Face Database.



Classical collaborative filtering, and content-based filtering methods try to
learn a static recommendation model given training data. These approaches are
far from ideal in highly dynamic recommendation domains such as news
recommendation and computational advertisement, where the set of items and
users is very fluid. In this work, we investigate an adaptive clustering
technique for content recommendation based on exploration-exploitation
strategies in contextual multi-armed bandit settings. Our algorithm takes into
account the collaborative effects that arise due to the interaction of the
users with the items, by dynamically grouping users based on the items under
consideration and, at the same time, grouping items based on the similarity of
the clusterings induced over the users. The resulting algorithm thus takes
advantage of preference patterns in the data in a way akin to collaborative
filtering methods. We provide an empirical analysis on medium-size real-world
datasets, showing scalability and increased prediction performance (as measured
by click-through rate) over state-of-the-art methods for clustering bandits. We
also provide a regret analysis within a standard linear stochastic noise
setting.



Multiple hypothesis testing is a significant problem in nearly all
neuroimaging studies. In order to correct for this phenomena, we require a
reliable estimate of the Family-Wise Error Rate (FWER). The well known
Bonferroni correction method, while simple to implement, is quite conservative,
and can substantially under-power a study because it ignores dependencies
between test statistics. Permutation testing, on the other hand, is an exact,
non-parametric method of estimating the FWER for a given $\alpha$-threshold,
but for acceptably low thresholds the computational burden can be prohibitive.
In this paper, we show that permutation testing in fact amounts to populating
the columns of a very large matrix ${\bf P}$. By analyzing the spectrum of this
matrix, under certain conditions, we see that ${\bf P}$ has a low-rank plus a
low-variance residual decomposition which makes it suitable for highly
sub--sampled --- on the order of $0.5\%$ --- matrix completion methods. Based
on this observation, we propose a novel permutation testing methodology which
offers a large speedup, without sacrificing the fidelity of the estimated FWER.
Our evaluations on four different neuroimaging datasets show that a
computational speedup factor of roughly $50\times$ can be achieved while
recovering the FWER distribution up to very high accuracy. Further, we show
that the estimated $\alpha$-threshold is also recovered faithfully, and is
stable.



With the advances in information technology (IT) criminals are using
cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly
vulnerable to intrusions and other threats. Physical devices and human
intervention are not sufficient for monitoring and protection of these
infrastructures; hence, there is a need for more sophisticated cyber defense
systems that need to be flexible, adaptable and robust, and able to detect a
wide variety of threats and make intelligent real-time decisions. Numerous
bio-inspired computing methods of Artificial Intelligence have been
increasingly playing an important role in cyber crime detection and prevention.
The purpose of this study is to present advances made so far in the field of
applying AI techniques for combating cyber crimes, to demonstrate how these
techniques can be an effective tool for detection and prevention of cyber
attacks, as well as to give the scope for future work.



We introduce a class of extensive form games where players might not be able
to foresee the possible consequences of their decisions and form a model of
their opponents which they exploit to achieve a more profitable outcome. We
improve upon existing models of games with limited foresight, endowing players
with the ability of higher-order reasoning and proposing a novel solution
concept to address intuitions coming from real game play. We analyse the
resulting equilibria, devising an effective procedure to compute them.



Variable or value elimination in a constraint satisfaction problem (CSP) can
be used in preprocessing or during search to reduce search space size. A
variable elimination rule (value elimination rule) allows the polynomial-time
identification of certain variables (domain elements) whose elimination,
without the introduction of extra compensatory constraints, does not affect the
satisfiability of an instance. We show that there are essentially just four
variable elimination rules and three value elimination rules defined by
forbidding generic sub-instances, known as irreducible existential patterns, in
arc-consistent CSP instances. One of the variable elimination rules is the
already-known Broken Triangle Property, whereas the other three are novel. The
three value elimination rules can all be seen as strict generalisations of
neighbourhood substitution.



Several authors have recently developed risk-sensitive policy gradient
methods that augment the standard expected cost minimization problem with a
measure of variability in cost. These studies have focused on specific
risk-measures, such as the variance or conditional value at risk (CVaR). In
this work, we extend the policy gradient method to the whole class of coherent
risk measures, which is widely accepted in finance and operations research,
among other fields. We consider both static and time-consistent dynamic risk
measures. For static risk measures, our approach is in the spirit of policy
gradient algorithms and combines a standard sampling approach with convex
programming. For dynamic risk measures, our approach is actor-critic style and
involves explicit approximation of value function. Most importantly, our
contribution presents a unified approach to risk-sensitive reinforcement
learning that generalizes and extends previous results.



Motivated by recent progress on pricing in the AI literature, we study
marketplaces that contain multiple vendors offering identical or similar
products and unit-demand buyers with different valuations on these vendors. The
objective of each vendor is to set the price of its product to a fixed value so
that its profit is maximized. The profit depends on the vendor's price itself
and the total volume of buyers that find the particular price more attractive
than the price of the vendor's competitors. We model the behaviour of buyers
and vendors as a two-stage full-information game and study a series of
questions related to the existence, efficiency (price of anarchy) and
computational complexity of equilibria in this game. To overcome situations
where equilibria do not exist or exist but are highly inefficient, we consider
the scenario where some of the vendors are subsidized in order to keep prices
low and buyers highly satisfied.



Electronic health records capture patient information using structured
controlled vocabularies and unstructured narrative text. While structured data
typically encodes lab values, encounters and medication lists, unstructured
data captures the physician's interpretation of the patient's condition,
prognosis, and response to therapeutic intervention. In this paper, we
demonstrate that information extraction from unstructured clinical narratives
is essential to most clinical applications. We perform an empirical study to
validate the argument and show that structured data alone is insufficient in
resolving eligibility criteria for recruiting patients onto clinical trials for
chronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is
essential to solving 59% of the CLL trial criteria and 77% of the prostate
cancer trial criteria. More specifically, for resolving eligibility criteria
with temporal constraints, we show the need for temporal reasoning and
information integration with medical events within and across unstructured
clinical narratives and structured data.



Monaural source separation is important for many real world applications. It
is challenging because, with only a single channel of information available,
without any constraints, an infinite number of solutions are possible. In this
paper, we explore joint optimization of masking functions and deep recurrent
neural networks for monaural source separation tasks, including monaural speech
separation, monaural singing voice separation, and speech denoising. The joint
optimization of the deep recurrent neural networks with an extra masking layer
enforces a reconstruction constraint. Moreover, we explore a discriminative
criterion for training neural networks to further enhance the separation
performance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT
datasets for speech separation, singing voice separation, and speech denoising
tasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared to
NMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and
4.32--5.42 dB GSIR gain compared to existing models in the singing voice
separation task, and outperform NMF and DNN baselines in the speech denoising
task.



In this work, a nonlinear model predictive controller is developed for a
batch polymerization process. The physical model of the process is
parameterized along a desired trajectory resulting in a trajectory linearized
piecewise model (a multiple linear model bank) and the parameters are
identified for an experimental polymerization reactor. Then, a multiple model
adaptive predictive controller is designed for thermal trajectory tracking of
the MMA polymerization. The input control signal to the process is constrained
by the maximum thermal power provided by the heaters. The constrained
optimization in the model predictive controller is solved via genetic
algorithms to minimize a DMC cost function in each sampling interval.



Belief Propagation (BP) is a widely used approximation for exact
probabilistic inference in graphical models, such as Markov Random Fields
(MRFs). In graphs with cycles, however, no exact convergence guarantees for BP
are known, in general. For the case when all edges in the MRF carry the same
symmetric, doubly stochastic potential, recent works have proposed to
approximate BP by linearizing the update equations around default values, which
was shown to work well for the problem of node classification. The present
paper generalizes all prior work and derives an approach that approximates
loopy BP on any pairwise MRF with the problem of solving a linear equation
system. This approach combines exact convergence guarantees and a fast matrix
implementation with the ability to model heterogenous networks. Experiments on
synthetic graphs with planted edge potentials show that the linearization has
comparable labeling accuracy as BP for graphs with weak potentials, while
speeding-up inference by orders of magnitude.



The growing need for labeled training data has made crowdsourcing an
important part of machine learning. The quality of crowdsourced labels is,
however, adversely affected by three factors: (1) the workers are not experts;
(2) the incentives of the workers are not aligned with those of the requesters;
and (3) the interface does not allow workers to convey their knowledge
accurately, by forcing them to make a single choice among a set of options. In
this paper, we address these issues by introducing approval voting to utilize
the expertise of workers who have partial knowledge of the true answer, and
coupling it with a ("strictly proper") incentive-compatible compensation
mechanism. We show rigorous theoretical guarantees of optimality of our
mechanism together with a simple axiomatic characterization. We also conduct
preliminary empirical studies on Amazon Mechanical Turk which validate our
approach.



One long-term goal of machine learning research is to produce methods that
are applicable to reasoning and natural language, in particular building an
intelligent dialogue agent. To measure progress towards that goal, we argue for
the usefulness of a set of proxy tasks that evaluate reading comprehension via
question answering. Our tasks measure understanding in several ways: whether a
system is able to answer questions via chaining facts, simple induction,
deduction and many more. The tasks are designed to be prerequisites for any
system that aims to be capable of conversing with a human. We believe many
existing learning systems can currently not solve them, and hence our aim is to
classify these tasks into skill sets, so that researchers can identify (and
then rectify) the failings of their systems. We also extend and improve the
recently introduced Memory Networks model, and show it is able to solve some,
but not all, of the tasks.



We design mechanisms for online procurement of data held by strategic agents
for machine learning tasks. The challenge is to use past data to actively price
future data and give learning guarantees even when an agent's cost for
revealing her data may depend arbitrarily on the data itself. We achieve this
goal by showing how to convert a large class of no-regret algorithms into
online posted-price and learning mechanisms. Our results in a sense parallel
classic sample complexity guarantees, but with the key resource being money
rather than quantity of data: With a budget constraint $B$, we give robust risk
(predictive error) bounds on the order of $1/\sqrt{B}$. Because we use an
active approach, we can often guarantee to do significantly better by
leveraging correlations between costs and data.
  Our algorithms and analysis go through a model of no-regret learning with $T$
arriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds
for this model are on the order of $T/\sqrt{B}$ and we give lower bounds on the
same order.



Rapid growth of documents, web pages, and other types of text content is a
huge challenge for the modern content management systems. One of the problems
in the areas of information storage and retrieval is the lacking of semantic
data. Ontologies can present knowledge in sharable and repeatedly usable manner
and provide an effective way to reduce the data volume overhead by encoding the
structure of a particular domain. Metadata in relational databases can be used
to extract ontology from database in a special domain. According to solve the
problem of sharing and reusing of data, approaches based on transforming
relational database to ontology are proposed. In this paper we propose a method
for automatic ontology construction based on relational database. Mining and
obtaining further components from relational database leads to obtain knowledge
with high semantic power and more expressiveness. Triggers are one of the
database components which could be transformed to the ontology model and
increase the amount of power and expressiveness of knowledge by presenting part
of the knowledge dynamically



The focus of this paper is on solving multi-robot planning problems in
continuous spaces with partial observability. Decentralized partially
observable Markov decision processes (Dec-POMDPs) are general models for
multi-robot coordination problems, but representing and solving Dec-POMDPs is
often intractable for large problems. To allow for a high-level representation
that is natural for multi-robot problems and scalable to large discrete and
continuous problems, this paper extends the Dec-POMDP model to the
decentralized partially observable semi-Markov decision process (Dec-POSMDP).
The Dec-POSMDP formulation allows asynchronous decision-making by the robots,
which is crucial in multi-robot domains. We also present an algorithm for
solving this Dec-POSMDP which is much more scalable than previous methods since
it can incorporate closed-loop belief space macro-actions in planning. These
macro-actions are automatically constructed to produce robust solutions. The
proposed method's performance is evaluated on a complex multi-robot package
delivery problem under uncertainty, showing that our approach can naturally
represent multi-robot problems and provide high-quality solutions for
large-scale problems.



We propose a self-organizing memory architecture for perceptual experience,
capable of supporting autonomous learning and goal-directed problem solving in
the absence of any prior information about the agent's environment. The
architecture is simple enough to ensure (1) a quadratic bound (in the number of
available sensors) on space requirements, and (2) a quadratic bound on the
time-complexity of the update-execute cycle. At the same time, it is
sufficiently complex to provide the agent with an internal representation which
is (3) minimal among all representations of its class which account for every
sensory equivalence class subject to the agent's belief state; (4) capable, in
principle, of recovering the homotopy type of the system's state space; (5)
learnable with arbitrary precision through a random application of the
available actions. The provable properties of an effectively trained memory
structure exploit a duality between weak poc sets -- a symbolic (discrete)
representation of subset nesting relations -- and non-positively curved cubical
complexes, whose rich convexity theory underlies the planning cycle of the
proposed architecture.



Automatic reconstruction of 3D models from images using multi-view
Structure-from-Motion methods has been one of the most fruitful outcomes of
computer vision. These advances combined with the growing popularity of Micro
Aerial Vehicles as an autonomous imaging platform, have made 3D vision tools
ubiquitous for large number of Architecture, Engineering and Construction
applications among audiences, mostly unskilled in computer vision. However, to
obtain high-resolution and accurate reconstructions from a large-scale object
using SfM, there are many critical constraints on the quality of image data,
which often become sources of inaccuracy as the current 3D reconstruction
pipelines do not facilitate the users to determine the fidelity of input data
during the image acquisition. In this paper, we present and advocate a
closed-loop interactive approach that performs incremental reconstruction in
real-time and gives users an online feedback about the quality parameters like
Ground Sampling Distance (GSD), image redundancy, etc on a surface mesh. We
also propose a novel multi-scale camera network design to prevent scene drift
caused by incremental map building, and release the first multi-scale image
sequence dataset as a benchmark. Further, we evaluate our system on real
outdoor scenes, and show that our interactive pipeline combined with a
multi-scale camera network approach provides compelling accuracy in multi-view
reconstruction tasks when compared against the state-of-the-art methods.



We study an online model of fair division designed to capture features of a
real world charity problem. We consider two simple mechanisms for this model in
which agents simply declare what items they like. We analyse several axiomatic
properties of these mechanisms like strategy-proofness and envy-freeness.
Finally, we perform a competitive analysis and compute the price of anarchy.



In this paper we propose a non-metric ranking-based representation of
semantic similarity that allows natural aggregation of semantic information
from multiple heterogeneous sources. We apply the ranking-based representation
to zero-shot learning problems, and present deterministic and probabilistic
zero-shot classifiers which can be built from pre-trained classifiers without
retraining. We demonstrate their the advantages on two large real-world image
datasets. In particular, we show that aggregating different sources of semantic
information, including crowd-sourcing, leads to more accurate classification.



Recent applications of Stackelberg Security Games (SSG), from wildlife crime
to urban crime, have employed machine learning tools to learn and predict
adversary behavior using available data about defender-adversary interactions.
Given these recent developments, this paper commits to an approach of directly
learning the response function of the adversary. Using the PAC model, this
paper lays a firm theoretical foundation for learning in SSGs (e.g.,
theoretically answer questions about the numbers of samples required to learn
adversary behavior) and provides utility guarantees when the learned adversary
model is used to plan the defender's strategy. The paper also aims to answer
practical questions such as how much more data is needed to improve an
adversary model's accuracy. Additionally, we explain a recently observed
phenomenon that prediction accuracy of learned adversary behavior is not enough
to discover the utility maximizing defender strategy. We provide four main
contributions: (1) a PAC model of learning adversary response functions in
SSGs; (2) PAC-model analysis of the learning of key, existing bounded
rationality models in SSGs; (3) an entirely new approach to adversary modeling
based on a non-parametric class of response functions with PAC-model analysis
and (4) identification of conditions under which computing the best defender
strategy against the learned adversary behavior is indeed the optimal strategy.
Finally, we conduct experiments with real-world data from a national park in
Uganda, showing the benefit of our new adversary modeling approach and
verification of our PAC model predictions.



This article provides a formalization of the W3C Draft Core SHACL Semantics
specification using Z notation. This formalization exercise has identified a
number of quality issues in the draft. It has also established that the
recursive definitions in the draft are well-founded. Further formal validation
of the draft will require the use of an executable specification technology.



Large unweighted directed graphs are commonly used to capture relations
between entities. A fundamental problem in the analysis of such networks is to
properly define the similarity or dissimilarity between any two vertices.
Despite the significance of this problem, statistical characterization of the
proposed metrics has been limited. We introduce and develop a class of
techniques for analyzing random walks on graphs using stochastic calculus.
Using these techniques we generalize results on the degeneracy of hitting times
and analyze a metric based on the Laplace transformed hitting time (LTHT). The
metric serves as a natural, provably well-behaved alternative to the expected
hitting time. We establish a general correspondence between hitting times of
the Brownian motion and analogous hitting times on the graph. We show that the
LTHT is consistent with respect to the underlying metric of a geometric graph,
preserves clustering tendency, and remains robust against random addition of
non-geometric edges. Tests on simulated and real-world data show that the LTHT
matches theoretical predictions and outperforms alternatives.



Classification is a fundamental task in machine learning and artificial
intelligence. Existing classification methods are designed to classify unknown
instances within a set of previously known classes that are seen in training.
Such classification takes the form of prediction within a closed-set. However,
a more realistic scenario that fits the ground truth of real world applications
is to consider the possibility of encountering instances that do not belong to
any of the classes that are seen in training, $i.e.$, an open-set
classification. In such situation, existing closed-set classification methods
will assign a training label to these instances resulting in a
misclassification. In this paper, we introduce Galaxy-X, a novel multi-class
classification method for open-set problem. For each class of the training set,
Galaxy-X creates a minimum bounding hyper-sphere that encompasses the
distribution of the class by enclosing all of its instances. In such manner,
our method is able to distinguish instances resembling previously seen classes
from those that are of unseen classes. To adequately evaluate open-set
classification, we introduce a novel evaluation procedure. Experimental results
on benchmark datasets as well as on synthetic datasets show the efficiency of
our approach in classifying novel instances from known as well as unknown
classes.



Recent works have highlighted scale invariance or symmetry present in the
weight space of a typical deep network and the adverse effect it has on the
Euclidean gradient based stochastic gradient descent optimization. In this
work, we show that a commonly used deep network, which uses convolution, batch
normalization, reLU, max-pooling, and sub-sampling pipeline, possess more
complex forms of symmetry arising from scaling-based reparameterization of the
network weights. We propose to tackle the issue of the weight space symmetry by
constraining the filters to lie on the unit-norm manifold. Consequently,
training the network boils down to using stochastic gradient descent updates on
the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows
that the proposed updates improve the test performance beyond what is achieved
with batch normalization and without sacrificing the computational efficiency
of the weight updates.



A line of recent work provides welfare guarantees of simple combinatorial
auction formats, such as selling m items via simultaneous second price auctions
(SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et
al. 2013). These guarantees hold even when the auctions are repeatedly executed
and players use no-regret learning algorithms. Unfortunately, off-the-shelf
no-regret algorithms for these auctions are computationally inefficient as the
number of actions is exponential. We show that this obstacle is insurmountable:
there are no polynomial-time no-regret algorithms for SiSPAs, unless
RP$\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises
the question of how good outcomes polynomially-bounded bidders may discover in
such auctions.
  To answer this question, we propose a novel concept of learning in auctions,
termed "no-envy learning." This notion is founded upon Walrasian equilibrium,
and we show that it is both efficiently implementable and results in
approximately optimal welfare, even when the bidders have fractionally
subadditive (XOS) valuations (assuming demand oracles) or coverage valuations
(without demand oracles). No-envy learning outcomes are a relaxation of
no-regret outcomes, which maintain their approximate welfare optimality while
endowing them with computational tractability. Our results extend to other
auction formats that have been studied in the literature via the smoothness
paradigm.
  Our results for XOS valuations are enabled by a novel
Follow-The-Perturbed-Leader algorithm for settings where the number of experts
is infinite, and the payoff function of the learner is non-linear. This
algorithm has applications outside of auction settings, such as in security
games. Our result for coverage valuations is based on a novel use of convex
rounding schemes and a reduction to online convex optimization.



Structured prediction is used in areas such as computer vision and natural
language processing to predict structured outputs such as segmentations or
parse trees. In these settings, prediction is performed by MAP inference or,
equivalently, by solving an integer linear program. Because of the complex
scoring functions required to obtain accurate predictions, both learning and
inference typically require the use of approximate solvers. We propose a
theoretical explanation to the striking observation that approximations based
on linear programming (LP) relaxations are often tight on real-world instances.
In particular, we show that learning with LP relaxed inference encourages
integrality of training instances, and that tightness generalizes from train to
test data.



Recent works have highlighted scale invariance or symmetry that is present in
the weight space of a typical deep network and the adverse effect that it has
on the Euclidean gradient based stochastic gradient descent optimization. In
this work, we show that these and other commonly used deep networks, such as
those which use a max-pooling and sub-sampling layer, possess more complex
forms of symmetry arising from scaling based reparameterization of the network
weights. We then propose two symmetry-invariant gradient based weight updates
for stochastic gradient descent based learning. Our empirical evidence based on
the MNIST dataset shows that these updates improve the test performance without
sacrificing the computational efficiency of the weight updates. We also show
the results of training with one of the proposed weight updates on an image
segmentation problem.



We introduce a framework and early results for massively scalable Gaussian
processes (MSGP), significantly extending the KISS-GP approach of Wilson and
Nickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs)
on billions of datapoints, without requiring distributed inference, or severe
assumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GP
learning and inference to $O(n)$, and the standard $O(n^2)$ complexity per test
point prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices as
Kronecker products of Toeplitz matrices approximated by circulant matrices.
This multi-level circulant approximation allows one to unify the orthogonal
computational benefits of fast Kronecker and Toeplitz approaches, and is
significantly faster than either approach in isolation; 2) local kernel
interpolation and inducing points to allow for arbitrarily located data inputs,
and $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-block
structure (BTTB), which enables fast inference and learning when
multidimensional Kronecker structure is not present; and 4) projections of the
input space to flexibly model correlated inputs and high dimensional data. The
ability to handle many ($m \approx n$) inducing points allows for near-exact
accuracy and large scale kernel learning.



We show that there is a largely unexplored class of functions (positive
polymatroids) that can define proper discrete metrics over pairs of binary
vectors and that are fairly tractable to optimize over. By exploiting
submodularity, we are able to give hardness results and approximation
algorithms for optimizing over such metrics. Additionally, we demonstrate
empirically the effectiveness of these metrics and associated algorithms on
both a metric minimization task (a form of clustering) and also a metric
maximization task (generating diverse k-best lists).



We introduce scalable deep kernels, which combine the structural properties
of deep learning architectures with the non-parametric flexibility of kernel
methods. Specifically, we transform the inputs of a spectral mixture base
kernel with a deep architecture, using local kernel interpolation, inducing
points, and structure exploiting (Kronecker and Toeplitz) algebra for a
scalable kernel representation. These closed-form kernels can be used as
drop-in replacements for standard kernels, with benefits in expressive power
and scalability. We jointly learn the properties of these kernels through the
marginal likelihood of a Gaussian process. Inference and learning cost $O(n)$
for $n$ training points, and predictions cost $O(1)$ per test point. On a large
and diverse collection of applications, including a dataset with 2 million
examples, we show improved performance over scalable Gaussian processes with
flexible kernel learning models, and stand-alone deep architectures.



We propose an effective technique to solving review-level sentiment
classification problem by using sentence-level polarity correction. Our
polarity correction technique takes into account the consistency of the
polarities (positive and negative) of sentences within each product review
before performing the actual machine learning task. While sentences with
inconsistent polarities are removed, sentences with consistent polarities are
used to learn state-of-the-art classifiers. The technique achieved better
results on different types of products reviews and outperforms baseline models
without the correction technique. Experimental results show an average of 82%
F-measure on four different product review domains.



Measuring the naturalness of images is important to generate realistic images
or to detect unnatural regions in images. Additionally, a method to measure
naturalness can be complementary to Convolutional Neural Network (CNN) based
features, which are known to be insensitive to the naturalness of images.
However, most probabilistic image models have insufficient capability of
modeling the complex and abstract naturalness that we feel because they are
built directly on raw image pixels. In this work, we assume that naturalness
can be measured by the predictability on high-level features during eye
movement. Based on this assumption, we propose a novel method to evaluate the
naturalness by building a variant of Recurrent Neural Network Language Models
on pre-trained CNN representations. Our method is applied to two tasks,
demonstrating that 1) using our method as a regularizer enables us to generate
more understandable images from image features than existing approaches, and 2)
unnaturalness maps produced by our method achieve state-of-the-art eye fixation
prediction performance on two well-studied datasets.



Hyperspectral image (HSI) classification is a hot topic in the remote sensing
community. This paper proposes a new framework of spectral-spatial feature
extraction for HSI classification, in which for the first time the concept of
deep learning is introduced. Specifically, the model of autoencoder is
exploited in our framework to extract various kinds of features. First we
verify the eligibility of autoencoder by following classical spectral
information based classification and use autoencoders with different depth to
classify hyperspectral image. Further in the proposed framework, we combine PCA
on spectral dimension and autoencoder on the other two spatial dimensions to
extract spectral-spatial information for classification. The experimental
results show that this framework achieves the highest classification accuracy
among all methods, and outperforms classical classifiers such as SVM and
PCA-based SVM.



Our aim is to extract information about literary characters in unstructured
texts. We employ natural language processing and reasoning on domain
ontologies. The first task is to identify the main characters and the parts of
the story where these characters are described or act. We illustrate the system
in a scenario in the folktale domain. The system relies on a folktale ontology
that we have developed based on Propp's model for folktales morphology.



In this paper we propose the construction of linguistic descriptions of
images. This is achieved through the extraction of scene description graphs
(SDGs) from visual scenes using an automatically constructed knowledge base.
SDGs are constructed using both vision and reasoning. Specifically, commonsense
reasoning is applied on (a) detections obtained from existing perception
methods on given images, (b) a "commonsense" knowledge base constructed using
natural language processing of image annotations and (c) lexical ontological
knowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based
evaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most
cases, sentences auto-constructed from SDGs obtained by our method give a more
relevant and thorough description of an image than a recent state-of-the-art
image caption based approach. Our Image-Sentence Alignment Evaluation results
are also comparable to that of the recent state-of-the art approaches.



In this paper, we present a model which takes as input a corpus of images
with relevant spoken captions and finds a correspondence between the two
modalities. We employ a pair of convolutional neural networks to model visual
objects and speech signals at the word level, and tie the networks together
with an embedding and alignment model which learns a joint semantic space over
both modalities. We evaluate our model using image search and annotation tasks
on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000
spoken captions using Amazon Mechanical Turk.



Learning about the social structure of hidden and hard-to-reach populations
--- such as drug users and sex workers --- is a major goal of epidemiological
and public health research on risk behaviors and disease prevention.
Respondent-driven sampling (RDS) is a peer-referral process widely used by many
health organizations, where research subjects recruit other subjects from their
social network. In such surveys, researchers observe who recruited whom, along
with the time of recruitment and the total number of acquaintances (network
degree) of respondents. However, due to privacy concerns, the identities of
acquaintances are not disclosed. In this work, we show how to reconstruct the
underlying network structure through which the subjects are recruited. We
formulate the dynamics of RDS as a continuous-time diffusion process over the
underlying graph and derive the likelihood for the recruitment time series
under an arbitrary recruitment time distribution. We develop an efficient
stochastic optimization algorithm called RENDER (REspoNdent-Driven nEtwork
Reconstruction) that finds the network that best explains the collected data.
We support our analytical results through an exhaustive set of experiments on
both synthetic and real data.



Recent work has shown that deep neural networks are capable of approximating
both value functions and policies in reinforcement learning domains featuring
continuous state and action spaces. However, to the best of our knowledge no
previous work has succeeded at using deep neural networks in structured
(parameterized) continuous action spaces. To fill this gap, this paper focuses
on learning within the domain of simulated RoboCup soccer, which features a
small set of discrete action types, each of which is parameterized with
continuous variables. The best learned agent can score goals more reliably than
the 2012 RoboCup champion agent. As such, this paper represents a successful
extension of deep reinforcement learning to the class of parameterized action
space MDPs.



Multi-mode resource and precedence-constrained project scheduling is a
well-known challenging real-world optimisation problem. An important variant of
the problem requires scheduling of activities for multiple projects considering
availability of local and global resources while respecting a range of
constraints. A critical aspect of the benchmarks addressed in this paper is
that the primary objective is to minimise the sum of the project completion
times, with the usual makespan minimisation as a secondary objective. We
observe that this leads to an expected different overall structure of good
solutions and discuss the effects this has on the algorithm design. This paper
presents a carefully designed hybrid of Monte-Carlo tree search, novel
neighbourhood moves, memetic algorithms, and hyper-heuristic methods. The
implementation is also engineered to increase the speed with which iterations
are performed, and to exploit the computing power of multicore machines.
Empirical evaluation shows that the resulting information-sharing
multi-component algorithm significantly outperforms other solvers on a set of
"hidden" instances, i.e. instances not available at the algorithm design phase.



Sum-Product Networks (SPN) have recently emerged as a new class of tractable
probabilistic graphical models. Unlike Bayesian networks and Markov networks
where inference may be exponential in the size of the network, inference in
SPNs is in time linear in the size of the network. Since SPNs represent
distributions over a fixed set of variables only, we propose dynamic sum
product networks (DSPNs) as a generalization of SPNs for sequence data of
varying length. A DSPN consists of a template network that is repeated as many
times as needed to model data sequences of any length. We present a local
search technique to learn the structure of the template network. In contrast to
dynamic Bayesian networks for which inference is generally exponential in the
number of variables per time slice, DSPNs inherit the linear inference
complexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and other
models on several datasets of sequence data.



This paper introduces a novel architecture for reinforcement learning with
deep neural networks designed to handle state and action spaces characterized
by natural language, as found in text-based games. Termed a deep reinforcement
relevance network (DRRN), the architecture represents action and state spaces
with separate embedding vectors, which are combined with an interaction
function to approximate the Q-function in reinforcement learning. We evaluate
the DRRN on two popular text games, showing superior performance over other
deep Q-learning architectures. Experiments with paraphrased action descriptions
show that the model is extracting meaning rather than simply memorizing strings
of text.



Emotional content is a key element in user-generated videos. However, it is
difficult to understand emotions conveyed in such videos due to the complex and
unstructured nature of user-generated content and the sparsity of video frames
that express emotion. In this paper, for the first time, we study the problem
of transferring knowledge from heterogeneous external sources, including image
and textual data, to facilitate three related tasks in video emotion
understanding: emotion recognition, emotion attribution and emotion-oriented
summarization. Specifically, our framework (1) learns a video encoding from an
auxiliary emotional image dataset in order to improve supervised video emotion
recognition, and (2) transfers knowledge from an auxiliary textual corpus for
zero-shot \pl{recognition} of emotion classes unseen during training. The
proposed technique for knowledge transfer facilitates novel applications of
emotion attribution and emotion-oriented summarization. A comprehensive set of
experiments on multiple datasets demonstrate the effectiveness of our
framework.



In the task of Object Recognition, there exists a dichotomy between the
categorization of objects and estimating object pose, where the former
necessitates a view-invariant representation, while the latter requires a
representation capable of capturing pose information over different categories
of objects. With the rise of deep architectures, the prime focus has been on
object category recognition. Deep learning methods have achieved wide success
in this task. In contrast, object pose regression using these approaches has
received relatively much less attention. In this paper we show how deep
architectures, specifically Convolutional Neural Networks (CNN), can be adapted
to the task of simultaneous categorization and pose estimation of objects. We
investigate and analyze the layers of various CNN models and extensively
compare between them with the goal of discovering how the layers of distributed
representations of CNNs represent object pose information and how this
contradicts with object category representations. We extensively experiment on
two recent large and challenging multi-view datasets. Our models achieve better
than state-of-the-art performance on both datasets.



We address the problem of Visual Question Answering (VQA), which requires
joint image and language understanding to answer a question about a given
photograph. Recent approaches have applied deep image captioning methods based
on convolutional-recurrent networks to this problem, but have failed to model
spatial inference. To remedy this, we propose a model we call the Spatial
Memory Network and apply it to the VQA task. Memory networks are recurrent
neural networks with an explicit attention mechanism that selects certain parts
of the information stored in memory. Our Spatial Memory Network stores neuron
activations from different spatial regions of the image in its memory, and uses
the question to choose relevant regions for computing the answer, a process of
which constitutes a single "hop" in the network. We propose a novel spatial
attention architecture that aligns words with image patches in the first hop,
and obtain improved results by adding a second attention hop which considers
the whole question to choose visual evidence based on the results of the first
hop. To better understand the inference process learned by the network, we
design synthetic questions that specifically require spatial inference and
visualize the attention weights. We evaluate our model on two published visual
question answering datasets, DAQUAR [1] and VQA [2], and obtain improved
results compared to a strong deep baseline model (iBOWIMG) which concatenates
image and question features to predict the answer [3].



Bayesian Optimization (BO) is a data-efficient method for global black-box
optimization of an expensive-to-evaluate fitness function. BO typically assumes
that computation cost of BO is cheap, but experiments are time consuming or
costly. In practice, this allows us to optimize ten or fewer critical
parameters in up to 1,000 experiments. But experiments may be less expensive
than BO methods assume: In some simulation models, we may be able to conduct
multiple thousands of experiments in a few hours, and the computational burden
of BO is no longer negligible compared to experimentation time. To address this
challenge we introduce a new Dimension Scheduling Algorithm (DSA), which
reduces the computational burden of BO for many experiments. The key idea is
that DSA optimizes the fitness function only along a small set of dimensions at
each iteration. This DSA strategy (1) reduces the necessary computation time,
(2) finds good solutions faster than the traditional BO method, and (3) can be
parallelized straightforwardly. We evaluate the DSA in the context of
optimizing parameters of dynamic models of microalgae metabolism and show
faster convergence than traditional BO.



Traditional algorithms for robots who need to integrate into a wireless
network often focus on one specific task. In this work we want to develop
simple, adaptive and reusable algorithms for real world applications for this
scenario. Starting with the most basic task for mobile wireless network nodes,
finding the position of another node, we introduce an algorithm able to solve
this task. We then show how this algorithm can readily be employed to solve a
large number of other related tasks like finding the optimal position to bridge
two static network nodes. For this we first introduce a meta-algorithm inspired
by autonomous robot learning strategies and the concept of internal models
which yields a class of source seeking algorithms for mobile nodes. The
effectiveness of this algorithm is demonstrated in real world experiments using
a physical mobile robot and standard 802.11 wireless LAN in an office
environment. We also discuss the differences to conventional algorithms and
give the robotics perspective on this class of algorithms. Then we proceed to
show how more complex tasks, which might be encountered by mobile nodes, can be
encoded in the same framework and how the introduced algorithm can solve them.
These tasks can be direct (cross layer) optimization tasks or can also encode
more complex tasks like bridging two network nodes. We choose the bridging
scenario as an example, implemented on a real physical robot, and show how the
robot can solve it in a real world experiment.



Graph-structured data appears frequently in domains including chemistry,
natural language semantics, social networks, and knowledge bases. In this work,
we study feature learning techniques for graph-structured inputs. Our starting
point is previous work on Graph Neural Networks (Scarselli et al., 2009), which
we modify to use gated recurrent units and modern optimization techniques and
then extend to output sequences. The result is a flexible and broadly useful
class of neural network models that has favorable inductive biases relative to
purely sequence-based models (e.g., LSTMs) when the problem is
graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and
graph algorithm learning tasks. We then show it achieves state-of-the-art
performance on a problem from program verification, in which subgraphs need to
be matched to abstract data structures.



Unlike human learning, machine learning often fails to handle changes between
training (source) and test (target) input distributions. Such domain shifts,
common in practical scenarios, severely damage the performance of conventional
machine learning methods. Supervised domain adaptation methods have been
proposed for the case when the target data have labels, including some that
perform very well despite being "frustratingly easy" to implement. However, in
practice, the target domain is often unlabeled, requiring unsupervised
adaptation. We propose a simple, effective, and efficient method for
unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL
minimizes domain shift by aligning the second-order statistics of source and
target distributions, without requiring any target labels. Even though it is
extraordinarily simple--it can be implemented in four lines of Matlab
code--CORAL performs remarkably well in extensive evaluations on standard
benchmark datasets.



We examine a new form of smooth approximation to the zero one loss in which
learning is performed using a reformulation of the widely used logistic
function. Our approach is based on using the posterior mean of a novel
generalized Beta-Bernoulli formulation. This leads to a generalized logistic
function that approximates the zero one loss, but retains a probabilistic
formulation conferring a number of useful properties. The approach is easily
generalized to kernel logistic regression and easily integrated into methods
for structured prediction. We present experiments in which we learn such models
using an optimization method consisting of a combination of gradient descent
and coordinate descent using localized grid search so as to escape from local
minima. Our experiments indicate that optimization quality is improved when
learning meta-parameters are themselves optimized using a validation set. Our
experiments show improved performance relative to widely used logistic and
hinge loss methods on a wide variety of problems ranging from standard UC
Irvine and libSVM evaluation datasets to product review predictions and a
visual information extraction task. We observe that the approach: 1) is more
robust to outliers compared to the logistic and hinge losses; 2) outperforms
comparable logistic and max margin models on larger scale benchmark problems;
3) when combined with Gaussian- Laplacian mixture prior on parameters the
kernelized version of our formulation yields sparser solutions than Support
Vector Machine classifiers; and 4) when integrated into a probabilistic
structured prediction technique our approach provides more accurate
probabilities yielding improved inference and increasing information extraction
performance.



In practice, there are often explicit constraints on what representations or
decisions are acceptable in an application of machine learning. For example it
may be a legal requirement that a decision must not favour a particular group.
Alternatively it can be that that representation of data must not have
identifying information. We address these two related issues by learning
flexible representations that minimize the capability of an adversarial critic.
This adversary is trying to predict the relevant sensitive variable from the
representation, and so minimizing the performance of the adversary ensures
there is little or no information in the representation about the sensitive
variable. We demonstrate this adversarial approach on two problems: making
decisions free from discrimination and removing private information from
images. We formulate the adversarial model as a minimax problem, and optimize
that minimax objective using a stochastic gradient alternate min-max optimizer.
We demonstrate the ability to provide discriminant free representations for
standard test problems, and compare with previous state of the art methods for
fairness, showing statistically significant improvement across most cases. The
flexibility of this method is shown via a novel problem: removing annotations
from images, from unaligned training examples of annotated and unannotated
images, and with no a priori knowledge of the form of annotation provided to
the model.



Computer system monitoring generates huge amounts of logs that record the
interaction of system entities. How to query such data to better understand
system behaviors and identify potential system risks and malicious behaviors
becomes a challenging task for system administrators due to the dynamics and
heterogeneity of the data. System monitoring data are essentially heterogeneous
temporal graphs with nodes being system entities and edges being their
interactions over time. Given the complexity of such graphs, it becomes
time-consuming for system administrators to manually formulate useful queries
in order to examine abnormal activities, attacks, and vulnerabilities in
computer systems.
  In this work, we investigate how to query temporal graphs and treat query
formulation as a discriminative temporal graph pattern mining problem. We
introduce TGMiner to mine discriminative patterns from system logs, and these
patterns can be taken as templates for building more complex queries. TGMiner
leverages temporal information in graphs to prune graph patterns that share
similar growth trend without compromising pattern quality. Experimental results
on real system data show that TGMiner is 6-32 times faster than baseline
methods. The discovered patterns were verified by system experts; they achieved
high precision (97%) and recall (91%).



Variation in language is ubiquitous, particularly in newer forms of writing
such as social media. Fortunately, variation is not random, it is often linked
to social properties of the author. In this paper, we show how to exploit
social networks to make sentiment analysis more robust to social language
variation. The key idea is linguistic homophily: the tendency of socially
linked individuals to use language in similar ways. We formalize this idea in a
novel attention-based neural network architecture, in which attention is
divided among several basis models, depending on the author's position in the
social network. This has the effect of smoothing the classification function
across the social network, and makes it possible to induce personalized
classifiers even for authors for whom there is no labeled data or demographic
metadata. This model significantly improves the accuracies of sentiment
analysis on Twitter and on review data.



The ability to predict future states of the environment is a central pillar
of intelligence. At its core, effective prediction requires an internal model
of the world and an understanding of the rules by which the world changes.
Here, we explore the internal models developed by deep neural networks trained
using a loss based on predicting future frames in synthetic video sequences,
using a CNN-LSTM-deCNN framework. We first show that this architecture can
achieve excellent performance in visual sequence prediction tasks, including
state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever
et al., 2009). Using a weighted mean-squared error and adversarial loss
(Goodfellow et al., 2014), the same architecture successfully extrapolates
out-of-the-plane rotations of computer-generated faces. Furthermore, despite
being trained end-to-end to predict only pixel-level information, our
Predictive Generative Networks learn a representation of the latent structure
of the underlying three-dimensional objects themselves. Importantly, we find
that this representation is naturally tolerant to object transformations, and
generalizes well to new tasks, such as classification of static images. Similar
models trained solely with a reconstruction loss fail to generalize as
effectively. We argue that prediction can serve as a powerful unsupervised loss
for learning rich internal representations of high-level object features.



Although the traits emerged in a mass gathering are often non-deliberative,
the act of mass impulse may lead to irre- vocable crowd disasters. The two-fold
increase of carnage in crowd since the past two decades has spurred significant
advances in the field of computer vision, towards effective and proactive crowd
surveillance. Computer vision stud- ies related to crowd are observed to
resonate with the understanding of the emergent behavior in physics (complex
systems) and biology (animal swarm). These studies, which are inspired by
biology and physics, share surprisingly common insights, and interesting
contradictions. However, this aspect of discussion has not been fully explored.
Therefore, this survey provides the readers with a review of the
state-of-the-art methods in crowd behavior analysis from the physics and
biologically inspired perspectives. We provide insights and comprehensive
discussions for a broader understanding of the underlying prospect of blending
physics and biology studies in computer vision.



We propose a method for hand pose estimation based on a deep regressor
trained on two different kinds of input. Raw depth data is fused with an
intermediate representation in the form of a segmentation of the hand into
parts. This intermediate representation contains important topological
information and provides useful cues for reasoning about joint locations. The
mapping from raw depth to segmentation maps is learned in a
semi/weakly-supervised way from two different datasets: (i) a synthetic dataset
created through a rendering pipeline including densely labeled ground truth
(pixelwise segmentations); and (ii) a dataset with real images for which ground
truth joint positions are available, but not dense segmentations. Loss for
training on real images is generated from a patch-wise restoration process,
which aligns tentative segmentation maps with a large dictionary of synthetic
poses. The underlying premise is that the domain shift between synthetic and
real data is smaller in the intermediate representation, where labels carry
geometric and topological meaning, than in the raw input domain. Experiments on
the NYU dataset show that the proposed training method decreases error on
joints over direct regression of joints from depth data by 15.7%.



This paper presents a novel nonmyopic adaptive Gaussian process planning
(GPP) framework endowed with a general class of Lipschitz continuous reward
functions that can unify some active learning/sensing and Bayesian optimization
criteria and offer practitioners some flexibility to specify their desired
choices for defining new tasks/problems. In particular, it utilizes a
principled Bayesian sequential decision problem framework for jointly and
naturally optimizing the exploration-exploitation trade-off. In general, the
resulting induced GPP policy cannot be derived exactly due to an uncountable
set of candidate observations. A key contribution of our work here thus lies in
exploiting the Lipschitz continuity of the reward functions to solve for a
nonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real
time, we further propose an asymptotically optimal, branch-and-bound anytime
variant of epsilon-GPP with performance guarantee. We empirically demonstrate
the effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian
optimization and an energy harvesting task.



This paper addresses the problem of active learning of a multi-output
Gaussian process (MOGP) model representing multiple types of coexisting
correlated environmental phenomena. In contrast to existing works, our active
learning problem involves selecting not just the most informative sampling
locations to be observed but also the types of measurements at each selected
location for minimizing the predictive uncertainty (i.e., posterior joint
entropy) of a target phenomenon of interest given a sampling budget.
Unfortunately, such an entropy criterion scales poorly in the numbers of
candidate sampling locations and selected observations when optimized. To
resolve this issue, we first exploit a structure common to sparse MOGP models
for deriving a novel active learning criterion. Then, we exploit a relaxed form
of submodularity property of our new criterion for devising a polynomial-time
approximation algorithm that guarantees a constant-factor approximation of that
achieved by the optimal set of selected observations. Empirical evaluation on
real-world datasets shows that our proposed approach outperforms existing
algorithms for active learning of MOGP and single-output GP models.



We introduce the concept of continuous transportation task to the context of
multi-agent systems. A continuous transportation task is one in which a
multi-agent team visits a number of fixed locations, picks up objects, and
delivers them to a final destination. The goal is to maximize the rate of
transportation while the objects are replenished over time. Examples of
problems that need continuous transportation are foraging, area sweeping, and
first/last mile problem. Previous approaches typically neglect the interference
and are highly dependent on communications among agents. Some also incorporate
an additional reconnaissance agent to gather information. In this paper, we
present a hybrid of centralized and distributed approaches that minimize the
interference and communications in the multi-agent team without the need for a
reconnaissance agent. We contribute two partitioning-transportation algorithms
inspired by existing algorithms, and contribute one novel online
partitioning-transportation algorithm with information gathering in the
multi-agent team. Our algorithms have been implemented and tested extensively
in the simulation. The results presented in this paper demonstrate the
effectiveness of our algorithms that outperform the existing algorithms, even
without any communications between the agents and without the presence of a
reconnaissance agent.



We address the problem of maximizing an unknown submodular function that can
only be accessed via noisy evaluations. Our work is motivated by the task of
summarizing content, e.g., image collections, by leveraging users' feedback in
form of clicks or ratings. For summarization tasks with the goal of maximizing
coverage and diversity, submodular set functions are a natural choice. When the
underlying submodular function is unknown, users' feedback can provide noisy
evaluations of the function that we seek to maximize. We provide a generic
algorithm -- \submM{} -- for maximizing an unknown submodular function under
cardinality constraints. This algorithm makes use of a novel exploration module
-- \blbox{} -- that proposes good elements based on adaptively sampling noisy
function evaluations. \blbox{} is able to accommodate different kinds of
observation models such as value queries and pairwise comparisons. We provide
PAC-style guarantees on the quality and sampling cost of the solution obtained
by \submM{}. We demonstrate the effectiveness of our approach in an
interactive, crowdsourced image collection summarization application.



This paper introduces MazeBase: an environment for simple 2D games, designed
as a sandbox for machine learning approaches to reasoning and planning. Within
it, we create 10 simple games embodying a range of algorithmic tasks (e.g.
if-then statements or set negation). A variety of neural models (fully
connected, convolutional network, memory network) are deployed via
reinforcement learning on these games, with and without a procedurally
generated curriculum. Despite the tasks' simplicity, the performance of the
models is far from optimal, suggesting directions for future development. We
also demonstrate the versatility of MazeBase by using it to emulate small
combat scenarios from StarCraft. Models trained on the MazeBase version can be
directly applied to StarCraft, where they consistently beat the in-game AI.



Many real-world relations can be represented by signed networks with positive
and negative links, as a result of which signed network analysis has attracted
increasing attention from multiple disciplines. With the increasing prevalence
of social media networks, signed network analysis has evolved from developing
and measuring theories to mining tasks. In this article, we present a review of
mining signed networks in the context of social media and discuss some
promising research directions and new frontiers. We begin by giving basic
concepts and unique properties and principles of signed networks. Then we
classify and review tasks of signed network mining with representative
algorithms. We also delineate some tasks that have not been extensively studied
with formal definitions and also propose research directions to expand the
field of signed network mining.



Embedding learning, a.k.a. representation learning, has been shown to be able
to model large-scale semantic knowledge graphs. A key concept is a mapping of
the knowledge graph to a tensor representation whose entries are predicted by
models using latent representations of generalized entities. Latent variable
models are well suited to deal with the high dimensionality and sparsity of
typical knowledge graphs. In recent publications the embedding models were
extended to also consider time evolutions, time patterns and subsymbolic
representations. In this paper we map embedding models, which were developed
purely as solutions to technical problems for modelling temporal knowledge
graphs, to various cognitive memory functions, in particular to semantic and
concept memory, episodic memory, sensory memory, short-term memory, and working
memory. We discuss learning, query answering, the path from sensory input to
semantic decoding, and the relationship between episodic memory and semantic
memory. We introduce a number of hypotheses on human memory that can be derived
from the developed mathematical models.



The construction of efficient and effective decision trees remains a key
topic in machine learning because of their simplicity and flexibility. A lot of
heuristic algorithms have been proposed to construct near-optimal decision
trees. ID3, C4.5 and CART are classical decision tree algorithms and the split
criteria they used are Shannon entropy, Gain Ratio and Gini index respectively.
All the split criteria seem to be independent, actually, they can be unified in
a Tsallis entropy framework. Tsallis entropy is a generalization of Shannon
entropy and provides a new approach to enhance decision trees' performance with
an adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC)
algorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index,
which generalizes the split criteria of decision trees. More importantly, we
reveal the relations between Tsallis entropy with different $q$ and other split
criteria. Experimental results on UCI data sets indicate that the TEC algorithm
achieves statistically significant improvement over the classical algorithms.



Matching natural language sentences is central for many applications such as
information retrieval and question answering. Existing deep models rely on a
single sentence representation or multiple granularity representations for
matching. However, such methods cannot well capture the contextualized local
information in the matching process. To tackle this problem, we present a new
deep architecture to match two sentences with multiple positional sentence
representations. Specifically, each positional sentence representation is a
sentence representation at this position, generated by a bidirectional long
short term memory (Bi-LSTM). The matching score is finally produced by
aggregating interactions between these different positional sentence
representations, through $k$-Max pooling and a multi-layer perceptron. Our
model has several advantages: (1) By using Bi-LSTM, rich context of the whole
sentence is leveraged to capture the contextualized local information in each
positional sentence representation; (2) By matching with multiple positional
sentence representations, it is flexible to aggregate different important
contextualized local information in a sentence to support the matching; (3)
Experiments on different tasks such as question answering and sentence
completion demonstrate the superiority of our model.



Multiagent systems appear in most social, economical, and political
situations. In the present work we extend the Deep Q-Learning Network
architecture proposed by Google DeepMind to multiagent environments and
investigate how two agents controlled by independent Deep Q-Networks interact
in the classic videogame Pong. By manipulating the classical rewarding scheme
of Pong we demonstrate how competitive and collaborative behaviors emerge.
Competitive agents learn to play and score efficiently. Agents trained under
collaborative rewarding schemes find an optimal strategy to keep the ball in
the game as long as possible. We also describe the progression from competitive
to collaborative behavior. The present work demonstrates that Deep Q-Networks
can become a practical tool for studying the decentralized learning of
multiagent systems living in highly complex environments.



Human language is recognized as a very complex domain since decades. No
computer system has been able to reach human levels of performance so far. The
only known computational system capable of proper language processing is the
human brain. While we gather more and more data about the brain, its
fundamental computational processes still remain obscure. The lack of a sound
computational brain theory also prevents the fundamental understanding of
Natural Language Processing. As always when science lacks a theoretical
foundation, statistical modeling is applied to accommodate as many sampled
real-world data as possible. An unsolved fundamental issue is the actual
representation of language (data) within the brain, denoted as the
Representational Problem. Starting with Jeff Hawkins' Hierarchical Temporal
Memory (HTM) theory, a consistent computational theory of the human cortex, we
have developed a corresponding theory of language data representation: The
Semantic Folding Theory. The process of encoding words, by using a topographic
semantic space as distributional reference frame into a sparse binary
representational vector is called Semantic Folding and is the central topic of
this document. Semantic Folding describes a method of converting language from
its symbolic representation (text) into an explicit, semantically grounded
representation that can be generically processed by Hawkins' HTM networks. As
it turned out, this change in representation, by itself, can solve many complex
NLP problems by applying Boolean operators and a generic similarity function
like the Euclidian Distance. Many practical problems of statistical NLP
systems, like the high cost of computation, the fundamental incongruity of
precision and recall , the complex tuning procedures etc., can be elegantly
overcome by applying Semantic Folding.



Reinforcement learning (RL) is a general and well-known method that a robot
can use to learn an optimal control policy to solve a particular task. We would
like to build a versatile robot that can learn multiple tasks, but using RL for
each of them would be prohibitively expensive in terms of both time and
wear-and-tear on the robot. To remedy this problem, we use the Policy Gradient
Efficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RL
algorithm that enables the robot to efficiently learn multiple consecutive
tasks by sharing knowledge between these tasks to accelerate learning and
improve performance. We implemented and evaluated three RL methods--Q-learning,
policy gradient RL, and PG-ELLA--on a ground robot whose task is to find a
target object in an environment under different surface conditions. In this
paper, we discuss our implementations as well as present an empirical analysis
of their learning performance.



Automated sentiment analysis and opinion mining is a complex process
concerning the extraction of useful subjective information from text. The
explosion of user generated content on the Web, especially the fact that
millions of users, on a daily basis, express their opinions on products and
services to blogs, wikis, social networks, message boards, etc., render the
reliable, automated export of sentiments and opinions from unstructured text
crucial for several commercial applications. In this paper, we present a novel
hybrid vectorization approach for textual resources that combines a weighted
variant of the popular Word2Vec representation (based on Term Frequency-Inverse
Document Frequency) representation and with a Bag- of-Words representation and
a vector of lexicon-based sentiment values. The proposed text representation
approach is assessed through the application of several machine learning
classification algorithms on a dataset that is used extensively in literature
for sentiment detection. The classification accuracy derived through the
proposed hybrid vectorization approach is higher than when its individual
components are used for text represenation, and comparable with
state-of-the-art sentiment detection methodologies.



This paper addresses the general problem of reinforcement learning (RL) in
partially observable environments. In 2013, our large RL recurrent neural
networks (RNNs) learned from scratch to drive simulated cars from
high-dimensional video input. However, real brains are more powerful in many
ways. In particular, they learn a predictive model of their initially unknown
environment, and somehow use it for abstract (e.g., hierarchical) planning and
reasoning. Guided by algorithmic information theory, we describe RNN-based AIs
(RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending
sequences of tasks, some of them provided by the user, others invented by the
RNNAI itself in a curious, playful fashion, to improve its RNN-based world
model. Unlike our previous model-building RNN-based RL machines dating back to
1990, the RNNAI learns to actively query its model for abstract reasoning and
planning and decision making, essentially "learning to think." The basic ideas
of this report can be applied to many other cases where one RNN-like system
exploits the algorithmic information content of another. They are taken from a
grant proposal submitted in Fall 2014, and also explain concepts such as
"mirror neurons." Experimental results will be described in separate papers.



Information hierarchies are organizational structures that often used to
organize and present large and complex information as well as provide a
mechanism for effective human navigation. Fortunately, many statistical and
computational models exist that automatically generate hierarchies; however,
the existing approaches do not consider linkages in information {\em networks}
that are increasingly common in real-world scenarios. Current approaches also
tend to present topics as an abstract probably distribution over words, etc
rather than as tangible nodes from the original network. Furthermore, the
statistical techniques present in many previous works are not yet capable of
processing data at Web-scale. In this paper we present the Hierarchical
Document Topic Model (HDTM), which uses a distributed vertex-programming
process to calculate a nonparametric Bayesian generative model. Experiments on
three medium size data sets and the entire Wikipedia dataset show that HDTM can
infer accurate hierarchies even over large information networks.



An important problem for both graphics and vision is to synthesize novel
views of a 3D object from a single image. This is particularly challenging due
to the partial observability inherent in projecting a 3D object onto the image
space, and the ill-posedness of inferring object shape and pose. However, we
can train a neural network to address the problem if we restrict our attention
to specific object categories (in our case faces and chairs) for which we can
gather ample training data. In this paper, we propose a novel recurrent
convolutional encoder-decoder network that is trained end-to-end on the task of
rendering rotated objects starting from a single image. The recurrent structure
allows our model to capture long-term dependencies along a sequence of
transformations. We demonstrate the quality of its predictions for human faces
on the Multi-PIE dataset and for a dataset of 3D chair models, and also show
its ability to disentangle latent factors of variation (e.g., identity and
pose) without using full supervision.



Multi-tenancy hosting of users in cloud NoSQL data stores is favored by cloud
providers because it enables resource sharing at low operating cost.
Multi-tenancy takes several forms depending on whether the back-end file system
is a local file system (LFS) or a parallel file system (PFS), and on whether
tenants are independent or share data across tenants. In this thesis I focus on
and propose solutions to two cases: independent data-local file system, and
shared data-parallel file system.
  In the independent data-local file system case, resource contention occurs
under certain conditions in Cassandra and HBase, two state-of-the-art NoSQL
stores, causing performance degradation for one tenant by another. We
investigate the interference and propose two approaches. The first provides a
scheduling scheme that can approximate resource consumption, adapt to workload
dynamics and work in a distributed fashion. The second introduces a
workload-aware resource reservation approach to prevent interference. The
approach relies on a performance model obtained offline and plans the
reservation according to different workload resource demands. Results show the
approaches together can prevent interference and adapt to dynamic workloads
under multi-tenancy.
  In the shared data-parallel file system case, it has been shown that running
a distributed NoSQL store over PFS for shared data across tenants is not cost
effective. Overheads are introduced due to the unawareness of the NoSQL store
of PFS. This dissertation targets the key-value store (KVS), a specific form of
NoSQL stores, and proposes a lightweight KVS over a parallel file system to
improve efficiency. The solution is built on an embedded KVS for high
performance but uses novel data structures to support concurrent writes.
Results show the proposed system outperforms Cassandra and Voldemort in several
different workloads.



We consider the problem of learning preferences over trajectories for mobile
manipulators such as personal robots and assembly line robots. The preferences
we learn are more intricate than simple geometric constraints on trajectories;
they are rather governed by the surrounding context of various objects and
human interactions in the environment. We propose a coactive online learning
framework for teaching preferences in contextually rich environments. The key
novelty of our approach lies in the type of feedback expected from the user:
the human user does not need to demonstrate optimal trajectories as training
data, but merely needs to iteratively provide trajectories that slightly
improve over the trajectory currently proposed by the system. We argue that
this coactive preference feedback can be more easily elicited than
demonstrations of optimal trajectories. Nevertheless, theoretical regret bounds
of our algorithm match the asymptotic rates of optimal trajectory algorithms.
  We implement our algorithm on two high degree-of-freedom robots, PR2 and
Baxter, and present three intuitive mechanisms for providing such incremental
feedback. In our experimental evaluation we consider two context rich settings
-- household chores and grocery store checkout -- and show that users are able
to train the robot with just a few feedbacks (taking only a few
minutes).\footnote{Parts of this work has been published at NIPS and ISRR
conferences~\citep{Jain13,Jain13b}. This journal submission presents a
consistent full paper, and also includes the proof of regret bounds, more
details of the robotic system, and a thorough related work.}



This article discusses open scientific challenges for understanding
development and evolution of speech forms, as a commentary to Moulin-Frier et
al. (Moulin-Frier et al., 2015). Based on the analysis of mathematical models
of the origins of speech forms, with a focus on their assumptions , we study
the fundamental question of how speech can be formed out of non--speech, at
both developmental and evolutionary scales. In particular, we emphasize the
importance of embodied self-organization , as well as the role of mechanisms of
motivation and active curiosity-driven exploration in speech formation. Finally
, we discuss an evolutionary-developmental perspective of the origins of
speech.



We present a new concept - Wikiometrics - the derivation of metrics and
indicators from Wikipedia. Wikipedia provides an accurate representation of the
real world due to its size, structure, editing policy and popularity. We
demonstrate an innovative mining methodology, where different elements of
Wikipedia - content, structure, editorial actions and reader reviews - are used
to rank items in a manner which is by no means inferior to rankings produced by
experts or other methods. We test our proposed method by applying it to two
real-world ranking problems: top world universities and academic journals. Our
proposed ranking methods were compared to leading and widely accepted
benchmarks, and were found to be extremely correlative but with the advantage
of the data being publically available.



The developpment of the Internet of Things (IoT) concept revives Responsive
Environments (RE) technologies. Nowadays, the idea of a permanent connection
between physical and digital world is technologically possible. The capillar
Internet relates to the Internet extension into daily appliances such as they
become actors of Internet like any hu-man. The parallel development of
Machine-to-Machine communications and Arti cial Intelligence (AI) technics
start a new area of cybernetic. This paper presents an approach for Cybernetic
Organism (Cyborg) for RE based on Organic Computing (OC). In such approach,
each appli-ance is a part of an autonomic system in order to control a physical
environment. The underlying idea is that such systems must have self-x
properties in order to adapt their behavior to external disturbances with a
high-degree of autonomy.



In the Internet of Things (IoT) domain, various heterogeneous ubiquitous
devices would be able to connect and communicate with each other seamlessly,
irrespective of the domain. Semantic representation of data through detailed
standardized annotation has shown to improve the integration of the
interconnected heterogeneous devices. However, the semantic representation of
these heterogeneous data sources for environmental monitoring systems is not
yet well supported. To achieve the maximum benefits of IoT for drought
forecasting, a dedicated semantic middleware solution is required. This
research proposes a middleware that semantically represents and integrates
heterogeneous data sources with indigenous knowledge based on a unified
ontology for an accurate IoT-based drought early warning system (DEWS).



For time series comparisons, it has often been observed that z-score
normalized Euclidean distances far outperform the unnormalized variant. In this
paper we show that a z-score normalized, squared Euclidean Distance is, in
fact, equal to a distance based on Pearson Correlation. This has profound
impact on many distance-based classification or clustering methods. In addition
to this theoretically sound result we also show that the often used k-Means
algorithm formally needs a mod ification to keep the interpretation as Pearson
correlation strictly valid. Experimental results demonstrate that in many cases
the standard k-Means algorithm generally produces the same results.



Collaborative vocabulary development in the context of data integration is
the process of finding consensus between the experts of the different systems
and domains. The complexity of this process is increased with the number of
involved people, the variety of the systems to be integrated and the dynamics
of their domain. In this paper we advocate that the realization of a powerful
version control system is the heart of the problem. Driven by this idea and the
success of Git in the context of software development, we investigate the
applicability of Git for collaborative vocabulary development. Even though
vocabulary development and software development have much more similarities
than differences there are still important differences. These need to be
considered within the development of a successful versioning and collaboration
system for vocabulary development. Therefore, this paper starts by presenting
the challenges we were faced with during the creation of vocabularies
collaboratively and discusses its distinction to software development. Based on
these insights we propose Git4Voc which comprises guidelines how Git can be
adopted to vocabulary development. Finally, we demonstrate how Git hooks can be
implemented to go beyond the plain functionality of Git by realizing
vocabulary-specific features like syntactic validation and semantic diffs.



Speech based solutions have taken center stage with growth in the services
industry where there is a need to cater to a very large number of people from
all strata of the society. While natural language speech interfaces are the
talk in the research community, yet in practice, menu based speech solutions
thrive. Typically in a menu based speech solution the user is required to
respond by speaking from a closed set of words when prompted by the system. A
sequence of human speech response to the IVR prompts results in the completion
of a transaction. A transaction is deemed successful if the speech solution can
correctly recognize all the spoken utterances of the user whenever prompted by
the system. The usual mechanism to evaluate the performance of a speech
solution is to do an extensive test of the system by putting it to actual
people use and then evaluating the performance by analyzing the logs for
successful transactions. This kind of evaluation could lead to dissatisfied
test users especially if the performance of the system were to result in a poor
transaction completion rate. To negate this the Wizard of Oz approach is
adopted during evaluation of a speech system. Overall this kind of evaluations
is an expensive proposition both in terms of time and cost. In this paper, we
propose a method to evaluate the performance of a speech solution without
actually putting it to people use. We first describe the methodology and then
show experimentally that this can be used to identify the performance
bottlenecks of the speech solution even before the system is actually used thus
saving evaluation time and expenses.



This paper presents inference rules for Resource Description Framework (RDF),
RDF Schema (RDFS) and Web Ontology Language (OWL). Our formalization is based
on Notation 3 Logic, which extended RDF by logical symbols and created Semantic
Web logic for deductive RDF graph stores. We also propose OWL-P that is a
lightweight formalism of OWL and supports soft inferences by omitting complex
language constructs.



There is a large variety of objects and appliances in human environments,
such as stoves, coffee dispensers, juice extractors, and so on. It is
challenging for a roboticist to program a robot for each of these object types
and for each of their instantiations. In this work, we present a novel approach
to manipulation planning based on the idea that many household objects share
similarly-operated object parts. We formulate the manipulation planning as a
structured prediction problem and learn to transfer manipulation strategy
across different objects by embedding point-cloud, natural language, and
manipulation trajectory data into a shared embedding space using a deep neural
network. In order to learn semantically meaningful spaces throughout our
network, we introduce a method for pre-training its lower layers for multimodal
feature embedding and a method for fine-tuning this embedding space using a
loss-based margin. In order to collect a large number of manipulation
demonstrations for different objects, we develop a new crowd-sourcing platform
called Robobarista. We test our model on our dataset consisting of 116 objects
and appliances with 249 parts along with 250 language instructions, for which
there are 1225 crowd-sourced manipulation demonstrations. We further show that
our robot with our model can even prepare a cup of a latte with appliances it
has never seen before.



Finding inclusion-minimal "hitting sets" for a given collection of sets is a
fundamental combinatorial problem with applications in domains as diverse as
Boolean algebra, computational biology, and data mining. Much of the
algorithmic literature focuses on the problem of *recognizing* the collection
of minimal hitting sets; however, in many of the applications, it is more
important to *generate* these hitting sets. We survey twenty algorithms from
across a variety of domains, considering their history, classification, useful
features, and computational performance on a variety of synthetic and
real-world inputs. We also provide a suite of implementations of these
algorithms with a ready-to-use, platform-agnostic interface based on Docker
containers and the AlgoRun framework, so that interested computational
scientists can easily perform similar tests with inputs from their own research
areas on their own computers or through a convenient Web interface.



We consider the problem of maximizing a monotone submodular function under
noise. There has been a great deal of work on optimization of submodular
functions under various constraints, resulting in algorithms that provide
desirable approximation guarantees. In many applications, however, we do not
have access to the submodular function we aim to optimize, but rather to some
erroneous or noisy version of it. This raises the question of whether provable
guarantees are obtainable in presence of error and noise. We provide initial
answers, by focusing on the question of maximizing a monotone submodular
function under a cardinality constraint when given access to a noisy oracle of
the function. We show that:
  - For a cardinality constraint $k \geq 2$, there is an approximation
algorithm whose approximation ratio is arbitrarily close to $1-1/e$;
  - For $k=1$ there is an algorithm whose approximation ratio is arbitrarily
close to $1/2$. No randomized algorithm can obtain an approximation ratio
better than $1/2+o(1)$;
  -If the noise is adversarial, no non-trivial approximation guarantee can be
obtained.



Link prediction, or predicting the likelihood of a link in a knowledge graph
based on its existing state is a key research task. It differs from a
traditional link prediction task in that the links in a knowledge graph are
categorized into different predicates and the link prediction performance of
different predicates in a knowledge graph generally varies widely. In this
work, we propose a latent feature embedding based link prediction model which
considers the prediction task for each predicate disjointly. To learn the model
parameters it utilizes a Bayesian personalized ranking based optimization
technique. Experimental results on large-scale knowledge bases such as YAGO2
show that our link prediction approach achieves substantially higher
performance than several state-of-art approaches. We also show that for a given
predicate the topological properties of the knowledge graph induced by the
given predicate edges are key indicators of the link prediction performance of
that predicate in the knowledge graph.



Machine learning algorithms are increasingly influencing our decisions and
interacting with us in all parts of our daily lives. Therefore, just like for
power plants, highways, and myriad other engineered sociotechnical systems, we
must consider the safety of systems involving machine learning. In this paper,
we first discuss the definition of safety in terms of risk, epistemic
uncertainty, and the harm incurred by unwanted outcomes. Then we examine
dimensions, such as the choice of cost function and the appropriateness of
minimizing the empirical average training cost, along which certain real-world
applications may not be completely amenable to the foundational principle of
modern statistical machine learning: empirical risk minimization. In
particular, we note an emerging dichotomy of applications: ones in which safety
is important and risk minimization is not the complete story (we name these
Type A applications), and ones in which safety is not so critical and risk
minimization is sufficient (we name these Type B applications). Finally, we
discuss how four different strategies for achieving safety in engineering
(inherently safe design, safety reserves, safe fail, and procedural safeguards)
can be mapped to the machine learning context through interpretability and
causality of predictive models, objectives beyond expected prediction accuracy,
human involvement for labeling difficult or rare examples, and user experience
design of software.



In this paper, we design a Deep Dual-Domain ($\mathbf{D^3}$) based fast
restoration model to remove artifacts of JPEG compressed images. It leverages
the large learning capacity of deep networks, as well as the problem-specific
expertise that was hardly incorporated in the past design of deep
architectures. For the latter, we take into consideration both the prior
knowledge of the JPEG compression scheme, and the successful practice of the
sparsity-based dual-domain approach. We further design the One-Step Sparse
Inference (1-SI) module, as an efficient and light-weighted feed-forward
approximation of sparse coding. Extensive experiments verify the superiority of
the proposed $D^3$ model over several state-of-the-art methods. Specifically,
our best model is capable of outperforming the latest deep model for around 1
dB in PSNR, and is 30 times faster.



Visual recognition research often assumes a sufficient resolution of the
region of interest (ROI). That is usually violated in practice, inspiring us to
explore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROI
in a VLRR problem can be smaller than $16 \times 16$ pixels, and is challenging
to be recognized even by human experts. We attempt to solve the VLRR problem
using deep learning methods. Taking advantage of techniques primarily in super
resolution, domain adaptation and robust regression, we formulate a dedicated
deep learning method and demonstrate how these techniques are incorporated step
by step. Any extra complexity, when introduced, is fully justified by both
analysis and simulation results. The resulting \textit{Robust Partially Coupled
Networks} achieves feature enhancement and recognition simultaneously. It
allows for both the flexibility to combat the LR-HR domain mismatch, and the
robustness to outliers. Finally, the effectiveness of the proposed models is
evaluated on three different VLRR tasks, including face identification, digit
recognition and font recognition, all of which obtain very impressive
performances.



Top-N recommender systems have been investigated widely both in industry and
academia. However, the recommendation quality is far from satisfactory. In this
paper, we propose a simple yet promising algorithm. We fill the user-item
matrix based on a low-rank assumption and simultaneously keep the original
information. To do that, a nonconvex rank relaxation rather than the nuclear
norm is adopted to provide a better rank approximation and an efficient
optimization strategy is designed. A comprehensive set of experiments on real
datasets demonstrates that our method pushes the accuracy of Top-N
recommendation to a new level.



Location tagging, also known as geotagging or geolocation, is the process of
assigning geographical coordinates to input data. In this paper we present an
algorithm for location tagging of textual documents. Our approach makes use of
previous work in natural language processing by using a state-of-the-art
part-of-speech tagger and named entity recognizer to find blocks of text which
may refer to locations. A knowledge base (OpenStreatMap) is then used to find a
list of possible locations for each block. Finally, one location is chosen for
each block by assigning distance-based scores to each location and repeatedly
selecting the location and block with the best score. We tested our geolocation
algorithm with Wikipedia articles about topics with a well-defined geographical
location that are geotagged by the articles' authors, where classification
approaches have achieved median errors as low as 11 km, with attainable
accuracy limited by the class size. Our approach achieved a 10th percentile
error of 490 metres and median error of 54 kilometres on the Wikipedia dataset
we used. When considering the five location tags with the greatest scores, 50%
of articles were assigned at least one tag within 8.5 kilometres of the
article's author-assigned true location. We also tested our approach on Twitter
messages that are tagged with the location from which the message was sent.
Twitter texts are challenging because they are short and unstructured and often
do not contain words referring to the location they were sent from, but we
obtain potentially useful results. We explain how we use the Spark framework
for data analytics to collect and process our test data. In general,
classification-based approaches for location tagging may be reaching their
upper accuracy limit, but our precision-focused approach has high accuracy for
some texts and shows significant potential for improvement overall.



Based on the assumption that there exists a neural network that efficiently
represents a set of Boolean functions between all binary inputs and outputs, we
propose a process for developing and deploying neural networks whose weight
parameters, bias terms, input, and intermediate hidden layer output signals,
are all binary-valued, and require only basic bit logic for the feedforward
pass. The proposed Bitwise Neural Network (BNN) is especially suitable for
resource-constrained environments, since it replaces either floating or
fixed-point arithmetic with significantly more efficient bitwise operations.
Hence, the BNN requires for less spatial complexity, less memory bandwidth, and
less power consumption in hardware. In order to design such networks, we
propose to add a few training schemes, such as weight compression and noisy
backpropagation, which result in a bitwise network that performs almost as well
as its corresponding real-valued network. We test the proposed network on the
MNIST dataset, represented using binary features, and show that BNNs result in
competitive performance while offering dramatic computational savings.



For vehicle sharing schemes, where drop-off positions are not fixed, we
propose a pricing scheme, where the price depends in part on the distance
between where a vehicle is being dropped off and where the closest shared
vehicle is parked. Under certain restrictive assumptions, we show that this
pricing leads to a socially optimal spread of the vehicles within a region.



This thesis investigates the generation of new concepts from combinations of
existing concepts as a language evolves. We give a method for combining
concepts, and will be investigating the utility of composite concepts in
language evolution and thence the utility of concept generation.



We investigate the emergence of shared concepts in a community of language
users using a multi-agent simulation. We extend results showing that negated
assertions are of use in developing shared categories, to include assertions
modified by linguistic hedges. Results show that using hedged assertions
positively affects the emergence of shared categories in two distinct ways.
Firstly, using contraction hedges like `very' gives better convergence over
time. Secondly, using expansion hedges such as `quite' reduces concept overlap.
However, both these improvements come at a cost of slower speed of development.



We investigate the generation of new concepts from combinations of properties
as an artificial language develops. To do so, we have developed a new framework
for conjunctive concept combination. This framework gives a semantic grounding
to the weighted sum approach to concept combination seen in the literature. We
implement the framework in a multi-agent simulation of language evolution and
show that shared combination weights emerge. The expected value and the
variance of these weights across agents may be predicted from the distribution
of elements in the conceptual space, as determined by the underlying
environment, together with the rate at which agents adopt others' concepts.
When this rate is smaller, the agents are able to converge to weights with
lower variance. However, the time taken to converge to a steady state
distribution of weights is longer.



We consider a general class of models, where a reinforcement learning (RL)
agent learns from cyclic interactions with an external environment via
classical signals. Perceptual inputs are encoded as quantum states, which are
subsequently transformed by a quantum channel representing the agent's memory,
while the outcomes of measurements performed at the channel's output determine
the agent's actions. The learning takes place via stepwise modifications of the
channel properties. They are described by an update rule that is inspired by
the projective simulation (PS) model and equipped with a glow mechanism that
allows for a backpropagation of policy changes, analogous to the eligibility
traces in RL and edge glow in PS. In this way, the model combines features of
PS with the ability for generalization, offered by its physical embodiment as a
quantum system. We apply the agent to various setups of an invasion game and a
grid world, which serve as elementary model tasks allowing a direct comparison
with a basic classical PS agent.



In this paper, the idea of client verification in distributed systems is
presented. The proposed solution presents a sample system where client
verification through cloud resources using input signature is discussed. For
different signatures the proposed method has been examined. Research results
are presented and discussed to show potential advantages.



This preprint has been withdrawn by the author for revision



A mechanism called Eligibility Propagation is proposed to speed up the Time
Hopping technique used for faster Reinforcement Learning in simulations.
Eligibility Propagation provides for Time Hopping similar abilities to what
eligibility traces provide for conventional Reinforcement Learning. It
propagates values from one state to all of its temporal predecessors using a
state transitions graph. Experiments on a simulated biped crawling robot
confirm that Eligibility Propagation accelerates the learning process more than
3 times.



We study the derivational complexity induced by the dependency pair method,
enhanced with standard refinements. We obtain upper bounds on the derivational
complexity induced by the dependency pair method in terms of the derivational
complexity of the base techniques employed. In particular we show that the
derivational complexity induced by the dependency pair method based on some
direct technique, possibly refined by argument filtering, the usable rules
criterion, or dependency graphs, is primitive recursive in the derivational
complexity induced by the direct method. This implies that the derivational
complexity induced by a standard application of the dependency pair method
based on traditional termination orders like KBO, LPO, and MPO is exactly the
same as if those orders were applied as the only termination technique.



We reformulate Pratt's tableau decision procedure of checking satisfiability
of a set of formulas in PDL. Our formulation is simpler and more direct for
implementation. Extending the method we give the first EXPTIME (optimal)
tableau decision procedure not based on transformation for checking consistency
of an ABox w.r.t. a TBox in PDL (here, PDL is treated as a description logic).
We also prove the new result that the data complexity of the instance checking
problem in PDL is coNP-complete.



We show how polynomial path orders can be employed efficiently in conjunction
with weak innermost dependency pairs to automatically certify polynomial
runtime complexity of term rewrite systems and the polytime computability of
the functions computed. The established techniques have been implemented and we
provide ample experimental data to assess the new method.



Household robots need to communicate with human beings in a friendly fashion.
To achieve better understanding of displayed information, an importance and a
certainty of the information should be communicated together with the main
information. The proposed intent expression system aims to convey this
additional information using an eye robot. The eye motions are represented as
states in a pleasure-arousal space model. Change of the model state is
calculated by fuzzy inference according to the importance and certainty of the
displayed information. This change influences the arousal-sleep coordinate in
the space which corresponds to activeness in communication. The eye robot
provides a basic interface for the mascot robot system which is an easy to
understand information terminal for home environments in a humatronics society.



An intent expression system using eye robots is proposed for a mascot robot
system from a viewpoint of humatronics. The eye robot aims at providing a basic
interface method for an information terminal robot system. To achieve better
understanding of the displayed information, the importance and the degree of
certainty of the information should be communicated along with the main
content. The proposed intent expression system aims at conveying this
additional information using the eye robot system. Eye motions are represented
as the states in a pleasure-arousal space model. Changes in the model state are
calculated by fuzzy inference according to the importance and degree of
certainty of the displayed information. These changes influence the
arousal-sleep coordinates in the space that corresponds to levels of liveliness
during communication. The eye robot provides a basic interface for the mascot
robot system that is easy to be understood as an information terminal for home
environments in a humatronics society.



Subspace clustering has gained increasing popularity in the analysis of gene
expression data. Among subspace cluster models, the recently introduced
order-preserving sub-matrix (OPSM) has demonstrated high promise. An OPSM,
essentially a pattern-based subspace cluster, is a subset of rows and columns
in a data matrix for which all the rows induce the same linear ordering of
columns. Existing OPSM discovery methods do not scale well to increasingly
large expression datasets. In particular, twig clusters having few genes and
many experiments incur explosive computational costs and are completely pruned
off by existing methods. However, it is of particular interest to determine
small groups of genes that are tightly coregulated across many conditions. In
this paper, we present KiWi, an OPSM subspace clustering algorithm that is
scalable to massive datasets, capable of discovering twig clusters and
identifying negative as well as positive correlations. We extensively validate
KiWi using relevant biological datasets and show that KiWi correctly assigns
redundant probes to the same cluster, groups experiments with common clinical
annotations, differentiates real promoter sequences from negative control
sequences, and shows good association with cis-regulatory motif predictions.



Maximal frequent patterns superset checking plays an important role in the
efficient mining of complete Maximal Frequent Itemsets (MFI) and maximal search
space pruning. In this paper we present a new indexing approach, FastLMFI for
local maximal frequent patterns (itemset) propagation and maximal patterns
superset checking. Experimental results on different sparse and dense datasets
show that our work is better than the previous well known progressive focusing
technique. We have also integrated our superset checking approach with an
existing state of the art maximal itemsets algorithm Mafia, and compare our
results with current best maximal itemsets algorithms afopt-max and FP
(zhu)-max. Our results outperform afopt-max and FP (zhu)-max on dense (chess
and mushroom) datasets on almost all support thresholds, which shows the
effectiveness of our approach.



In this paper we present a novel hybrid (arraybased layout and vertical
bitmap layout) database representation approach for mining complete Maximal
Frequent Itemset (MFI) on sparse and large datasets. Our work is novel in terms
of scalability, item search order and two horizontal and vertical projection
techniques. We also present a maximal algorithm using this hybrid database
representation approach. Different experimental results on real and sparse
benchmark datasets show that our approach is better than previous state of art
maximal algorithms.



Mining frequent itemset using bit-vector representation approach is very
efficient for dense type datasets, but highly inefficient for sparse datasets
due to lack of any efficient bit-vector projection technique. In this paper we
present a novel efficient bit-vector projection technique, for sparse and dense
datasets. To check the efficiency of our bit-vector projection technique, we
present a new frequent itemset mining algorithm Ramp (Real Algorithm for Mining
Patterns) build upon our bit-vector projection technique. The performance of
the Ramp is compared with the current best (all, maximal and closed) frequent
itemset mining algorithms on benchmark datasets. Different experimental results
on sparse and dense datasets show that mining frequent itemset using Ramp is
faster than the current best algorithms, which show the effectiveness of our
bit-vector projection idea. We also present a new local maximal frequent
itemsets propagation and maximal itemset superset checking approach FastLMFI,
build upon our PBR bit-vector projection technique. Our different computational
experiments suggest that itemset maximality checking using FastLMFI is fast and
efficient than a previous will known progressive focusing approach.



Real world datasets are sparse, dirty and contain hundreds of items. In such
situations, discovering interesting rules (results) using traditional frequent
itemset mining approach by specifying a user defined input support threshold is
not appropriate. Since without any domain knowledge, setting support threshold
small or large can output nothing or a large number of redundant uninteresting
results. Recently a novel approach of mining only N-most/Top-K interesting
frequent itemsets has been proposed, which discovers the top N interesting
results without specifying any user defined support threshold. However, mining
interesting frequent itemsets without minimum support threshold are more costly
in terms of itemset search space exploration and processing cost. Thereby, the
efficiency of their mining highly depends upon three main factors (1) Database
representation approach used for itemset frequency counting, (2) Projection of
relevant transactions to lower level nodes of search space and (3) Algorithm
implementation technique. Therefore, to improve the efficiency of mining
process, in this paper we present two novel algorithms called (N-MostMiner and
Top-K-Miner) using the bit-vector representation approach which is very
efficient in terms of itemset frequency counting and transactions projection.
In addition to this, several efficient implementation techniques of N-MostMiner
and Top-K-Miner are also present which we experienced in our implementation.
Our experimental results on benchmark datasets suggest that the NMostMiner and
Top-K-Miner are very efficient in terms of processing time as compared to
current best algorithms BOMO and TFP.



The quality of training data for knowledge discovery in databases (KDD) and
data mining depends upon many factors, but handling missing values is
considered to be a crucial factor in overall data quality. Today real world
datasets contains missing values due to human, operational error, hardware
malfunctioning and many other factors. The quality of knowledge extracted,
learning and decision problems depend directly upon the quality of training
data. By considering the importance of handling missing values in KDD and data
mining tasks, in this paper we propose a novel Hybrid Missing values Imputation
Technique (HMiT) using association rules mining and hybrid combination of
k-nearest neighbor approach. To check the effectiveness of our HMiT missing
values imputation technique, we also perform detail experimental results on
real world datasets. Our results suggest that the HMiT technique is not only
better in term of accuracy but it also take less processing time as compared to
current best missing values imputation technique based on k-nearest neighbor
approach, which shows the effectiveness of our missing values imputation
technique.



Handling missing values in training datasets for constructing learning models
or extracting useful information is considered to be an important research task
in data mining and knowledge discovery in databases. In recent years, lot of
techniques are proposed for imputing missing values by considering attribute
relationships with missing value observation and other observations of training
dataset. The main deficiency of such techniques is that, they depend upon
single approach and do not combine multiple approaches, that why they are less
accurate. To improve the accuracy of missing values imputation, in this paper
we introduce a novel partial matching concept in association rules mining,
which shows better results as compared to full matching concept that we
described in our previous work. Our imputation technique combines the partial
matching concept in association rules with k-nearest neighbor approach. Since
this is a hybrid technique, therefore its accuracy is much better than as
compared to those techniques which depend upon single approach. To check the
efficiency of our technique, we also provide detail experimental results on
number of benchmark datasets which show better results as compared to previous
approaches.



We present a method for hedging in continuous time.



Computability logic (CL) (see http://www.cis.upenn.edu/~giorgi/cl.html ) is a
research program for redeveloping logic as a formal theory of computability, as
opposed to the formal theory of truth which it has more traditionally been.
Formulas in CL stand for interactive computational problems, seen as games
between a machine and its environment; logical operators represent operations
on such entities; and "truth" is understood as existence of an effective
solution. The formalism of CL is open-ended, and may undergo series of
extensions as the studies of the subject advance. So far three -- parallel,
sequential and choice -- sorts of conjunction and disjunction have been
studied. The present paper adds one more natural kind to this collection,
termed toggling. The toggling operations can be characterized as lenient
versions of choice operations where choices are retractable, being allowed to
be reconsidered any finite number of times. This way, they model
trial-and-error style decision steps in interactive computation. The main
technical result of this paper is constructing a sound and complete
axiomatization for the propositional fragment of computability logic whose
vocabulary, together with negation, includes all four -- parallel, toggling,
sequential and choice -- kinds of conjunction and disjunction. Along with
toggling conjunction and disjunction, the paper also introduces the toggling
versions of quantifiers and recurrence operations.



The problem of multi-agent learning and adaptation has attracted a great deal
of attention in recent years. It has been suggested that the dynamics of multi
agent learning can be studied using replicator equations from population
biology. Most existing studies so far have been limited to discrete strategy
spaces with a small number of available actions. In many cases, however, the
choices available to agents are better characterized by continuous spectra.
This paper suggests a generalization of the replicator framework that allows to
study the adaptive dynamics of Q-learning agents with continuous strategy
spaces. Instead of probability vectors, agents strategies are now characterized
by probability measures over continuous variables. As a result, the ordinary
differential equations for the discrete case are replaced by a system of
coupled integral--differential replicator equations that describe the mutual
evolution of individual agent strategies. We derive a set of functional
equations describing the steady state of the replicator dynamics, examine their
solutions for several two-player games, and confirm our analytical results
using simulations.



This paper studies the stable model semantics of logic programs with
(abstract) constraint atoms and their properties. We introduce a succinct
abstract representation of these constraint atoms in which a constraint atom is
represented compactly. We show two applications. First, under this
representation of constraint atoms, we generalize the Gelfond-Lifschitz
transformation and apply it to define stable models (also called answer sets)
for logic programs with arbitrary constraint atoms. The resulting semantics
turns out to coincide with the one defined by Son et al., which is based on a
fixpoint approach. One advantage of our approach is that it can be applied, in
a natural way, to define stable models for disjunctive logic programs with
constraint atoms, which may appear in the disjunctive head as well as in the
body of a rule. As a result, our approach to the stable model semantics for
logic programs with constraint atoms generalizes a number of previous
approaches. Second, we show that our abstract representation of constraint
atoms provides a means to characterize dependencies of atoms in a program with
constraint atoms, so that some standard characterizations and properties
relying on these dependencies in the past for logic programs with ordinary
atoms can be extended to logic programs with constraint atoms.



Our project aims at supporting the creation of sustainable and meaningful
longer-term human-robot relationships through the creation of embodied robots
with face recognition and natural language dialogue capabilities, which exploit
and publish social information available on the web (Facebook). Our main
underlying experimental hypothesis is that such relationships can be
significantly enhanced if the human and the robot are gradually creating a pool
of shared episodic memories that they can co-refer to (shared memories), and if
they are both embedded in a social web of other humans and robots they both
know and encounter (shared friends). In this paper, we are presenting such a
robot, which as we will see achieves two significant novelties.



Gesture recognition is mainly apprehensive on analyzing the functionality of
human wits. The main goal of gesture recognition is to create a system which
can recognize specific human gestures and use them to convey information or for
device control. Hand gestures provide a separate complementary modality to
speech for expressing ones ideas. Information associated with hand gestures in
a conversation is degree,discourse structure, spatial and temporal structure.
The approaches present can be mainly divided into Data-Glove Based and Vision
Based approaches. An important face feature point is the nose tip. Since nose
is the highest protruding point from the face. Besides that, it is not affected
by facial expressions.Another important function of the nose is that it is able
to indicate the head pose. Knowledge of the nose location will enable us to
align an unknown 3D face with those in a face database. Eye detection is
divided into eye position detection and eye contour detection. Existing works
in eye detection can be classified into two major categories: traditional
image-based passive approaches and the active IR based approaches. The former
uses intensity and shape of eyes for detection and the latter works on the
assumption that eyes have a reflection under near IR illumination and produce
bright/dark pupil effect. The traditional methods can be broadly classified
into three categories: template based methods,appearance based methods and
feature based methods. The purpose of this paper is to compare various human
Gesture recognition systems for interfacing machines directly to human wits
without any corporeal media in an ambient environment.



Recent years have witnessed the popularity of using rank minimization as a
regularizer for various signal processing and machine learning problems. As
rank minimization problems are often converted to nuclear norm minimization
(NNM) problems, they have to be solved iteratively and each iteration requires
computing a singular value decomposition (SVD). Therefore, their solution
suffers from the high computation cost of multiple SVDs. To relieve this issue,
we propose using the block Lanczos method to compute the partial SVDs, where
the principal singular subspaces obtained in the previous iteration are used to
start the block Lanczos procedure. To avoid the expensive reorthogonalization
in the Lanczos procedure, the block Lanczos procedure is performed for only a
few steps. Our block Lanczos with warm start (BLWS) technique can be adopted by
different algorithms that solve NNM problems. We present numerical results on
applying BLWS to Robust PCA and Matrix Completion problems. Experimental
results show that our BLWS technique usually accelerates its host algorithm by
at least two to three times.



We prove the following strong hardness result for learning: Given a
distribution of labeled examples from the hypercube such that there exists a
monomial consistent with $(1-\eps)$ of the examples, it is NP-hard to find a
halfspace that is correct on $(1/2+\eps)$ of the examples, for arbitrary
constants $\eps > 0$. In learning theory terms, weak agnostic learning of
monomials is hard, even if one is allowed to output a hypothesis from the much
bigger concept class of halfspaces. This hardness result subsumes a long line
of previous results, including two recent hardness results for the proper
learning of monomials and halfspaces. As an immediate corollary of our result
we show that weak agnostic learning of decision lists is NP-hard.
  Our techniques are quite different from previous hardness proofs for
learning. We define distributions on positive and negative examples for
monomials whose first few moments match. We use the invariance principle to
argue that regular halfspaces (all of whose coefficients have small absolute
value relative to the total $\ell_2$ norm) cannot distinguish between
distributions whose first few moments match. For highly non-regular subspaces,
we use a structural lemma from recent work on fooling halfspaces to argue that
they are ``junta-like'' and one can zero out all but the top few coefficients
without affecting the performance of the halfspace. The top few coefficients
form the natural list decoding of a halfspace in the context of dictatorship
tests/Label Cover reductions.
  We note that unlike previous invariance principle based proofs which are only
known to give Unique-Games hardness, we are able to reduce from a version of
Label Cover problem that is known to be NP-hard. This has inspired follow-up
work on bypassing the Unique Games conjecture in some optimal geometric
inapproximability results.



The output of an association rule miner is often huge in practice. This is
why several concise lossless representations have been proposed, such as the
"essential" or "representative" rules. We revisit the algorithm given by
Kryszkiewicz (Int. Symp. Intelligent Data Analysis 2001, Springer-Verlag LNCS
2189, 350-359) for mining representative rules. We show that its output is
sometimes incomplete, due to an oversight in its mathematical validation. We
propose alternative complete generators and we extend the approach to an
existing closure-aware basis similar to, and often smaller than, the
representative rules, namely the basis B*.



The Border algorithm and the iPred algorithm find the Hasse diagrams of FCA
lattices. We show that they can be generalized to arbitrary lattices. In the
case of iPred, this requires the identification of a join-semilattice
homomorphism into a distributive lattice.



Most of the existing information retrieval systems are based on bag of words
model and are not equipped with common world knowledge. Work has been done
towards improving the efficiency of such systems by using intelligent
algorithms to generate search queries, however, not much research has been done
in the direction of incorporating human-and-society level knowledge in the
queries. This paper is one of the first attempts where such information is
incorporated into the search queries using Wikipedia semantics. The paper
presents an essential shift from conventional token based queries to concept
based queries, leading to an enhanced efficiency of information retrieval
systems. To efficiently handle the automated query learning problem, we propose
Wikipedia-based Evolutionary Semantics (Wiki-ES) framework where concept based
queries are learnt using a co-evolving evolutionary procedure. Learning concept
based queries using an intelligent evolutionary procedure yields significant
improvement in performance which is shown through an extensive study using
Reuters newswire documents. Comparison of the proposed framework is performed
with other information retrieval systems. Concept based approach has also been
implemented on other information retrieval systems to justify the effectiveness
of a transition from token based queries to concept based queries.



Knowledge Representation is important issue in reinforcement learning. In
this paper, we bridge the gap between reinforcement learning and knowledge
representation, by providing a rich knowledge representation framework, based
on normal logic programs with answer set semantics, that is capable of solving
model-free reinforcement learning problems for more complex do-mains and
exploits the domain-specific knowledge. We prove the correctness of our
approach. We show that the complexity of finding an offline and online policy
for a model-free reinforcement learning problem in our approach is NP-complete.
Moreover, we show that any model-free reinforcement learning problem in MDP
environment can be encoded as a SAT problem. The importance of that is
model-free reinforcement



Temporal Logic Model Checking is a verification method in which we describe a
system, the model, and then we verify whether some properties, expressed in a
temporal logic formula, hold in the system. It has many industrial
applications. In order to improve performance, some tools allow preprocessing
of the model, verifying on-line a set of properties reusing the same compiled
model; we prove that the complexity of the Model Checking problem, without any
preprocessing or preprocessing the model or the formula in a polynomial data
structure, is the same. As a result preprocessing does not always exponentially
improve performance.
  Symbolic Model Checking algorithms work by manipulating sets of states, and
these sets are often represented by BDDs. It has been observed that the size of
BDDs may grow exponentially as the model and formula increase in size. As a
side result, we formally prove that a superpolynomial increase of the size of
these BDDs is unavoidable in the worst case. While this exponential growth has
been empirically observed, to the best of our knowledge it has never been
proved so far in general terms. This result not only holds for all types of
BDDs regardless of the variable ordering, but also for more powerful data
structures, such as BEDs, RBCs, MTBDDs, and ADDs.



In this paper, the uplink direct sequence code division multiple access
(DS-CDMA) multiuser detection problem (MuD) is studied into heuristic
perspective, named particle swarm optimization (PSO). Regarding different
system improvements for future technologies, such as high-order modulation and
diversity exploitation, a complete parameter optimization procedure for the PSO
applied to MuD problem is provided, which represents the major contribution of
this paper. Furthermore, the performance of the PSO-MuD is briefly analyzed via
Monte-Carlo simulations. Simulation results show that, after convergence, the
performance reached by the PSO-MuD is much better than the conventional
detector, and somewhat close to the single user bound (SuB). Rayleigh flat
channel is initially considered, but the results are further extend to
diversity (time and spatial) channels.



The ever-increasing amount of data in biomedical research, and in cancer
research in particular, needs to be managed to support efficient data access,
exchange and integration. Existing software infrastructures, such caGrid,
support access to distributed information annotated with a domain ontology.
However, caGrid's current querying functionality depends on the structure of
individual data resources without exploiting the semantic annotations. In this
paper, we present the design and development of an ontology-based querying
functionality that consists of: the generation of OWL2 ontologies from the
underlying data resources metadata and a query rewriting and translation
process based on reasoning, which converts a query at the domain ontology level
into queries at the software infrastructure level. We present a detailed
analysis of our approach as well as an extensive performance evaluation. While
the implementation and evaluation was performed for the caGrid infrastructure,
the approach could be applicable to other model and metadata-driven
environments for data sharing.



Mining association rules is a task of data mining, which extracts knowledge
in the form of significant implication relation of useful items (objects) from
a database. Mining multilevel association rules uses concept hierarchies, also
called taxonomies and defined as relations of type 'is-a' between objects, to
extract rules that items belong to different levels of abstraction. These rules
are more useful, more refined and more interpretable by the user. Several
algorithms have been proposed in the literature to discover the multilevel
association rules. In this article, we are interested in the problem of
discovering multi-level frequent itemsets under constraints, involving the user
in the research process. We proposed a technique for modeling and
interpretation of constraints in a context of use of concept hierarchies. Three
approaches for discovering multi-level frequent itemsets under constraints were
proposed and discussed: Basic approach, "Test and Generate" approach and
Pruning based Approach.



One of the most interesting scientific challenges nowadays deals with the
analysis and the understanding of complex networks' dynamics and how their
processes lead to emergence according to the interactions among their
components. In this paper we approach the definition of new methodologies for
the visualization and the exploration of the dynamics at play in real dynamic
social networks. We present a recently introduced formalism called TVG (for
time-varying graphs), which was initially developed to model and analyze
highly-dynamic and infrastructure-less communication networks such as mobile
ad-hoc networks, wireless sensor networks, or vehicular networks. We discuss
its applicability to complex networks in general, and social networks in
particular, by showing how it enables the specification and analysis of complex
dynamic phenomena in terms of temporal interactions, and allows to easily
switch the perspective between local and global dynamics. As an example, we
chose the case of scientific communities by analyzing portion of the ArXiv
repository (ten years of publications in physics) focusing on the social
determinants (e.g. goals and potential interactions among individuals) behind
the emergence and the resilience of scientific communities. We consider that
scientific communities are at the same time communities of practice (through
co-authorship) and that they exist also as representations in the scientists'
mind, since references to other scientists' works is not merely an objective
link to a relevant work, but it reveals social objects that one manipulates,
select and refers to. In the paper we show the emergence/selection of a
community as a goal-driven preferential attachment toward a set of authors
among which there are some key scientists (Nobel prizes).



Most instruments - formalisms, concepts, and metrics - for social networks
analysis fail to capture their dynamics. Typical systems exhibit different
scales of dynamics, ranging from the fine-grain dynamics of interactions (which
recently led researchers to consider temporal versions of distance,
connectivity, and related indicators), to the evolution of network properties
over longer periods of time. This paper proposes a general approach to study
that evolution for both atemporal and temporal indicators, based respectively
on sequences of static graphs and sequences of time-varying graphs that cover
successive time-windows. All the concepts and indicators, some of which are
new, are expressed using a time-varying graph formalism.



The paper addresses a new class of combinatorial problems which consist in
restructuring of solutions (as structures) in combinatorial optimization. Two
main features of the restructuring process are examined: (i) a cost of the
restructuring, (ii) a closeness to a goal solution. This problem corresponds to
redesign (improvement, upgrade) of modular systems or solutions. The
restructuring approach is described and illustrated for the following
combinatorial optimization problems: knapsack problem, multiple choice problem,
assignment problem, spanning tree problems. Examples illustrate the
restructuring processes.



In this paper we introduce the olog, or ontology log, a category-theoretic
model for knowledge representation (KR). Grounded in formal mathematics, ologs
can be rigorously formulated and cross-compared in ways that other KR models
(such as semantic networks) cannot. An olog is similar to a relational database
schema; in fact an olog can serve as a data repository if desired. Unlike
database schemas, which are generally difficult to create or modify, ologs are
designed to be user-friendly enough that authoring or reconfiguring an olog is
a matter of course rather than a difficult chore. It is hoped that learning to
author ologs is much simpler than learning a database definition language,
despite their similarity. We describe ologs carefully and illustrate with many
examples. As an application we show that any primitive recursive function can
be described by an olog. We also show that ologs can be aligned or connected
together into a larger network using functors. The various methods of
information flow and institutions can then be used to integrate local and
global world-views. We finish by providing several different avenues for future
research.



Despite the increasing diffusion of the Internet technology, TV remains the
principal medium of communication. People's perceptions, knowledge, beliefs and
opinions about matter of facts get (in)formed through the information reported
on by the mass-media. However, a single source of information (and consensus)
could be a potential cause of anomalies in the structure and evolution of a
society. Hence, as the information available (and the way it is reported) is
fundamental for our perceptions and opinions, the definition of conditions
allowing for a good information to be disseminated is a pressing challenge. In
this paper starting from a report on the last Italian political campaign in
2008, we derive a socio-cognitive computational model of opinion dynamics where
agents get informed by different sources of information. Then, a what-if
analysis, performed trough simulations on the model's parameters space, is
shown. In particular, the scenario implemented includes three main streams of
information acquisition, differing in both the contents and the perceived
reliability of the messages spread. Agents' internal opinion is updated either
by accessing one of the information sources, namely media and experts, or by
exchanging information with one another. They are also endowed with cognitive
mechanisms to accept, reject or partially consider the acquired information.



This article is a brief personal account of the past, present, and future of
algorithmic randomness, emphasizing its role in inductive inference and
artificial intelligence. It is written for a general audience interested in
science and philosophy. Intuitively, randomness is a lack of order or
predictability. If randomness is the opposite of determinism, then algorithmic
randomness is the opposite of computability. Besides many other things, these
concepts have been used to quantify Ockham's razor, solve the induction
problem, and define intelligence.



This paper addresses combinatorial optimization scheme for solving the
multicriteria Steiner tree problem for communication network topology design
(e.g., wireless mesh network). The solving scheme is based on several models:
multicriteria ranking, clustering, minimum spanning tree, and minimum Steiner
tree problem. An illustrative numerical example corresponds to designing a
covering long-distance Wi-Fi network (static Ad-Hoc network). The set of
criteria (i.e., objective functions) involves the following: total cost, total
edge length, overall throughput (capacity), and estimate of QoS. Obtained
computing results show the suggested solving scheme provides good network
topologies which can be compared with minimum spanning trees.



This study is focused on the development of the cortex-like visual object
recognition system. We propose a general framework, which consists of three
hierarchical levels (modules). These modules functionally correspond to the V1,
V4 and IT areas. Both bottom-up and top-down connections between the
hierarchical levels V4 and IT are employed. The higher the degree of matching
between the input and the preferred stimulus, the shorter the response time of
the neuron. Therefore information about a single stimulus is distributed in
time and is transmitted by the waves of spikes. The reciprocal connections and
waves of spikes implement predictive coding: an initial hypothesis is generated
on the basis of information delivered by the first wave of spikes and is tested
with the information carried by the consecutive waves. The development is
considered as extraction and accumulation of features in V4 and objects in IT.
Once stored a feature can be disposed, if rarely activated. This cause update
of feature repository. Consequently, objects in IT are also updated. This
illustrates the growing process and dynamical change of topological structures
of V4, IT and connections between these areas.



This article is concerned with automated complexity analysis of term rewrite
systems. Since these systems underlie much of declarative programming, time
complexity of functions defined by rewrite systems is of particular interest.
Among other results, we present a variant of the dependency pair method for
analysing runtime complexities of term rewrite systems automatically. The
established results significantly extent previously known techniques: we give
examples of rewrite systems subject to our methods that could previously not
been analysed automatically. Furthermore, the techniques have been implemented
in the Tyrolean Complexity Tool. We provide ample numerical data for assessing
the viability of the method.



We present the first method to handle curvature regularity in region-based
image segmentation and inpainting that is independent of initialization.
  To this end we start from a new formulation of length-based optimization
schemes, based on surface continuation constraints, and discuss the connections
to existing schemes. The formulation is based on a \emph{cell complex} and
considers basic regions and boundary elements. The corresponding optimization
problem is cast as an integer linear program.
  We then show how the method can be extended to include curvature regularity,
again cast as an integer linear program. Here, we are considering pairs of
boundary elements to reflect curvature. Moreover, a constraint set is derived
to ensure that the boundary variables indeed reflect the boundary of the
regions described by the region variables.
  We show that by solving the linear programming relaxation one gets quite
close to the global optimum, and that curvature regularity is indeed much
better suited in the presence of long and thin objects compared to standard
length regularity.



A computational challenge to validate the candidate disease genes identified
in a high-throughput genomic study is to elucidate the associations between the
set of candidate genes and disease phenotypes. The conventional gene set
enrichment analysis often fails to reveal associations between disease
phenotypes and the gene sets with a short list of poorly annotated genes,
because the existing annotations of disease causative genes are incomplete. We
propose a network-based computational approach called rcNet to discover the
associations between gene sets and disease phenotypes. Assuming coherent
associations between the genes ranked by their relevance to the query gene set,
and the disease phenotypes ranked by their relevance to the hidden target
disease phenotypes of the query gene set, we formulate a learning framework
maximizing the rank coherence with respect to the known disease phenotype-gene
associations. An efficient algorithm coupling ridge regression with label
propagation, and two variants are introduced to find the optimal solution of
the framework. We evaluated the rcNet algorithms and existing baseline methods
with both leave-one-out cross-validation and a task of predicting recently
discovered disease-gene associations in OMIM. The experiments demonstrated that
the rcNet algorithms achieved the best overall rankings compared to the
baselines. To further validate the reproducibility of the performance, we
applied the algorithms to identify the target diseases of novel candidate
disease genes obtained from recent studies of GWAS, DNA copy number variation
analysis, and gene expression profiling. The algorithms ranked the target
disease of the candidate genes at the top of the rank list in many cases across
all the three case studies. The rcNet algorithms are available as a webtool for
disease and gene set association analysis at
http://compbio.cs.umn.edu/dgsa_rcNet.



The paper describes a general glance to the use of element exchange
techniques for optimization over permutations. A multi-level description of
problems is proposed which is a fundamental to understand nature and complexity
of optimization problems over permutations (e.g., ordering, scheduling,
traveling salesman problem). The description is based on permutation
neighborhoods of several kinds (e.g., by improvement of an objective function).
Our proposed operational digraph and its kinds can be considered as a way to
understand convexity and polynomial solvability for combinatorial optimization
problems over permutations. Issues of an analysis of problems and a design of
hierarchical heuristics are discussed. The discussion leads to a multi-level
adaptive algorithm system which analyzes an individual problem and
selects/designs a solving strategy (trajectory).



Technology has had an unquestionable impact on the way people watch sports.
Along with this technological evolution has come a higher standard to ensure a
good viewing experience for the casual sports fan. It can be argued that the
pervasion of statistical analysis in sports serves to satiate the fan's desire
for detailed sports statistics. The goal of statistical analysis in sports is a
simple one: to eliminate subjective analysis. In this paper, we review previous
work that attempts to analyze various aspects in sports by using ideas from
Markov Chains, Bayesian Inference and Markov Chain Monte Carlo (MCMC) methods.
The unifying goal of these works is to achieve an accurate representation of
the player's ability, the sport, or the environmental effects on the player's
performance. With the prevalence of cheap computation, it is possible that
using techniques in Artificial Intelligence could improve the result of
statistical analysis in sport. This is best illustrated when evaluating
football using Neuro Dynamic Programming, a Control Theory paradigm heavily
based on theory in Stochastic processes. The results from this method suggest
that statistical analysis in sports may benefit from using ideas from the area
of Control Theory or Machine Learning



The number of publicly available Web services (WS) is continuously growing,
and in parallel, we are witnessing a rapid development in semantic-related web
technologies. The intersection of the semantic web and WS allows the
development of semantic WS. In this work, we adopt a complex network
perspective to perform a comparative analysis of the syntactic and semantic
approaches used to describe WS. From a collection of publicly available WS
descriptions, we extract syntactic and semantic WS interaction networks. We
take advantage of tools from the complex network field to analyze them and
determine their properties. We show that WS interaction networks exhibit some
of the typical characteristics observed in real-world networks, such as short
average distance between nodes and community structure. By comparing syntactic
and semantic networks through their properties, we show the introduction of
semantics in WS descriptions should improve the composition process.



Do two data samples come from different distributions? Recent studies of this
fundamental problem focused on embedding probability distributions into
sufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), to
compare distributions by the distance between their embeddings. We show that
Regularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-based
hypothesis testing, yields substantial improvements even when sample sizes are
small, and excels at hypothesis tests involving multiple comparisons with power
control. We derive asymptotic distributions under the null and alternative
hypotheses, and assess power control. Outstanding results are obtained on:
challenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solar
dataset.



In this paper, we firstly give a brief introduction of expectation
maximization (EM) algorithm, and then discuss the initial value sensitivity of
expectation maximization algorithm. Subsequently, we give a short proof of EM's
convergence. Then, we implement experiments with the expectation maximization
algorithm (We implement all the experiments on Gaussion mixture model (GMM)).
Our experiment with expectation maximization is performed in the following
three cases: initialize randomly; initialize with result of K-means; initialize
with result of K-medoids. The experiment result shows that expectation
maximization algorithm depend on its initial state or parameters. And we found
that EM initialized with K-medoids performed better than both the one
initialized with K-means and the one initialized randomly.



The extended mind hypothesis has stimulated much interest in cognitive
science. However, its core claim, i.e. that the process of cognition can extend
beyond the brain via the body and into the environment, has been heavily
criticized. A prominent critique of this claim holds that when some part of the
world is coupled to a cognitive system this does not necessarily entail that
the part is also constitutive of that cognitive system. This critique is known
as the "coupling-constitution fallacy". In this paper we respond to this
reductionist challenge by using an evolutionary robotics approach to create a
minimal model of two acoustically coupled agents. We demonstrate how the
interaction process as a whole has properties that cannot be reduced to the
contributions of the isolated agents. We also show that the neural dynamics of
the coupled agents has formal properties that are inherently impossible for
those neural networks in isolation. By keeping the complexity of the model to
an absolute minimum, we are able to illustrate how the coupling-constitution
fallacy is in fact based on an inadequate understanding of the constitutive
role of nonlinear interactions in dynamical systems theory.



We describe how the powerful "Divide and Concur" algorithm for constraint
satisfaction can be derived as a special case of a message-passing version of
the Alternating Direction Method of Multipliers (ADMM) algorithm for convex
optimization, and introduce an improved message-passing algorithm based on
ADMM/DC by introducing three distinct weights for messages, with "certain" and
"no opinion" weights, as well as the standard weight used in ADMM/DC. The
"certain" messages allow our improved algorithm to implement constraint
propagation as a special case, while the "no opinion" messages speed
convergence for some problems by making the algorithm focus only on active
constraints. We describe how our three-weight version of ADMM/DC can give
greatly improved performance for non-convex problems such as circle packing and
solving large Sudoku puzzles, while retaining the exact performance of ADMM for
convex problems. We also describe the advantages of our algorithm compared to
other message-passing algorithms based upon belief propagation.



We examine three different algorithms that enable the collision certificate
method from [Bialkowski, et al.] to handle the case of a centralized
multi-robot team. By taking advantage of symmetries in the configuration space
of multi-robot teams, our methods can significantly reduce the number of
collision checks vs. both [Bialkowski, et al.] and standard collision checking
implementations.



Here we describe the SHARE system, a web service based framework for
distributed querying and reasoning on the semantic web. The main innovations of
SHARE are: (1) the extension of a SPARQL query engine to perform on-demand data
retrieval from web services, and (2) the extension of an OWL reasoner to test
property restrictions by means of web service invocations. In addition to
enabling queries across distributed datasets, the system allows for a target
dataset that is significantly larger than is possible under current,
centralized approaches. Although the architecture is equally applicable to all
types of data, the SHARE system targets bioinformatics, due to the large number
of interoperable web services that are already available in this area. SHARE is
built entirely on semantic web standards, and is the successor of the BioMOBY
project.



We advance a doxastic interpretation for many of the logical connectives
considered in Dependence Logic and in its extensions, and we argue that Team
Semantics is a natural framework for reasoning about beliefs and belief
updates.



Annotation errors can significantly hurt classifier performance, yet datasets
are only growing noisier with the increased use of Amazon Mechanical Turk and
techniques like distant supervision that automatically generate labels. In this
paper, we present a robust extension of logistic regression that incorporates
the possibility of mislabelling directly into the objective. Our model can be
trained through nearly the same means as logistic regression, and retains its
efficiency on high-dimensional datasets. Through named entity recognition
experiments, we demonstrate that our approach can provide a significant
improvement over the standard model when annotation errors are present.



The intention of the present study is to establish general framework for
automated problem solving by approaching the task universal algebraically
introducing knowledge as realizations of generalized free algebra based nets,
graphs with gluing forms connecting in- and out-edges to nodes. Nets are caused
to undergo transformations in conceptual level by type wise differentiated
intervening net rewriting systems dispersing problems to abstract parts,
matching being determined by substitution relations. Achieved sets of
conceptual nets constitute congruent classes. New results are obtained within
construction of problem solving systems where solution algorithms are derived
parallel with other candidates applied to the same net classes. By applying
parallel transducer paths consisting of net rewriting systems to net classes
congruent quotient algebras are established and the manifested class rewriting
comprises all solution candidates whenever produced nets are in anticipated
languages liable to acceptance of net automata.



Recent research in robot exploration and mapping has focused on sampling
environmental hotspot fields. This exploration task is formalized by Low,
Dolan, and Khosla (2008) in a sequential decision-theoretic planning under
uncertainty framework called MASP. The time complexity of solving MASP
approximately depends on the map resolution, which limits its use in
large-scale, high-resolution exploration and mapping. To alleviate this
computational difficulty, this paper presents an information-theoretic approach
to MASP (iMASP) for efficient adaptive path planning; by reformulating the
cost-minimizing iMASP as a reward-maximizing problem, its time complexity
becomes independent of map resolution and is less sensitive to increasing robot
team size as demonstrated both theoretically and empirically. Using the
reward-maximizing dual, we derive a novel adaptive variant of maximum entropy
sampling, thus improving the induced exploration policy performance. It also
allows us to establish theoretical bounds quantifying the performance advantage
of optimal adaptive over non-adaptive policies and the performance quality of
approximately optimal vs. optimal adaptive policies. We show analytically and
empirically the superior performance of iMASP-based policies for sampling the
log-Gaussian process to that of policies for the widely-used Gaussian process
in mapping the hotspot field. Lastly, we provide sufficient conditions that,
when met, guarantee adaptivity has no benefit under an assumed environment
model.



This article presents the top-level of an ontology categorizing and
generalizing best practices and quality criteria or measures for Linked Data.
It permits to compare these techniques and have a synthetic organized view of
what can or should be done for knowledge sharing purposes. This ontology is
part of a general knowledge base that can be accessed and complemented by any
Web user. Thus, it can be seen as a cooperatively built library for the above
cited elements. Since they permit to evaluate information objects and create
better ones, these elements also permit knowledge-based tools and techniques -
as well as knowledge providers - to be evaluated and categorized based on their
input/output information objects. One top-level distinction permitting to
organize this ontology is the one between content, medium and containers of
descriptions. Various structural, ontological, syntactical and lexical
distinctions are then used.



Modelling and simulating the traffic of heavily used but secure environments
such as seaports and airports is of increasing importance. Errors made when
simulating these environments can have long standing economic, social and
environmental implications. This paper discusses issues and problems that may
arise when designing a simulation strategy. Data for the Port is presented,
methods for lightweight vehicle assessment that can be used to calibrate and
validate simulations are also discussed along with a diagnosis of
overcalibration issues. We show that decisions about where the intelligence
lies in a system has important repercussions for the reliability of system
statistics. Finally, conclusions are drawn about how microsimulations can be
moved forward as a robust planning tool for the 21st century.



Given a Markov Decision Process (MDP) with $n$ states and a totalnumber $m$
of actions, we study the number of iterations needed byPolicy Iteration (PI)
algorithms to converge to the optimal$\gamma$-discounted policy. We consider
two variations of PI: Howard'sPI that changes the actions in all states with a
positive advantage,and Simplex-PI that only changes the action in the state
with maximaladvantage. We show that Howard's PI terminates after at most
$O\left(\frac{m}{1-\gamma}\log\left(\frac{1}{1-\gamma}\right)\right)$iterations,
improving by a factor $O(\log n)$ a result by Hansen etal., while Simplex-PI
terminates after at most
$O\left(\frac{nm}{1-\gamma}\log\left(\frac{1}{1-\gamma}\right)\right)$iterations,
improving by a factor $O(\log n)$ a result by Ye. Undersome structural
properties of the MDP, we then consider bounds thatare independent of the
discount factor~$\gamma$: quantities ofinterest are bounds $\tau\_t$ and
$\tau\_r$---uniform on all states andpolicies---respectively on the
\emph{expected time spent in transientstates} and \emph{the inverse of the
frequency of visits in recurrentstates} given that the process starts from the
uniform distribution.Indeed, we show that Simplex-PI terminates after at most
$\tilde O\left(n^3 m^2 \tau\_t \tau\_r \right)$ iterations. This extends
arecent result for deterministic MDPs by Post & Ye, in which $\tau\_t\le 1$ and
$\tau\_r \le n$, in particular it shows that Simplex-PI isstrongly polynomial
for a much larger class of MDPs. We explain whysimilar results seem hard to
derive for Howard's PI. Finally, underthe additional (restrictive) assumption
that the state space ispartitioned in two sets, respectively states that are
transient andrecurrent for all policies, we show that both Howard's PI
andSimplex-PI terminate after at most $\tilde
O(m(n^2\tau\_t+n\tau\_r))$iterations.



Online learning with delayed feedback has received increasing attention
recently due to its several applications in distributed, web-based learning
problems. In this paper we provide a systematic study of the topic, and analyze
the effect of delay on the regret of online learning algorithms. Somewhat
surprisingly, it turns out that delay increases the regret in a multiplicative
way in adversarial problems, and in an additive way in stochastic problems. We
give meta-algorithms that transform, in a black-box fashion, algorithms
developed for the non-delayed case into ones that can handle the presence of
delays in the feedback loop. Modifications of the well-known UCB algorithm are
also developed for the bandit problem with delayed feedback, with the advantage
over the meta-algorithms that they can be implemented with lower complexity.



We aim to reduce the burden of programming and deploying autonomous systems
to work in concert with people in time-critical domains, such as military field
operations and disaster response. Deployment plans for these operations are
frequently negotiated on-the-fly by teams of human planners. A human operator
then translates the agreed upon plan into machine instructions for the robots.
We present an algorithm that reduces this translation burden by inferring the
final plan from a processed form of the human team's planning conversation. Our
approach combines probabilistic generative modeling with logical plan
validation used to compute a highly structured prior over possible plans. This
hybrid approach enables us to overcome the challenge of performing inference
over the large solution space with only a small amount of noisy data from the
team planning session. We validate the algorithm through human subject
experimentation and show we are able to infer a human team's final plan with
83% accuracy on average. We also describe a robot demonstration in which two
people plan and execute a first-response collaborative task with a PR2 robot.
To the best of our knowledge, this is the first work that integrates a logical
planning technique within a generative model to perform plan inference.



We construe smart meeting cinematography with a focus on professional
situations such as meetings and seminars, possibly conducted in a distributed
manner across socio-spatially separated groups. The basic objective in smart
meeting cinematography is to interpret professional interactions involving
people, and automatically produce dynamic recordings of discussions, debates,
presentations etc in the presence of multiple communication modalities. Typical
modalities include gestures (e.g., raising one's hand for a question,
applause), voice and interruption, electronic apparatus (e.g., pressing a
button), movement (e.g., standing-up, moving around) etc. ROTUNDE, an instance
of smart meeting cinematography concept, aims to: (a) develop
functionality-driven benchmarks with respect to the interpretation and control
capabilities of human-cinematographers, real-time video editors, surveillance
personnel, and typical human performance in everyday situations; (b) Develop
general tools for the commonsense cognitive interpretation of dynamic scenes
from the viewpoint of visuo-spatial cognition centred perceptual
narrativisation. Particular emphasis is placed on declarative representations
and interfacing mechanisms that seamlessly integrate within large-scale
cognitive (interaction) systems and companion technologies consisting of
diverse AI sub-components. For instance, the envisaged tools would provide
general capabilities for high-level commonsense reasoning about space, events,
actions, change, and interaction.



This manuscript discusses computation of the Partition Function (PF) and the
Minimum Weight Perfect Matching (MWPM) on arbitrary, non-bipartite graphs. We
present two novel problem formulations - one for computing the PF of a Perfect
Matching (PM) and one for finding MWPMs - that build upon the inter-related
Bethe Free Energy, Belief Propagation (BP), Loop Calculus (LC), Integer Linear
Programming (ILP) and Linear Programming (LP) frameworks. First, we describe an
extension of the LC framework to the PM problem. The resulting formulas, coined
(fractional) Bootstrap-BP, express the PF of the original model via the BFE of
an alternative PM problem. We then study the zero-temperature version of this
Bootstrap-BP formula for approximately solving the MWPM problem. We do so by
leveraging the Bootstrap-BP formula to construct a sequence of MWPM problems,
where each new problem in the sequence is formed by contracting odd-sized
cycles (or blossoms) from the previous problem. This Bootstrap-and-Contract
procedure converges reliably and generates an empirically tight upper bound for
the MWPM. We conclude by discussing the relationship between our iterative
procedure and the famous Blossom Algorithm of Edmonds '65 and demonstrate the
performance of the Bootstrap-and-Contract approach on a variety of weighted PM
problems.



Competition between a complex system's constituents and a corresponding
reward mechanism based on it have profound influence on the functioning,
stability, and evolution of the system. But determining the dominance hierarchy
or ranking among the constituent parts from the strongest to the weakest --
essential in determining reward or penalty -- is almost always an ambiguous
task due to the incomplete nature of competition networks. Here we introduce
``Natural Ranking," a desirably unambiguous ranking method applicable to a
complete (full) competition network, and formulate an analytical model based on
the Bayesian formula inferring the expected mean and error of the natural
ranking of nodes from an incomplete network. We investigate its potential and
uses in solving issues in ranking by applying to a real-world competition
network of economic and social importance.



Local Policy Search is a popular reinforcement learning approach for handling
large state spaces. Formally, it searches locally in a paramet erized policy
space in order to maximize the associated value function averaged over some
predefined distribution. It is probably commonly b elieved that the best one
can hope in general from such an approach is to get a local optimum of this
criterion. In this article, we show th e following surprising result:
\emph{any} (approximate) \emph{local optimum} enjoys a \emph{global performance
guarantee}. We compare this g uarantee with the one that is satisfied by Direct
Policy Iteration, an approximate dynamic programming algorithm that does some
form of Poli cy Search: if the approximation error of Local Policy Search may
generally be bigger (because local search requires to consider a space of s
tochastic policies), we argue that the concentrability coefficient that appears
in the performance bound is much nicer. Finally, we discuss several practical
and theoretical consequences of our analysis.



The paper presents an approach to olfactory search for a diffusive emitting
source of tracer (e.g. aerosol, gas) in an environment with unknown map of
randomly placed and shaped obstacles.
  The measurements of tracer concentration are sporadic, noisy and without
directional information. The search domain is discretised and modelled by a
finite two-dimensional lattice. The links is the lattice represent the
traversable paths for emitted particles and for the searcher. A missing link in
the lattice indicates a blocked paths, due to the walls or obstacles. The
searcher must simultaneously estimate the source parameters, the map of the
search domain and its own location within the map. The solution is formulated
in the sequential Bayesian framework and implemented as a Rao-Blackwellised
particle filter with information-driven motion control. The numerical results
demonstrate the concept and its performance.



We consider a voting setting where candidates have preferences about the
outcome of the election and are free to join or leave the election. The
corresponding candidacy game, where candidates choose strategically to
participate or not, has been studied %initially by Dutta et al., who showed
that no non-dictatorial voting procedure satisfying unanimity is
candidacy-strategyproof, that is, is such that the joint action where all
candidates enter the election is always a pure strategy Nash equilibrium. Dutta
et al. also showed that for some voting tree procedures, there are candidacy
games with no pure Nash equilibria, and that for the rule that outputs the
sophisticated winner of voting by successive elimination, all games have a pure
Nash equilibrium. No results were known about other voting rules. Here we prove
several such results. For four candidates, the message is, roughly, that most
scoring rules (with the exception of Borda) do not guarantee the existence of a
pure Nash equilibrium but that Condorcet-consistent rules, for an odd number of
voters, do. For five candidates, most rules we study no longer have this
guarantee. Finally, we identify one prominent rule that guarantees the
existence of a pure Nash equilibrium for any number of candidates (and for an
odd number of voters): the Copeland rule. We also show that under mild
assumptions on the voting rule, the existence of strong equilibria cannot be
guaranteed.



The random drift particle swarm optimization (RDPSO) algorithm, inspired by
the free electron model in metal conductors placed in an external electric
field, is presented, systematically analyzed and empirically studied in this
paper. The free electron model considers that electrons have both a thermal and
a drift motion in a conductor that is placed in an external electric field. The
motivation of the RDPSO algorithm is described first, and the velocity equation
of the particle is designed by simulating the thermal motion as well as the
drift motion of the electrons, both of which lead the electrons to a location
with minimum potential energy in the external electric field. Then, a
comprehensive analysis of the algorithm is made, in order to provide a deep
insight into how the RDPSO algorithm works. It involves a theoretical analysis
and the simulation of the stochastic dynamical behavior of a single particle in
the RDPSO algorithm. The search behavior of the algorithm itself is also
investigated in detail, by analyzing the interaction between the particles.
Some variants of the RDPSO algorithm are proposed by incorporating different
random velocity components with different neighborhood topologies. Finally,
empirical studies on the RDPSO algorithm are performed by using a set of
benchmark functions from the CEC2005 benchmark suite. Based on the theoretical
analysis of the particle's behavior, two methods of controlling the algorithmic
parameters are employed, followed by an experimental analysis on how to select
the parameter values, in order to obtain a good overall performance of the
RDPSO algorithm and its variants in real-world applications. A further
performance comparison between the RDPSO algorithms and other variants of PSO
is made to prove the efficiency of the RDPSO algorithms.



In this paper we give a partially mechanized proof of the correctness of
Steane's 7-qubit error correcting code, using the tool Quantomatic. To the best
of our knowledge, this represents the largest and most complicated verification
task yet carried out using Quantomatic.



Complementarity problems and variational inequalities arise in a wide variety
of areas, including machine learning, planning, game theory, and physical
simulation. In all of these areas, to handle large-scale problem instances, we
need fast approximate solution methods. One promising idea is Galerkin
approximation, in which we search for the best answer within the span of a
given set of basis functions. Bertsekas proposed one possible Galerkin method
for variational inequalities. However, this method can exhibit two problems in
practice: its approximation error is worse than might be expected based on the
ability of the basis to represent the desired solution, and each iteration
requires a projection step that is not always easy to implement efficiently.
So, in this paper, we present a new Galerkin method with improved behavior: our
new error bounds depend directly on the distance from the true solution to the
subspace spanned by our basis, and the only projections we require are onto the
feasible region or onto the span of our basis.



We position a narrative-centred computational model for high-level knowledge
representation and reasoning in the context of a range of assistive
technologies concerned with "visuo-spatial perception and cognition" tasks. Our
proposed narrative model encompasses aspects such as \emph{space, events,
actions, change, and interaction} from the viewpoint of commonsense reasoning
and learning in large-scale cognitive systems. The broad focus of this paper is
on the domain of "human-activity interpretation" in smart environments, ambient
intelligence etc. In the backdrop of a "smart meeting cinematography" domain,
we position the proposed narrative model, preliminary work on perceptual
narrativisation, and the immediate outlook on constructing general-purpose
open-source tools for perceptual narrativisation.
  ACM Classification: I.2 Artificial Intelligence: I.2.0 General -- Cognitive
Simulation, I.2.4 Knowledge Representation Formalisms and Methods, I.2.10
Vision and Scene Understanding: Architecture and control structures, Motion,
Perceptual reasoning, Shape, Video analysis
  General keywords: cognitive systems; human-computer interaction; spatial
cognition and computation; commonsense reasoning; spatial and temporal
reasoning; assistive technologies



Many tasks in human environments require performing a sequence of navigation
and manipulation steps involving objects. In unstructured human environments,
the location and configuration of the objects involved often change in
unpredictable ways. This requires a high-level planning strategy that is robust
and flexible in an uncertain environment. We propose a novel dynamic planning
strategy, which can be trained from a set of example sequences. High level
tasks are expressed as a sequence of primitive actions or controllers (with
appropriate parameters). Our score function, based on Markov Random Field
(MRF), captures the relations between environment, controllers, and their
arguments. By expressing the environment using sets of attributes, the approach
generalizes well to unseen scenarios. We train the parameters of our MRF using
a maximum margin learning method. We provide a detailed empirical validation of
our overall framework demonstrating successful plan strategies for a variety of
tasks.



The objective of the paper is to design an agent which provides efficient
response to the caller when a call goes unanswered in smartphones. The agent
provides responses through text messages, email etc stating the most likely
reason as to why the callee is unable to answer a call. Responses are composed
taking into consideration the importance of the present call and the situation
the callee is in at the moment like driving, sleeping, at work etc. The agent
makes decisons in the compostion of response messages based on the patterns it
has come across in the learning environment. Initially the user helps the agent
to compose response messages. The agent associates this message to the percept
it recieves with respect to the environment the callee is in. The user may
thereafter either choose to make to response system automatic or choose to
recieve suggestions from the agent for responses messages and confirm what is
to be sent to the caller.



We consider the problem of learning good trajectories for manipulation tasks.
This is challenging because the criterion defining a good trajectory varies
with users, tasks and environments. In this paper, we propose a co-active
online learning framework for teaching robots the preferences of its users for
object manipulation tasks. The key novelty of our approach lies in the type of
feedback expected from the user: the human user does not need to demonstrate
optimal trajectories as training data, but merely needs to iteratively provide
trajectories that slightly improve over the trajectory currently proposed by
the system. We argue that this co-active preference feedback can be more easily
elicited from the user than demonstrations of optimal trajectories, which are
often challenging and non-intuitive to provide on high degrees of freedom
manipulators. Nevertheless, theoretical regret bounds of our algorithm match
the asymptotic rates of optimal trajectory algorithms. We demonstrate the
generalizability of our algorithm on a variety of grocery checkout tasks, for
whom, the preferences were not only influenced by the object being manipulated
but also by the surrounding environment.\footnote{For more details and a
demonstration video, visit: \url{http://pr.cs.cornell.edu/coactive}}



The idea of computer vision as the Bayesian inverse problem to computer
graphics has a long history and an appealing elegance, but it has proved
difficult to directly implement. Instead, most vision tasks are approached via
complex bottom-up processing pipelines. Here we show that it is possible to
write short, simple probabilistic graphics programs that define flexible
generative models and to automatically invert them to interpret real-world
images. Generative probabilistic graphics programs consist of a stochastic
scene generator, a renderer based on graphics software, a stochastic likelihood
model linking the renderer's output and the data, and latent variables that
adjust the fidelity of the renderer and the tolerance of the likelihood model.
Representations and algorithms from computer graphics, originally designed to
produce high-quality images, are instead used as the deterministic backbone for
highly approximate and stochastic generative models. This formulation combines
probabilistic programming, computer graphics, and approximate Bayesian
computation, and depends only on general-purpose, automatic inference
techniques. We describe two applications: reading sequences of degraded and
adversarially obscured alphanumeric characters, and inferring 3D road models
from vehicle-mounted camera images. Each of the probabilistic graphics programs
we present relies on under 20 lines of probabilistic code, and supports
accurate, approximately Bayesian inferences about ambiguous real-world images.



For most problems in science and engineering we can obtain data sets that
describe the observed system from various perspectives and record the behavior
of its individual components. Heterogeneous data sets can be collectively mined
by data fusion. Fusion can focus on a specific target relation and exploit
directly associated data together with contextual data and data about system's
constraints. In the paper we describe a data fusion approach with penalized
matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to
reveal hidden associations. The approach can directly consider any data that
can be expressed in a matrix, including those from feature-based
representations, ontologies, associations and networks. We demonstrate the
utility of DFMF for gene function prediction task with eleven different data
sources and for prediction of pharmacologic actions by fusing six data sources.
Our data fusion algorithm compares favorably to alternative data integration
approaches and achieves higher accuracy than can be obtained from any single
data source alone.



Learning policies that generalize across multiple tasks is an important and
challenging research topic in reinforcement learning and robotics. Training
individual policies for every single potential task is often impractical,
especially for continuous task variations, requiring more principled approaches
to share and transfer knowledge among similar tasks. We present a novel
approach for learning a nonlinear feedback policy that generalizes across
multiple tasks. The key idea is to define a parametrized policy as a function
of both the state and the task, which allows learning a single policy that
generalizes across multiple known and unknown tasks. Applications of our novel
approach to reinforcement and imitation learning in real-robot experiments are
shown.



The intuitive notion of evidence has both semantic and syntactic features. In
this paper, we develop an {\em evidence logic} for epistemic agents faced with
possibly contradictory evidence from different sources. The logic is based on a
neighborhood semantics, where a neighborhood $N$ indicates that the agent has
reason to believe that the true state of the world lies in $N$. Further notions
of relative plausibility between worlds and beliefs based on the latter
ordering are then defined in terms of this evidence structure, yielding our
intended models for evidence-based beliefs. In addition, we also consider a
second more general flavor, where belief and plausibility are modeled using
additional primitive relations, and we prove a representation theorem showing
that each such general model is a $p$-morphic image of an intended one. This
semantics invites a number of natural special cases, depending on how uniform
we make the evidence sets, and how coherent their total structure. We give a
structural study of the resulting `uniform' and `flat' models. Our main result
are sound and complete axiomatizations for the logics of all four major model
classes with respect to the modal language of evidence, belief and safe belief.
We conclude with an outlook toward logics for the dynamics of changing
evidence, and the resulting language extensions and connections with logics of
plausibility change.



The LCF tradition of interactive theorem proving, which was started by Milner
in the 1970-ies, appears to be tied to the classic READ-EVAL-PRINT-LOOP of
sequential and synchronous evaluation of prover commands. We break up this loop
and retrofit the read-eval-print phases into a model of parallel and
asynchronous proof processing. Thus we explain some key concepts of the
Isabelle/Scala approach to prover interaction and integration, and the
Isabelle/jEdit Prover IDE as front-end technology. We hope to open up the
scientific discussion about non-trivial interaction models for ITP systems
again, and help getting other old-school proof assistants on a similar track.



The modelling, analysis, and visualisation of dynamic geospatial phenomena
has been identified as a key developmental challenge for next-generation
Geographic Information Systems (GIS). In this context, the envisaged
paradigmatic extensions to contemporary foundational GIS technology raises
fundamental questions concerning the ontological, formal representational, and
(analytical) computational methods that would underlie their spatial
information theoretic underpinnings.
  We present the conceptual overview and architecture for the development of
high-level semantic and qualitative analytical capabilities for dynamic
geospatial domains. Building on formal methods in the areas of commonsense
reasoning, qualitative reasoning, spatial and temporal representation and
reasoning, reasoning about actions and change, and computational models of
narrative, we identify concrete theoretical and practical challenges that
accrue in the context of formal reasoning about `space, events, actions, and
change'. With this as a basis, and within the backdrop of an illustrated
scenario involving the spatio-temporal dynamics of urban narratives, we address
specific problems and solutions techniques chiefly involving `qualitative
abstraction', `data integration and spatial consistency', and `practical
geospatial abduction'. From a broad topical viewpoint, we propose that
next-generation dynamic GIS technology demands a transdisciplinary scientific
perspective that brings together Geography, Artificial Intelligence, and
Cognitive Science.
  Keywords: artificial intelligence; cognitive systems; human-computer
interaction; geographic information systems; spatio-temporal dynamics;
computational models of narrative; geospatial analysis; geospatial modelling;
ontology; qualitative spatial modelling and reasoning; spatial assistance
systems



There is growing interest in representing image data and feature descriptors
using compact binary codes for fast near neighbor search. Although binary codes
are motivated by their use as direct indices (addresses) into a hash table,
codes longer than 32 bits are not being used as such, as it was thought to be
ineffective. We introduce a rigorous way to build multiple hash tables on
binary code substrings that enables exact k-nearest neighbor search in Hamming
space. The approach is storage efficient and straightforward to implement.
Theoretical analysis shows that the algorithm exhibits sub-linear run-time
behavior for uniformly distributed codes. Empirical results show dramatic
speedups over a linear scan baseline for datasets of up to one billion codes of
64, 128, or 256 bits.



Graph comparison plays a major role in many network applications. We often
need a similarity metric for comparing networks according to their structural
properties. Various network features - such as degree distribution and
clustering coefficient - provide measurements for comparing networks from
different points of view, but a global and integrated distance metric is still
missing. In this paper, we employ distance metric learning algorithms in order
to construct an integrated distance metric for comparing structural properties
of complex networks. According to natural witnesses of network similarities
(such as network categories) the distance metric is learned by the means of a
dataset of some labeled real networks. For evaluating our proposed method which
is called NetDistance, we applied it as the distance metric in
K-nearest-neighbors classification. Empirical results show that NetDistance
outperforms previous methods, at least 20 percent, with respect to precision.



This paper proposes the meeting of fuzzy logic with paraconsistency in a very
precise and foundational way. Specifically, in this paper we introduce
expansions of the fuzzy logic MTL by means of primitive operators for
consistency and inconsistency in the style of the so-called Logics of Formal
Inconsistency (LFIs). The main novelty of the present approach is the
definition of postulates for this type of operators over MTL-algebras, leading
to the definition and axiomatization of a family of logics, expansions of MTL,
whose degree-preserving counterpart are paraconsistent and moreover LFIs.



The language of probability is used to define several different types of
conditional statements. There are four principal types: subjunctive, material,
existential, and feasibility. Two further types of conditionals are defined
using the propositional calculus and Boole's mathematical logic:
truth-functional and Boolean feasibility (which turn out to be special cases of
probabilistic conditionals). Each probabilistic conditional is quantified by a
fractional parameter between zero and one that says whether it is purely
affirmative, purely negative, or intermediate in its sense. Conditionals can be
specialized further by their content to express factuality and
counterfactuality, and revised or reformulated to account for exceptions and
confounding factors. The various conditionals have distinct mathematical
representations: through intermediate probability expressions and logical
formulas, each conditional is eventually translated into a set of polynomial
equations and inequalities (with real coefficients). The polynomial systems
from different types of conditionals exhibit different patterns of behavior,
concerning for example opposing conditionals or false antecedents. Interesting
results can be computed from the relevant polynomial systems using well-known
methods from algebra and computer science. Among other benefits, the proposed
framework of analysis offers paraconsistent procedures for logical deduction
that produce such familiar results as modus ponens, transitivity, disjunction
introduction, and disjunctive syllogism; all while avoiding any explosion of
consequences from inconsistent premises. Several example problems from Goodman
and Adams are analyzed. A new perspective called polylogicism is presented:
mathematical logic that respects the diversity among conditionals in particular
and logic problems in general.



Learning the Markov network structure from data is a problem that has
received considerable attention in machine learning, and in many other
application fields. This work focuses on a particular approach for this purpose
called independence-based learning. Such approach guarantees the learning of
the correct structure efficiently, whenever data is sufficient for representing
the underlying distribution. However, an important issue of such approach is
that the learned structures are encoded in an undirected graph. The problem
with graphs is that they cannot encode some types of independence relations,
such as the context-specific independences. They are a particular case of
conditional independences that is true only for a certain assignment of its
conditioning set, in contrast to conditional independences that must hold for
all its assignments. In this work we present CSPC, an independence-based
algorithm for learning structures that encode context-specific independences,
and encoding them in a log-linear model, instead of a graph. The central idea
of CSPC is combining the theoretical guarantees provided by the
independence-based approach with the benefits of representing complex
structures by using features in a log-linear model. We present experiments in a
synthetic case, showing that CSPC is more accurate than the state-of-the-art IB
algorithms when the underlying distribution contains CSIs.



Verification of multi-agents systems (MAS) has been recently studied taking
into account the need of expressing resource bounds. Several logics for
specifying properties of MAS have been presented in quite a variety of
scenarios with bounded resources. In this paper, we study a different
formalism, called Priced Resource-Bounded Alternating-time Temporal Logic
(PRBATL), whose main novelty consists in moving the notion of resources from a
syntactic level (part of the formula) to a semantic one (part of the model).
This allows us to track the evolution of the resource availability along the
computations and provides us with a formalisms capable to model a number of
real-world scenarios. Two relevant aspects are the notion of global
availability of the resources on the market, that are shared by the agents, and
the notion of price of resources, depending on their availability. In a
previous work of ours, an initial step towards this new formalism was
introduced, along with an EXPTIME algorithm for the model checking problem. In
this paper we better analyze the features of the proposed formalism, also in
comparison with previous approaches. The main technical contribution is the
proof of the EXPTIME-hardness of the the model checking problem for PRBATL,
based on a reduction from the acceptance problem for Linearly-Bounded
Alternating Turing Machines. In particular, since the problem has multiple
parameters, we show two fixed-parameter reductions.



We consider the problem of reinforcement learning over episodes of a
finite-horizon deterministic system and as a solution propose optimistic
constraint propagation (OCP), an algorithm designed to synthesize efficient
exploration and value function generalization. We establish that when the true
value function lies within a given hypothesis class, OCP selects optimal
actions over all but at most K episodes, where K is the eluder dimension of the
given hypothesis class. We establish further efficiency and asymptotic
performance guarantees that apply even if the true value function does not lie
in the given hypothesis class, for the special case where the hypothesis class
is the span of pre-specified indicator functions over disjoint sets. We also
discuss the computational complexity of OCP and present computational results
involving two illustrative examples.



Navigating through a visual maze relies on the strategic use of eye movements
to select and identify the route. When navigating the maze, there are
trade-offs between exploring to the environment and relying on memory. This
study examined strategies used to navigating through novel and familiar mazes
that were viewed from above and traversed by a mouse cursor. Eye and mouse
movements revealed two modes that almost never occurred concurrently:
exploration and guidance. Analyses showed that people learned mazes and were
able to devise and carry out complex, multi-faceted strategies that traded-off
visual exploration against active motor performance. These strategies took into
account available visual information, memory, confidence, the estimated cost in
time for exploration, and idiosyncratic tolerance for error. Understanding the
strategies humans used for maze solving is valuable for applications in
cognitive neuroscience as well as in AI, robotics and human-robot interactions.



In this paper, we define a new information theoretic measure that we call the
"uprooted information". We show that a necessary and sufficient condition for a
probability $P(s|do(t))$ to be "identifiable" (in the sense of Pearl) in a
graph $G$ is that its uprooted information be non-negative for all models of
the graph $G$. In this paper, we also give a new algorithm for deciding, for a
Bayesian net that is semi-Markovian, whether a probability $P(s|do(t))$ is
identifiable, and, if it is identifiable, for expressing it without allusions
to confounding variables. Our algorithm is closely based on a previous
algorithm by Tian and Pearl, but seems to correct a small flaw in theirs. In
this paper, we also find a {\it necessary and sufficient graphical condition}
for a probability $P(s|do(t))$ to be identifiable when $t$ is a singleton set.
So far, in the prior literature, it appears that only a {\it sufficient
graphical condition} has been given for this. By "graphical" we mean that it is
directly based on Judea Pearl's 3 rules of do-calculus.



Time-series classification has attracted considerable research attention due
to the various domains where time-series data are observed, ranging from
medicine to econometrics. Traditionally, the focus of time-series
classification has been on short time-series data composed of a unique pattern
with intraclass pattern distortions and variations, while recently there have
been attempts to focus on longer series composed of various local patterns.
This study presents a novel method which can detect local patterns in long
time-series via fitting local polynomial functions of arbitrary degrees. The
coefficients of the polynomial functions are converted to symbolic words via
equivolume discretizations of the coefficients' distributions. The symbolic
polynomial words enable the detection of similar local patterns by assigning
the same words to similar polynomials. Moreover, a histogram of the frequencies
of the words is constructed from each time-series' bag of words. Each row of
the histogram enables a new representation for the series and symbolize the
existence of local patterns and their frequencies. Experimental evidence
demonstrates outstanding results of our method compared to the state-of-art
baselines, by exhibiting the best classification accuracies in all the datasets
and having statistically significant improvements in the absolute majority of
experiments.



We investigate different approaches to integrating object recognition and
planning in a tabletop manipulation domain with the set of objects used in the
2012 RoboCup@Work competition. Results of our preliminary experiments show
that, with some approaches, close integration of perception and planning
improves the quality of plans, as well as the computation times of feasible
plans.



We present ReAct!, an interactive tool for high-level reasoning for cognitive
robotic applications. ReAct! enables robotic researchers to describe robots'
actions and change in dynamic domains, without having to know about the
syntactic and semantic details of the underlying formalism in advance, and
solve planning problems using state-of-the-art automated reasoners, without
having to learn about their input/output language or usage. In particular,
ReAct! can be used to represent sophisticated dynamic domains that feature
concurrency, indirect effects of actions, and state/transition constraints. It
allows for embedding externally defined calculations (e.g., checking for
collision-free continuous trajectories) into representations of hybrid domains
that require a tight integration of (discrete) high-level reasoning with
(continuous) geometric reasoning. ReAct! also enables users to solve planning
problems that involve complex goals. Such variety of utilities are useful for
robotic researchers to work on interesting and challenging domains, ranging
from service robotics to cognitive factories. ReAct! provides sample
formalizations of some action domains (e.g., multi-agent path planning, Tower
of Hanoi), as well as dynamic simulations of plans computed by a
state-of-the-art automated reasoner (e.g., a SAT solver or an ASP solver).



Research on distributed machine learning algorithms has focused primarily on
one of two extremes - algorithms that obey strict concurrency constraints or
algorithms that obey few or no such constraints. We consider an intermediate
alternative in which algorithms optimistically assume that conflicts are
unlikely and if conflicts do arise a conflict-resolution protocol is invoked.
We view this "optimistic concurrency control" paradigm as particularly
appropriate for large-scale machine learning algorithms, particularly in the
unsupervised setting. We demonstrate our approach in three problem areas:
clustering, feature learning and online facility location. We evaluate our
methods via large-scale experiments in a cluster computing environment.



In this paper, we study how an agent's belief is affected by her neighbors in
a social network. We first introduce a general framework, where every agent has
an initial belief on a statement, and updates her belief according to her and
her neighbors' current beliefs under some belief evolution functions, which,
arguably, should satisfy some basic properties. Then, we focus on the majority
rule belief evolution function, that is, an agent will (dis)believe the
statement iff more than half of her neighbors (dis)believe it. We consider some
fundamental issues about majority rule belief evolution, for instance, whether
the belief evolution process will eventually converge. The answer is no in
general. However, for random asynchronous belief evolution, this is indeed the
case.



The purpose of this paper is twofold. On one side, we present a general
framework for Bayesian optimization and we compare it with some related fields
in active learning and Bayesian numerical analysis. On the other hand, Bayesian
optimization and related problems (bandits, sequential experimental design) are
highly dependent on the surrogate model that is selected. However, there is no
clear standard in the literature. Thus, we present a fast and flexible toolbox
that allows to test and combine different models and criteria with little
effort. It includes most of the state-of-the-art contributions, algorithms and
models. Its speed also removes part of the stigma that Bayesian optimization
methods are only good for "expensive functions". The software is free and it
can be used in many operating systems and computer languages.



Stochastic models such as Continuous-Time Markov Chains (CTMC) and Stochastic
Hybrid Automata (SHA) are powerful formalisms to model and to reason about the
dynamics of biological systems, due to their ability to capture the
stochasticity inherent in biological processes. A classical question in formal
modelling with clear relevance to biological modelling is the model checking
problem. i.e. calculate the probability that a behaviour, expressed for
instance in terms of a certain temporal logic formula, may occur in a given
stochastic process. However, one may not only be interested in the notion of
satisfiability, but also in the capacity of a system to mantain a particular
emergent behaviour unaffected by the perturbations, caused e.g. from extrinsic
noise, or by possible small changes in the model parameters. To address this
issue, researchers from the verification community have recently proposed
several notions of robustness for temporal logic providing suitable definitions
of distance between a trajectory of a (deterministic) dynamical system and the
boundaries of the set of trajectories satisfying the property of interest. The
contributions of this paper are twofold. First, we extend the notion of
robustness to stochastic systems, showing that this naturally leads to a
distribution of robustness scores. By discussing two examples, we show how to
approximate the distribution of the robustness score and its key indicators:
the average robustness and the conditional average robustness. Secondly, we
show how to combine these indicators with the satisfaction probability to
address the system design problem, where the goal is to optimize some control
parameters of a stochastic model in order to best maximize robustness of the
desired specifications.



We present general principles underlying analysis of the dependence of random
variables (outputs) on deterministic conditions (inputs). Random outputs
recorded under mutually exclusive input values are labeled by these values and
considered stochastically unrelated, possessing no joint distribution. An input
that does not directly influence an output creates a context for the latter.
Any constraint imposed on the dependence of random outputs on inputs can be
characterized by considering all possible couplings (joint distributions)
imposed on stochastically unrelated outputs. The target application of these
principles is a quantum mechanical system of entangled particles, with
directions of spin measurements chosen for each particle being inputs and the
spins recorded outputs. The sphere of applicability, however, spans systems
across physical, biological, and behavioral sciences.



We review attempts that have been made towards understanding the
computational properties and mechanisms of input-driven dynamical systems like
RNNs, and reservoir computing networks in particular. We provide details on
methods that have been developed to give quantitative answers to the questions
above. Following this, we show how self-organization may be used to improve
reservoirs for better performance, in some cases guided by the measures
presented before. We also present a possible way to quantify task performance
using an information-theoretic approach, and finally discuss promising future
directions aimed at a better understanding of how these systems perform their
computations and how to best guide self-organized processes for their
optimization.



In several applications of automatic diagnosis and active learning a central
problem is the evaluation of a discrete function by adaptively querying the
values of its variables until the values read uniquely determine the value of
the function. In general, the process of reading the value of a variable might
involve some cost, computational or even a fee to be paid for the experiment
required for obtaining the value. This cost should be taken into account when
deciding the next variable to read. The goal is to design a strategy for
evaluating the function incurring little cost (in the worst case or in
expectation according to a prior distribution on the possible variables'
assignments). Our algorithm builds a strategy (decision tree) which attains a
logarithmic approxima- tion simultaneously for the expected and worst cost
spent. This is best possible under the assumption that $P \neq NP.$



In this paper we present a neural oscillator model of stimulus response
theory that exhibits quantum-like behavior. We then show that without adding
any additional assumptions, a quantum model constructed to fit observable
pairwise correlations has no predictive power over the unknown triple moment,
obtainable through the activation of multiple oscillators. We compare this with
the results obtained in de Barros (2013), where a criteria of rationality gives
optimal ranges for the triple moment.



Given appropriate representations of the semantic relations between carpenter
and wood and between mason and stone (for example, vectors in a vector space
model), a suitable algorithm should be able to recognize that these relations
are highly similar (carpenter is to wood as mason is to stone; the relations
are analogous). Likewise, with representations of dog, house, and kennel, an
algorithm should be able to recognize that the semantic composition of dog and
house, dog house, is highly similar to kennel (dog house and kennel are
synonymous). It seems that these two tasks, recognizing relations and
compositions, are closely connected. However, up to now, the best models for
relations are significantly different from the best models for compositions. In
this paper, we introduce a dual-space model that unifies these two tasks. This
model matches the performance of the best previous models for relations and
compositions. The dual-space model consists of a space for measuring domain
similarity and a space for measuring function similarity. Carpenter and wood
share the same domain, the domain of carpentry. Mason and stone share the same
domain, the domain of masonry. Carpenter and mason share the same function, the
function of artisans. Wood and stone share the same function, the function of
materials. In the composition dog house, kennel has some domain overlap with
both dog and house (the domains of pets and buildings). The function of kennel
is similar to the function of house (the function of shelters). By combining
domain and function similarities in various ways, we can model relations,
compositions, and other aspects of semantics.



We introduce a class of models for multidimensional control problems which we
call skip-free Markov decision processes on trees. We describe and analyse an
algorithm applicable to Markov decision processes of this type that are
skip-free in the negative direction. Starting with the finite average cost
case, we show that the algorithm combines the advantages of both value
iteration and policy iteration -- it is guaranteed to converge to an optimal
policy and optimal value function after a finite number of iterations but the
computational effort required for each iteration step is comparable with that
for value iteration. We show that the algorithm can also be used to solve
discounted cost models and continuous time models, and that a suitably modified
algorithm can be used to solve communicating models.



In this work we explore the use of reinforcement learning (RL) to help with
human decision making, combining state-of-the-art RL algorithms with an
application to prosthetics. Managing human-machine interaction is a problem of
considerable scope, and the simplification of human-robot interfaces is
especially important in the domains of biomedical technology and rehabilitation
medicine. For example, amputees who control artificial limbs are often required
to quickly switch between a number of control actions or modes of operation in
order to operate their devices. We suggest that by learning to anticipate
(predict) a user's behaviour, artificial limbs could take on an active role in
a human's control decisions so as to reduce the burden on their users.
Recently, we showed that RL in the form of general value functions (GVFs) could
be used to accurately detect a user's control intent prior to their explicit
control choices. In the present work, we explore the use of temporal-difference
learning and GVFs to predict when users will switch their control influence
between the different motor functions of a robot arm. Experiments were
performed using a multi-function robot arm that was controlled by muscle
signals from a user's body (similar to conventional artificial limb control).
Our approach was able to acquire and maintain forecasts about a user's
switching decisions in real time. It also provides an intuitive and reward-free
way for users to correct or reinforce the decisions made by the machine
learning system. We expect that when a system is certain enough about its
predictions, it can begin to take over switching decisions from the user to
streamline control and potentially decrease the time and effort needed to
complete tasks. This preliminary study therefore suggests a way to naturally
integrate human- and machine-based decision making systems.



We present a complete finite axiomatization of the unrestricted implication
problem for inclusion and conditional independence atoms in the context of
dependence logic. For databases, our result implies a finite axiomatization of
the unrestricted implication problem for inclusion, functional, and embedded
multivalued dependencies in the unirelational case.



We present the concept of Semantic Advertising which we see as the future of
online advertising. Semantic Advertising is online advertising powered by
semantic technology which essentially enables us to represent and reason with
concepts and the meaning of things. This paper aims to 1) Define semantic
advertising, 2) Place it in the context of broader and more widely used
concepts such as the Semantic Web and Semantic Search, 3) Provide a survey of
work in related areas such as context matching, and 4) Provide a perspective on
successful emerging technologies and areas of future work. We base our work on
our experience as a company developing semantic technologies aimed at realizing
the full potential of online advertising.



We propose a new family of message passing techniques for MAP estimation in
graphical models which we call {\em Sequential Reweighted Message Passing}
(SRMP). Special cases include well-known techniques such as {\em Min-Sum
Diffusion} (MSD) and a faster {\em Sequential Tree-Reweighted Message Passing}
(TRW-S). Importantly, our derivation is simpler than the original derivation of
TRW-S, and does not involve a decomposition into trees. This allows easy
generalizations. We present such a generalization for the case of higher-order
graphical models, and test it on several real-world problems with promising
results.



This paper presents a novel meta algorithm, Partition-Merge (PM), which takes
existing centralized algorithms for graph computation and makes them
distributed and faster. In a nutshell, PM divides the graph into small
subgraphs using our novel randomized partitioning scheme, runs the centralized
algorithm on each partition separately, and then stitches the resulting
solutions to produce a global solution. We demonstrate the efficiency of the PM
algorithm on two popular problems: computation of Maximum A Posteriori (MAP)
assignment in an arbitrary pairwise Markov Random Field (MRF), and modularity
optimization for community detection. We show that the resulting distributed
algorithms for these problems essentially run in time linear in the number of
nodes in the graph, and perform as well -- or even better -- than the original
centralized algorithm as long as the graph has geometric structures. Here we
say a graph has geometric structures, or polynomial growth property, when the
number of nodes within distance r of any given node grows no faster than a
polynomial function of r. More precisely, if the centralized algorithm is a
C-factor approximation with constant C \ge 1, the resulting distributed
algorithm is a (C+\delta)-factor approximation for any small \delta>0; but if
the centralized algorithm is a non-constant (e.g. logarithmic) factor
approximation, then the resulting distributed algorithm becomes a constant
factor approximation. For general graphs, we compute explicit bounds on the
loss of performance of the resulting distributed algorithm with respect to the
centralized algorithm.



The best current methods for exactly computing the number of satisfying
assignments, or the satisfying probability, of Boolean formulas can be seen,
either directly or indirectly, as building 'decision-DNNF' (decision
decomposable negation normal form) representations of the input Boolean
formulas. Decision-DNNFs are a special case of 'd-DNNF's where 'd' stands for
'deterministic'. We show that any decision-DNNF can be converted into an
equivalent 'FBDD' (free binary decision diagram) -- also known as a 'read-once
branching program' (ROBP or 1-BP) -- with only a quasipolynomial increase in
representation size in general, and with only a polynomial increase in size in
the special case of monotone k-DNF formulas. Leveraging known exponential lower
bounds for FBDDs, we then obtain similar exponential lower bounds for
decision-DNNFs which provide lower bounds for the recent algorithms. We also
separate the power of decision-DNNFs from d-DNNFs and a generalization of
decision-DNNFs known as AND-FBDDs. Finally we show how these imply exponential
lower bounds for natural problems associated with probabilistic databases.



We give a new consistent scoring function for structure learning of Bayesian
networks. In contrast to traditional approaches to scorebased structure
learning, such as BDeu or MDL, the complexity penalty that we propose is
data-dependent and is given by the probability that a conditional independence
test correctly shows that an edge cannot exist. What really distinguishes this
new scoring function from earlier work is that it has the property of becoming
computationally easier to maximize as the amount of data increases. We prove a
polynomial sample complexity result, showing that maximizing this score is
guaranteed to correctly learn a structure with no false edges and a
distribution close to the generating distribution, whenever there exists a
Bayesian network which is a perfect map for the data generating distribution.
Although the new score can be used with any search algorithm, we give empirical
results showing that it is particularly effective when used together with a
linear programming relaxation approach to Bayesian network structure learning.



We consider the problem of maximum a posteriori (MAP) inference in discrete
graphical models. We present a parallel MAP inference algorithm called
Bethe-ADMM based on two ideas: tree-decomposition of the graph and the
alternating direction method of multipliers (ADMM). However, unlike the
standard ADMM, we use an inexact ADMM augmented with a Bethe-divergence based
proximal function, which makes each subproblem in ADMM easy to solve in
parallel using the sum-product algorithm. We rigorously prove global
convergence of Bethe-ADMM. The proposed algorithm is extensively evaluated on
both synthetic and real datasets to illustrate its effectiveness. Further, the
parallel Bethe-ADMM is shown to scale almost linearly with increasing number of
cores.



We propose a method for learning cyclic causal models from a combination of
observational and interventional equilibrium data. Novel aspects of the
proposed method are its ability to work with continuous data (without assuming
linearity) and to deal with feedback loops. Within the context of biochemical
reactions, we also propose a novel way of modeling interventions that modify
the activity of compounds instead of their abundance. For computational
reasons, we approximate the nonlinear causal mechanisms by (coupled) local
linearizations, one for each experimental condition. We apply the method to
reconstruct a cellular signaling network from the flow cytometry data measured
by Sachs et al. (2005). We show that our method finds evidence in the data for
feedback loops and that it gives a more accurate quantitative description of
the data at comparable model complexity.



Consider a collection of weighted subsets of a ground set N. Given a query
subset Q of N, how fast can one (1) find the weighted sum over all subsets of
Q, and (2) sample a subset of Q proportionally to the weights? We present a
tree-based greedy heuristic, Treedy, that for a given positive tolerance d
answers such counting and sampling queries to within a guaranteed relative
error d and total variation distance d, respectively. Experimental results on
artificial instances and in application to Bayesian structure discovery in
Bayesian networks show that approximations yield dramatic savings in running
time compared to exact computation, and that Treedy typically outperforms a
previously proposed sorting-based heuristic.



We propose a kernel method to identify finite mixtures of nonparametric
product distributions. It is based on a Hilbert space embedding of the joint
distribution. The rank of the constructed tensor is equal to the number of
mixture components. We present an algorithm to recover the components by
partitioning the data points into clusters such that the variables are jointly
conditionally independent given the cluster. This method can be used to
identify finite confounders.



Hidden variables are ubiquitous in practical data analysis, and therefore
modeling marginal densities and doing inference with the resulting models is an
important problem in statistics, machine learning, and causal inference.
Recently, a new type of graphical model, called the nested Markov model, was
developed which captures equality constraints found in marginals of directed
acyclic graph (DAG) models. Some of these constraints, such as the so called
`Verma constraint', strictly generalize conditional independence. To make
modeling and inference with nested Markov models practical, it is necessary to
limit the number of parameters in the model, while still correctly capturing
the constraints in the marginal of a DAG model. Placing such limits is similar
in spirit to sparsity methods for undirected graphical models, and regression
models. In this paper, we give a log-linear parameterization which allows
sparse modeling with nested Markov models. We illustrate the advantages of this
parameterization with a simulation study.



Computational creativity is an emerging branch of artificial intelligence
that places computers in the center of the creative process. Broadly,
creativity involves a generative step to produce many ideas and a selective
step to determine the ones that are the best. Many previous attempts at
computational creativity, however, have not been able to achieve a valid
selective step. This work shows how bringing data sources from the creative
domain and from hedonic psychophysics together with big data analytics
techniques can overcome this shortcoming to yield a system that can produce
novel and high-quality creative artifacts. Our data-driven approach is
demonstrated through a computational creativity system for culinary recipes and
menus we developed and deployed, which can operate either autonomously or
semi-autonomously with human interaction. We also comment on the volume,
velocity, variety, and veracity of data in computational creativity.



We develop a Bayesian Poisson matrix factorization model for forming
recommendations from sparse user behavior data. These data are large user/item
matrices where each user has provided feedback on only a small subset of items,
either explicitly (e.g., through star ratings) or implicitly (e.g., through
views or purchases). In contrast to traditional matrix factorization
approaches, Poisson factorization implicitly models each user's limited
attention to consume items. Moreover, because of the mathematical form of the
Poisson likelihood, the model needs only to explicitly consider the observed
entries in the matrix, leading to both scalable computation and good predictive
performance. We develop a variational inference algorithm for approximate
posterior inference that scales up to massive data sets. This is an efficient
algorithm that iterates over the observed entries and adjusts an approximate
posterior over the user/item representations. We apply our method to large
real-world user data containing users rating movies, users listening to songs,
and users reading scientific papers. In all these settings, Bayesian Poisson
factorization outperforms state-of-the-art matrix factorization methods.



We investigate two new optimization problems -- minimizing a submodular
function subject to a submodular lower bound constraint (submodular cover) and
maximizing a submodular function subject to a submodular upper bound constraint
(submodular knapsack). We are motivated by a number of real-world applications
in machine learning including sensor placement and data subset selection, which
require maximizing a certain submodular function (like coverage or diversity)
while simultaneously minimizing another (like cooperative cost). These problems
are often posed as minimizing the difference between submodular functions [14,
35] which is in the worst case inapproximable. We show, however, that by
phrasing these problems as constrained optimization, which is more natural for
many applications, we achieve a number of bounded approximation guarantees. We
also show that both these problems are closely related and an approximation
algorithm solving one can be used to obtain an approximation guarantee for the
other. We provide hardness results for both problems thus showing that our
approximation factors are tight up to log-factors. Finally, we empirically
demonstrate the performance and good scalability properties of our algorithms.



Many database resources, such as Reactome, collect manually annotated
reactions, interactions, and pathways from peer-reviewed publications. The
interactors (e.g., a protein), interactions, and pathways in these data
resources are often represented as instances in using BioPAX, a standard
pathway data exchange format. However, these interactions are better
represented as classes (or universals) since they always occur given
appropriate conditions. This study aims to represent various human interaction
pathways and networks as classes via a formal ontology aligned with the Basic
Formal Ontology (BFO). Towards this goal, the Human Interaction Network
Ontology (HINO) was generated by extending the BFO-aligned Interaction Network
Ontology (INO). All human pathways and associated processes and interactors
listed in Reactome and represented in BioPAX were first converted to ontology
classes by aligning them under INO. Related terms and associated relations and
hierarchies from external ontologies (e.g., CHEBI and GO) were also retrieved
and imported into HINO. HINO ontology terms were resolved in the linked
ontology data server Ontobee. The RDF triples stored in the RDF triple store
are queryable through a SPARQL program. Such an ontology system supports
advanced pathway data integration and applications.



Belief Propagation has been widely used for marginal inference, however it is
slow on problems with large-domain variables and high-order factors. Previous
work provides useful approximations to facilitate inference on such models, but
lacks important anytime properties such as: 1) providing accurate and
consistent marginals when stopped early, 2) improving the approximation when
run longer, and 3) converging to the fixed point of BP. To this end, we propose
a message passing algorithm that works on sparse (partially instantiated)
domains, and converges to consistent marginals using dynamic message
scheduling. The algorithm grows the sparse domains incrementally, selecting the
next value to add using prioritization schemes based on the gradients of the
marginal inference objective. Our experiments demonstrate local anytime
consistency and fast convergence, providing significant speedups over BP to
obtain low-error marginals: up to 25 times on grid models, and up to 6 times on
a real-world natural language processing task.



Dempster-Shafer theory of evidence (D-S theory) is widely used in uncertain
information process. The basic probability assignment(BPA) is a key element in
D-S theory. How to measure the distance between two BPAs is an open issue. In
this paper, a new method to measure the distance of two BPAs is proposed. The
proposed method is a generalized of existing evidence distance. Numerical
examples are illustrated that the proposed method can overcome the shortcomings
of existing methods.



Knowledge Representation and Reasoning and Machine Learning are two important
fields in AI. Nonmonotonic logic programming (NMLP) and Answer Set Programming
(ASP) provide formal languages for representing and reasoning with commonsense
knowledge and realize declarative problem solving in AI. On the other side,
Inductive Logic Programming (ILP) realizes Machine Learning in logic
programming, which provides a formal background to inductive learning and the
techniques have been applied to the fields of relational learning and data
mining. Generally speaking, NMLP and ASP realize nonmonotonic reasoning while
lack the ability of learning. By contrast, ILP realizes inductive learning
while most techniques have been developed under the classical monotonic logic.
With this background, some researchers attempt to combine techniques in the
context of nonmonotonic ILP. Such combination will introduce a learning
mechanism to programs and would exploit new applications on the NMLP side,
while on the ILP side it will extend the representation language and enable us
to use existing solvers. Cross-fertilization between learning and nonmonotonic
reasoning can also occur in such as the use of answer set solvers for ILP,
speed-up learning while running answer set solvers, learning action theories,
learning transition rules in dynamical systems, abductive learning, learning
biological networks with inhibition, and applications involving default and
negation. This workshop is the first attempt to provide an open forum for the
identification of problems and discussion of possible collaborations among
researchers with complementary expertise. The workshop was held on September
15th of 2013 in Corunna, Spain. This post-proceedings contains five technical
papers (out of six accepted papers) and the abstract of the invited talk by Luc
De Raedt.



Non-linear dimensionality reduction techniques such as manifold learning
algorithms have become a common way for processing and analyzing
high-dimensional patterns that often have attached a target that corresponds to
the value of an unknown function. Their application to new points consists in
two steps: first, embedding the new data point into the low dimensional space
and then, estimating the function value on the test point from its neighbors in
the embedded space.
  However, finding the low dimension representation of a test point, while easy
for simple but often not powerful enough procedures such as PCA, can be much
more complicated for methods that rely on some kind of eigenanalysis, such as
Spectral Clustering (SC) or Diffusion Maps (DM). Similarly, when a target
function is to be evaluated, averaging methods like nearest neighbors may give
unstable results if the function is noisy. Thus, the smoothing of the target
function with respect to the intrinsic, low-dimensional representation that
describes the geometric structure of the examined data is a challenging task.
  In this paper we propose Auto-adaptive Laplacian Pyramids (ALP), an extension
of the standard Laplacian Pyramids model that incorporates a modified LOOCV
procedure that avoids the large cost of the standard one and offers the
following advantages: (i) it selects automatically the optimal function
resolution (stopping time) adapted to the data and its noise, (ii) it is easy
to apply as it does not require parameterization, (iii) it does not overfit the
training set and (iv) it adds no extra cost compared to other classical
interpolation methods. We illustrate numerically ALP's behavior on a synthetic
problem and apply it to the computation of the DM projection of new patterns
and to the extension to them of target function values on a radiation
forecasting problem over very high dimensional patterns.



Community Question Answering (CQA) websites have become valuable repositories
which host a massive volume of human knowledge. To maximize the utility of such
knowledge, it is essential to evaluate the quality of an existing question or
answer, especially soon after it is posted on the CQA website.
  In this paper, we study the problem of inferring the quality of questions and
answers through a case study of a software CQA (Stack Overflow). Our key
finding is that the quality of an answer is strongly positively correlated with
that of its question. Armed with this observation, we propose a family of
algorithms to jointly predict the quality of questions and answers, for both
quantifying numerical quality scores and differentiating the high-quality
questions/answers from those of low quality. We conduct extensive experimental
evaluations to demonstrate the effectiveness and efficiency of our methods.



Linear Dynamical System (LDS) is an elegant mathematical framework for
modeling and learning multivariate time series. However, in general, it is
difficult to set the dimension of its hidden state space. A small number of
hidden states may not be able to model the complexities of a time series, while
a large number of hidden states can lead to overfitting. In this paper, we
study methods that impose an $\ell_1$ regularization on the transition matrix
of an LDS model to alleviate the problem of choosing the optimal number of
hidden states. We incorporate a generalized gradient descent method into the
Maximum a Posteriori (MAP) framework and use Expectation Maximization (EM) to
iteratively achieve sparsity on the transition matrix of an LDS model. We show
that our Sparse Linear Dynamical System (SLDS) improves the predictive
performance when compared to ordinary LDS on a multivariate clinical time
series dataset.



We consider the quantified constraint satisfaction problem (QCSP) which is to
decide, given a structure and a first-order sentence (not assumed here to be in
prenex form) built from conjunction and quantification, whether or not the
sentence is true on the structure. We present a proof system for certifying the
falsity of QCSP instances and develop its basic theory; for instance, we
provide an algorithmic interpretation of its behavior. Our proof system places
the established Q-resolution proof system in a broader context, and also allows
us to derive QCSP tractability results.



Forward inference techniques such as sequential Monte Carlo and particle
Markov chain Monte Carlo for probabilistic programming can be implemented in
any programming language by creative use of standardized operating system
functionality including processes, forking, mutexes, and shared memory.
Exploiting this we have defined, developed, and tested a probabilistic
programming language intermediate representation language we call probabilistic
C, which itself can be compiled to machine code by standard compilers and
linked to operating system libraries yielding an efficient, scalable, portable
probabilistic programming compilation target. This opens up a new hardware and
systems research path for optimizing probabilistic programming systems.



Biological organisms are composed of numerous interconnected biochemical
processes. Diseases occur when normal functionality of these processes is
disrupted. Thus, understanding these biochemical processes and their
interrelationships is a primary task in biomedical research and a prerequisite
for diagnosing diseases, and drug development. Scientists studying these
processes have identified various pathways responsible for drug metabolism, and
signal transduction, etc.
  Newer techniques and speed improvements have resulted in deeper knowledge
about these pathways, resulting in refined models that tend to be large and
complex, making it difficult for a person to remember all aspects of it. Thus,
computer models are needed to analyze them. We want to build such a system that
allows modeling of biological systems and pathways in such a way that we can
answer questions about them.
  Many existing models focus on structural and/or factoid questions, using
surface-level knowledge that does not require understanding the underlying
model. We believe these are not the kind of questions that a biologist may ask
someone to test their understanding of the biological processes. We want our
system to answer the kind of questions a biologist may ask. Such questions
appear in early college level text books.
  Thus the main goal of our thesis is to develop a system that allows us to
encode knowledge about biological pathways and answer such questions about them
demonstrating understanding of the pathway. To that end, we develop a language
that will allow posing such questions and illustrate the utility of our
framework with various applications in the biological domain. We use some
existing tools with modifications to accomplish our goal.
  Finally, we apply our system to real world applications by extracting pathway
knowledge from text and answering questions related to drug development.



Sparse representation based classification (SRC) has been proved to be a
simple, effective and robust solution to face recognition. As it gets popular,
doubts on the necessity of enforcing sparsity starts coming up, and primary
experimental results showed that simply changing the $l_1$-norm based
regularization to the computationally much more efficient $l_2$-norm based
non-sparse version would lead to a similar or even better performance. However,
that's not always the case. Given a new classification task, it's still unclear
which regularization strategy (i.e., making the coefficients sparse or
non-sparse) is a better choice without trying both for comparison. In this
paper, we present as far as we know the first study on solving this issue,
based on plenty of diverse classification experiments. We propose a scoring
function for pre-selecting the regularization strategy using only the dataset
size, the feature dimensionality and a discrimination score derived from a
given feature representation. Moreover, we show that when dictionary learning
is taking into account, non-sparse representation has a more significant
superiority to sparse representation. This work is expected to enrich our
understanding of sparse/non-sparse collaborative representation for
classification and motivate further research activities.



Optimizing an interactive system against a predefined online metric is
particularly challenging, when the metric is computed from user feedback such
as clicks and payments. The key challenge is the counterfactual nature: in the
case of Web search, any change to a component of the search engine may result
in a different search result page for the same query, but we normally cannot
infer reliably from search log how users would react to the new result page.
Consequently, it appears impossible to accurately estimate online metrics that
depend on user feedback, unless the new engine is run to serve users and
compared with a baseline in an A/B test. This approach, while valid and
successful, is unfortunately expensive and time-consuming. In this paper, we
propose to address this problem using causal inference techniques, under the
contextual-bandit framework. This approach effectively allows one to run
(potentially infinitely) many A/B tests offline from search log, making it
possible to estimate and optimize online metrics quickly and inexpensively.
Focusing on an important component in a commercial search engine, we show how
these ideas can be instantiated and applied, and obtain very promising results
that suggest the wide applicability of these techniques.



It is widely known in the machine learning community that class noise can be
(and often is) detrimental to inducing a model of the data. Many current
approaches use a single, often biased, measurement to determine if an instance
is noisy. A biased measure may work well on certain data sets, but it can also
be less effective on a broader set of data sets. In this paper, we present
noise identification using classifier diversity (NICD) -- a method for deriving
a less biased noise measurement and integrating it into the learning process.
To lessen the bias of the noise measure, NICD selects a diverse set of
classifiers (based on their predictions of novel instances) to determine which
instances are noisy. We examine NICD as a technique for filtering, instance
weighting, and selecting the base classifiers of a voting ensemble. We compare
NICD with several other noise handling techniques that do not consider
classifier diversity on a set of 54 data sets and 5 learning algorithms. NICD
significantly increases the classification accuracy over the other considered
approaches and is effective across a broad set of data sets and learning
algorithms.



Dynamic geometry systems (DGS) have become basic tools in many areas of
geometry as, for example, in education. Geometry Automated Theorem Provers
(GATP) are an active area of research and are considered as being basic tools
in future enhanced educational software as well as in a next generation of
mechanized mathematics assistants. Recently emerged Web repositories of
geometric knowledge, like TGTP and Intergeo, are an attempt to make the already
vast data set of geometric knowledge widely available. Considering the large
amount of geometric information already available, we face the need of a query
mechanism for descriptions of geometric constructions.
  In this paper we discuss two approaches for describing geometric figures
(declarative and procedural), and present algorithms for querying geometric
figures in declaratively and procedurally described corpora, by using a DGS or
a dedicated controlled natural language for queries.



The selection of an appropriate competition format is critical for both the
success and credibility of any competition, both real and simulated. In this
paper, the automated parallelism offered by the RoboCupSoccer 2D simulation
league is leveraged to conduct a 28,000 game round-robin between the top 8
teams from RoboCup 2012 and 2013. A proposed new competition format is found to
reduce variation from the resultant statistically significant team performance
rankings by 75% and 67%, when compared to the actual competition results from
RoboCup 2012 and 2013 respectively. These results are statistically validated
by generating 10,000 random tournaments for each of the three considered
formats and comparing the respective distributions of ranking discrepancy.



High-throughput mRNA sequencing (RNA-Seq) is widely used for transcript
quantification of gene isoforms. Since RNA-Seq data alone is often not
sufficient to accurately identify the read origins from the isoforms for
quantification, we propose to explore protein domain-domain interactions as
prior knowledge for integrative analysis with RNA-seq data. We introduce a
Network-based method for RNA-Seq-based Transcript Quantification (Net-RSTQ) to
integrate protein domain-domain interaction network with short read alignments
for transcript abundance estimation. Based on our observation that the
abundances of the neighboring isoforms by domain-domain interactions in the
network are positively correlated, Net-RSTQ models the expression of the
neighboring transcripts as Dirichlet priors on the likelihood of the observed
read alignments against the transcripts in one gene. The transcript abundances
of all the genes are then jointly estimated with alternating optimization of
multiple EM problems. In simulation Net-RSTQ effectively improved isoform
transcript quantifications when isoform co-expressions correlate with their
interactions. qRT-PCR results on 25 multi-isoform genes in a stem cell line, an
ovarian cancer cell line, and a breast cancer cell line also showed that
Net-RSTQ estimated more consistent isoform proportions with RNA-Seq data. In
the experiments on the RNA-Seq data in The Cancer Genome Atlas (TCGA), the
transcript abundances estimated by Net-RSTQ are more informative for patient
sample classification of ovarian cancer, breast cancer and lung cancer. All
experimental results collectively support that Net-RSTQ is a promising approach
for isoform quantification.



A matroid is a notion of independence in combinatorial optimization which is
closely related to computational efficiency. In particular, it is well known
that the maximum of a constrained modular function can be found greedily if and
only if the constraints are associated with a matroid. In this paper, we bring
together the ideas of bandits and matroids, and propose a new class of
combinatorial bandits, matroid bandits. The objective in these problems is to
learn how to maximize a modular function on a matroid. This function is
stochastic and initially unknown. We propose a practical algorithm for solving
our problem, Optimistic Matroid Maximization (OMM); and prove two upper bounds,
gap-dependent and gap-free, on its regret. Both bounds are sublinear in time
and at most linear in all other quantities of interest. The gap-dependent upper
bound is tight and we prove a matching lower bound on a partition matroid
bandit. Finally, we evaluate our method on three real-world problems and show
that it is practical.



In this paper we introduce a Bayesian framework for solving a class of
problems termed Multi-agent Inverse Reinforcement Learning (MIRL). Compared to
the well-known Inverse Reinforcement Learning (IRL) problem, MIRL is formalized
in the context of a stochastic game rather than a Markov decision process
(MDP). Games bring two primary challenges: First, the concept of optimality,
central to MDPs, loses its meaning and must be replaced with a more general
solution concept, such as the Nash equilibrium. Second, the non-uniqueness of
equilibria means that in MIRL, in addition to multiple reasonable solutions for
a given inversion model, there may be multiple inversion models that are all
equally sensible approaches to solving the problem. We establish a theoretical
foundation for competitive two-agent MIRL problems and propose a Bayesian
optimization algorithm to solve the problem. We focus on the case of two-person
zero-sum stochastic games, developing a generative model for the likelihood of
unknown rewards of agents given observed game play assuming that the two agents
follow a minimax bipolicy. As a numerical illustration, we apply our method in
the context of an abstract soccer game. For the soccer game, we investigate
relationships between the extent of prior information and the quality of
learned rewards. Results suggest that covariance structure is more important
than mean value in reward priors.



A formal framework is given for the characterizability of a class of belief
revision operators, defined using minimization over a class of partial
preorders, by postulates. It is shown that for partial orders
characterizability implies a definability property of the class of partial
orders in monadic second-order logic. Based on a non-definability result for a
class of partial orders, an example is given of a non-characterizable class of
revision operators. This appears to be the first non-characterizability result
in belief revision.



There are plenty of problems where the data available is scarce and
expensive. We propose a generator of semi-artificial data with similar
properties to the original data which enables development and testing of
different data mining algorithms and optimization of their parameters. The
generated data allow a large scale experimentation and simulations without
danger of overfitting. The proposed generator is based on RBF networks which
learn sets of Gaussian kernels. Learned Gaussian kernels can be used in a
generative mode to generate the data from the same distributions. To asses
quality of the generated data we developed several workflows and used them to
evaluate the statistical properties of the generated data, structural
similarity, and predictive similarity using supervised and unsupervised
learning techniques. To determine usability of the proposed generator we
conducted a large scale evaluation using 51 UCI data sets. The results show a
considerable similarity between the original and generated data and indicate
that the method can be useful in several development and simulation scenarios.



We present chemlambda (or the chemical concrete machine), an artificial
chemistry with the following properties: (a) is Turing complete, (b) has a
model of decentralized, distributed computing associated to it, (c) works at
the level of individual (artificial) molecules, subject of reversible, but
otherwise deterministic interactions with a small number of enzymes, (d)
encodes information in the geometrical structure of the molecules and not in
their numbers, (e) all interactions are purely local in space and time. This is
part of a larger project to create computing, artificial chemistry and
artificial life in a distributed context, using topological and graphical
languages.



We extend the notion of anti-unification to cover equational theories and
present a method based on regular tree grammars to compute a finite
representation of E-generalization sets. We present a framework to combine
Inductive Logic Programming and E-generalization that includes an extension of
Plotkin's lgg theorem to the equational case. We demonstrate the potential
power of E-generalization by three example applications: computation of
suggestions for auxiliary lemmas in equational inductive proofs, computation of
construction laws for given term sequences, and learning of screen editor
command sequences.



We describe Venture, an interactive virtual machine for probabilistic
programming that aims to be sufficiently expressive, extensible, and efficient
for general-purpose use. Like Church, probabilistic models and inference
problems in Venture are specified via a Turing-complete, higher-order
probabilistic language descended from Lisp. Unlike Church, Venture also
provides a compositional language for custom inference strategies built out of
scalable exact and approximate techniques. We also describe four key aspects of
Venture's implementation that build on ideas from probabilistic graphical
models. First, we describe the stochastic procedure interface (SPI) that
specifies and encapsulates primitive random variables. The SPI supports custom
control flow, higher-order probabilistic procedures, partially exchangeable
sequences and ``likelihood-free'' stochastic simulators. It also supports
external models that do inference over latent variables hidden from Venture.
Second, we describe probabilistic execution traces (PETs), which represent
execution histories of Venture programs. PETs capture conditional dependencies,
existential dependencies and exchangeable coupling. Third, we describe
partitions of execution histories called scaffolds that factor global inference
problems into coherent sub-problems. Finally, we describe a family of
stochastic regeneration algorithms for efficiently modifying PET fragments
contained within scaffolds. Stochastic regeneration linear runtime scaling in
cases where many previous approaches scaled quadratically. We show how to use
stochastic regeneration and the SPI to implement general-purpose inference
strategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals
based on particle Markov chain Monte Carlo and mean-field variational inference
techniques.



In many "smart city" applications, congestion arises in part due to the
nature of signals received by individuals from a central authority. In the
model of Marecek et al. [arXiv:1406.7639, Int. J. Control 88(10), 2015], each
agent uses one out of multiple resources at each time instant. The per-use cost
of a resource depends on the number of concurrent users. A central authority
has up-to-date knowledge of the congestion across all resources and uses
randomisation to provide a scalar or an interval for each resource at each
time. In this paper, the interval to broadcast per resource is obtained by
taking the minima and maxima of costs observed within a time window of length
r, rather than by randomisation. We show that the resulting distribution of
agents across resources also converges in distribution, under plausible
assumptions about the evolution of the population over time.



Social status, defined as the relative rank or position that an individual
holds in a social hierarchy, is known to be among the most important motivating
forces in social behaviors. In this paper, we consider the notion of status
from the perspective of a position or title held by a person in an enterprise.
We study the intersection of social status and social networks in an
enterprise. We study whether enterprise communication logs can help reveal how
social interactions and individual status manifest themselves in social
networks. To that end, we use two enterprise datasets with three communication
channels --- voice call, short message, and email --- to demonstrate the
social-behavioral differences among individuals with different status. We have
several interesting findings and based on these findings we also develop a
model to predict social status. On the individual level, high-status
individuals are more likely to be spanned as structural holes by linking to
people in parts of the enterprise networks that are otherwise not well
connected to one another. On the community level, the principle of homophily,
social balance and clique theory generally indicate a "rich club" maintained by
high-status individuals, in the sense that this community is much more
connected, balanced and dense. Our model can predict social status of
individuals with 93% accuracy.



Conditional Value at Risk (CVaR) is a prominent risk measure that is being
used extensively in various domains. We develop a new formula for the gradient
of the CVaR in the form of a conditional expectation. Based on this formula, we
propose a novel sampling-based estimator for the CVaR gradient, in the spirit
of the likelihood-ratio method. We analyze the bias of the estimator, and prove
the convergence of a corresponding stochastic gradient descent algorithm to a
local CVaR optimum. Our method allows to consider CVaR optimization in new
domains. As an example, we consider a reinforcement learning application, and
learn a risk-sensitive controller for the game of Tetris.



The intent of this research is to generate a set of non-dominated policies
from which one of two agents (the leader) can select a most preferred policy to
control a dynamic system that is also affected by the control decisions of the
other agent (the follower). The problem is described by an infinite horizon,
partially observed Markov game (POMG). At each decision epoch, each agent
knows: its past and present states, its past actions, and noise corrupted
observations of the other agent's past and present states. The actions of each
agent are determined at each decision epoch based on these data. The leader
considers multiple objectives in selecting its policy. The follower considers a
single objective in selecting its policy with complete knowledge of and in
response to the policy selected by the leader. This leader-follower assumption
allows the POMG to be transformed into a specially structured, partially
observed Markov decision process (POMDP). This POMDP is used to determine the
follower's best response policy. A multi-objective genetic algorithm (MOGA) is
used to create the next generation of leader policies based on the fitness
measures of each leader policy in the current generation. Computing a fitness
measure for a leader policy requires a value determination calculation, given
the leader policy and the follower's best response policy. The policies from
which the leader can select a most preferred policy are the non-dominated
policies of the final generation of leader policies created by the MOGA. An
example is presented that illustrates how these results can be used to support
a manager of a liquid egg production process (the leader) in selecting a
sequence of actions to best control this process over time, given that there is
an attacker (the follower) who seeks to contaminate the liquid egg production
process with a chemical or biological toxin.



The interaction of two binary variables, assumed to be empirical
observations, has three degrees of freedom when expressed as a matrix of
frequencies. Usually, the size of causal influence of one variable on the other
is calculated as a single value, as increase in recovery rate for a medical
treatment, for example. We examine what is lost in this simplification, and
propose using two interface constants to represent positive and negative
implications separately. Given certain assumptions about non-causal outcomes,
the set of resulting epistemologies is a continuum. We derive a variety of
particular measures and contrast them with the one-dimensional index.



Continuous time Bayesian network classifiers are designed for temporal
classification of multivariate streaming data when time duration of events
matters and the class does not change over time. This paper introduces the
CTBNCToolkit: an open source Java toolkit which provides a stand-alone
application for temporal classification and a library for continuous time
Bayesian network classifiers. CTBNCToolkit implements the inference algorithm,
the parameter learning algorithm, and the structural learning algorithm for
continuous time Bayesian network classifiers. The structural learning algorithm
is based on scoring functions: the marginal log-likelihood score and the
conditional log-likelihood score are provided. CTBNCToolkit provides also an
implementation of the expectation maximization algorithm for clustering
purpose. The paper introduces continuous time Bayesian network classifiers. How
to use the CTBNToolkit from the command line is described in a specific
section. Tutorial examples are included to facilitate users to understand how
the toolkit must be used. A section dedicate to the Java library is proposed to
help further code extensions.



We propose a representation of graph as a functional object derived from the
power iteration of the underlying adjacency matrix. The proposed functional
representation is a graph invariant, i.e., the functional remains unchanged
under any reordering of the vertices. This property eliminates the difficulty
of handling exponentially many isomorphic forms. Bhattacharyya kernel
constructed between these functionals significantly outperforms the
state-of-the-art graph kernels on 3 out of the 4 standard benchmark graph
classification datasets, demonstrating the superiority of our approach. The
proposed methodology is simple and runs in time linear in the number of edges,
which makes our kernel more efficient and scalable compared to many widely
adopted graph kernels with running time cubic in the number of vertices.



Attributing a cyber-operation through the use of multiple pieces of technical
evidence (i.e., malware reverse-engineering and source tracking) and
conventional intelligence sources (i.e., human or signals intelligence) is a
difficult problem not only due to the effort required to obtain evidence, but
the ease with which an adversary can plant false evidence. In this paper, we
introduce a formal reasoning system called the InCA (Intelligent Cyber
Attribution) framework that is designed to aid an analyst in the attribution of
a cyber-operation even when the available information is conflicting and/or
uncertain. Our approach combines argumentation-based reasoning, logic
programming, and probabilistic models to not only attribute an operation but
also explain to the analyst why the system reaches its conclusions.



Our winning submission to the 2014 Kaggle competition for Large Scale
Hierarchical Text Classification (LSHTC) consists mostly of an ensemble of
sparse generative models extending Multinomial Naive Bayes. The
base-classifiers consist of hierarchically smoothed models combining document,
label, and hierarchy level Multinomials, with feature pre-processing using
variants of TF-IDF and BM25. Additional diversification is introduced by
different types of folds and random search optimization for different measures.
The ensemble algorithm optimizes macroFscore by predicting the documents for
each label, instead of the usual prediction of labels per document. Scores for
documents are predicted by weighted voting of base-classifier outputs with a
variant of Feature-Weighted Linear Stacking. The number of documents per label
is chosen using label priors and thresholding of vote scores. This document
describes the models and software used to build our solution. Reproducing the
results for our solution can be done by running the scripts included in the
Kaggle package. A package omitting precomputed result files is also
distributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0
for Weka and Meka dependencies.



Rare data in a large-scale database are called outliers that reveal
significant information in the real world. The subspace-based outlier detection
is regarded as a feasible approach in very high dimensional space. However, the
outliers found in subspaces are only part of the true outliers in high
dimensional space, indeed. The outliers hidden in normal-clustered points are
sometimes neglected in the projected dimensional subspace. In this paper, we
propose a robust subspace method for detecting such inner outliers in a given
dataset, which uses two dimensional-projections: detecting outliers in
subspaces with local density ratio in the first projected dimensions; finding
outliers by comparing neighbor's positions in the second projected dimensions.
Each point's weight is calculated by summing up all related values got in the
two steps projected dimensions, and then the points scoring the largest weight
values are taken as outliers. By taking a series of experiments with the number
of dimensions from 10 to 10000, the results show that our proposed method
achieves high precision in the case of extremely high dimensional space, and
works well in low dimensional space.



Finding rare information hidden in a huge amount of data from the Internet is
a necessary but complex issue. Many researchers have studied this issue and
have found effective methods to detect anomaly data in low dimensional space.
However, as the dimension increases, most of these existing methods perform
poorly in detecting outliers because of "high dimensional curse". Even though
some approaches aim to solve this problem in high dimensional space, they can
only detect some anomaly data appearing in low dimensional space and cannot
detect all of anomaly data which appear differently in high dimensional space.
To cope with this problem, we propose a new k-nearest section-based method
(k-NS) in a section-based space. Our proposed approach not only detects
outliers in low dimensional space with section-density ratio but also detects
outliers in high dimensional space with the ratio of k-nearest section against
average value. After taking a series of experiments with the dimension from 10
to 10000, the experiment results show that our proposed method achieves 100%
precision and 100% recall result in the case of extremely high dimensional
space, and better improvement in low dimensional space compared to our
previously proposed method.



In this paper, a mathematical theory of learning is proposed that has many
parallels with information theory. We consider Vapnik's General Setting of
Learning in which the learning process is defined to be the act of selecting a
hypothesis in response to a given training set. Such hypothesis can, for
example, be a decision boundary in classification, a set of centroids in
clustering, or a set of frequent item-sets in association rule mining.
Depending on the hypothesis space and how the final hypothesis is selected, we
show that a learning process can be assigned a numeric score, called learning
capacity, which is analogous to Shannon's channel capacity and satisfies
similar interesting properties as well such as the data-processing inequality
and the information-cannot-hurt inequality. In addition, learning capacity
provides the tightest possible bound on the difference between true risk and
empirical risk of the learning process for all loss functions that are
parametrized by the chosen hypothesis. It is also shown that the notion of
learning capacity equivalently quantifies how sensitive the choice of the final
hypothesis is to a small perturbation in the training set. Consequently,
algorithmic stability is both necessary and sufficient for generalization.
While the theory does not rely on concentration inequalities, we finally show
that analogs to classical results in learning theory using the Probably
Approximately Correct (PAC) model can be immediately deduced using this theory,
and conclude with information-theoretic bounds to learning capacity.



In this treatise we aim to build a hybrid network automated (self-adaptive)
security threats discovery and prevention system; by using unconventional
techniques and methods, including fuzzy logic and biological inspired
algorithms under the context of soft computing.



Many machine learning algorithms are based on the assumption that training
examples are drawn independently. However, this assumption does not hold
anymore when learning from a networked sample because two or more training
examples may share some common objects, and hence share the features of these
shared objects. We show that the classic approach of ignoring this problem
potentially can have a harmful effect on the accuracy of statistics, and then
consider alternatives. One of these is to only use independent examples,
discarding other information. However, this is clearly suboptimal. We analyze
sample error bounds in this networked setting, providing significantly improved
results. An important component of our approach is formed by efficient sample
weighting schemes, which leads to novel concentration inequalities.



The maximum mean discrepancy (MMD) is a recently proposed test statistic for
two-sample test. Its quadratic time complexity, however, greatly hampers its
availability to large-scale applications. To accelerate the MMD calculation, in
this study we propose an efficient method called FastMMD. The core idea of
FastMMD is to equivalently transform the MMD with shift-invariant kernels into
the amplitude expectation of a linear combination of sinusoid components based
on Bochner's theorem and Fourier transform (Rahimi & Recht, 2007). Taking
advantage of sampling of Fourier transform, FastMMD decreases the time
complexity for MMD calculation from $O(N^2 d)$ to $O(L N d)$, where $N$ and $d$
are the size and dimension of the sample set, respectively. Here $L$ is the
number of basis functions for approximating kernels which determines the
approximation accuracy. For kernels that are spherically invariant, the
computation can be further accelerated to $O(L N \log d)$ by using the Fastfood
technique (Le et al., 2013). The uniform convergence of our method has also
been theoretically proved in both unbiased and biased estimates. We have
further provided a geometric explanation for our method, namely ensemble of
circular discrepancy, which facilitates us to understand the insight of MMD,
and is hopeful to help arouse more extensive metrics for assessing two-sample
test. Experimental results substantiate that FastMMD is with similar accuracy
as exact MMD, while with faster computation speed and lower variance than the
existing MMD approximation methods.



In this paper, we present a novel two-stage metric learning algorithm. We
first map each learning instance to a probability distribution by computing its
similarities to a set of fixed anchor points. Then, we define the distance in
the input data space as the Fisher information distance on the associated
statistical manifold. This induces in the input data space a new family of
distance metric with unique properties. Unlike kernelized metric learning, we
do not require the similarity measure to be positive semi-definite. Moreover,
it can also be interpreted as a local metric learning algorithm with well
defined distance approximation. We evaluate its performance on a number of
datasets. It outperforms significantly other metric learning methods and SVM.



In both quantum mechanics and corpus linguistics based on vector spaces, the
notion of entanglement provides a means for the various subsystems to
communicate with each other. In this paper we examine a number of
implementations of the categorical framework of Coecke, Sadrzadeh and Clark
(2010) for natural language, from an entanglement perspective. Specifically,
our goal is to better understand in what way the level of entanglement of the
relational tensors (or the lack of it) affects the compositional structures in
practical situations. Our findings reveal that a number of proposals for verb
construction lead to almost separable tensors, a fact that considerably
simplifies the interactions between the words. We examine the ramifications of
this fact, and we show that the use of Frobenius algebras mitigates the
potential problems to a great extent. Finally, we briefly examine a machine
learning method that creates verb tensors exhibiting a sufficient level of
entanglement.



We consider the infinite-horizon discounted optimal control problem
formalized by Markov Decision Processes. We focus on several approximate
variations of the Policy Iteration algorithm: Approximate Policy Iteration,
Conservative Policy Iteration (CPI), a natural adaptation of the Policy Search
by Dynamic Programming algorithm to the infinite-horizon case (PSDP$_\infty$),
and the recently proposed Non-Stationary Policy iteration (NSPI(m)). For all
algorithms, we describe performance bounds, and make a comparison by paying a
particular attention to the concentrability constants involved, the number of
iterations and the memory required. Our analysis highlights the following
points: 1) The performance guarantee of CPI can be arbitrarily better than that
of API/API($\alpha$), but this comes at the cost of a relative---exponential in
$\frac{1}{\epsilon}$---increase of the number of iterations. 2) PSDP$_\infty$
enjoys the best of both worlds: its performance guarantee is similar to that of
CPI, but within a number of iterations similar to that of API. 3) Contrary to
API that requires a constant memory, the memory needed by CPI and PSDP$_\infty$
is proportional to their number of iterations, which may be problematic when
the discount factor $\gamma$ is close to 1 or the approximation error
$\epsilon$ is close to $0$; we show that the NSPI(m) algorithm allows to make
an overall trade-off between memory and performance. Simulations with these
schemes confirm our analysis.



In this paper we study lifted inference for the Weighted First-Order Model
Counting problem (WFOMC), which counts the assignments that satisfy a given
sentence in first-order logic (FOL); it has applications in Statistical
Relational Learning (SRL) and Probabilistic Databases (PDB). We present several
results. First, we describe a lifted inference algorithm that generalizes prior
approaches in SRL and PDB. Second, we provide a novel dichotomy result for a
non-trivial fragment of FO CNF sentences, showing that for each sentence the
WFOMC problem is either in PTIME or #P-hard in the size of the input domain; we
prove that, in the first case our algorithm solves the WFOMC problem in PTIME,
and in the second case it fails. Third, we present several properties of the
algorithm. Finally, we discuss limitations of lifted inference for symmetric
probabilistic databases (where the weights of ground literals depend only on
the relation name, and not on the constants of the domain), and prove the
impossibility of a dichotomy result for the complexity of probabilistic
inference for the entire language FOL.



Bayesian model averaging (BMA) is the state of the art approach for
overcoming model uncertainty. Yet, especially on small data sets, the results
yielded by BMA might be sensitive to the prior over the models. Credal Model
Averaging (CMA) addresses this problem by substituting the single prior over
the models by a set of priors (credal set). Such approach solves the problem of
how to choose the prior over the models and automates sensitivity analysis. We
discuss various CMA algorithms for building an ensemble of logistic regressors
characterized by different sets of covariates. We show how CMA can be
appropriately tuned to the case in which one is prior-ignorant and to the case
in which instead domain knowledge is available. CMA detects prior-dependent
instances, namely instances in which a different class is more probable
depending on the prior over the models. On such instances CMA suspends the
judgment, returning multiple classes. We thoroughly compare different BMA and
CMA variants on a real case study, predicting presence of Alpine marmot burrows
in an Alpine valley. We find that BMA is almost a random guesser on the
instances recognized as prior-dependent by CMA.



Extensional higher-order logic programming has been introduced as a
generalization of classical logic programming. An important characteristic of
this paradigm is that it preserves all the well-known properties of traditional
logic programming. In this paper we consider the semantics of negation in the
context of the new paradigm. Using some recent results from non-monotonic
fixed-point theory, we demonstrate that every higher-order logic program with
negation has a unique minimum infinite-valued model. In this way we obtain the
first purely model-theoretic semantics for negation in extensional higher-order
logic programming. Using our approach, we resolve an old paradox that was
introduced by W. W. Wadge in order to demonstrate the semantic difficulties of
higher-order logic programming.



This paper describe about a new methodology for developing and improving the
robotics field via artificial intelligence and internet of things. Now a day,
we can say Artificial Intelligence take the world into robotics. Almost all
industries use robots for lot of works. They are use co-operative robots to
make different kind of works. But there was some problem to make robot for
multi tasks. So there was a necessary new methodology to made multi tasking
robots. It will be done only by artificial intelligence and internet of things.



Many machine learning algorithms require the input to be represented as a
fixed-length feature vector. When it comes to texts, one of the most common
fixed-length features is bag-of-words. Despite their popularity, bag-of-words
features have two major weaknesses: they lose the ordering of the words and
they also ignore semantics of the words. For example, "powerful," "strong" and
"Paris" are equally distant. In this paper, we propose Paragraph Vector, an
unsupervised algorithm that learns fixed-length feature representations from
variable-length pieces of texts, such as sentences, paragraphs, and documents.
Our algorithm represents each document by a dense vector which is trained to
predict words in the document. Its construction gives our algorithm the
potential to overcome the weaknesses of bag-of-words models. Empirical results
show that Paragraph Vectors outperform bag-of-words models as well as other
techniques for text representations. Finally, we achieve new state-of-the-art
results on several text classification and sentiment analysis tasks.



The constraint satisfaction problem (CSP) on a relational structure B is to
decide, given a set of constraints on variables where the relations come from
B, whether or not there is a assignment to the variables satisfying all of the
constraints; the surjective CSP is the variant where one decides the existence
of a surjective satisfying assignment onto the universe of B. We present an
algebraic condition on the polymorphism clone of B and prove that it is
sufficient for the hardness of the surjective CSP on a finite structure B, in
the sense that this problem admits a reduction from a certain fixed-structure
CSP. To our knowledge, this is the first result that allows one to use
algebraic information from a relational structure B to infer information on the
complexity hardness of surjective constraint satisfaction on B. A corollary of
our result is that, on any finite non-trivial structure having only essentially
unary polymorphisms, surjective constraint satisfaction is NP-complete.



The Collective Graphical Model (CGM) models a population of independent and
identically distributed individuals when only collective statistics (i.e.,
counts of individuals) are observed. Exact inference in CGMs is intractable,
and previous work has explored Markov Chain Monte Carlo (MCMC) and MAP
approximations for learning and inference. This paper studies Gaussian
approximations to the CGM. As the population grows large, we show that the CGM
distribution converges to a multivariate Gaussian distribution (GCGM) that
maintains the conditional independence properties of the original CGM. If the
observations are exact marginals of the CGM or marginals that are corrupted by
Gaussian noise, inference in the GCGM approximation can be computed efficiently
in closed form. If the observations follow a different noise model (e.g.,
Poisson), then expectation propagation provides efficient and accurate
approximate inference. The accuracy and speed of GCGM inference is compared to
the MCMC and MAP methods on a simulated bird migration problem. The GCGM
matches or exceeds the accuracy of the MAP method while being significantly
faster.



We present a framework for learning human user models from joint-action
demonstrations that enables the robot to compute a robust policy for a
collaborative task with a human. The learning takes place completely
automatically, without any human intervention. First, we describe the
clustering of demonstrated action sequences into different human types using an
unsupervised learning algorithm. These demonstrated sequences are also used by
the robot to learn a reward function that is representative for each type,
through the employment of an inverse reinforcement learning algorithm. The
learned model is then used as part of a Mixed Observability Markov Decision
Process formulation, wherein the human type is a partially observable variable.
With this framework, we can infer, either offline or online, the human type of
a new user that was not included in the training set, and can compute a policy
for the robot that will be aligned to the preference of this new user and will
be robust to deviations of the human actions from prior demonstrations. Finally
we validate the approach using data collected in human subject experiments, and
conduct proof-of-concept demonstrations in which a person performs a
collaborative task with a small industrial robot.



Many important optimization problems, such as the minimum spanning tree and
minimum-cost flow, can be solved optimally by a greedy method. In this work, we
study a learning variant of these problems, where the model of the problem is
unknown and has to be learned by interacting repeatedly with the environment in
the bandit setting. We formalize our learning problem quite generally, as
learning how to maximize an unknown modular function on a known polymatroid. We
propose a computationally efficient algorithm for solving our problem and bound
its expected cumulative regret. Our gap-dependent upper bound is tight up to a
constant and our gap-free upper bound is tight up to polylogarithmic factors.
Finally, we evaluate our method on three problems and demonstrate that it is
practical.



Semantic composition is the task of understanding the meaning of text by
composing the meanings of the individual words in the text. Semantic
decomposition is the task of understanding the meaning of an individual word by
decomposing it into various aspects (factors, constituents, components) that
are latent in the meaning of the word. We take a distributional approach to
semantics, in which a word is represented by a context vector. Much recent work
has considered the problem of recognizing compositions and decompositions, but
we tackle the more difficult generation problem. For simplicity, we focus on
noun-modifier bigrams and noun unigrams. A test for semantic composition is,
given context vectors for the noun and modifier in a noun-modifier bigram ("red
salmon"), generate a noun unigram that is synonymous with the given bigram
("sockeye"). A test for semantic decomposition is, given a context vector for a
noun unigram ("snifter"), generate a noun-modifier bigram that is synonymous
with the given unigram ("brandy glass"). With a vocabulary of about 73,000
unigrams from WordNet, there are 73,000 candidate unigram compositions for a
bigram and 5,300,000,000 (73,000 squared) candidate bigram decompositions for a
unigram. We generate ranked lists of potential solutions in two passes. A fast
unsupervised learning algorithm generates an initial list of candidates and
then a slower supervised learning algorithm refines the list. We evaluate the
candidate solutions by comparing them to WordNet synonym sets. For
decomposition (unigram to bigram), the top 100 most highly ranked bigrams
include a WordNet synonym of the given unigram 50.7% of the time. For
composition (bigram to unigram), the top 100 most highly ranked unigrams
include a WordNet synonym of the given bigram 77.8% of the time.



Due to advances in sensors, growing large and complex medical image data have
the ability to visualize the pathological change in the cellular or even the
molecular level or anatomical changes in tissues and organs. As a consequence,
the medical images have the potential to enhance diagnosis of disease,
prediction of clinical outcomes, characterization of disease progression,
management of health care and development of treatments, but also pose great
methodological and computational challenges for representation and selection of
features in image cluster analysis. To address these challenges, we first
extend one dimensional functional principal component analysis to the two
dimensional functional principle component analyses (2DFPCA) to fully capture
space variation of image signals. Image signals contain a large number of
redundant and irrelevant features which provide no additional or no useful
information for cluster analysis. Widely used methods for removing redundant
and irrelevant features are sparse clustering algorithms using a lasso-type
penalty to select the features. However, the accuracy of clustering using a
lasso-type penalty depends on how to select penalty parameters and a threshold
for selecting features. In practice, they are difficult to determine. Recently,
randomized algorithms have received a great deal of attention in big data
analysis. This paper presents a randomized algorithm for accurate feature
selection in image cluster analysis. The proposed method is applied to ovarian
and kidney cancer histology image data from the TCGA database. The results
demonstrate that the randomized feature selection method coupled with
functional principal component analysis substantially outperforms the current
sparse clustering algorithms in image cluster analysis.



Exact Bayesian structure discovery in Bayesian networks requires exponential
time and space. Using dynamic programming (DP), the fastest known sequential
algorithm computes the exact posterior probabilities of structural features in
$O(2(d+1)n2^n)$ time and space, if the number of nodes (variables) in the
Bayesian network is $n$ and the in-degree (the number of parents) per node is
bounded by a constant $d$. Here we present a parallel algorithm capable of
computing the exact posterior probabilities for all $n(n-1)$ edges with optimal
parallel space efficiency and nearly optimal parallel time efficiency. That is,
if $p=2^k$ processors are used, the run-time reduces to
$O(5(d+1)n2^{n-k}+k(n-k)^d)$ and the space usage becomes $O(n2^{n-k})$ per
processor. Our algorithm is based the observation that the subproblems in the
sequential DP algorithm constitute a $n$-$D$ hypercube. We take a delicate way
to coordinate the computation of correlated DP procedures such that large
amount of data exchange is suppressed. Further, we develop parallel techniques
for two variants of the well-known \emph{zeta transform}, which have
applications outside the context of Bayesian networks. We demonstrate the
capability of our algorithm on datasets with up to 33 variables and its
scalability on up to 2048 processors. We apply our algorithm to a biological
data set for discovering the yeast pheromone response pathways.



Many people suffer from the loss of a limb. Learning to get by without an arm
or hand can be very challenging, and existing prostheses do not yet fulfil the
needs of individuals with amputations. One promising solution is to provide
greater communication between a prosthesis and its user. Towards this end, we
present a simple machine learning interface to supplement the control of a
robotic limb with feedback to the user about what the limb will be experiencing
in the near future. A real-time prediction learner was implemented to predict
impact-related electrical load experienced by a robot limb; the learning
system's predictions were then communicated to the device's user to aid in
their interactions with a workspace. We tested this system with five
able-bodied subjects. Each subject manipulated the robot arm while receiving
different forms of vibrotactile feedback regarding the arm's contact with its
workspace. Our trials showed that communicable predictions could be learned
quickly during human control of the robot arm. Using these predictions as a
basis for feedback led to a statistically significant improvement in task
performance when compared to purely reactive feedback from the device. Our
study therefore contributes initial evidence that prediction learning and
machine intelligence can benefit not just control, but also feedback from an
artificial limb. We expect that a greater level of acceptance and ownership can
be achieved if the prosthesis itself takes an active role in transmitting
learned knowledge about its state and its situation of use.



Probability theory as extended logic is completed such that essentially any
probability may be determined. This is done by considering propositional logic
(as opposed to predicate logic) as syntactically suffcient and imposing a
symmetry from propositional logic. It is shown how the notions of `possibility'
and `property' may be suffciently represented in propositional logic such that
1) the principle of indifference drops out and becomes essentially combinatoric
in nature and 2) one may appropriately represent assumptions where one assumes
there is a space of possibilities but does not assume the size of the space.



Editing faces in videos is a popular yet challenging aspect of computer
vision and graphics, which encompasses several applications including facial
attractiveness enhancement, makeup transfer, face replacement, and expression
manipulation. Simply applying image-based warping algorithms to video-based
face editing produces temporal incoherence in the synthesized videos because it
is impossible to consistently localize facial features in two frames
representing two different faces in two different videos (or even two
consecutive frames representing the same face in one video). Therefore, high
performance face editing usually requires significant manual manipulation. In
this paper we propose a novel temporal-spatial-smooth warping (TSSW) algorithm
to effectively exploit the temporal information in two consecutive frames, as
well as the spatial smoothness within each frame. TSSW precisely estimates two
control lattices in the horizontal and vertical directions respectively from
the corresponding control lattices in the previous frame, by minimizing a novel
energy function that unifies a data-driven term, a smoothness term, and feature
point constraints. Corresponding warping surfaces then precisely map source
frames to the target frames. Experimental testing on facial attractiveness
enhancement, makeup transfer, face replacement, and expression manipulation
demonstrates that the proposed approaches can effectively preserve spatial
smoothness and temporal coherence in editing facial geometry, skin detail,
identity, and expression, which outperform the existing face editing methods.
In particular, TSSW is robust to subtly inaccurate localization of feature
points and is a vast improvement over image-based warping methods.



Matrix completion under interval uncertainty can be cast as matrix completion
with element-wise box constraints. We present an efficient
alternating-direction parallel coordinate-descent method for the problem. We
show that the method outperforms any other known method on a benchmark in image
in-painting in terms of signal-to-noise ratio, and that it provides
high-quality solutions for an instance of collaborative filtering with
100,198,805 recommendations within 5 minutes.



One of the main concepts in quantum physics is a density matrix, which is a
symmetric positive definite matrix of trace one. Finite probability
distributions are a special case where the density matrix is restricted to be
diagonal. Density matrices are mixtures of dyads, where a dyad has the form uu'
for any any unit column vector u. These unit vectors are the elementary events
of the generalized probability space. Perhaps the simplest case to see that
something unusual is going on is the case of uniform density matrix, i.e. 1/n
times identity. This matrix assigns probability 1/n to every unit vector, but
of course there are infinitely many of them. The new normalization rule thus
says that sum of probabilities over any orthonormal basis of directions is one.
We develop a probability calculus based on these more general distributions
that includes definitions of joints, conditionals and formulas that relate
these, i.e. analogs of the theorem of total probability, various Bayes rules
for the calculation of posterior density matrices, etc. The resulting calculus
parallels the familiar 'classical' probability calculus and always retains the
latter as a special case when all matrices are diagonal.
  Whereas the classical Bayesian methods maintain uncertainty about which model
is 'best', the generalization maintains uncertainty about which unit direction
has the largest variance. Surprisingly the bounds also generalize: as in the
classical setting we bound the negative log likelihood of the data by the
negative log likelihood of the MAP estimator.



We evaluate a version of the recently-proposed classification system named
Optimized Dissimilarity Space Embedding (ODSE) that operates in the input space
of sequences of generic objects. The ODSE system has been originally presented
as a classification system for patterns represented as labeled graphs. However,
since ODSE is founded on the dissimilarity space representation of the input
data, the classifier can be easily adapted to any input domain where it is
possible to define a meaningful dissimilarity measure. Here we demonstrate the
effectiveness of the ODSE classifier for sequences by considering an
application dealing with the recognition of the solubility degree of the
Escherichia coli proteome. Solubility, or analogously aggregation propensity,
is an important property of protein molecules, which is intimately related to
the mechanisms underlying the chemico-physical process of folding. Each protein
of our dataset is initially associated with a solubility degree and it is
represented as a sequence of symbols, denoting the 20 amino acid residues. The
herein obtained computational results, which we stress that have been achieved
with no context-dependent tuning of the ODSE system, confirm the validity and
generality of the ODSE-based approach for structured data classification.



The use of short text messages in social media and instant messaging has
become a popular communication channel during the last years. This rising
popularity has caused an increment in messaging threats such as spam, phishing
or malware as well as other threats. The processing of these short text message
threats could pose additional challenges such as the presence of lexical
variants, SMS-like contractions or advanced obfuscations which can degrade the
performance of traditional filtering solutions. By using a real-world SMS data
set from a large telecommunications operator from the US and a social media
corpus, in this paper we analyze the effectiveness of machine learning filters
based on linguistic and behavioral patterns in order to detect short text spam
and abusive users in the network. We have also explored different ways to deal
with short text message challenges such as tokenization and entity detection by
using text normalization and substring clustering techniques. The obtained
results show the validity of the proposed solution by enhancing baseline
approaches.



We propose and evaluate a number of solutions to the problem of calculating
the cost to serve each location in a single-vehicle transport setting. Such
cost to serve analysis has application both strategically and operationally in
transportation. The problem is formally given by the traveling salesperson game
(TSG), a cooperative total utility game in which agents correspond to locations
in a traveling salesperson problem (TSP). The cost to serve a location is an
allocated portion of the cost of an optimal tour. The Shapley value is one of
the most important normative division schemes in cooperative games, giving a
principled and fair allocation both for the TSG and more generally. We consider
a number of direct and sampling-based procedures for calculating the Shapley
value, and present the first proof that approximating the Shapley value of the
TSG within a constant factor is NP-hard. Treating the Shapley value as an ideal
baseline allocation, we then develop six proxies for that value which are
relatively easy to compute. We perform an experimental evaluation using
Synthetic Euclidean games as well as games derived from real-world tours
calculated for fast-moving consumer goods scenarios. Our experiments show that
several computationally tractable allocation techniques correspond to good
proxies for the Shapley value.



The survey methodological paper addresses a glance to a general decision
support platform technology for modular systems (modular/composite
alterantives/solutions) in various applied domains. The decision support
platform consists of seven basic combinatorial engineering frameworks (system
synthesis, system modeling, evaluation, detection of bottleneck,
improvement/extension, multistage design, combinatorial evolution and
forecasting). The decision support platform is based on decision support
procedures (e.g., multicriteria selection/sorting, clustering), combinatorial
optimization problems (e.g., knapsack, multiple choice problem, clique,
assignment/allocation, covering, spanning trees), and their combinations. The
following is described: (1) general scheme of the decision support platform
technology; (2) brief descriptions of modular (composite) systems (or composite
alternatives); (3) trends in moving from chocie/selection of alternatives to
processing of composite alternatives which correspond to hierarchical modular
products/systems; (4) scheme of resource requirements (i.e., human,
information-computer); and (5) basic combinatorial engineering frameworks and
their applications in various domains.



The paper provides a survey of semantic methods for solution of fundamental
tasks in mathematical knowledge management. Ontological models and formalisms
are discussed. We propose an ontology of mathematical knowledge, covering a
wide range of fields of mathematics. We demonstrate applications of this
representation in mathematical formula search, and learning.



Map matching of the GPS trajectory serves the purpose of recovering the
original route on a road network from a sequence of noisy GPS observations. It
is a fundamental technique to many Location Based Services. However, map
matching of a low sampling rate on urban road network is still a challenging
task. In this paper, the characteristics of Conditional Random Fields with
regard to inducing many contextual features and feature selection are explored
for the map matching of the GPS trajectories at a low sampling rate.
Experiments on a taxi trajectory dataset show that our method may achieve
competitive results along with the success of reducing model complexity for
computation-limited applications.



For the last ten years, CAPTCHAs have been widely used by websites to prevent
their data being automatically updated by machines. By supposedly allowing only
humans to do so, CAPTCHAs take advantage of the reverse Turing test (TT),
knowing that humans are more intelligent than machines. Generally, CAPTCHAs
have defeated machines, but things are changing rapidly as technology improves.
Hence, advanced research into optical character recognition (OCR) is overtaking
attempts to strengthen CAPTCHAs against machine-based attacks. This paper
investigates the immunity of CAPTCHA, which was built on the failure of the TT.
We show that some CAPTCHAs are easily broken using a simple OCR machine built
for the purpose of this study. By reviewing other techniques, we show that even
more difficult CAPTCHAs can be broken using advanced OCR machines. Current
advances in OCR should enable machines to pass the TT in the image recognition
domain, which is exactly where machines are seeking to overcome CAPTCHAs. We
enhance traditional CAPTCHAs by employing not only characters, but also natural
language and multiple objects within the same CAPTCHA. The proposed CAPTCHAs
might be able to hold out against machines, at least until the advent of a
machine that passes the TT completely.



Many diseases cause significant changes to the concentrations of small
molecules (aka metabolites) that appear in a person's biofluids, which means
such diseases can often be readily detected from a person's "metabolic
profile". This information can be extracted from a biofluid's NMR spectrum.
Today, this is often done manually by trained human experts, which means this
process is relatively slow, expensive and error-prone. This paper presents a
tool, Bayesil, that can quickly, accurately and autonomously produce a complex
biofluid's (e.g., serum or CSF) metabolic profile from a 1D1H NMR spectrum.
This requires first performing several spectral processing steps then matching
the resulting spectrum against a reference compound library, which contains the
"signatures" of each relevant metabolite. Many of these steps are novel
algorithms and our matching step views spectral matching as an inference
problem within a probabilistic graphical model that rapidly approximates the
most probable metabolic profile. Our extensive studies on a diverse set of
complex mixtures, show that Bayesil can autonomously find the concentration of
all NMR-detectable metabolites accurately (~90% correct identification and ~10%
quantification error), in <5minutes on a single CPU. These results demonstrate
that Bayesil is the first fully-automatic publicly-accessible system that
provides quantitative NMR spectral profiling effectively -- with an accuracy
that meets or exceeds the performance of trained experts. We anticipate this
tool will usher in high-throughput metabolomics and enable a wealth of new
applications of NMR in clinical settings. Available at http://www.bayesil.ca.



The Gaussian process latent variable model (GP-LVM) provides a flexible
approach for non-linear dimensionality reduction that has been widely applied.
However, the current approach for training GP-LVMs is based on maximum
likelihood, where the latent projection variables are maximized over rather
than integrated out. In this paper we present a Bayesian method for training
GP-LVMs by introducing a non-standard variational inference framework that
allows to approximately integrate out the latent variables and subsequently
train a GP-LVM by maximizing an analytic lower bound on the exact marginal
likelihood. We apply this method for learning a GP-LVM from iid observations
and for learning non-linear dynamical systems where the observations are
temporally correlated. We show that a benefit of the variational Bayesian
procedure is its robustness to overfitting and its ability to automatically
select the dimensionality of the nonlinear latent space. The resulting
framework is generic, flexible and easy to extend for other purposes, such as
Gaussian process regression with uncertain inputs and semi-supervised Gaussian
processes. We demonstrate our method on synthetic data and standard machine
learning benchmarks, as well as challenging real world datasets, including high
resolution video data.



An important capability of autonomous multi-robot systems is to prevent
collision among the individual robots. One approach to this problem is to plan
conflict-free trajectories and let each of the robots follow its pre-planned
trajectory. A widely used practical method for multi-robot trajectory planning
is prioritized planning, which has been shown to be effective in practice, but
is in general incomplete. Formal analysis of instances that are provably
solvable by prioritized planning is still missing. Moreover, prioritized
planning is a centralized algorithm, which may be in many situations
undesirable.
  In this paper we a) propose a revised version of prioritized planning and
characterize the class of instances that are provably solvable by the algorithm
and b) propose an asynchronous decentralized variant of prioritized planning,
which maintains the desirable properties of the centralized version and in the
same time exploits the distributed computational power of the individual
robots, which in most situations allows to find the joint trajectories faster.
  The experimental evaluation performed on real-world indoor maps shows that a)
the revised version of prioritized planning reliably solves a wide class of
instances on which both classical prioritized planning and popular reactive
technique ORCA fail and b) the asynchronous decentralized algorithm provides
solution faster than the previously proposed synchronized decentralized
algorithm.



We propose and evaluate alternative ensemble schemes for a new instance based
learning classifier, the Randomised Sphere Cover (RSC) classifier. RSC fuses
instances into spheres, then bases classification on distance to spheres rather
than distance to instances. The randomised nature of RSC makes it ideal for use
in ensembles. We propose two ensemble methods tailored to the RSC classifier;
$\alpha \beta$RSE, an ensemble based on instance resampling and $\alpha$RSSE, a
subspace ensemble. We compare $\alpha \beta$RSE and $\alpha$RSSE to tree based
ensembles on a set of UCI datasets and demonstrates that RSC ensembles perform
significantly better than some of these ensembles, and not significantly worse
than the others. We demonstrate via a case study on six gene expression data
sets that $\alpha$RSSE can outperform other subspace ensemble methods on high
dimensional data when used in conjunction with an attribute filter. Finally, we
perform a set of Bias/Variance decomposition experiments to analyse the source
of improvement in comparison to a base classifier.



Local field potentials (LFPs) sampled with extracellular electrodes are
frequently used as a measure of population neuronal activity. However, relating
such measurements to underlying neuronal behaviour and connectivity is
non-trivial. To help study this link, we developed the Virtual Electrode
Recording Tool for EXtracellular potentials (VERTEX). We first identified a
reduced neuron model that retained the spatial and frequency filtering
characteristics of extracellular potentials from neocortical neurons. We then
developed VERTEX as an easy-to-use Matlab tool for simulating LFPs from large
populations (>100 000 neurons). A VERTEX-based simulation successfully
reproduced features of the LFPs from an in vitro multi-electrode array
recording of macaque neocortical tissue. Our model, with virtual electrodes
placed anywhere in 3D, allows direct comparisons with the in vitro recording
setup. We envisage that VERTEX will stimulate experimentalists, clinicians, and
computational neuroscientists to use models to understand the mechanisms
underlying measured brain dynamics in health and disease.



This paper presents a study of the dynamic coupling between a user and a
virtual character during body interaction. Coupling is directly linked with
other dimensions, such as co-presence, engagement, and believability, and was
measured in an experiment that allowed users to describe their subjective
feelings about those dimensions of interest. The experiment was based on a
theatrical game involving the imitation of slow upper-body movements and the
proposal of new movements by the user and virtual agent. The agent's behaviour
varied in autonomy: the agent could limit itself to imitating the user's
movements only, initiate new movements, or combine both behaviours. After the
game, each participant completed a questionnaire regarding their engagement in
the interaction, their subjective feeling about the co-presence of the agent,
etc. Based on four main dimensions of interest, we tested several hypotheses
against our experimental results, which are discussed here.



This paper studies the form and complexity of inference in graphical models
using the abstraction offered by algebraic structures. In particular, we
broadly formalize inference problems in graphical models by viewing them as a
sequence of operations based on commutative semigroups. We then study the
computational complexity of inference by organizing various problems into an
"inference hierarchy". When the underlying structure of an inference problem is
a commutative semiring -- i.e. a combination of two commutative semigroups with
the distributive law -- a message passing procedure called belief propagation
can leverage this distributive law to perform polynomial-time inference for
certain problems. After establishing the NP-hardness of inference in any
commutative semiring, we investigate the relation between algebraic properties
in this setting and further show that polynomial-time inference using
distributive law does not (trivially) extend to inference problems that are
expressed using more than two commutative semigroups. We then extend the
algebraic treatment of message passing procedures to survey propagation,
providing a novel perspective using a combination of two commutative semirings.
This formulation generalizes the application of survey propagation to new
settings.



Finding the physical location of a specific network node is a prototypical
task for navigation inside a wireless network. In this paper, we consider in
depth the implications of wireless communication as a measurement input of
gradient-based taxis algorithms. We discuss how gradients can be measured and
determine the errors of this estimation. We then introduce a gradient-based
taxis algorithm as an example of a family of gradient-based, convergent
algorithms and discuss its convergence in the context of network robotics. We
also conduct an exemplary experiment to show how to overcome some of the
specific problems related to network robotics. Finally, we show how to adapt
this framework to more complex objectives.



Serial pattern mining consists in extracting the frequent sequential patterns
from a unique sequence of itemsets. This paper explores the ability of a
declarative language, such as Answer Set Programming (ASP), to solve this issue
efficiently. We propose several ASP implementations of the frequent sequential
pattern mining task: a non-incremental and an incremental resolution. The
results show that the incremental resolution is more efficient than the
non-incremental one, but both ASP programs are less efficient than dedicated
algorithms. Nonetheless, this approach can be seen as a first step toward a
generic framework for sequential pattern mining with constraints.



The Shapley value has been recently advocated as a method to choose the seed
nodes for the process of information diffusion. Intuitively, since the Shapley
value evaluates the average marginal contribution of a player to the
coalitional game, it can be used in the network context to evaluate the
marginal contribution of a node in the process of information diffusion given
various groups of already 'infected' nodes. Although the above direction of
research seems promising, the current liter- ature is missing a throughout
assessment of its performance. The aim of this work is to provide such an
assessment of the existing Shapley value-based approaches to information
diffusion.



We explore the idea that authoring a piece of text is an act of maximizing
one's expected utility. To make this idea concrete, we consider the societally
important decisions of the Supreme Court of the United States. Extensive past
work in quantitative political science provides a framework for empirically
modeling the decisions of justices and how they relate to text. We incorporate
into such a model texts authored by amici curiae ("friends of the court"
separate from the litigants) who seek to weigh in on the decision, then
explicitly model their goals in a random utility model. We demonstrate the
benefits of this approach in improved vote prediction and the ability to
perform counterfactual analysis.



In repeated stochastic games (RSGs), an agent must quickly adapt to the
behavior of previously unknown associates, who may themselves be learning. This
machine-learning problem is particularly challenging due, in part, to the
presence of multiple (even infinite) equilibria and inherently large strategy
spaces. In this paper, we introduce a method to reduce the strategy space of
two-player general-sum RSGs to a handful of expert strategies. This process,
called Mega, effectually reduces an RSG to a bandit problem. We show that the
resulting strategy space preserves several important properties of the original
RSG, thus enabling a learner to produce robust strategies within a reasonably
small number of interactions. To better establish strengths and weaknesses of
this approach, we empirically evaluate the resulting learning system against
other algorithms in three different RSGs.



We introduce the notion of online reactive planning with sensing actions for
systems with temporal logic constraints in partially observable and dynamic
environments. With incomplete information on the dynamic environment, reactive
controller synthesis amounts to solving a two-player game with partial
observations, which has impractically computational complexity. To alleviate
the high computational burden, online replanning via sensing actions avoids
solving the strategy in the reactive system under partial observations.
Instead, we only solve for a strategy that ensures a given temporal logic
specification can be satisfied had the system have complete observations of its
environment. Such a strategy is then transformed into one which makes control
decisions based on the observed sequence of states (of the interacting system
and its environment). When the system encounters a belief---a set including all
possible hypotheses the system has for the current state---for which the
observation-based strategy is undefined, a sequence of sensing actions are
triggered, chosen by an active sensing strategy, to reduce the uncertainty in
the system's belief. We show that by alternating between the observation-based
strategy and the active sensing strategy, under a mild technical assumption of
the set of sensors in the system, the given temporal logic specification can be
satisfied with probability 1.



We propose a method for automatically answering questions about images by
bringing together recent advances from natural language processing and computer
vision. We combine discrete reasoning with uncertain predictions by a
multi-world approach that represents uncertainty about the perceived world in a
bayesian framework. Our approach can handle human questions of high complexity
about realistic scenes and replies with range of answer like counts, object
classes, instances and lists of them. The system is directly trained from
question-answer pairs. We establish a first benchmark for this task that can be
seen as a modern attempt at a visual turing test.



We introduce a new framework to model interactions among agents which seek to
trade to minimize their risk with respect to some future outcome. We quantify
this risk using the concept of risk measures from finance, and introduce a
class of trade dynamics which allow agents to trade contracts contingent upon
the future outcome. We then show that these trade dynamics exactly correspond
to a variant of randomized coordinate descent. By extending the analysis of
these coordinate descent methods to account for our more organic setting, we
are able to show convergence rates for very general trade dynamics, showing
that the market or network converges to a unique steady state. Applying these
results to prediction markets, we expand on recent results by adding
convergence rates and general aggregation properties. Finally, we illustrate
the generality of our framework by applying it to agent interactions on a
scale-free network.



An initial study of surrogate-assisted evolutionary algorithms used to design
vertical-axis wind turbines wherein candidate prototypes are evaluated under
fan generated wind conditions after being physically instantiated by a 3D
printer has recently been presented. Unlike other approaches, such as
computational fluid dynamics simulations, no mathematical formulations were
used and no model assumptions were made. This paper extends that work by
exploring alternative surrogate modelling and evolutionary techniques. The
accuracy of various modelling algorithms used to estimate the fitness of
evaluated individuals from the initial experiments is compared. The effect of
temporally windowing surrogate model training samples is explored. A
surrogate-assisted approach based on an enhanced local search is introduced;
and alternative coevolution collaboration schemes are examined.



Rough sets over generalized transitive relations like proto-transitive ones
had been initiated by the present author in the year 2012. Subsequently,
approximation of proto-transitive relations by other relations was investigated
and the relation with rough approximations was developed towards constructing
semantics that can handle fragments of structure. It was also proved that
difference of approximations induced by some approximate relations need not
induce rough structures. In this research we develop different semantics of
proto transitive rough sets (PRAX) after characterizing the structure of rough
objects and also develop a theory of dependence for general rough sets and use
it to internalize the Nelson-algebra based approximate semantics developed
earlier. The theory of rough dependence initiated later by the present author
is extended in the process. This monograph is reasonably self-contained and
includes proofs and extensions of representation of objects that were not part
of earlier papers.



A stochastic combinatorial semi-bandit is an online learning problem where at
each step a learning agent chooses a subset of ground items subject to
constraints, and then observes stochastic weights of these items and receives
their sum as a payoff. In this paper, we close the problem of computationally
and sample efficient learning in stochastic combinatorial semi-bandits. In
particular, we analyze a UCB-like algorithm for solving the problem, which is
known to be computationally efficient; and prove $O(K L (1 / \Delta) \log n)$
and $O(\sqrt{K L n \log n})$ upper bounds on its $n$-step regret, where $L$ is
the number of ground items, $K$ is the maximum number of chosen items, and
$\Delta$ is the gap between the expected returns of the optimal and best
suboptimal solutions. The gap-dependent bound is tight up to a constant factor
and the gap-free bound is tight up to a polylogarithmic factor.



While most Bayesian nonparametric models in machine learning have focused on
the Dirichlet process, the beta process, or their variants, the gamma process
has recently emerged as a useful nonparametric prior in its own right. Current
inference schemes for models involving the gamma process are restricted to
MCMC-based methods, which limits their scalability. In this paper, we present a
variational inference framework for models involving gamma process priors. Our
approach is based on a novel stick-breaking constructive definition of the
gamma process. We prove correctness of this stick-breaking process by using the
characterization of the gamma process as a completely random measure (CRM), and
we explicitly derive the rate measure of our construction using Poisson process
machinery. We also derive error bounds on the truncation of the infinite
process required for variational inference, similar to the truncation analyses
for other nonparametric models based on the Dirichlet and beta processes. Our
representation is then used to derive a variational inference algorithm for a
particular Bayesian nonparametric latent structure formulation known as the
infinite Gamma-Poisson model, where the latent variables are drawn from a gamma
process prior with Poisson likelihoods. Finally, we present results for our
algorithms on nonnegative matrix factorization tasks on document corpora, and
show that we compare favorably to both sampling-based techniques and
variational approaches based on beta-Bernoulli priors.



It is well-known that neural networks are computationally hard to train. On
the other hand, in practice, modern day neural networks are trained efficiently
using SGD and a variety of tricks that include different activation functions
(e.g. ReLU), over-specification (i.e., train networks which are larger than
needed), and regularization. In this paper we revisit the computational
complexity of training neural networks from a modern perspective. We provide
both positive and negative results, some of them yield new provably efficient
and practical algorithms for training certain types of neural networks.



In this paper, we discuss the method of Bayesian regression and its efficacy
for predicting price variation of Bitcoin, a recently popularized virtual,
cryptographic currency. Bayesian regression refers to utilizing empirical data
as proxy to perform Bayesian inference. We utilize Bayesian regression for the
so-called "latent source model". The Bayesian regression for "latent source
model" was introduced and discussed by Chen, Nikolov and Shah (2013) and
Bresler, Chen and Shah (2014) for the purpose of binary classification. They
established theoretical as well as empirical efficacy of the method for the
setting of binary classification.
  In this paper, instead we utilize it for predicting real-valued quantity, the
price of Bitcoin. Based on this price prediction method, we devise a simple
strategy for trading Bitcoin. The strategy is able to nearly double the
investment in less than 60 day period when run against real data trace.



Bipartite ranking aims to learn a real-valued ranking function that orders
positive instances before negative instances. Recent efforts of bipartite
ranking are focused on optimizing ranking accuracy at the top of the ranked
list. Most existing approaches are either to optimize task specific metrics or
to extend the ranking loss by emphasizing more on the error associated with the
top ranked instances, leading to a high computational cost that is super-linear
in the number of training instances. We propose a highly efficient approach,
titled TopPush, for optimizing accuracy at the top that has computational
complexity linear in the number of training instances. We present a novel
analysis that bounds the generalization error for the top ranked instances for
the proposed approach. Empirical study shows that the proposed approach is
highly competitive to the state-of-the-art approaches and is 10-100 times
faster.



We propose relational linear programming, a simple framework for combing
linear programs (LPs) and logic programs. A relational linear program (RLP) is
a declarative LP template defining the objective and the constraints through
the logical concepts of objects, relations, and quantified variables. This
allows one to express the LP objective and constraints relationally for a
varying number of individuals and relations among them without enumerating
them. Together with a logical knowledge base, effectively a logical program
consisting of logical facts and rules, it induces a ground LP. This ground LP
is solved using lifted linear programming. That is, symmetries within the
ground LP are employed to reduce its dimensionality, if possible, and the
reduced program is solved using any off-the-shelf LP solver. In contrast to
mainstream LP template languages like AMPL, which features a mixture of
declarative and imperative programming styles, RLP's relational nature allows a
more intuitive representation of optimization problems over relational domains.
We illustrate this empirically by experiments on approximate inference in
Markov logic networks using LP relaxations, on solving Markov decision
processes, and on collective inference using LP support vector machines.



Ambiguity or uncertainty is a pervasive element of many real world decision
making processes. Variation in decisions is a norm in this situation when the
same problem is posed to different subjects. Psychological and metaphysical
research had proven that decision making by human is subjective. It is
influenced by many factors such as experience, age, background, etc. Scene
understanding is one of the computer vision problems that fall into this
category. Conventional methods relax this problem by assuming scene images are
mutually exclusive; and therefore, focus on developing different approaches to
perform the binary classification tasks. In this paper, we show that scene
images are non-mutually exclusive, and propose the Fuzzy Qualitative Rank
Classifier (FQRC) to tackle the aforementioned problems. The proposed FQRC
provides a ranking interpretation instead of binary decision. Evaluations in
term of qualitative and quantitative using large numbers and challenging public
scene datasets have shown the effectiveness of our proposed method in modeling
the non-mutually exclusive scene images.



We describe a new class of learning models called memory networks. Memory
networks reason with inference components combined with a long-term memory
component; they learn how to use these jointly. The long-term memory can be
read and written to, with the goal of using it for prediction. We investigate
these models in the context of question answering (QA) where the long-term
memory effectively acts as a (dynamic) knowledge base, and the output is a
textual response. We evaluate them on a large-scale QA task, and a smaller, but
more complex, toy task generated from a simulated world. In the latter, we show
the reasoning power of such models by chaining multiple supporting sentences to
answer questions that require understanding the intension of verbs.



Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are
widely used because they are expressive and are easy to train. Our interest
lies in empirically evaluating the expressiveness and the learnability of LSTMs
in the sequence-to-sequence regime by training them to evaluate short computer
programs, a domain that has traditionally been seen as too complex for neural
networks. We consider a simple class of programs that can be evaluated with a
single left-to-right pass using constant memory. Our main result is that LSTMs
can learn to map the character-level representations of such programs to their
correct outputs. Notably, it was necessary to use curriculum learning, and
while conventional curriculum learning proved ineffective, we developed a new
variant of curriculum learning that improved our networks' performance in all
experimental conditions. The improved curriculum had a dramatic impact on an
addition problem, making it possible to train an LSTM to add two 9-digit
numbers with 99% accuracy.



Functional Magnetic Resonance Imaging (fMRI) is a powerful non-invasive tool
for localizing and analyzing brain activity. This study focuses on one very
important aspect of the functional properties of human brain, specifically the
estimation of the level of parallelism when performing complex cognitive tasks.
Using fMRI as the main modality, the human brain activity is investigated
through a purely data-driven signal processing and dimensionality analysis
approach. Specifically, the fMRI signal is treated as a multi-dimensional data
space and its intrinsic `complexity' is studied via dataset fractal analysis
and blind-source separation (BSS) methods. One simulated and two real fMRI
datasets are used in combination with Independent Component Analysis (ICA) and
fractal analysis for estimating the intrinsic (true) dimensionality, in order
to provide data-driven experimental evidence on the number of independent brain
processes that run in parallel when visual or visuo-motor tasks are performed.
Although this number is can not be defined as a strict threshold but rather as
a continuous range, when a specific activation level is defined, a
corresponding number of parallel processes or the casual equivalent of `cpu
cores' can be detected in normal human brain activity.



In this paper, we propose an approach to the unsupervised segmentation of
images using Markov Random Field. The proposed approach is based on the idea of
Bit Plane Slicing. We use the planes as initial labellings for an ensemble of
segmentations. With pixelwise voting, a robust segmentation approach can be
achieved, which we demonstrate on microscope cell images. We tested our
approach on a publicly available database, where it proven to be competitive
with other methods and manual segmentation.



Generative models provide a powerful framework for probabilistic reasoning.
However, in many domains their use has been hampered by the practical
difficulties of inference. This is particularly the case in computer vision,
where models of the imaging process tend to be large, loopy and layered. For
this reason bottom-up conditional models have traditionally dominated in such
domains. We find that widely-used, general-purpose message passing inference
algorithms such as Expectation Propagation (EP) and Variational Message Passing
(VMP) fail on the simplest of vision models. With these models in mind, we
introduce a modification to message passing that learns to exploit their
layered structure by passing 'consensus' messages that guide inference towards
good solutions. Experiments on a variety of problems show that the proposed
technique leads to significantly more accurate inference results, not only when
compared to standard EP and VMP, but also when compared to competitive
bottom-up conditional models.



We introduce a family of adaptive estimators on graphs, based on penalizing
the $\ell_1$ norm of discrete graph differences. This generalizes the idea of
trend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate
nonparametric regression, to graphs. Analogous to the univariate case, graph
trend filtering exhibits a level of local adaptivity unmatched by the usual
$\ell_2$-based graph smoothers. It is also defined by a convex minimization
problem that is readily solved (e.g., by fast ADMM or Newton algorithms). We
demonstrate the merits of graph trend filtering through examples and theory.



In this work, we propose a generalized product of experts (gPoE) framework
for combining the predictions of multiple probabilistic models. We identify
four desirable properties that are important for scalability, expressiveness
and robustness, when learning and inferring with a combination of multiple
models. Through analysis and experiments, we show that gPoE of Gaussian
processes (GP) have these qualities, while no other existing combination
schemes satisfy all of them at the same time. The resulting GP-gPoE is highly
scalable as individual GP experts can be independently learned in parallel;
very expressive as the way experts are combined depends on the input rather
than fixed; the combined prediction is still a valid probabilistic model with
natural interpretation; and finally robust to unreliable predictions from
individual experts.



In this paper, we propose an approach for modeling and analysis of a number
of phenomena of collective behavior. By collectives we mean multi-agent systems
that transition from one state to another at discrete moments of time. The
behavior of a member of a collective (agent) is called conforming if the
opinion of this agent at current time moment conforms to the opinion of some
other agents at the previous time moment. We presume that at each moment of
time every agent makes a decision by choosing from the set {0,1} (where
1-decision corresponds to action and 0-decision corresponds to inaction). In
our approach we model collective behavior with synchronous Boolean networks. We
presume that in a network there can be agents that act at every moment of time.
Such agents are called instigators. Also there can be agents that never act.
Such agents are called loyalists. Agents that are neither instigators nor
loyalists are called simple agents. We study two combinatorial problems. The
first problem is to find a disposition of instigators that in several time
moments transforms a network from a state where a majority of simple agents are
inactive to a state with a majority of active agents. The second problem is to
find a disposition of loyalists that returns the network to a state with a
majority of inactive agents. Similar problems are studied for networks in which
simple agents demonstrate the contrary to conforming behavior that we call
anticonforming. We obtained several theoretical results regarding the behavior
of collectives of agents with conforming or anticonforming behavior. In
computational experiments we solved the described problems for randomly
generated networks with several hundred vertices. We reduced corresponding
combinatorial problems to the Boolean satisfiability problem (SAT) and used
modern SAT solvers to solve the instances obtained.



Everyday activities performed by artificial assistants can potentially be
executed naively and dangerously given their lack of common sense knowledge.
This paper presents conceptual work towards obtaining prior knowledge on the
usual modality (passive or active) of any given entity, and their affordance
estimates, by extracting high-confidence ability modality semantic relations (X
can Y relationship) from non-figurative texts, by analyzing co-occurrence of
grammatical instances of subjects and verbs, and verbs and objects. The
discussion includes an outline of the concept, potential and limitations, and
possible feature and learning framework adoption.



Reliable microaneurysm detection in digital fundus images is still an open
issue in medical image processing. We propose an ensemble-based framework to
improve microaneurysm detection. Unlike the well-known approach of considering
the output of multiple classifiers, we propose a combination of internal
components of microaneurysm detectors, namely preprocessing methods and
candidate extractors. We have evaluated our approach for microaneurysm
detection in an online competition, where this algorithm is currently ranked as
first and also on two other databases. Since microaneurysm detection is
decisive in diabetic retinopathy grading, we also tested the proposed method
for this task on the publicly available Messidor database, where a promising
AUC 0.90 with 0.01 uncertainty is achieved in a 'DR/non-DR'-type classification
based on the presence or absence of the microaneurysms.



In this paper, we study the impact of selection methods in the context of
on-line on-board distributed evolutionary algorithms. We propose a variant of
the mEDEA algorithm in which we add a selection operator, and we apply it in a
taskdriven scenario. We evaluate four selection methods that induce different
intensity of selection pressure in a multi-robot navigation with obstacle
avoidance task and a collective foraging task. Experiments show that a small
intensity of selection pressure is sufficient to rapidly obtain good
performances on the tasks at hand. We introduce different measures to compare
the selection methods, and show that the higher the selection pressure, the
better the performances obtained, especially for the more challenging food
foraging task.



This work presents how persistent predicates have been included in the
in-memory deductive system DES by relying on external SQL database management
systems. We introduce how persistence is supported from a user-point of view
and the possible applications the system opens up, as the deductive expressive
power is projected to relational databases. Also, we describe how it is
possible to intermix computations of the deductive engine and the external
database, explaining its implementation and some optimizations. Finally, a
performance analysis is undertaken, comparing the system with current
relational database systems.



Planned experiments are the gold standard in reliably comparing the causal
effect of switching from a baseline policy to a new policy. One critical
shortcoming of classical experimental methods, however, is that they typically
do not take into account the dynamic nature of response to policy changes. For
instance, in an experiment where we seek to understand the effects of a new ad
pricing policy on auction revenue, agents may adapt their bidding in response
to the experimental pricing changes. Thus, causal effects of the new pricing
policy after such adaptation period, the {\em long-term causal effects}, are
not captured by the classical methodology even though they clearly are more
indicative of the value of the new policy. Here, we formalize a framework to
define and estimate long-term causal effects of policy changes in multiagent
economies. Central to our approach is behavioral game theory, which we leverage
to formulate the ignorability assumptions that are necessary for causal
inference. Under such assumptions we estimate long-term causal effects through
a latent space approach, where a behavioral model of how agents act conditional
on their latent behaviors is combined with a temporal model of how behaviors
evolve over time.



Topic models are a way to discover underlying themes in an otherwise
unstructured collection of documents. In this study, we specifically used the
Latent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to
classify restaurants based off of their reviews. Furthermore, we hypothesize
that within a city, restaurants can be grouped into similar "clusters" based on
both location and similarity. We used several different clustering methods,
including K-means Clustering and a Probabilistic Mixture Model, in order to
uncover and classify districts, both well-known and hidden (i.e. cultural areas
like Chinatown or hearsay like "the best street for Italian restaurants")
within a city. We use these models to display and label different clusters on a
map. We also introduce a topic similarity heatmap that displays the similarity
distribution in a city to a new restaurant.



Progress in language and image understanding by machines has sparkled the
interest of the research community in more open-ended, holistic tasks, and
refueled an old AI dream of building intelligent machines. We discuss a few
prominent challenges that characterize such holistic tasks and argue for
"question answering about images" as a particular appealing instance of such a
holistic task. In particular, we point out that it is a version of a Turing
Test that is likely to be more robust to over-interpretations and contrast it
with tasks like grounding and generation of descriptions. Finally, we discuss
tools to measure progress in this field.



This paper presents a way of solving Markov Decision Processes that combines
state abstraction and temporal abstraction. Specifically, we combine state
aggregation with the options framework and demonstrate that they work well
together and indeed it is only after one combines the two that the full benefit
of each is realized. We introduce a hierarchical value iteration algorithm
where we first coarsely solve subgoals and then use these approximate solutions
to exactly solve the MDP. This algorithm solved several problems faster than
vanilla value iteration.



While computer and communication technologies have provided effective means
to scale up many aspects of education, the submission and grading of
assessments such as homework assignments and tests remains a weak link. In this
paper, we study the problem of automatically grading the kinds of open response
mathematical questions that figure prominently in STEM (science, technology,
engineering, and mathematics) courses. Our data-driven framework for
mathematical language processing (MLP) leverages solution data from a large
number of learners to evaluate the correctness of their solutions, assign
partial-credit scores, and provide feedback to each learner on the likely
locations of any errors. MLP takes inspiration from the success of natural
language processing for text data and comprises three main steps. First, we
convert each solution to an open response mathematical question into a series
of numerical features. Second, we cluster the features from several solutions
to uncover the structures of correct, partially correct, and incorrect
solutions. We develop two different clustering approaches, one that leverages
generic clustering algorithms and one based on Bayesian nonparametrics. Third,
we automatically grade the remaining (potentially large number of) solutions
based on their assigned cluster and one instructor-provided grade per cluster.
As a bonus, we can track the cluster assignment of each step of a multistep
solution and determine when it departs from a cluster of correct solutions,
which enables us to indicate the likely locations of errors to learners. We
test and validate MLP on real-world MOOC data to demonstrate how it can
substantially reduce the human effort required in large-scale educational
platforms.



We study the Bayesian model averaging approach to learning Bayesian network
structures (DAGs) from data. We develop new algorithms including the first
algorithm that is able to efficiently sample DAGs according to the exact
structure posterior. The DAG samples can then be used to construct estimators
for the posterior of any feature. We theoretically prove good properties of our
estimators and empirically show that our estimators considerably outperform the
estimators from the previous state-of-the-art methods.



In view of the paradigm shift that makes science ever more data-driven, in
this thesis we propose a synthesis method for encoding and managing large-scale
deterministic scientific hypotheses as uncertain and probabilistic data.
  In the form of mathematical equations, hypotheses symmetrically relate
aspects of the studied phenomena. For computing predictions, however,
deterministic hypotheses can be abstracted as functions. We build upon Simon's
notion of structural equations in order to efficiently extract the (so-called)
causal ordering between variables, implicit in a hypothesis structure (set of
mathematical equations).
  We show how to process the hypothesis predictive structure effectively
through original algorithms for encoding it into a set of functional
dependencies (fd's) and then performing causal reasoning in terms of acyclic
pseudo-transitive reasoning over fd's. Such reasoning reveals important causal
dependencies implicit in the hypothesis predictive data and guide our synthesis
of a probabilistic database. Like in the field of graphical models in AI, such
a probabilistic database should be normalized so that the uncertainty arisen
from competing hypotheses is decomposed into factors and propagated properly
onto predictive data by recovering its joint probability distribution through a
lossless join. That is motivated as a design-theoretic principle for
data-driven hypothesis management and predictive analytics.
  The method is applicable to both quantitative and qualitative deterministic
hypotheses and demonstrated in realistic use cases from computational science.



Nowadays, social networks such as Twitter, Facebook and LinkedIn become
increasingly popular. In fact, they introduced new habits, new ways of
communication and they collect every day several information that have
different sources. Most existing research works fo-cus on the analysis of
homogeneous social networks, i.e. we have a single type of node and link in the
network. However, in the real world, social networks offer several types of
nodes and links. Hence, with a view to preserve as much information as
possible, it is important to consider so-cial networks as heterogeneous and
uncertain. The goal of our paper is to classify the social message based on its
spreading in the network and the theory of belief functions. The proposed
classifier interprets the spread of messages on the network, crossed paths and
types of links. We tested our classifier on a real word network that we
collected from Twitter, and our experiments show the performance of our belief
classifier.



Web services allow communication between heterogeneous systems in a
distributed environment. Their enormous success and their increased use led to
the fact that thousands of Web services are present on the Internet. This
significant number of Web services which not cease to increase has led to
problems of the difficulty in locating and classifying web services, these
problems are encountered mainly during the operations of web services discovery
and substitution. Traditional ways of search based on keywords are not
successful in this context, their results do not support the structure of Web
services and they consider in their search only the identifiers of the web
service description language (WSDL) interface elements. The methods based on
semantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web
service with a semantic description allow raising partially this problem, but
their complexity and difficulty delays their adoption in real cases. Measuring
the similarity between the web services interfaces is the most suitable
solution for this kind of problems, it will classify available web services so
as to know those that best match the searched profile and those that do not
match. Thus, the main goal of this work is to study the degree of similarity
between any two web services by offering a new method that is more effective
than existing works.



We introduce and study methods for inferring and learning from
correspondences among neurons. The approach enables alignment of data from
distinct multiunit studies of nervous systems. We show that the methods for
inferring correspondences combine data effectively from cross-animal studies to
make joint inferences about behavioral decision making that are not possible
with the data from a single animal. We focus on data collection, machine
learning, and prediction in the representative and long-studied invertebrate
nervous system of the European medicinal leech. Acknowledging the computational
intractability of the general problem of identifying correspondences among
neurons, we introduce efficient computational procedures for matching neurons
across animals. The methods include techniques that adjust for missing cells or
additional cells in the different data sets that may reflect biological or
experimental variation. The methods highlight the value harnessing inference
and learning in new kinds of computational microscopes for multiunit
neurobiological studies.



The dynamics of belief and knowledge is one of the major components of any
autonomous system that should be able to incorporate new pieces of information.
In order to apply the rationality result of belief dynamics theory to various
practical problems, it should be generalized in two respects: first it should
allow a certain part of belief to be declared as immutable; and second, the
belief state need not be deductively closed. Such a generalization of belief
dynamics, referred to as base dynamics, is presented in this paper, along with
the concept of a generalized revision algorithm for knowledge bases (Horn or
Horn logic with stratified negation). We show that knowledge base dynamics has
an interesting connection with kernel change via hitting set and abduction. In
this paper, we show how techniques from disjunctive logic programming can be
used for efficient (deductive) database updates. The key idea is to transform
the given database together with the update request into a disjunctive
(datalog) logic program and apply disjunctive techniques (such as minimal model
reasoning) to solve the original update problem. The approach extends and
integrates standard techniques for efficient query answering and integrity
checking. The generation of a hitting set is carried out through a hyper
tableaux calculus and magic set that is focused on the goal of minimality. The
present paper provides a comparative study of view update algorithms in
rational approach. For, understand the basic concepts with abduction, we
provide an abductive framework for knowledge base dynamics. Finally, we
demonstrate how belief base dynamics can provide an axiomatic characterization
for insertion a view atom to the database. We give a quick overview of the main
operators for belief change, in particular, belief update versus database
update.



Particle Markov chain Monte Carlo techniques rank among current
state-of-the-art methods for probabilistic program inference. A drawback of
these techniques is that they rely on importance resampling, which results in
degenerate particle trajectories and a low effective sample size for variables
sampled early in a program. We here develop a formalism to adapt ancestor
resampling, a technique that mitigates particle degeneracy, to the
probabilistic programming setting. We present empirical results that
demonstrate nontrivial performance gains.



Given the large number of new musical tracks released each year, automated
approaches to plagiarism detection are essential to help us track potential
violations of copyright. Most current approaches to plagiarism detection are
based on musical similarity measures, which typically ignore the issue of
polyphony in music. We present a novel feature space for audio derived from
compositional modelling techniques, commonly used in signal separation, that
provides a mechanism to account for polyphony without incurring an inordinate
amount of computational overhead. We employ this feature representation in
conjunction with traditional audio feature representations in a classification
framework which uses an ensemble of distance features to characterize pairs of
songs as being plagiarized or not. Our experiments on a database of about 3000
musical track pairs show that the new feature space characterization produces
significant improvements over standard baselines.



We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks.



In many applications, an anomaly detection system presents the most anomalous
data instance to a human analyst, who then must determine whether the instance
is truly of interest (e.g. a threat in a security setting). Unfortunately, most
anomaly detectors provide no explanation about why an instance was considered
anomalous, leaving the analyst with no guidance about where to begin the
investigation. To address this issue, we study the problems of computing and
evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE
of an anomaly is a sequence of features, which are presented to the analyst one
at a time (in order) until the information contained in the highlighted
features is enough for the analyst to make a confident judgement about the
anomaly. Since analyst effort is related to the amount of information that they
consider in an investigation, an explanation's quality is related to the number
of features that must be revealed to attain confidence. One of our main
contributions is to present a novel framework for large scale quantitative
evaluations of SFEs, where the quality measure is based on analyst effort. To
do this we construct anomaly detection benchmarks from real data sets along
with artificial experts that can be simulated for evaluation. Our second
contribution is to evaluate several novel explanation approaches within the
framework and on traditional anomaly detection benchmarks, offering several
insights into the approaches.



Because of their superior ability to preserve sequence information over time,
Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with
a more complex computational unit, have obtained strong results on a variety of
sequence modeling tasks. The only underlying LSTM structure that has been
explored so far is a linear chain. However, natural language exhibits syntactic
properties that would naturally combine words to phrases. We introduce the
Tree-LSTM, a generalization of LSTMs to tree-structured network topologies.
Tree-LSTMs outperform all existing systems and strong LSTM baselines on two
tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task
1) and sentiment classification (Stanford Sentiment Treebank).



This paper presents a novel approach for automatic recognition of group
activities for video surveillance applications. We propose to use a group
representative to handle the recognition with a varying number of group
members, and use an Asynchronous Hidden Markov Model (AHMM) to model the
relationship between people. Furthermore, we propose a group activity detection
algorithm which can handle both symmetric and asymmetric group activities, and
demonstrate that this approach enables the detection of hierarchical
interactions between people. Experimental results show the effectiveness of our
approach.



In this paper we study multi robot cooperative task allocation issue in a
situation where a swarm of robots is deployed in a confined unknown environment
where the number of colored spots which represent tasks and the ratios of them
are unknown. The robots should cover this spots as far as possible to do
cleaning and sampling actions desirably. It means that they should discover the
spots cooperatively and spread proportional to the spots area and avoid from
remaining idle. We proposed 4 self-organized distributed methods which are
called hybrid methods for coping with this scenario. In two different
experiments the performance of the methods is analyzed. We compared them with
each other and investigated their scalability and robustness in term of single
point of failure.



The global influence of Big Data is not only growing but seemingly endless.
The trend is leaning towards knowledge that is attained easily and quickly from
massive pools of Big Data. Today we are living in the technological world that
Dr. Usama Fayyad and his distinguished research fellows discussed in the
introductory explanations of Knowledge Discovery in Databases (KDD) predicted
nearly two decades ago. Indeed, they were precise in their outlook on Big Data
analytics. In fact, the continued improvement of the interoperability of
machine learning, statistics, database building and querying fused to create
this increasingly popular science- Data Mining and Knowledge Discovery. The
next generation computational theories are geared towards helping to extract
insightful knowledge from even larger volumes of data at higher rates of speed.
As the trend increases in popularity, the need for a highly adaptive solution
for knowledge discovery will be necessary. In this research paper, we are
introducing the investigation and development of 23 bit-questions for a
Metaknowledge template for Big Data Processing and clustering purposes. This
research aims to demonstrate the construction of this methodology and proves
the validity and the beneficial utilization that brings Knowledge Discovery
from Big Data.



Past research has challenged us with the task of showing relational patterns
between text-based data and then clustering for predictive analysis using Golay
Code technique. We focus on a novel approach to extract metaknowledge in
multimedia datasets. Our collaboration has been an on-going task of studying
the relational patterns between datapoints based on metafeatures extracted from
metaknowledge in multimedia datasets. Those selected are significant to suit
the mining technique we applied, Golay Code algorithm. In this research paper
we summarize findings in optimization of metaknowledge representation for
23-bit representation of structured and unstructured multimedia data in order
to



Prior knowledge has been shown very useful to address many natural language
processing tasks. Many approaches have been proposed to formalise a variety of
knowledge, however, whether the proposed approach is robust or sensitive to the
knowledge supplied to the model has rarely been discussed. In this paper, we
propose three regularization terms on top of generalized expectation criteria,
and conduct extensive experiments to justify the robustness of the proposed
methods. Experimental results demonstrate that our proposed methods obtain
remarkable improvements and are much more robust than baselines.



This article provides a thorough meta-analysis of the anomaly detection
problem. To accomplish this we first identify approaches to benchmarking
anomaly detection algorithms across the literature and produce a large corpus
of anomaly detection benchmarks that vary in their construction across several
dimensions we deem important to real-world applications: (a) point difficulty,
(b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d)
relevance of features. We apply a representative set of anomaly detection
algorithms to this corpus, yielding a very large collection of experimental
results. We analyze these results to understand many phenomena observed in
previous work. First we observe the effects of experimental design on
experimental results. Second, results are evaluated with two metrics, ROC Area
Under the Curve and Average Precision. We employ statistical hypothesis testing
to demonstrate the value (or lack thereof) of our benchmarks. We then offer
several approaches to summarizing our experimental results, drawing several
conclusions about the impact of our methodology as well as the strengths and
weaknesses of some algorithms. Last, we compare results against a trivial
solution as an alternate means of normalizing the reported performance of
algorithms. The intended contributions of this article are many; in addition to
providing a large publicly-available corpus of anomaly detection benchmarks, we
provide an ontology for describing anomaly detection contexts, a methodology
for controlling various aspects of benchmark creation, guidelines for future
experimental design and a discussion of the many potential pitfalls of trying
to measure success in this field.



In this work we consider the problem of preparation of the stationary
distribution of irreducible, time-reversible Markov chains, which is a
fundamental task in algorithmic Markov chain theory. For the classical setting,
this task has a complexity lower bound of $\Omega(1/\delta)$, where $\delta$ is
the spectral gap of the Markov chain, and other dependencies contribute only
logarithmically. In the quantum case, the conjectured complexity is
$O(\sqrt{\delta^{-1}})$ (with other dependencies contributing only
logarithmically). However, this bound has only been achieved for a few special
classes of Markov chains.
  In this work, we provide a method for the sequential preparation of
stationary distributions for sequences of general time-reversible $N-$state
Markov chains, akin to the setting of simulated annealing methods.
  The complexity of preparation we achieve is $O(\sqrt{\delta^{-1}} N^{1/4})$,
neglecting logarithmic factors. While this result falls short of the
conjectured optimal time, it still provides at least a quadratic improvement
over other straightforward approaches for quantum mixing applied in this
setting.



For assignment problems where agents, specifying ordinal preferences, are
allocated indivisible objects, two widely studied randomized mechanisms are the
Random Serial Dictatorship (RSD) and Probabilistic Serial Rule (PS). These two
mechanisms both have desirable economic and computational properties, but the
outcomes they induce can be incomparable in many instances, thus creating
challenges in deciding which mechanism to adopt in practice. In this paper we
first look at the space of lexicographic preferences and show that, as opposed
to the general preference domain, RSD satisfies envyfreeness. Moreover, we show
that although under lexicographic preferences PS is strategyproof when the
number of objects is less than or equal agents, it is strictly manipulable when
there are more objects than agents. In the space of general preferences, we
provide empirical results on the (in)comparability of RSD and PS, analyze
economic properties, and provide further insights on the applicability of each
mechanism in different application domains.



Similarity between objects is multi-faceted and it can be easier for human
annotators to measure it when the focus is on a specific aspect. We consider
the problem of mapping objects into view-specific embeddings where the distance
between them is consistent with the similarity comparisons of the form "from
the t-th view, object A is more similar to B than to C". Our framework jointly
learns view-specific embeddings exploiting correlations between views.
Experiments on a number of datasets, including one of multi-view crowdsourced
comparison on bird images, show the proposed method achieves lower triplet
generalization error when compared to both learning embeddings independently
for each view and all views pooled into one view. Our method can also be used
to learn multiple measures of similarity over input features taking class
labels into account and compares favorably to existing approaches for
multi-task metric learning on the ISOLET dataset.



Conjunctive database queries have been extended with a mechanism for object
creation to capture important applications such as data exchange, data
integration, and ontology-based data access. Object creation generates new
object identifiers in the result, that do not belong to the set of constants in
the source database. The new object identifiers can be also seen as Skolem
terms. Hence, object-creating conjunctive queries can also be regarded as
restricted second-order tuple-generating dependencies (SO tgds), considered in
the data exchange literature.
  In this paper, we focus on the class of single-function object-creating
conjunctive queries, or sifo CQs for short. We give a new characterization for
oid-equivalence of sifo CQs that is simpler than the one given by Hull and
Yoshikawa and places the problem in the complexity class NP. Our
characterization is based on Cohen's equivalence notions for conjunctive
queries with multiplicities. We also solve the logical entailment problem for
sifo CQs, showing that also this problem belongs to NP. Results by Pichler et
al. have shown that logical equivalence for more general classes of SO tgds is
either undecidable or decidable with as yet unknown complexity upper bounds.



We present a novel hierarchical model for human activity recognition. In
contrast to approaches that successively recognize actions and activities, our
approach jointly models actions and activities in a unified framework, and
their labels are simultaneously predicted. The model is embedded with a latent
layer that is able to capture a richer class of contextual information in both
state-state and observation-state pairs. Although loops are present in the
model, the model has an overall linear-chain structure, where the exact
inference is tractable. Therefore, the model is very efficient in both
inference and learning. The parameters of the graphical model are learned with
a Structured Support Vector Machine (Structured-SVM). A data-driven approach is
used to initialize the latent variables; therefore, no manual labeling for the
latent states is required. The experimental results from using two benchmark
datasets show that our model outperforms the state-of-the-art approach, and our
model is computationally more efficient.



According to E.T. Jaynes and E.P. Wigner, entropy is an anthropomorphic
concept in the sense that in a physical system correspond many thermodynamic
systems. The physical system can be examined from many points of view each time
examining different variables and calculating entropy differently. In this
paper we discuss how this concept may be applied in information entropy; how
Shannon's definition of entropy can fit in Jayne's and Wigner's statement. This
is achieved by generalizing Shannon's notion of information entropy and this is
the main contribution of the paper. Then we discuss how entropy under these
considerations may be used for the comparison of password complexity and as a
measure of diversity useful in the analysis of the behavior of genetic
algorithms.



This paper considers the problem of estimating multiple related Gaussian
graphical models from a $p$-dimensional dataset consisting of different
classes. Our work is based upon the formulation of this problem as group
graphical lasso. This paper proposes a novel hybrid covariance thresholding
algorithm that can effectively identify zero entries in the precision matrices
and split a large joint graphical lasso problem into small subproblems. Our
hybrid covariance thresholding method is superior to existing uniform
thresholding methods in that our method can split the precision matrix of each
individual class using different partition schemes and thus split group
graphical lasso into much smaller subproblems, each of which can be solved very
fast. In addition, this paper establishes necessary and sufficient conditions
for our hybrid covariance thresholding algorithm. The superior performance of
our thresholding method is thoroughly analyzed and illustrated by a few
experiments on simulated data and real gene expression data.



Learning the network structure underlying data is an important problem in
machine learning. This paper introduces a novel prior to study the inference of
scale-free networks, which are widely used to model social and biological
networks. The prior not only favors a desirable global node degree
distribution, but also takes into consideration the relative strength of all
the possible edges adjacent to the same node and the estimated degree of each
individual node.
  To fulfill this, ranking is incorporated into the prior, which makes the
problem challenging to solve. We employ an ADMM (alternating direction method
of multipliers) framework to solve the Gaussian Graphical model regularized by
this prior. Our experiments on both synthetic and real data show that our prior
not only yields a scale-free network, but also produces many more correctly
predicted edges than the others such as the scale-free inducing prior, the
hub-inducing prior and the $l_1$ norm.



We propose Neural Responding Machine (NRM), a neural network-based response
generator for Short-Text Conversation. NRM takes the general encoder-decoder
framework: it formalizes the generation of response as a decoding process based
on the latent representation of the input text, while both encoding and
decoding are realized with recurrent neural networks (RNN). The NRM is trained
with a large amount of one-round conversation data collected from a
microblogging service. Empirical study shows that NRM can generate
grammatically correct and content-wise appropriate responses to over 75% of the
input text, outperforming state-of-the-arts in the same setting, including
retrieval-based and SMT-based models.



We are proposing an extension of the recursive neural network that makes use
of a variant of the long short-term memory architecture. The extension allows
information low in parse trees to be stored in a memory register (the `memory
cell') and used much later higher up in the parse tree. This provides a
solution to the vanishing gradient problem and allows the network to capture
long range dependencies. Experimental results show that our composition
outperformed the traditional neural-network composition on the Stanford
Sentiment Treebank.



This paper investigates the effectiveness of factorial speech processing
models in noise-robust automatic speech recognition tasks. For this purpose,
the paper proposes an idealistic approach for modeling state-conditional
observation distribution of factorial models based on weighted stereo samples.
This approach is an extension to previous single pass retraining for ideal
model compensation which is extended here to support multiple audio sources.
Non-stationary noises can be considered as one of these audio sources with
multiple states. Experiments of this paper over the set A of the Aurora 2
dataset show that recognition performance can be improved by this
consideration. The improvement is significant in low signal to noise energy
conditions, up to 4% absolute word recognition accuracy. In addition to the
power of the proposed method in accurate representation of state-conditional
observation distribution, it has an important advantage over previous methods
by providing the opportunity to independently select feature spaces for both
source and corrupted features. This opens a new window for seeking better
feature spaces appropriate for noisy speech, independent from clean speech
features.



Several efforts to predict student failure rate (SFR) at school accurately
still remains a core problem area faced by many in the educational sector. The
procedure for forecasting SFR are rigid and most often times require data
scaling or conversion into binary form such as is the case of the logistic
model which may lead to lose of information and effect size attenuation. Also,
the high number of factors, incomplete and unbalanced dataset, and black boxing
issues as in Artificial Neural Networks and Fuzzy logic systems exposes the
need for more efficient tools. Currently the application of Genetic Programming
(GP) holds great promises and has produced tremendous positive results in
different sectors. In this regard, this study developed GPSFARPS, a software
application to provide a robust solution to the prediction of SFR using an
evolutionary algorithm known as multi-gene genetic programming. The approach is
validated by feeding a testing data set to the evolved GP models. Result
obtained from GPSFARPS simulations show its unique ability to evolve a suitable
failure rate expression with a fast convergence at 30 generations from a
maximum specified generation of 500. The multi-gene system was also able to
minimize the evolved model expression and accurately predict student failure
rate using a subset of the original expression



High computational costs of manifold learning prohibit its application for
large point sets. A common strategy to overcome this problem is to perform
dimensionality reduction on selected landmarks and to successively embed the
entire dataset with the Nystr\"om method. The two main challenges that arise
are: (i) the landmarks selected in non-Euclidean geometries must result in a
low reconstruction error, (ii) the graph constructed from sparsely sampled
landmarks must approximate the manifold well. We propose the sampling of
landmarks from determinantal distributions on non-Euclidean spaces. Since
current determinantal sampling algorithms have the same complexity as those for
manifold learning, we present an efficient approximation running in linear
time. Further, we recover the local geometry after the sparsification by
assigning each landmark a local covariance matrix, estimated from the original
point set. The resulting neighborhood selection based on the Bhattacharyya
distance improves the embedding of sparsely sampled manifolds. Our experiments
show a significant performance improvement compared to state-of-the-art
landmark selection techniques.



We obtain the conditions for the emergence of the swarm intelligence effect
in an interactive game of restless multi-armed bandit (rMAB). A player competes
with multiple agents. Each bandit has a payoff that changes with a probability
$p_{c}$ per round. The agents and player choose one of three options: (1)
Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among
$n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a good
bandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision:
(i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probability
for Observe in learning. The parameters $(c,p_{obs})$ are uniformly
distributed. We determine the optimal strategies for the player using complete
knowledge about the rMAB. We show whether or not social or asocial learning is
more optimal in the $(p_{c},n_{I})$ space and define the swarm intelligence
effect. We conduct a laboratory experiment (67 subjects) and observe the swarm
intelligence effect only if $(p_{c},n_{I})$ are chosen so that social learning
is far more optimal than asocial learning.



We study probabilistically informative (weak) versions of transitivity, by
using suitable definitions of defaults and negated defaults, in the setting of
coherence and imprecise probabilities. We represent p-consistent sequences of
defaults and/or negated defaults by g-coherent imprecise probability
assessments on the respective sequences of conditional events. Finally, we
prove the coherent probability propagation rules for Weak Transitivity and the
validity of selected inference patterns by proving the p-entailment for the
associated knowledge bases.



A system with artificial intelligence usually relies on symbol manipulation,
at least partly and implicitly. However, the interpretation of the symbols -
what they represent and what they are about - is ultimately left to humans, as
designers and users of the system. How symbols can acquire meaning for the
system itself, independent of external interpretation, is an unsolved problem.
Some grounding of symbols can be obtained by embodiment, that is, by causally
connecting symbols (or sub-symbolic variables) to the physical environment,
such as in a robot with sensors and effectors. However, a causal connection as
such does not produce representation and aboutness of the kind that symbols
have for humans. Here I present a theory that explains how humans and other
living organisms have acquired the capability to have symbols and sub-symbolic
variables that represent, refer to, and are about something else. The theory
shows how reference can be to physical objects, but also to abstract objects,
and even how it can be misguided (errors in reference) or be about non-existing
objects. I subsequently abstract the primary components of the theory from
their biological context, and discuss how and under what conditions the theory
could be implemented in artificial agents. A major component of the theory is
the strong nonlinearity associated with (potentially unlimited)
self-reproduction. The latter is likely not acceptable in artificial systems.
It remains unclear if goals other than those inherently serving
self-reproduction can have aboutness and if such goals could be stabilized.



The question how an agent is affected by its embodiment has attracted growing
attention in recent years. A new field of artificial intelligence has emerged,
which is based on the idea that intelligence cannot be understood without
taking into account embodiment. We believe that a formal approach to
quantifying the embodiment's effect on the agent's behaviour is beneficial to
the fields of artificial life and artificial intelligence. The contribution of
an agent's body and environment to its behaviour is also known as morphological
computation. Therefore, in this work, we propose a quantification of
morphological computation, which is based on an information decomposition of
the sensorimotor loop into shared, unique and synergistic information. In
numerical simulation based on a formal representation of the sensorimotor loop,
we show that the unique information of the body and environment is a good
measure for morphological computation. The results are compared to our
previously derived quantification of morphological computation.



We introduce a new representation and feature extraction method for
biological sequences. Named bio-vectors (BioVec) to refer to biological
sequences in general with protein-vectors (ProtVec) for proteins (amino-acid
sequences) and gene-vectors (GeneVec) for gene sequences, this representation
can be widely used in applications of deep learning in proteomics and genomics.
In the present paper, we focus on protein-vectors that can be utilized in a
wide array of bioinformatics investigations such as family classification,
protein visualization, structure prediction, disordered protein identification,
and protein-protein interaction prediction. In this method, we adopt artificial
neural network approaches and represent a protein sequence with a single dense
n-dimensional vector. To evaluate this method, we apply it in classification of
324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein
families, where an average family classification accuracy of 93%+-0.06% is
obtained, outperforming existing family classification methods. In addition, we
use ProtVec representation to predict disordered proteins from structured
proteins. Two databases of disordered sequences are used: the DisProt database
as well as a database featuring the disordered regions of nucleoporins rich
with phenylalanine-glycine repeats (FG-Nups). Using support vector machine
classifiers, FG-Nup sequences are distinguished from structured protein
sequences found in Protein Data Bank (PDB) with a 99.8% accuracy, and
unstructured DisProt sequences are differentiated from structured DisProt
sequences with 100.0% accuracy. These results indicate that by only providing
sequence data for various proteins into this model, accurate information about
protein structure can be determined.



We consider the problem of recovering a low-rank tensor from its noisy
observation. Previous work has shown a recovery guarantee with signal to noise
ratio $O(n^{\lceil K/2 \rceil /2})$ for recovering a $K$th order rank one
tensor of size $n\times \cdots \times n$ by recursive unfolding. In this paper,
we first improve this bound to $O(n^{K/4})$ by a much simpler approach, but
with a more careful analysis. Then we propose a new norm called the subspace
norm, which is based on the Kronecker products of factors obtained by the
proposed simple estimator. The imposed Kronecker structure allows us to show a
nearly ideal $O(\sqrt{n}+\sqrt{H^{K-1}})$ bound, in which the parameter $H$
controls the blend from the non-convex estimator to mode-wise nuclear norm
minimization. Furthermore, we empirically demonstrate that the subspace norm
achieves the nearly ideal denoising performance even with $H=O(1)$.



Dimension reduction is often needed in the area of data mining. The goal of
these methods is to map the given high-dimensional data into a low-dimensional
space preserving certain properties of the initial data. There are two kinds of
techniques for this purpose. The first, projective methods, builds an explicit
linear projection from the high-dimensional space to the low-dimensional one.
On the other hand, the nonlinear methods utilizes nonlinear and implicit
mapping between the two spaces. In both cases, the methods considered in
literature have usually relied on computationally very intensive matrix
factorizations, frequently the Singular Value Decomposition (SVD). The
computational burden of SVD quickly renders these dimension reduction methods
infeasible thanks to the ever-increasing sizes of the practical datasets.
  In this paper, we present a new decomposition strategy, Reduced Basis
Decomposition (RBD), which is inspired by the Reduced Basis Method (RBM). Given
$X$ the high-dimensional data, the method approximates it by $Y \, T (\approx
X)$ with $Y$ being the low-dimensional surrogate and $T$ the transformation
matrix. $Y$ is obtained through a greedy algorithm thus extremely efficient. In
fact, it is significantly faster than SVD with comparable accuracy. $T$ can be
computed on the fly. Moreover, unlike many compression algorithms, it easily
finds the mapping for an arbitrary ``out-of-sample'' vector and it comes with
an ``error indicator'' certifying the accuracy of the compression. Numerical
results are shown validating these claims.



Existing MAP inference algorithms for determinantal point processes (DPPs)
need to calculate determinants or conduct eigenvalue decomposition generally at
the scale of the full kernel, which presents a great challenge for real-world
applications. In this paper, we introduce a class of DPPs, called BwDPPs, that
are characterized by an almost block diagonal kernel matrix and thus can allow
efficient block-wise MAP inference. Furthermore, BwDPPs are successfully
applied to address the difficulty of selecting change-points in the problem of
change-point detection (CPD), which results in a new BwDPP-based CPD method,
named BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is
first created based on existing well-studied metrics. Then, these change-point
candidates are treated as DPP items, and DPP-based subset selection is
conducted to give the final estimate of the change-points that favours both
quality and diversity. The effectiveness of BwDppCpd is demonstrated through
extensive experiments on five real-world datasets.



Deep Convolutional Neural Networks (CNNs) have demonstrated excellent
performance in image classification, but still show room for improvement in
object-detection tasks with many categories, in particular for cluttered scenes
and occlusion. Modern detection algorithms like Regions with CNNs (Girshick et
al., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions
which with high probability represent objects, where in turn CNNs are deployed
for classification. Selective Search represents a family of sophisticated
algorithms that are engineered with multiple segmentation, appearance and
saliency cues, typically coming with a significant run-time overhead.
Furthermore, (Hosang et al., 2014) have shown that most methods suffer from low
reproducibility due to unstable superpixels, even for slight image
perturbations. Although CNNs are subsequently used for classification in
top-performing object-detection pipelines, current proposal methods are
agnostic to how these models parse objects and their rich learned
representations. As a result they may propose regions which may not resemble
high-level objects or totally miss some of them. To overcome these drawbacks we
propose a boosting approach which directly takes advantage of hierarchical CNN
features for detecting regions of interest fast. We demonstrate its performance
on ImageNet 2013 detection benchmark and compare it with state-of-the-art
methods.



Searching through a large volume of data is very critical for companies,
scientists, and searching engines applications due to time complexity and
memory complexity. In this paper, a new technique of generating FuzzyFind
Dictionary for text mining was introduced. We simply mapped the 23 bits of the
English alphabet into a FuzzyFind Dictionary or more than 23 bits by using more
FuzzyFind Dictionary, and reflecting the presence or absence of particular
letters. This representation preserves closeness of word distortions in terms
of closeness of the created binary vectors within Hamming distance of 2
deviations. This paper talks about the Golay Coding Transformation Hash Table
and how it can be used on a FuzzyFind Dictionary as a new technology for using
in searching through big data. This method is introduced by linear time
complexity for generating the dictionary and constant time complexity to access
the data and update by new data sets, also updating for new data sets is linear
time depends on new data points. This technique is based on searching only for
letters of English that each segment has 23 bits, and also we have more than
23-bit and also it could work with more segments as reference table.



Smoothed analysis is a framework for analyzing the complexity of an
algorithm, acting as a bridge between average and worst-case behaviour. For
example, Quicksort and the Simplex algorithm are widely used in practical
applications, despite their heavy worst-case complexity. Smoothed complexity
aims to better characterize such algorithms. Existing theoretical bounds for
the smoothed complexity of sorting algorithms are still quite weak.
Furthermore, empirically computing the smoothed complexity via its original
definition is computationally infeasible, even for modest input sizes. In this
paper, we focus on accurately predicting the smoothed complexity of sorting
algorithms, using machine learning techniques. We propose two regression models
that take into account various properties of sorting algorithms and some of the
known theoretical results in smoothed analysis to improve prediction quality.
We show experimental results for predicting the smoothed complexity of
Quicksort, Mergesort, and optimized Bubblesort for large input sizes, therefore
filling the gap between known theoretical and empirical results.



In pervasive computing environments, various entities often have to cooperate
and integrate seamlessly in a \emph{situation} which can, thus, be considered
as an amalgamation of the context of several entities interacting and
coordinating with each other, and often performing one or more activities.
However, none of the existing context models and ontologies address situation
modeling. In this paper, we describe the design, structure and implementation
of a generic, flexible and extensible context ontology called Rover Context
Model Ontology (RoCoMO) for context and situation modeling in pervasive
computing systems and environments. We highlight several limitations of the
existing context models and ontologies, such as lack of provision for
provenance, traceability, quality of context, multiple representation of
contextual information, as well as support for security, privacy and
interoperability, and explain how we are addressing these limitations in our
approach. We also illustrate the applicability and utility of RoCoMO using a
practical and extensive case study.



Interactive partially observable Markov decision processes (I-POMDP) provide
a formal framework for planning for a self-interested agent in multiagent
settings. An agent operating in a multiagent environment must deliberate about
the actions that other agents may take and the effect these actions have on the
environment and the rewards it receives. Traditional I-POMDPs model this
dependence on the actions of other agents using joint action and model spaces.
Therefore, the solution complexity grows exponentially with the number of
agents thereby complicating scalability. In this paper, we model and extend
anonymity and context-specific independence -- problem structures often present
in agent populations -- for computational gain. We empirically demonstrate the
efficiency from exploiting these problem structures by solving a new multiagent
problem involving more than 1,000 agents.



The paper describes a novel approach to categorize users' reviews according
to the three Quality in Use (QU) indicators defined in ISO: effectiveness,
efficiency and freedom from risk. With the tremendous amount of reviews
published each day, there is a need to automatically summarize user reviews to
inform us if any of the software able to meet requirement of a company
according to the quality requirements. We implemented the method of Latent
Semantic Analysis (LSA) and its subspace to predict QU indicators. We build a
reduced dimensionality universal semantic space from Information System
journals and Amazon reviews. Next, we projected set of indicators' measurement
scales into the universal semantic space and represent them as subspace. In the
subspace, we can map similar measurement scales to the unseen reviews and
predict the QU indicators. Our preliminary study able to obtain the average of
F-measure, 0.3627.



We consider the problem of learning convex aggregation of models, that is as
good as the best convex aggregation, for the binary classification problem.
Working in the stream based active learning setting, where the active learner
has to make a decision on-the-fly, if it wants to query for the label of the
point currently seen in the stream, we propose a stochastic-mirror descent
algorithm, called SMD-AMA, with entropy regularization. We establish an excess
risk bounds for the loss of the convex aggregate returned by SMD-AMA to be of
the order of $O\left(\sqrt{\frac{\log(M)}{{T^{1-\mu}}}}\right)$, where $\mu\in
[0,1)$ is an algorithm dependent parameter, that trades-off the number of
labels queried, and excess risk.



We present for the first time an asymptotic convergence analysis of two
time-scale stochastic approximation driven by `controlled' Markov noise. In
particular, both the faster and slower recursions have non-additive controlled
Markov noise components in addition to martingale difference noise. We analyze
the asymptotic behavior of our framework by relating it to limiting
differential inclusions in both time-scales that are defined in terms of the
ergodic occupation measures associated with the controlled Markov processes.
Finally, we present a solution to the off-policy convergence problem for
temporal difference learning with linear function approximation, using our
results.



Over the last decade there has been increasing concern about the biases
embodied in traditional evaluation methods for Natural Language
Processing/Learning, particularly methods borrowed from Information Retrieval.
Without knowledge of the Bias and Prevalence of the contingency being tested,
or equivalently the expectation due to chance, the simple conditional
probabilities Recall, Precision and Accuracy are not meaningful as evaluation
measures, either individually or in combinations such as F-factor. The
existence of bias in NLP measures leads to the 'improvement' of systems by
increasing their bias, such as the practice of improving tagging and parsing
scores by using most common value (e.g. water is always a Noun) rather than the
attempting to discover the correct one. The measures Cohen Kappa and Powers
Informedness are discussed as unbiased alternative to Recall and related to the
psychologically significant measure DeltaP. In this paper we will analyze both
biased and unbiased measures theoretically, characterizing the precise
relationship between all these measures as well as evaluating the evaluation
measures themselves empirically using a Monte Carlo simulation.



We address the problem of planning collision-free paths for multiple agents
using optimization methods known as proximal algorithms. Recently this approach
was explored in Bento et al. 2013, which demonstrated its ease of
parallelization and decentralization, the speed with which the algorithms
generate good quality solutions, and its ability to incorporate different
proximal operators, each ensuring that paths satisfy a desired property.
Unfortunately, the operators derived only apply to paths in 2D and require that
any intermediate waypoints we might want agents to follow be preassigned to
specific agents, limiting their range of applicability. In this paper we
resolve these limitations. We introduce new operators to deal with agents
moving in arbitrary dimensions that are faster to compute than their 2D
predecessors and we introduce landmarks, space-time positions that are
automatically assigned to the set of agents under different optimality
criteria. Finally, we report the performance of the new operators in several
numerical experiments.



We computed linguistic information at the lexical, syntactic, and semantic
levels for Recognizing Inference in Text (RITE) tasks for both traditional and
simplified Chinese in NTCIR-9 and NTCIR-10. Techniques for syntactic parsing,
named-entity recognition, and near synonym recognition were employed, and
features like counts of common words, statement lengths, negation words, and
antonyms were considered to judge the entailment relationships of two
statements, while we explored both heuristics-based functions and
machine-learning approaches. The reported systems showed robustness by
simultaneously achieving second positions in the binary-classification subtasks
for both simplified and traditional Chinese in NTCIR-10 RITE-2. We conducted
more experiments with the test data of NTCIR-9 RITE, with good results. We also
extended our work to search for better configurations of our classifiers and
investigated contributions of individual features. This extended work showed
interesting results and should encourage further discussion.



The ability to generalize is an important feature of any intelligent agent.
Not only because it may allow the agent to cope with large amounts of data, but
also because in some environments, an agent with no generalization capabilities
cannot learn. In this work we outline several criteria for generalization, and
present a dynamic and autonomous machinery that enables projective simulation
agents to meaningfully generalize. Projective simulation, a novel, physical
approach to artificial intelligence, was recently shown to perform well in
standard reinforcement learning problems, with applications in advanced
robotics as well as quantum experiments. Both the basic projective simulation
model and the presented generalization machinery are based on very simple
principles. This allows us to provide a full analytical analysis of the agent's
performance and to illustrate the benefit the agent gains by generalizing.
Specifically, we show that already in basic (but extreme) environments,
learning without generalization may be impossible, and demonstrate how the
presented generalization machinery enables the projective simulation agent to
learn.



Knowledge reduction of dynamic covering information systems involves with the
time in practical situations. In this paper, we provide incremental approaches
to computing the type-1 and type-2 characteristic matrices of dynamic coverings
because of varying attribute values. Then we present incremental algorithms of
constructing the second and sixth approximations of sets by using
characteristic matrices. We employ experimental results to illustrate that the
incremental approaches are effective to calculate approximations of sets in
dynamic covering information systems. Finally, we perform knowledge reduction
of dynamic covering information systems with the incremental approaches.



There is a large variety of objects and appliances in human environments,
such as stoves, coffee dispensers, juice extractors, and so on. It is
challenging for a roboticist to program a robot for each of these object types
and for each of their instantiations. In this work, we present a novel approach
to manipulation planning based on the idea that many household objects share
similarly-operated object parts. We formulate the manipulation planning as a
structured prediction problem and design a deep learning model that can handle
large noise in the manipulation demonstrations and learns features from three
different modalities: point-clouds, language and trajectory. In order to
collect a large number of manipulation demonstrations for different objects, we
developed a new crowd-sourcing platform called Robobarista. We test our model
on our dataset consisting of 116 objects with 249 parts along with 250 language
instructions, for which there are 1225 crowd-sourced manipulation
demonstrations. We further show that our robot can even manipulate objects it
has never seen before.



We consider a semantic class, weakly-chase-sticky (WChS), and a syntactic
subclass, jointly-weakly-sticky (JWS), of Datalog+- programs. Both extend that
of weakly-sticky (WS) programs, which appear in our applications to data
quality. For WChS programs we propose a practical, polynomial-time query
answering algorithm (QAA). We establish that the two classes are closed under
magic-sets rewritings. As a consequence, QAA can be applied to the optimized
programs. QAA takes as inputs the program (including the query) and semantic
information about the "finiteness" of predicate positions. For the syntactic
subclasses JWS and WS of WChS, this additional information is computable.



We present a computational framework for automatically quantifying verbal and
nonverbal behaviors in the context of job interviews. The proposed framework is
trained by analyzing the videos of 138 interview sessions with 69
internship-seeking undergraduates at the Massachusetts Institute of Technology
(MIT). Our automated analysis includes facial expressions (e.g., smiles, head
gestures, facial tracking points), language (e.g., word counts, topic
modeling), and prosodic information (e.g., pitch, intonation, and pauses) of
the interviewees. The ground truth labels are derived by taking a weighted
average over the ratings of 9 independent judges. Our framework can
automatically predict the ratings for interview traits such as excitement,
friendliness, and engagement with correlation coefficients of 0.75 or higher,
and can quantify the relative importance of prosody, language, and facial
expressions. By analyzing the relative feature weights learned by the
regression models, our framework recommends to speak more fluently, use less
filler words, speak as "we" (vs. "I"), use more unique words, and smile more.
We also find that the students who were rated highly while answering the first
interview question were also rated highly overall (i.e., first impression
matters). Finally, our MIT Interview dataset will be made available to other
researchers to further validate and expand our findings.



Our understanding about things is conceptual. By stating that we reason about
objects, it is in fact not the objects but concepts referring to them that we
manipulate. Now, so long just as we acknowledge infinitely extending notions
such as space, time, size, colour, etc, - in short, any reasonable quality -
into which an object is subjected, it becomes infeasible to affirm atomicity in
the concept referring to the object. However, formal/symbolic logics typically
presume atomic entities upon which other expressions are built. Can we reflect
our intuition about the concept onto formal/symbolic logics at all? I assure
that we can, but the usual perspective about the atomicity needs inspected. In
this work, I present gradual logic which materialises the observation that we
cannot tell apart whether a so-regarded atomic entity is atomic or is just
atomic enough not to be considered non-atomic. The motivation is to capture
certain phenomena that naturally occur around concepts with attributes,
including presupposition and contraries. I present logical particulars of the
logic, which is then mapped onto formal semantics. Two linguistically
interesting semantics will be considered. Decidability is shown.



Many fields use search algorithms, which automatically explore a search space
to find high-performing solutions: chemists search through the space of
molecules to discover new drugs; engineers search for stronger, cheaper, safer
designs, scientists search for models that best explain data, etc. The goal of
search algorithms has traditionally been to return the single
highest-performing solution in a search space. Here we describe a new,
fundamentally different type of algorithm that is more useful because it
provides a holistic view of how high-performing solutions are distributed
throughout a search space. It creates a map of high-performing solutions at
each point in a space defined by dimensions of variation that a user gets to
choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)
algorithm illuminates search spaces, allowing researchers to understand how
interesting attributes of solutions combine to affect performance, either
positively or, equally of interest, negatively. For example, a drug company may
wish to understand how performance changes as the size of molecules and their
cost-to-produce vary. MAP-Elites produces a large diversity of high-performing,
yet qualitatively different solutions, which can be more helpful than a single,
high-performing solution. Interestingly, because MAP-Elites explores more of
the search space, it also tends to find a better overall solution than
state-of-the-art search algorithms. We demonstrate the benefits of this new
algorithm in three different problem domains ranging from producing modular
neural networks to designing simulated and real soft robots. Because MAP-
Elites (1) illuminates the relationship between performance and dimensions of
interest in solutions, (2) returns a set of high-performing, yet diverse
solutions, and (3) improves finding a single, best solution, it will advance
science and engineering.



The amount of completely sequenced chloroplast genomes increases rapidly
every day, leading to the possibility to build large scale phylogenetic trees
of plant species. Considering a subset of close plant species defined according
to their chloroplasts, the phylogenetic tree that can be inferred by their core
genes is not necessarily well supported, due to the possible occurrence of
"problematic" genes (i.e., homoplasy, incomplete lineage sorting, horizontal
gene transfers, etc.) which may blur phylogenetic signal. However, a
trustworthy phylogenetic tree can still be obtained if the number of
problematic genes is low, the problem being to determine the largest subset of
core genes that produces the best supported tree. To discard problematic genes
and due to the overwhelming number of possible combinations, we propose an
hybrid approach that embeds both genetic algorithms and statistical tests.
Given a set of organisms, the result is a pipeline of many stages for the
production of well supported phylogenetic trees. The proposal has been applied
to different cases of plant families, leading to encouraging results for these
families.



Definition of an accurate system model for Automated Planner (AP) is often
impractical, especially for real-world problems. Conversely, off-the-shelf
planners fail to scale up and are domain dependent. These drawbacks are
inherited from conventional transition systems such as Finite State Machines
(FSMs) that describes the action-plan execution generated by the AP. On the
other hand, Behavior Trees (BTs) represent a valid alternative to FSMs
presenting many advantages in terms of modularity, reactiveness, scalability
and domain-independence. In this paper, we propose a model-free AP framework
using Genetic Programming (GP) to derive an optimal BT for an autonomous agent
to achieve a given goal in unknown (but fully observable) environments. We
illustrate the proposed framework using experiments conducted with an open
source benchmark Mario AI for automated generation of BTs that can play the
game character Mario to complete a certain level at various levels of
difficulty to include enemies and obstacles.



Every day, billions of mobile network events (i.e. CDRs) are generated by
cellular phone operator companies. Latent in this data are inspiring insights
about human actions and behaviors, the discovery of which is important because
context-aware applications and services hold the key to user-driven,
intelligent services, which can enhance our everyday lives such as social and
economic development, urban planning, and health prevention. The major
challenge in this area is that interpreting such a big stream of data requires
a deep understanding of mobile network events' context through available
background knowledge. This article addresses the issues in context awareness
given heterogeneous and uncertain data of mobile network events missing
reliable information on the context of this activity. The contribution of this
research is a model from a combination of logical and statistical reasoning
standpoints for enabling human activity inference in qualitative terms from
open geographical data that aimed at improving the quality of human behaviors
recognition tasks from CDRs. We use open geographical data, Openstreetmap
(OSM), as a proxy for predicting the content of human activity in the area. The
user study performed in Trento shows that predicted human activities (top
level) match the survey data with around 93% overall accuracy. The extensive
validation for predicting a more specific economic type of human activity
performed in Barcelona, by employing credit card transaction data. The analysis
identifies that appropriately normalized data on points of interest (POI) is a
good proxy for predicting human economical activities, with 84% accuracy on
average. So the model is proven to be efficient for predicting the context of
human activity, when its total level could be efficiently observed from cell
phone data records, missing contextual information however.



The problem of target localization with noise is addressed. The target is a
sample from a continuous random variable with known distribution and the goal
is to locate it with minimum mean squared error distortion. The localization
scheme or policy proceeds by queries, or questions, weather or not the target
belongs to some subset as it is addressed in the 20-question framework. These
subsets are not constrained to be intervals and the answers to the queries are
noisy. While this situation is well studied for adaptive querying, this paper
is focused on the non adaptive querying policies based on dyadic questions. The
asymptotic minimum achievable distortion under such policies is derived.
Furthermore, a policy named the Aurelian1 is exhibited which achieves
asymptotically this distortion.



It is known that there are uncoupled learning heuristics leading to Nash
equilibrium in all finite games. Why should players use such learning
heuristics and where could they come from? We show that there is no uncoupled
learning heuristic leading to Nash equilibrium in all finite games that a
player has an incentive to adopt, that would be evolutionary stable or that
could "learn itself". Rather, a player has an incentive to strategically teach
such a learning opponent in order secure at least the Stackelberg leader
payoff. The impossibility result remains intact when restricted to the classes
of generic games, two-player games, potential games, games with strategic
complements or 2x2 games, in which learning is known to be "nice". More
generally, it also applies to uncoupled learning heuristics leading to
correlated equilibria, rationalizable outcomes, iterated admissible outcomes,
or minimal curb sets. A possibility result restricted to "strategically
trivial" games fails if some generic games outside this class are considered as
well.



Social media is becoming an increasingly important source of information to
complement traditional pharmacovigilance methods. In order to identify signals
of potential adverse drug reactions, it is necessary to first identify medical
concepts in the social media text. Most of the existing studies use
dictionary-based methods which are not evaluated independently from the overall
signal detection task.
  We compare different approaches to automatically identify and normalise
medical concepts in consumer reviews in medical forums. Specifically, we
implement several dictionary-based methods popular in the relevant literature,
as well as a method we suggest based on a state-of-the-art machine learning
method for entity recognition. MetaMap, a popular biomedical concept extraction
tool, is used as a baseline. Our evaluations were performed in a controlled
setting on a common corpus which is a collection of medical forum posts
annotated with concepts and linked to controlled vocabularies such as MedDRA
and SNOMED CT.
  To our knowledge, our study is the first to systematically examine the effect
of popular concept extraction methods in the area of signal detection for
adverse reactions. We show that the choice of algorithm or controlled
vocabulary has a significant impact on concept extraction, which will impact
the overall signal detection process. We also show that our proposed machine
learning approach significantly outperforms all the other methods in
identification of both adverse reactions and drugs, even when trained with a
relatively small set of annotated text.



Bayesian max-margin models have shown superiority in various practical
applications, such as text categorization, collaborative prediction, social
network link prediction and crowdsourcing, and they conjoin the flexibility of
Bayesian modeling and predictive strengths of max-margin learning. However,
Monte Carlo sampling for these models still remains challenging, especially for
applications that involve large-scale datasets. In this paper, we present the
stochastic subgradient Hamiltonian Monte Carlo (HMC) methods, which are easy to
implement and computationally efficient. We show the approximate detailed
balance property of subgradient HMC which reveals a natural and validated
generalization of the ordinary HMC. Furthermore, we investigate the variants
that use stochastic subsampling and thermostats for better scalability and
mixing. Using stochastic subgradient Markov Chain Monte Carlo (MCMC), we
efficiently solve the posterior inference task of various Bayesian max-margin
models and extensive experimental results demonstrate the effectiveness of our
approach.



Community annotation of biological entities with concepts from multiple
bio-ontologies has created large and growing repositories of ontology-based
annotation data with embedded implicit relationships among orthogonal
ontologies. Development of efficient data mining methods and metrics to mine
and assess the quality of the mined relationships has not kept pace with the
growth of annotation data. In this study, we present a data mining method that
uses ontology-guided generalization to discover relationships across ontologies
along with a new interestingness metric based on information theory. We apply
our data mining algorithm and interestingness measures to datasets from the
Gene Expression Database at the Mouse Genome Informatics as a preliminary proof
of concept to mine relationships between developmental stages in the mouse
anatomy ontology and Gene Ontology concepts (biological process, molecular
function and cellular component). In addition, we present a comparison of our
interestingness metric to four existing metrics. Ontology-based annotation
datasets provide a valuable resource for discovery of relationships across
ontologies. The use of efficient data mining methods and appropriate
interestingness metrics enables the identification of high quality
relationships.



Expectation maximization (EM) has recently been shown to be an efficient
algorithm for learning finite-state controllers (FSCs) in large decentralized
POMDPs (Dec-POMDPs). However, current methods use fixed-size FSCs and often
converge to maxima that are far from optimal. This paper considers a
variable-size FSC to represent the local policy of each agent. These
variable-size FSCs are constructed using a stick-breaking prior, leading to a
new framework called \emph{decentralized stick-breaking policy representation}
(Dec-SBPR). This approach learns the controller parameters with a variational
Bayesian algorithm without having to assume that the Dec-POMDP model is
available. The performance of Dec-SBPR is demonstrated on several benchmark
problems, showing that the algorithm scales to large problems while
outperforming other state-of-the-art methods.



We present an approach for the detection of coordinate-term relationships
between entities from the software domain, that refer to Java classes. Usually,
relations are found by examining corpus statistics associated with text
entities. In some technical domains, however, we have access to additional
information about the real-world objects named by the entities, suggesting that
coupling information about the "grounded" entities with corpus statistics might
lead to improved methods for relation discovery. To this end, we develop a
similarity measure for Java classes using distributional information about how
they are used in software, which we combine with corpus statistics on the
distribution of contexts in which the classes appear in text. Using our
approach, cross-validation accuracy on this dataset can be improved
dramatically, from around 60% to 88%. Human labeling results show that our
classifier has an F1 score of 86% over the top 1000 predicted pairs.



The project of the Ontology Web Search Engine is presented in this paper. The
main purpose of this paper is to develop such a project that can be easily
implemented. Ontology Web Search Engine is software to look for and index
ontologies in the Web. OWL (Web Ontology Languages) ontologies are meant, and
they are necessary for the functioning of the SWES (Semantic Web Expert
System). SWES is an expert system that will use found ontologies from the Web,
generating rules from them, and will supplement its knowledge base with these
generated rules. It is expected that the SWES will serve as a universal expert
system for the average user.



Conditional Simple Temporal Network (CSTN) is a constraint-based
graph-formalism for conditional temporal planning. It offers a more flexible
formalism than the equivalent CSTP model of Tsamardinos, Vidal and Pollack,
from which it was derived mainly as a sound formalization. Three notions of
consistency arise for CSTNs and CSTPs: weak, strong, and dynamic. Dynamic
consistency is the most interesting notion, but it is also the most challenging
and it was conjectured to be hard to assess. Tsamardinos, Vidal and Pollack
gave a doubly-exponential time algorithm for deciding whether a CSTN is
dynamically-consistent and to produce, in the positive case, a dynamic
execution strategy of exponential size. In the present work we offer a proof
that deciding whether a CSTN is dynamically-consistent is coNP-hard and provide
the first singly-exponential time algorithm for this problem, also producing a
dynamic execution strategy whenever the input CSTN is dynamically-consistent.
The algorithm is based on a novel connection with Mean Payoff Games, a family
of two-player combinatorial games on graphs well known for having applications
in model-checking and formal verification. The presentation of such connection
is mediated by the Hyper Temporal Network model, a tractable generalization of
Simple Temporal Networks whose consistency checking is equivalent to
determining Mean Payoff Games. In order to analyze the algorithm we introduce a
refined notion of dynamic-consistency, named \epsilon-dynamic-consistency, and
present a sharp lower bounding analysis on the critical value of the reaction
time \hat{\varepsilon} where the CSTN transits from being, to not being,
dynamically-consistent. The proof technique introduced in this analysis of
\hat{\varepsilon} is applicable more in general when dealing with linear
difference constraints which include strict inequalities.



Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11.
In this paper, we describe our system for solving real-world university
entrance exam questions, which are related to world history. Wikipedia is used
as the main external resource for our system. Since problems with choosing
right/wrong sentence from multiple sentence choices account for about
two-thirds of the total, we individually design a classification based model
for solving this type of questions. For other types of questions, we also
design some simple methods.



Pervasive systems refers to context-aware systems that can sense their
context, and adapt their behavior accordingly to provide adaptable services.
Proactive adaptation of such systems allows changing the service and the
context based on prediction. However, the definition of the context is still
vague and not suitable to prediction. In this paper we discuss and classify
previous definitions of context. Then, we present a new definition which allows
pervasive systems to understand and predict their contexts. We analyze the
essential lines that fall within the context definition, and propose some
scenarios to make it clear our approach.



We address a question answering task on real-world images that is set up as a
Visual Turing Test. By combining latest advances in image representation and
natural language processing, we propose Neural-Image-QA, an end-to-end
formulation to this problem for which all parts are trained jointly. In
contrast to previous efforts, we are facing a multi-modal problem where the
language output (answer) is conditioned on visual and natural language input
(image and question). Our approach Neural-Image-QA doubles the performance of
the previous best approach on this problem. We provide additional insights into
the problem by analyzing how much information is contained only in the language
part for which we provide a new human baseline. To study human consensus, which
is related to the ambiguities inherent in this challenging task, we propose two
novel metrics and collect additional answers which extends the original DAQUAR
dataset to DAQUAR-Consensus.



Potential games, originally introduced in the early 1990's by Lloyd Shapley,
the 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are a
very important class of models in game theory. They have special properties
such as the existence of Nash equilibria in pure strategies. This note
introduces graphical versions of potential games. Special cases of graphical
potential games have already found applicability in many areas of science and
engineering beyond economics, including artificial intelligence, computer
vision, and machine learning. They have been effectively applied to the study
and solution of important real-world problems such as routing and congestion in
networks, distributed resource allocation (e.g., public goods), and
relaxation-labeling for image segmentation. Implicit use of graphical potential
games goes back at least 40 years. Several classes of games considered standard
in the literature, including coordination games, local interaction games,
lattice games, congestion games, and party-affiliation games, are instances of
graphical potential games. This note provides several characterizations of
graphical potential games by leveraging well-known results from the literature
on probabilistic graphical models. A major contribution of the work presented
here that particularly distinguishes it from previous work is establishing that
the convergence of certain type of game-playing rules implies that the
agents/players must be embedded in some graphical potential game.



LeoPARD supports the implementation of knowledge representation and reasoning
tools for higher-order logic(s). It combines a sophisticated data structure
layer (polymorphically typed {\lambda}-calculus with nameless spine notation,
explicit substitutions, and perfect term sharing) with an ambitious multi-agent
blackboard architecture (supporting prover parallelism at the term, clause, and
search level). Further features of LeoPARD include a parser for all TPTP
dialects, a command line interpreter, and generic means for the integration of
external reasoners.



Two recent approaches have achieved state-of-the-art results in image
captioning. The first uses a pipelined process where a set of candidate words
is generated by a convolutional neural network (CNN) trained on images, and
then a maximum entropy (ME) language model is used to arrange these words into
a coherent sentence. The second uses the penultimate activation layer of the
CNN as input to a recurrent neural network (RNN) that then generates the
caption sequence. In this paper, we compare the merits of these different
language modeling approaches for the first time by using the same
state-of-the-art CNN as input. We examine issues in the different approaches,
including linguistic irregularities, caption repetition, and data set overlap.
By combining key aspects of the ME and RNN methods, we achieve a new record
performance over previously published results on the benchmark COCO dataset.
However, the gains we see in BLEU do not translate to human judgments.



This report provides an overview of the current state of the art deep
learning architectures and optimisation techniques, and uses the ADNI
hippocampus MRI dataset as an example to compare the effectiveness and
efficiency of different convolutional architectures on the task of patch-based
3-dimensional hippocampal segmentation, which is important in the diagnosis of
Alzheimer's Disease. We found that a slightly unconventional "stacked 2D"
approach provides much better classification performance than simple 2D patches
without requiring significantly more computational power. We also examined the
popular "tri-planar" approach used in some recently published studies, and
found that it provides much better results than the 2D approaches, but also
with a moderate increase in computational power requirement. Finally, we
evaluated a full 3D convolutional architecture, and found that it provides
marginally better results than the tri-planar approach, but at the cost of a
very significant increase in computational power requirement.



This work aims to address the problem of image-based question-answering (QA)
with new models and datasets. In our work, we propose to use neural networks
and visual semantic embeddings, without intermediate stages such as object
detection and image segmentation, to predict answers to simple questions about
images. Our model performs 1.8 times better than the only published results on
an existing image QA dataset. We also present a question generation algorithm
that converts image descriptions, which are widely available, into QA form. We
used this algorithm to produce an order-of-magnitude larger dataset, with more
evenly distributed answers. A suite of baseline results on this new dataset are
also presented.



Understanding how images of objects and scenes behave in response to specific
ego-motions is a crucial aspect of proper visual development, yet existing
visual learning methods are conspicuously disconnected from the physical source
of their images. We propose to exploit proprioceptive motor signals to provide
unsupervised regularization in convolutional neural networks to learn visual
representations from egocentric video. Specifically, we enforce that our
learned features exhibit equivariance i.e. they respond predictably to
transformations associated with distinct ego-motions. With three datasets, we
show that our unsupervised feature learning approach significantly outperforms
previous approaches on visual recognition and next-best-view prediction tasks.
In the most challenging test, we show that features learned from video captured
on an autonomous driving platform improve large-scale scene recognition in
static images from a disjoint domain.



Compositional embedding models build a representation (or embedding) for a
linguistic structure based on its component word embeddings. We propose a
Feature-rich Compositional Embedding Model (FCM) for relation extraction that
is expressive, generalizes to new domains, and is easy-to-implement. The key
idea is to combine both (unlexicalized) hand-crafted features with learned word
embeddings. The model is able to directly tackle the difficulties met by
traditional compositional embeddings models, such as handling arbitrary types
of sentence annotations and utilizing global information for composition. We
test the proposed model on two relation extraction tasks, and demonstrate that
our model outperforms both previous compositional models and traditional
feature rich models on the ACE 2005 relation extraction task, and the SemEval
2010 relation classification task. The combination of our model and a
log-linear classifier with hand-crafted features gives state-of-the-art
results.



Fault Tree Analysis (FTA) is a dependability analysis technique that has been
widely used to predict reliability, availability and safety of many complex
engineering systems. Traditionally, these FTA-based analyses are done using
paper-and-pencil proof methods or computer simulations, which cannot ascertain
absolute correctness due to their inherent limitations. As a complementary
approach, we propose to use the higher-order-logic theorem prover HOL4 to
conduct the FTA-based analysis of safety-critical systems where accuracy of
failure analysis is a dire need. In particular, the paper presents a
higher-order-logic formalization of generic Fault Tree gates, i.e., AND, OR,
NAND, NOR, XOR and NOT and the formal verification of their failure probability
expressions. Moreover, we have formally verified the generic probabilistic
inclusion-exclusion principle, which is one of the foremost requirements for
conducting the FTA-based failure analysis of any given system. For illustration
purposes, we conduct the FTA-based failure analysis of a solar array that is
used as the main source of power for the Dong Fang Hong-3 (DFH-3) satellite.



Metric learning seeks a transformation of the feature space that enhances
prediction quality for the given task at hand. In this work we provide
PAC-style sample complexity rates for supervised metric learning. We give
matching lower- and upper-bounds showing that the sample complexity scales with
the representation dimension when no assumptions are made about the underlying
data distribution. However, by leveraging the structure of the data
distribution, we show that one can achieve rates that are fine-tuned to a
specific notion of intrinsic complexity for a given dataset. Our analysis
reveals that augmenting the metric learning optimization criterion with a
simple norm-based regularization can help adapt to a dataset's intrinsic
complexity, yielding better generalization. Experiments on benchmark datasets
validate our analysis and show that regularizing the metric can help discern
the signal even when the data contains high amounts of noise.



Outlier detection aims to identify unusual data instances that deviate from
expected patterns. The outlier detection is particularly challenging when
outliers are context dependent and when they are defined by unusual
combinations of multiple outcome variable values. In this paper, we develop and
study a new conditional outlier detection approach for multivariate outcome
spaces that works by (1) transforming the conditional detection to the outlier
detection problem in a new (unconditional) space and (2) defining outlier
scores by analyzing the data in the new space. Our approach relies on the
classifier chain decomposition of the multi-dimensional classification problem
that lets us transform the output space into a probability vector, one
probability for each dimension of the output space. Outlier scores applied to
these transformed vectors are then used to detect the outliers. Experiments on
multiple multi-dimensional classification problems with the different outlier
injection rates show that our methodology is robust and able to successfully
identify outliers when outliers are either sparse (manifested in one or very
few dimensions) or dense (affecting multiple dimensions).



We focus on the problem of finding a non-linear classification function that
lies in a Reproducing Kernel Hilbert Space (RKHS) both from the primal point of
view (finding a perfect separator when one exists) and the dual point of view
(giving a certificate of non-existence), with special focus on generalizations
of two classical schemes - the Perceptron (primal) and Von-Neumann (dual)
algorithms.
  We cast our problem as one of maximizing the regularized normalized
hard-margin ($\rho$) in an RKHS and %use the Representer Theorem to rephrase it
in terms of a Mahalanobis dot-product/semi-norm associated with the kernel's
(normalized and signed) Gram matrix. We derive an accelerated smoothed
algorithm with a convergence rate of $\tfrac{\sqrt {\log n}}{\rho}$ given $n$
separable points, which is strikingly similar to the classical kernelized
Perceptron algorithm whose rate is $\tfrac1{\rho^2}$. When no such classifier
exists, we prove a version of Gordan's separation theorem for RKHSs, and give a
reinterpretation of negative margins. This allows us to give guarantees for a
primal-dual algorithm that halts in $\min\{\tfrac{\sqrt n}{|\rho|},
\tfrac{\sqrt n}{\epsilon}\}$ iterations with a perfect separator in the RKHS if
the primal is feasible or a dual $\epsilon$-certificate of near-infeasibility.



Interesting theoretical associations have been established by recent papers
between the fields of active learning and stochastic convex optimization due to
the common role of feedback in sequential querying mechanisms. In this paper,
we continue this thread in two parts by exploiting these relations for the
first time to yield novel algorithms in both fields, further motivating the
study of their intersection. First, inspired by a recent optimization algorithm
that was adaptive to unknown uniform convexity parameters, we present a new
active learning algorithm for one-dimensional thresholds that can yield minimax
rates by adapting to unknown noise parameters. Next, we show that one can
perform $d$-dimensional stochastic minimization of smooth uniformly convex
functions when only granted oracle access to noisy gradient signs along any
coordinate instead of real-valued gradients, by using a simple randomized
coordinate descent procedure where each line search can be solved by
$1$-dimensional active learning, provably achieving the same error convergence
rate as having the entire real-valued gradient. Combining these two parts
yields an algorithm that solves stochastic convex optimization of uniformly
convex and smooth functions using only noisy gradient signs by repeatedly
performing active learning, achieves optimal rates and is adaptive to all
unknown convexity and smoothness parameters.



A fundamental challenge in developing high-impact machine learning
technologies is balancing the need to model rich, structured domains with the
ability to scale to big data. Many important problem areas are both richly
structured and large scale, from social and biological networks, to knowledge
graphs and the Web, to images, video, and natural language. In this paper, we
introduce two new formalisms for modeling structured data, and show that they
can both capture rich structure and scale to big data. The first, hinge-loss
Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model
that generalizes different approaches to convex inference. We unite three
approaches from the randomized algorithms, probabilistic graphical models, and
fuzzy logic communities, showing that all three lead to the same inference
objective. We then define HL-MRFs by generalizing this unified objective. The
second new formalism, probabilistic soft logic (PSL), is a probabilistic
programming language that makes HL-MRFs easy to define using a syntax based on
first-order logic. We introduce an algorithm for inferring most-probable
variable assignments (MAP inference) that is much more scalable than
general-purpose convex optimization methods, because it uses message passing to
take advantage of sparse dependency structures. We then show how to learn the
parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous
discrete models, but much more scalable. Together, these algorithms enable
HL-MRFs and PSL to model rich, structured data at scales not previously
possible.



Emergence is a phenomenon taken for granted in science but also still not
well understood. We have developed a model of artificial genetic evolution
intended to allow for emergence on genetic, population and social levels. We
present the details of the current state of our environment, agent, and
reproductive models. In developing our models we have relied on a principle of
using non-linear systems to model as many systems as possible including
mutation and recombination, gene-environment interaction, agent metabolism,
agent survival, resource gathering and sexual reproduction. In this paper we
review the genetic dynamics that have emerged in our system including
genotype-phenotype divergence, genetic drift, pseudogenes, and gene
duplication. We conclude that emergence-focused design in complex system
simulation is necessary to reproduce the multilevel emergence seen in the
natural world.



Distributed computing excels at processing large scale data, but the
communication cost for synchronizing the shared parameters may slow down the
overall performance. Fortunately, the interactions between parameter and data
in many problems are sparse, which admits efficient partition in order to
reduce the communication overhead.
  In this paper, we formulate data placement as a graph partitioning problem.
We propose a distributed partitioning algorithm. We give both theoretical
guarantees and a highly efficient implementation. We also provide a highly
efficient implementation of the algorithm and demonstrate its promising results
on both text datasets and social networks. We show that the proposed algorithm
leads to 1.6x speedup of a state-of-the-start distributed machine learning
system by eliminating 90\% of the network communication.



Analyzing huge amounts of spatial data plays an important role in many
emerging analysis and decision-making domains such as healthcare, urban
planning, agriculture and so on. For extracting meaningful knowledge from
geographical data, the relationships between spatial data objects need to be
analyzed. An important class of such relationships are topological relations
like the connectedness or overlap between regions. While real-world
geographical regions such as lakes or forests do not have exact boundaries and
are fuzzy, most of the existing analysis methods neglect this inherent feature
of topological relations. In this paper, we propose a method for handling the
topological relations in spatial databases based on fuzzy region connection
calculus (RCC). The proposed method is implemented in PostGIS spatial database
and evaluated in analyzing the relationship of diseases as an important
application domain. We also used our fuzzy RCC implementation for fuzzification
of the skyline operator in spatial databases. The results of the evaluation
show that our method provides a more realistic view of spatial relationships
and gives more flexibility to the data analyst to extract meaningful and
accurate results in comparison with the existing methods.



Writing rap lyrics requires both creativity to construct a meaningful,
interesting story and lyrical skills to produce complex rhyme patterns, which
form the cornerstone of good flow. We present a rap lyrics generation method
that captures both of these aspects. First, we develop a prediction model to
identify the next line of existing lyrics from a set of candidate next lines.
This model is based on two machine-learning techniques: the RankSVM algorithm
and a deep neural network model with a novel structure. Results show that the
prediction model can identify the true next line among 299 randomly selected
lines with an accuracy of 17%, i.e., over 50 times more likely than by random.
Second, we employ the prediction model to combine lines from existing songs,
producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics
shows that in terms of quantitative rhyme density, the method outperforms the
best human rappers by 21%. The rap lyrics generator has been deployed as an
online tool called DeepBeat, and the performance of the tool has been assessed
by analyzing its usage logs. This analysis shows that machine-learned rankings
correlate with user preferences.



Continued reliance on human operators for managing data centers is a major
impediment for them from ever reaching extreme dimensions. Large computer
systems in general, and data centers in particular, will ultimately be managed
using predictive computational and executable models obtained through
data-science tools, and at that point, the intervention of humans will be
limited to setting high-level goals and policies rather than performing
low-level operations. Data-driven autonomics, where management and control are
based on holistic predictive models that are built and updated using generated
data, opens one possible path towards limiting the role of operators in data
centers. In this paper, we present a data-science study of a public Google
dataset collected in a 12K-node cluster with the goal of building and
evaluating a predictive model for node failures. We use BigQuery, the big data
SQL platform from the Google Cloud suite, to process massive amounts of data
and generate a rich feature set characterizing machine state over time. We
describe how an ensemble classifier can be built out of many Random Forest
classifiers each trained on these features, to predict if machines will fail in
a future 24-hour window. Our evaluation reveals that if we limit false positive
rates to 5%, we can achieve true positive rates between 27% and 88% with
precision varying between 50% and 72%. We discuss the practicality of including
our predictive model as the central component of a data-driven autonomic
manager and operating it on-line with live data streams (rather than off-line
on data logs). All of the scripts used for BigQuery and classification analyses
are publicly available from the authors' website.



We present a novel hybrid algorithm for Bayesian network structure learning,
called Hybrid HPC (H2PC). It first reconstructs the skeleton of a Bayesian
network and then performs a Bayesian-scoring greedy hill-climbing search to
orient the edges. It is based on a subroutine called HPC, that combines ideas
from incremental and divide-and-conquer constraint-based methods to learn the
parents and children of a target variable. We conduct an experimental
comparison of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the
most powerful state-of-the-art algorithm for Bayesian network structure
learning, on several benchmarks with various data sizes. Our extensive
experiments show that H2PC outperforms MMHC both in terms of goodness of fit to
new data and in terms of the quality of the network structure itself, which is
closer to the true dependence structure of the data. The source code (in R) of
H2PC as well as all data sets used for the empirical tests are publicly
available.



The proliferation of the web presents an unsolved problem of automatically
analyzing billions of pages of natural language. We introduce a scalable
algorithm that clusters hundreds of millions of web pages into hundreds of
thousands of clusters. It does this on a single mid-range machine using
efficient algorithms and compressed document representations. It is applied to
two web-scale crawls covering tens of terabytes. ClueWeb09 and ClueWeb12
contain 500 and 733 million web pages and were clustered into 500,000 to
700,000 clusters. To the best of our knowledge, such fine grained clustering
has not been previously demonstrated. Previous approaches clustered a sample
that limits the maximum number of discoverable clusters. The proposed EM-tree
algorithm uses the entire collection in clustering and produces several orders
of magnitude more clusters than the existing algorithms. Fine grained
clustering is necessary for meaningful clustering in massive collections where
the number of distinct topics grows linearly with collection size. These
fine-grained clusters show an improved cluster quality when assessed with two
novel evaluations using ad hoc search relevance judgments and spam
classifications for external validation. These evaluations solve the problem of
assessing the quality of clusters where categorical labeling is unavailable and
unfeasible.



The categorical compositional distributional model of Coecke, Sadrzadeh and
Clark provides a linguistically motivated procedure for computing the meaning
of a sentence as a function of the distributional meaning of the words therein.
The theoretical framework allows for reasoning about compositional aspects of
language and offers structural ways of studying the underlying relationships.
While the model so far has been applied on the level of syntactic structures, a
sentence can bring extra information conveyed in utterances via intonational
means. In the current paper we extend the framework in order to accommodate
this additional information, using Frobenius algebraic structures canonically
induced over the basis of finite-dimensional vector spaces. We detail the
theory, provide truth-theoretic and distributional semantics for meanings of
intonationally-marked utterances, and present justifications and extensive
examples.



The Web has made it possible to harness human cognition en masse to achieve
new capabilities. Some of these successes are well known; for example Wikipedia
has become the go-to place for basic information on all things; Duolingo
engages millions of people in real-life translation of text, while
simultaneously teaching them to speak foreign languages; and fold.it has
enabled public-driven scientific discoveries by recasting complex biomedical
challenges into popular online puzzle games. These and other early successes
hint at the tremendous potential for future crowd-powered capabilities for the
benefit of health, education, science, and society. In the process, a new field
called Human Computation has emerged to better understand, replicate, and
improve upon these successes through scientific research. Human Computation
refers to the science that underlies online crowd-powered systems and was the
topic of a recent visioning activity in which a representative cross-section of
researchers, industry practitioners, visionaries, funding agency
representatives, and policy makers came together to understand what makes
crowd-powered systems successful. Teams of experts considered past, present,
and future human computation systems to explore which kinds of crowd-powered
systems have the greatest potential for societal impact and which kinds of
research will best enable the efficient development of new crowd-powered
systems to achieve this impact. This report summarize the products and findings
of those activities as well as the unconventional process and activities
employed by the workshop, which were informed by human computation research.



The paper describes clustering problems from the combinatorial viewpoint. A
brief systemic survey is presented including the following: (i) basic
clustering problems (e.g., classification, clustering, sorting, clustering with
an order over cluster), (ii) basic approaches to assessment of objects and
object proximities (i.e., scales, comparison, aggregation issues), (iii) basic
approaches to evaluation of local quality characteristics for clusters and
total quality characteristics for clustering solutions, (iv) clustering as
multicriteria optimization problem, (v) generalized modular clustering
framework, (vi) basic clustering models/methods (e.g., hierarchical clustering,
k-means clustering, minimum spanning tree based clustering, clustering as
assignment, detection of clisue/quasi-clique based clustering, correlation
clustering, network communities based clustering), Special attention is
targeted to formulation of clustering as multicriteria optimization models.
Combinatorial optimization models are used as auxiliary problems (e.g.,
assignment, partitioning, knapsack problem, multiple choice problem,
morphological clique problem, searching for consensus/median for structures).
Numerical examples illustrate problem formulations, solving methods, and
applications. The material can be used as follows: (a) a research survey, (b) a
fundamental for designing the structure/architecture of composite modular
clustering software, (c) a bibliography reference collection, and (d) a
tutorial.



Recurrent Neural Networks (RNNs) have become increasingly popular for the
task of language understanding. In this task, a semantic tagger is deployed to
associate a semantic label to each word in an input sequence. The success of
RNN may be attributed to its ability to memorize long-term dependence that
relates the current-time semantic label prediction to the observations many
time instances away. However, the memory capacity of simple RNNs is limited
because of the gradient vanishing and exploding problem. We propose to use an
external memory to improve memorization capability of RNNs. We conducted
experiments on the ATIS dataset, and observed that the proposed model was able
to achieve the state-of-the-art results. We compare our proposed model with
alternative models and report analysis results that may provide insights for
future research.



Due to rapid advancement in high-throughput techniques, such as microarrays
and next generation sequencing technologies, biological data are increasing
exponentially. The current challenge in computational biology and
bioinformatics research is how to analyze these huge raw biological data to
extract biologically meaningful knowledge. This review paper presents the
applications of formal concept analysis for the analysis and knowledge
discovery from biological data, including gene expression discretization, gene
co-expression mining, gene expression clustering, finding genes in gene
regulatory networks, enzyme/protein classifications, binding site
classifications, and so on. It also presents a list of FCA-based software tools
applied in biological domain and covers the challenges faced so far.



This work is concerned with regular languages defined over large alphabets,
either infinite or just too large to be expressed enumeratively. We define a
generic model where transitions are labeled by elements of a finite partition
of the alphabet. We then extend Angluin's L* algorithm for learning regular
languages from examples for such automata. We have implemented this algorithm
and we demonstrate its behavior where the alphabet is a subset of the natural
or real numbers. We sketch the extension of the algorithm to a class of
languages over partially ordered alphabets.



Suppose there is a large collection of items, each with an associated cost
and an inherent utility that is revealed only once we commit to selecting it.
Given a budget on the cumulative cost of the selected items, how can we pick a
subset of maximal value? This task generalizes several important problems such
as multi-arm bandits, active search and the knapsack problem. We present an
algorithm, GP-Select, which utilizes prior knowledge about similarity be- tween
items, expressed as a kernel function. GP-Select uses Gaussian process
prediction to balance exploration (estimating the unknown value of items) and
exploitation (selecting items of high value). We extend GP-Select to be able to
discover sets that simultaneously have high utility and are diverse. Our
preference for diversity can be specified as an arbitrary monotone submodular
function that quantifies the diminishing returns obtained when selecting
similar items. Furthermore, we exploit the structure of the model updates to
achieve an order of magnitude (up to 40X) speedup in our experiments without
resorting to approximations. We provide strong guarantees on the performance of
GP-Select and apply it to three real-world case studies of industrial
relevance: (1) Refreshing a repository of prices in a Global Distribution
System for the travel industry, (2) Identifying diverse, binding-affine
peptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scale
recommender system by recommending items to users.



This paper tackles the problem of endogenous link prediction for Knowledge
Base completion. Knowledge Bases can be represented as directed graphs whose
nodes correspond to entities and edges to relationships. Previous attempts
either consist of powerful systems with high capacity to model complex
connectivity patterns, which unfortunately usually end up overfitting on rare
relationships, or in approaches that trade capacity for simplicity in order to
fairly model all relationships, frequent or not. In this paper, we propose
Tatec a happy medium obtained by complementing a high-capacity model with a
simpler one, both pre-trained separately and then combined. We present several
variants of this model with different kinds of regularization and combination
strategies and show that this approach outperforms existing methods on
different types of relationships by achieving state-of-the-art results on four
benchmarks of the literature.



A neuromorphic chip that combines CMOS analog spiking neurons and memristive
synapses offers a promising solution to brain-inspired computing, as it can
provide massive neural network parallelism and density. Previous hybrid analog
CMOS-memristor approaches required extensive CMOS circuitry for training, and
thus eliminated most of the density advantages gained by the adoption of
memristor synapses. Further, they used different waveforms for pre and
post-synaptic spikes that added undesirable circuit overhead. Here we describe
a hardware architecture that can feature a large number of memristor synapses
to learn real-world patterns. We present a versatile CMOS neuron that combines
integrate-and-fire behavior, drives passive memristors and implements
competitive learning in a compact circuit module, and enables in-situ
plasticity in the memristor synapses. We demonstrate handwritten-digits
recognition using the proposed architecture using transistor-level circuit
simulations. As the described neuromorphic architecture is homogeneous, it
realizes a fundamental building block for large-scale energy-efficient
brain-inspired silicon chips that could lead to next-generation cognitive
computing.



Path queries on a knowledge graph can be used to answer compositional
questions such as "What languages are spoken by people living in Lisbon?".
However, knowledge graphs often have missing facts (edges) which disrupts path
queries. Recent models for knowledge base completion impute missing facts by
embedding knowledge graphs in vector spaces. We show that these models can be
recursively applied to answer path queries, but that they suffer from cascading
errors. This motivates a new "compositional" training objective, which
dramatically improves all models' ability to answer path queries, in some cases
more than doubling accuracy. On a standard knowledge base completion task, we
also demonstrate that compositional training acts as a novel form of structural
regularization, reliably improving performance across all base models (reducing
errors by up to 43%) and achieving new state-of-the-art results.



The ad hoc coordination problem is to design an autonomous agent which is
able to achieve optimal flexibility and efficiency in a multiagent system with
no mechanisms for prior coordination. We conceptualise this problem formally
using a game-theoretic model, called the stochastic Bayesian game, in which the
behaviour of a player is determined by its private information, or type. Based
on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc
Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in
a planning procedure to find optimal actions in the sense of Bellman optimal
control. We evaluate HBA in a multiagent logistics domain called level-based
foraging, showing that it achieves higher flexibility and efficiency than
several alternative algorithms. We also report on a human-machine experiment at
a public science exhibition in which the human participants played repeated
Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative
algorithms, showing that HBA achieves equal efficiency and a significantly
higher welfare and winning rate.



We assess the performance of generic text summarization algorithms applied to
films and documentaries, using the well-known behavior of summarization of news
articles as reference. We use three datasets: (i) news articles, (ii) film
scripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics
are used for comparing generated summaries against news abstracts, plot
summaries, and synopses. We show that the best performing algorithms are LSA,
for news articles and documentaries, and LexRank and Support Sets, for films.
Despite the different nature of films and documentaries, their relative
behavior is in accordance with that obtained for news articles.



A simple Neural Network model is presented for end-to-end visual learning of
arithmetic operations from pictures of numbers. The input consists of two
pictures, each showing a 7-digit number. The output, also a picture, displays
the number showing the result of an arithmetic operation (e.g., addition or
subtraction) on the two input numbers. The concepts of a number, or of an
operator, are not explicitly introduced. This indicates that addition is a
simple cognitive task, which can be learned visually using a very small number
of neurons.
  Other operations, e.g., multiplication, were not learnable using this
architecture. Some tasks were not learnable end-to-end (e.g., addition with
Roman numerals), but were easily learnable once broken into two separate
sub-tasks: a perceptual \textit{Character Recognition} and cognitive
\textit{Arithmetic} sub-tasks. This indicates that while some tasks may be
easily learnable end-to-end, other may need to be broken into sub-tasks.



Behavior Trees are commonly used to model agents for robotics and games,
where constrained behaviors must be designed by human experts in order to
guarantee that these agents will execute a specific chain of actions given a
specific set of perceptions. In such application areas, learning is a desirable
feature to provide agents with the ability to adapt and improve interactions
with humans and environment, but often discarded due to its unreliability. In
this paper, we propose a framework that uses Reinforcement Learning nodes as
part of Behavior Trees to address the problem of adding learning capabilities
in constrained agents. We show how this framework relates to Options in
Hierarchical Reinforcement Learning, ensuring convergence of nested learning
nodes, and we empirically show that the learning nodes do not affect the
execution of other nodes in the tree.



In Constraint Programming, global constraints allow to model and solve many
combinatorial problems. Among these constraints, several sortedness constraints
have been defined, for which propagation algorithms are available, but for
which the tractability is not settled. We show that the sort(U,V) constraint
(Older et. al, 1995) is intractable for integer variables whose domains are not
limited to intervals. As a consequence, the similar result holds for the
sort(U,V, P) constraint (Zhou, 1996). Moreover, the intractability holds even
under the stability condition present in the recently introduced
keysorting(U,V,Keys,P) constraint (Carlsson et al., 2014), and requiring that
the order of the variables with the same value in the list U be preserved in
the list V. Therefore, keysorting(U,V,Keys,P) is intractable as well.



Teaching machines to read natural language documents remains an elusive
challenge. Machine reading systems can be tested on their ability to answer
questions posed on the contents of documents that they have seen, but until now
large scale training and test datasets have been missing for this type of
evaluation. In this work we define a new methodology that resolves this
bottleneck and provides large scale supervised reading comprehension data. This
allows us to develop a class of attention based deep neural networks that learn
to read real documents and answer complex questions with minimal prior
knowledge of language structure.



We consider a contextual version of multi-armed bandit problem with global
knapsack constraints. In each round, the outcome of pulling an arm is a scalar
reward and a resource consumption vector, both dependent on the context, and
the global knapsack constraints require the total consumption for each resource
to be below some pre-fixed budget. The learning agent competes with an
arbitrary set of context-dependent policies. This problem was introduced by
Badanidiyuru et al. (2014), who gave a computationally inefficient algorithm
with near-optimal regret bounds for it. We give a computationally efficient
algorithm for this problem with slightly better regret bounds, by generalizing
the approach of Agarwal et al. (2014) for the non-constrained version of the
problem. The computational time of our algorithm scales logarithmically in the
size of the policy space. This answers the main open question of Badanidiyuru
et al. (2014). We also extend our results to a variant where there are no
knapsack constraints but the objective is an arbitrary Lipschitz concave
function of the sum of outcome vectors.



The empirically successful Thompson Sampling algorithm for stochastic bandits
has drawn much interest in understanding its theoretical properties. One
important benefit of the algorithm is that it allows domain knowledge to be
conveniently encoded as a prior distribution to balance exploration and
exploitation more effectively. While it is generally believed that the
algorithm's regret is low (high) when the prior is good (bad), little is known
about the exact dependence. In this paper, we fully characterize the
algorithm's worst-case dependence of regret on the choice of prior, focusing on
a special yet representative case. These results also provide insights into the
general sensitivity of the algorithm to the choice of priors. In particular,
with $p$ being the prior probability mass of the true reward-generating model,
we prove $O(\sqrt{T/p})$ and $O(\sqrt{(1-p)T})$ regret upper bounds for the
bad- and good-prior cases, respectively, as well as \emph{matching} lower
bounds. Our proofs rely on the discovery of a fundamental property of Thompson
Sampling and make heavy use of martingale theory, both of which appear novel in
the literature, to the best of our knowledge.



Declarative spatial reasoning denotes the ability to (declaratively) specify
and solve real-world problems related to geometric and qualitative spatial
representation and reasoning within standard knowledge representation and
reasoning (KR) based methods (e.g., logic programming and derivatives). One
approach for encoding the semantics of spatial relations within a declarative
programming framework is by systems of polynomial constraints. However, solving
such constraints is computationally intractable in general (i.e. the theory of
real-closed fields).
  We present a new algorithm, implemented within the declarative spatial
reasoning system CLP(QS), that drastically improves the performance of deciding
the consistency of spatial constraint graphs over conventional polynomial
encodings. We develop pruning strategies founded on spatial symmetries that
form equivalence classes (based on affine transformations) at the qualitative
spatial level. Moreover, pruning strategies are themselves formalised as
knowledge about the properties of space and spatial symmetries. We evaluate our
algorithm using a range of benchmarks in the class of contact problems, and
proofs in mereology and geometry. The empirical results show that CLP(QS) with
knowledge-based spatial pruning outperforms conventional polynomial encodings
by orders of magnitude, and can thus be applied to problems that are otherwise
unsolvable in practice.



This paper proposes a new approach to model the temporal dynamics of a
sequence of facial expressions. To this purpose, a sequence of Face Image
Descriptors (FID) is regarded as the output of a Linear Time Invariant (LTI)
system. The temporal dynamics of such sequence of descriptors are represented
by means of a Hankel matrix. The paper presents different strategies to compute
dynamics-based representation of a sequence of FID, and reports classification
accuracy values of the proposed representations within different standard
classification frameworks. The representations have been validated in two very
challenging application domains: emotion recognition and pain detection.
Experiments on two publicly available benchmarks and comparison with
state-of-the-art approaches demonstrate that the dynamics-based FID
representation attains competitive performance when off-the-shelf
classification tools are adopted.



In this paper, a method is proposed to detect the emotion of a song based on
its lyrical and audio features. Lyrical features are generated by segmentation
of lyrics during the process of data extraction. ANEW and WordNet knowledge is
then incorporated to compute Valence and Arousal values. In addition to this,
linguistic association rules are applied to ensure that the issue of ambiguity
is properly addressed. Audio features are used to supplement the lyrical ones
and include attributes like energy, tempo, and danceability. These features are
extracted from The Echo Nest, a widely used music intelligence platform.
Construction of training and test sets is done on the basis of social tags
extracted from the last.fm website. The classification is done by applying
feature weighting and stepwise threshold reduction on the k-Nearest Neighbors
algorithm to provide fuzziness in the classification.



The social network analysis (SNA), branch of complex systems can be used in
the construction of multiagent systems. This paper proposes a study of how
social network analysis can assist in modeling multiagent systems, while
addressing similarities and differences between the two theories. We built a
prototype of multi-agent systems for resolution of tasks through the formation
of teams of agents that are formed on the basis of the social network
established between agents. Agents make use of performance indicators to assess
when should change their social network to maximize the participation in teams



We present a novel hybrid algorithm for Bayesian network structure learning,
called H2PC. It first reconstructs the skeleton of a Bayesian network and then
performs a Bayesian-scoring greedy hill-climbing search to orient the edges.
The algorithm is based on divide-and-conquer constraint-based subroutines to
learn the local structure around a target variable. We conduct two series of
experimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is
currently the most powerful state-of-the-art algorithm for Bayesian network
structure learning. First, we use eight well-known Bayesian network benchmarks
with various data sizes to assess the quality of the learned structure returned
by the algorithms. Our extensive experiments show that H2PC outperforms MMHC in
terms of goodness of fit to new data and quality of the network structure with
respect to the true dependence structure of the data. Second, we investigate
H2PC's ability to solve the multi-label learning problem. We provide
theoretical results to characterize and identify graphically the so-called
minimal label powersets that appear as irreducible factors in the joint
distribution under the faithfulness condition. The multi-label learning problem
is then decomposed into a series of multi-class classification problems, where
each multi-class variable encodes a label powerset. H2PC is shown to compare
favorably to MMHC in terms of global classification accuracy over ten
multi-label data sets covering different application domains. Overall, our
experiments support the conclusions that local structural learning with H2PC in
the form of local neighborhood induction is a theoretically well-motivated and
empirically effective learning framework that is well suited to multi-label
learning. The source code (in R) of H2PC as well as all data sets used for the
empirical tests are publicly available.



Knowledge tracing---where a machine models the knowledge of a student as they
interact with coursework---is a well established problem in computer supported
education. Though effectively modeling student knowledge would have high
educational impact, the task has many inherent challenges. In this paper we
explore the utility of using Recurrent Neural Networks (RNNs) to model student
learning. The RNN family of models have important advantages over previous
methods in that they do not require the explicit encoding of human domain
knowledge, and can capture more complex representations of student knowledge.
Using neural networks results in substantial improvements in prediction
performance on a range of knowledge tracing datasets. Moreover the learned
model can be used for intelligent curriculum design and allows straightforward
interpretation and discovery of structure in student tasks. These results
suggest a promising new line of research for knowledge tracing and an exemplary
application task for RNNs.



We propose an original particle-based implementation of the Loopy Belief
Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a
continuous state space. The algorithm constructs adaptively efficient proposal
distributions approximating the local beliefs at each note of the MRF. This is
achieved by considering proposal distributions in the exponential family whose
parameters are updated iterately in an Expectation Propagation (EP) framework.
The proposed particle scheme provides consistent estimation of the LBP
marginals as the number of particles increases. We demonstrate that it provides
more accurate results than the Particle Belief Propagation (PBP) algorithm of
Ihler and McAllester (2009) at a fraction of the computational cost and is
additionally more robust empirically. The computational complexity of our
algorithm at each iteration is quadratic in the number of particles. We also
propose an accelerated implementation with sub-quadratic computational
complexity which still provides consistent estimates of the loopy BP marginal
distributions and performs almost as well as the original procedure.



Stock price forecasting is an important issue for investors since extreme
accuracy in forecasting can bring about high profits. Fuzzy Time Series (FTS)
and Longest Common/Repeated Sub-sequence (LCS/LRS) are two important issues for
forecasting prices. However, to the best of our knowledge, there are no
significant studies using LCS/LRS to predict stock prices. It is impossible
that prices stay exactly the same as historic prices. Therefore, this paper
proposes a state-of-the-art method which combines FTS and LCS/LRS to predict
stock prices. This method is based on the principle that history will repeat
itself. It uses different interval lengths in FTS to fuzzify the prices, and
LCS/LRS to look for the same pattern in the historical prices to predict future
stock prices. In the experiment, we examine various intervals of fuzzy time
sets in order to achieve high prediction accuracy. The proposed method
outperforms traditional methods in terms of prediction accuracy and,
furthermore, it is easy to implement.



Information Extraction (IE) aims to automatically generate a large knowledge
base from natural language text, but progress remains slow. Supervised learning
requires copious human annotation, while unsupervised and weakly supervised
approaches do not deliver competitive accuracy. As a result, most fielded
applications of IE, as well as the leading TAC-KBP systems, rely on significant
amounts of manual engineering. Even "Extreme" methods, such as those reported
in Freedman et al. 2011, require about 10 hours of expert labor per relation.
  This paper shows how to reduce that effort by an order of magnitude. We
present a novel system, InstaRead, that streamlines authoring with an ensemble
of methods: 1) encoding extraction rules in an expressive and compositional
representation, 2) guiding the user to promising rules based on corpus
statistics and mined resources, and 3) introducing a new interactive
development cycle that provides immediate feedback --- even on large datasets.
Experiments show that experts can create quality extractors in under an hour
and even NLP novices can author good extractors. These extractors equal or
outperform ones obtained by comparably supervised and state-of-the-art
distantly supervised approaches.



Human infants can discover words directly from unsegmented speech signals
without any explicitly labeled data. In this paper, we develop a novel machine
learning method called nonparametric Bayesian double articulation analyzer
(NPB-DAA) that can directly acquire language and acoustic models from observed
continuous speech signals. For this purpose, we propose an integrative
generative model that combines a language model and an acoustic model into a
single generative model called the "hierarchical Dirichlet process hidden
language model" (HDP-HLM). The HDP-HLM is obtained by extending the
hierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by
Johnson et al. An inference procedure for the HDP-HLM is derived using the
blocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure
enables the simultaneous and direct inference of language and acoustic models
from continuous speech signals. Based on the HDP-HLM and its inference
procedure, we developed a novel double articulation analyzer. By assuming
HDP-HLM as a generative model of observed time series data, and by inferring
latent variables of the model, the method can analyze latent double
articulation structure, i.e., hierarchically organized latent words and
phonemes, of the data in an unsupervised manner. The novel unsupervised double
articulation analyzer is called NPB-DAA.
  The NPB-DAA can automatically estimate double articulation structure embedded
in speech signals. We also carried out two evaluation experiments using
synthetic data and actual human continuous speech signals representing Japanese
vowel sequences. In the word acquisition and phoneme categorization tasks, the
NPB-DAA outperformed a conventional double articulation analyzer (DAA) and
baseline automatic speech recognition system whose acoustic model was trained
in a supervised manner.



We present a novel response generation system that can be trained end to end
on large quantities of unstructured Twitter conversations. A neural network
architecture is used to address sparsity issues that arise when integrating
contextual information into classic statistical models, allowing the system to
take into account previous dialog utterances. Our dynamic-context generative
models show consistent gains over both context-sensitive and
non-context-sensitive Machine Translation and Information Retrieval baselines.



Integrating vision and language has long been a dream in work on artificial
intelligence (AI). In the past two years, we have witnessed an explosion of
work that brings together vision and language from images to videos and beyond.
The available corpora have played a crucial role in advancing this area of
research. In this paper, we propose a set of quality metrics for evaluating and
analyzing the vision & language datasets and categorize them accordingly. Our
analyses show that the most recent datasets have been using more complex
language and more abstract concepts, however, there are different strengths and
weaknesses in each.



Financial news contains useful information on public companies and the
market. In this paper we apply the popular word embedding methods and deep
neural networks to leverage financial news to predict stock price movements in
the market. Experimental results have shown that our proposed methods are
simple but very effective, which can significantly improve the stock prediction
accuracy on a standard financial database over the baseline system using only
the historical price information.



This paper presents a framework for exact discovery of the top-k sequential
patterns under Leverage. It combines (1) a novel definition of the expected
support for a sequential pattern - a concept on which most interestingness
measures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for
the exact discovery of top-k sequential patterns under a given measure of
interest. Our interestingness measure employs the partition approach. A pattern
is interesting to the extent that it is more frequent than can be explained by
assuming independence between any of the pairs of patterns from which it can be
composed. The larger the support compared to the expectation under
independence, the more interesting is the pattern. We build on these two
elements to exactly extract the k sequential patterns with highest leverage,
consistent with our definition of expected support. We conduct experiments on
both synthetic data with known patterns and real-world datasets; both
experiments confirm the consistency and relevance of our approach with regard
to the state of the art. This article was published in Data Mining and
Knowledge Discovery and is accessible at
http://dx.doi.org/10.1007/s10618-016-0467-9.



The New Yorker publishes a weekly captionless cartoon. More than 5,000
readers submit captions for it. The editors select three of them and ask the
readers to pick the funniest one. We describe an experiment that compares a
dozen automatic methods for selecting the funniest caption. We show that
negative sentiment, human-centeredness, and lexical centrality most strongly
match the funniest captions, followed by positive sentiment. These results are
useful for understanding humor and also in the design of more engaging
conversational agents in text and multimodal (vision+text) systems. As part of
this work, a large set of cartoons and captions is being made available to the
community.



This paper studies convolutional neural networks (CNN) to learn unsupervised
feature representations for 44 different plant species, collected at the Royal
Botanic Gardens, Kew, England. To gain intuition on the chosen features from
the CNN model (opposed to a 'black box' solution), a visualisation technique
based on the deconvolutional networks (DN) is utilized. It is found that
venations of different order have been chosen to uniquely represent each of the
plant species. Experimental results using these CNN features with different
classifiers show consistency and superiority compared to the state-of-the art
solutions which rely on hand-crafted features.



Probabilistic graphical models offer a powerful framework to account for the
dependence structure between variables, which can be represented as a graph.
The dependence between variables may render inference tasks such as computing
normalizing constant, marginalization or optimization intractable. The
objective of this paper is to review techniques exploiting the graph structure
for exact inference borrowed from optimization and computer science. They are
not yet standard in the statistician toolkit, and we specify under which
conditions they are efficient in practice. They are built on the principle of
variable elimination whose complexity is dictated in an intricate way by the
order in which variables are eliminated in the graph. The so-called treewidth
of the graph characterizes this algorithmic complexity: low-treewidth graphs
can be processed efficiently. Algorithmic solutions derived from variable
elimination and the notion of treewidth are illustrated on problems of
treewidth computation and inference in challenging benchmarks from optimization
competitions. We also review how efficient techniques for approximate inference
such as loopy belief propagation and variational approaches can be linked to
variable elimination and we illustrate them in the context of
Expectation-Maximisation procedures for parameter estimation in coupled Hidden
Markov Models.



Design mining is the use of computational intelligence techniques to
iteratively search and model the attribute space of physical objects evaluated
directly through rapid prototyping to meet given objectives. It enables the
exploitation of novel materials and processes without formal models or complex
simulation. In this paper, we focus upon the coevolutionary nature of the
design process when it is decomposed into concurrent sub-design threads due to
the overall complexity of the task. Using an abstract, tuneable model of
coevolution we consider strategies to sample sub-thread designs for whole
system testing and how best to construct and use surrogate models within the
coevolutionary scenario. Drawing on our findings, the paper then describes the
effective design of an array of six heterogeneous vertical-axis wind turbines.



This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost
1 million multi-turn dialogues, with a total of over 7 million utterances and
100 million words. This provides a unique resource for research into building
dialogue managers based on neural language models that can make use of large
amounts of unlabeled data. The dataset has both the multi-turn property of
conversations in the Dialog State Tracking Challenge datasets, and the
unstructured nature of interactions from microblog services such as Twitter. We
also describe two neural learning architectures suitable for analyzing this
dataset, and provide benchmark performance on the task of selecting the best
next response.



Making recommendations in the presence of sparsity is known to present one of
the most challenging problems faced by collaborative filtering methods. In this
work we tackle this problem by exploiting the innately hierarchical structure
of the item space following an approach inspired by the theory of
Decomposability. We view the itemspace as a Nearly Decomposable system and we
define blocks of closely related elements and corresponding indirect proximity
components. We study the theoretical properties of the decomposition and we
derive sufficient conditions that guarantee full item space coverage even in
cold-start recommendation scenarios. A comprehensive set of experiments on the
MovieLens and the Yahoo!R2Music datasets, using several widely applied
performance metrics, support our model's theoretically predicted properties and
verify that NCDREC outperforms several state-of-the-art algorithms, in terms of
recommendation accuracy, diversity and sparseness insensitivity.



Cross-validation (CV) is one of the main tools for performance estimation and
parameter tuning in machine learning. The general recipe for computing CV
estimate is to run a learning algorithm separately for each CV fold, a
computationally expensive process. In this paper, we propose a new approach to
reduce the computational burden of CV-based performance estimation. As opposed
to all previous attempts, which are specific to a particular learning model or
problem domain, we propose a general method applicable to a large class of
incremental learning algorithms, which are uniquely fitted to big data
problems. In particular, our method applies to a wide range of supervised and
unsupervised learning tasks with different performance criteria, as long as the
base learning algorithm is incremental. We show that the running time of the
algorithm scales logarithmically, rather than linearly, in the number of CV
folds. Furthermore, the algorithm has favorable properties for parallel and
distributed implementation. Experiments with state-of-the-art incremental
learning algorithms confirm the practicality of the proposed method.



In this work we establish and investigate connections between causes for
query answers in databases, database repairs wrt. denial constraints, and
consistency-based diagnosis. The first two are relatively new research areas in
databases, and the third one is an established subject in knowledge
representation. We show how to obtain database repairs from causes, and the
other way around. Causality problems are formulated as diagnosis problems, and
the diagnoses provide causes and their responsibilities. The vast body of
research on database repairs can be applied to the newer problems of computing
actual causes for query answers and their responsibilities. These connections,
which are interesting per se, allow us, after a transition -inspired by
consistency-based diagnosis- to computational problems on hitting sets and
vertex covers in hypergraphs, to obtain several new algorithmic and complexity
results for database causality.



The true online TD({\lambda}) algorithm has recently been proposed (van
Seijen and Sutton, 2014) as a universal replacement for the popular
TD({\lambda}) algorithm, in temporal-difference learning and reinforcement
learning. True online TD({\lambda}) has better theoretical properties than
conventional TD({\lambda}), and the expectation is that it also results in
faster learning. In this paper, we put this hypothesis to the test.
Specifically, we compare the performance of true online TD({\lambda}) with that
of TD({\lambda}) on challenging examples, random Markov reward processes, and a
real-world myoelectric prosthetic arm. We use linear function approximation
with tabular, binary, and non-binary features. We assess the algorithms along
three dimensions: computational cost, learning speed, and ease of use. Our
results confirm the strength of true online TD({\lambda}): 1) for sparse
feature vectors, the computational overhead with respect to TD({\lambda}) is
minimal; for non-sparse features the computation time is at most twice that of
TD({\lambda}), 2) across all domains/representations the learning speed of true
online TD({\lambda}) is often better, but never worse than that of
TD({\lambda}), and 3) true online TD({\lambda}) is easier to use, because it
does not require choosing between trace types, and it is generally more stable
with respect to the step-size. Overall, our results suggest that true online
TD({\lambda}) should be the first choice when looking for an efficient,
general-purpose TD method.



We show that natural classes of regularized learning algorithms with a form
of recency bias achieve faster convergence rates to approximate efficiency and
to coarse correlated equilibria in multiplayer normal form games. When each
player in a game uses an algorithm from our class, their individual regret
decays at $O(T^{-3/4})$, while the sum of utilities converges to an approximate
optimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates.
We show a black-box reduction for any algorithm in the class to achieve
$\tilde{O}(T^{-1/2})$ rates against an adversary, while maintaining the faster
rates against algorithms in the class. Our results extend those of [Rakhlin and
Shridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-player
zero-sum games for specific algorithms.



Achieving efficient and scalable exploration in complex domains poses a major
challenge in reinforcement learning. While Bayesian and PAC-MDP approaches to
the exploration problem offer strong formal guarantees, they are often
impractical in higher dimensions due to their reliance on enumerating the
state-action space. Hence, exploration in complex domains is often performed
with simple epsilon-greedy methods. In this paper, we consider the challenging
Atari games domain, which requires processing raw pixel inputs and delayed
rewards. We evaluate several more sophisticated exploration strategies,
including Thompson sampling and Boltzman exploration, and propose a new
exploration method based on assigning exploration bonuses from a concurrently
learned model of the system dynamics. By parameterizing our learned model with
a neural network, we are able to develop a scalable and efficient approach to
exploration bonuses that can be applied to tasks with complex, high-dimensional
state spaces. In the Atari domain, our method provides the most consistent
improvement across a range of games that pose a major challenge for prior
methods. In addition to raw game-scores, we also develop an AUC-100 metric for
the Atari Learning domain to evaluate the impact of exploration on this
benchmark.



We introduce and demonstrate a new approach to inference in expressive
probabilistic programming languages based on particle Markov chain Monte Carlo.
Our approach is simple to implement and easy to parallelize. It applies to
Turing-complete probabilistic programming languages and supports accurate
inference in models that make use of complex control flow, including stochastic
recursion. It also includes primitives from Bayesian nonparametric statistics.
Our experiments show that this approach can be more efficient than previously
introduced single-site Metropolis-Hastings methods.



Recent work on language modelling has shifted focus from count-based models
to neural models. In these works, the words in each sentence are always
considered in a left-to-right order. In this paper we show how we can improve
the performance of the recurrent neural network (RNN) language model by
incorporating the syntactic dependencies of a sentence, which have the effect
of bringing relevant contexts closer to the word being predicted. We evaluate
our approach on the Microsoft Research Sentence Completion Challenge and show
that the dependency RNN proposed improves over the RNN by about 10 points in
accuracy. Furthermore, we achieve results comparable with the state-of-the-art
models on this task.



In this paper, we consider multi-sensor classification when there is a large
number of unlabeled samples. The problem is formulated under the multi-view
learning framework and a Consensus-based Multi-View Maximum Entropy
Discrimination (CMV-MED) algorithm is proposed. By iteratively maximizing the
stochastic agreement between multiple classifiers on the unlabeled dataset, the
algorithm simultaneously learns multiple high accuracy classifiers. We
demonstrate that our proposed method can yield improved performance over
previous multi-view learning approaches by comparing performance on three real
multi-sensor data sets.



As software systems are getting increasingly connected, there is a need for
equipping nonmonotonic logic programs with access to external sources that are
possibly remote and may contain information in heterogeneous formats. To cater
for this need, HEX programs were designed as a generalization of answer set
programs with an API style interface that allows to access arbitrary external
sources, providing great flexibility. Efficient evaluation of such programs
however is challenging, and it requires to interleave external computation and
model building; to decide when to switch between these tasks is difficult, and
existing approaches have limited scalability in many real-world application
scenarios. We present a new approach for the evaluation of logic programs with
external source access, which is based on a configurable framework for dividing
the non-ground program into possibly overlapping smaller parts called
evaluation units. The latter will be processed by interleaving external
evaluation and model building using an evaluation graph and a model graph,
respectively, and by combining intermediate results. Experiments with our
prototype implementation show a significant improvement compared to previous
approaches. While designed for HEX-programs, the new evaluation approach may be
deployed to related rule-based formalisms as well.



In this paper we propose a wet lab algorithm for prediction of radiation fog
by DNA computing. The concept of DNA computing is essentially exploited for
generating the classifier algorithm in the wet lab. The classifier is based on
a new concept of similarity based fuzzy reasoning suitable for wet lab
implementation. This new concept of similarity based fuzzy reasoning is
different from conventional approach to fuzzy reasoning based on similarity
measure and also replaces the logical aspect of classical fuzzy reasoning by
DNA chemistry. Thus, we add a new dimension to existing forms of fuzzy
reasoning by bringing it down to nanoscale. We exploit the concept of massive
parallelism of DNA computing by designing this new classifier in the wet lab.
This newly designed classifier is very much generalized in nature and apart
from prediction of radiation fog this methodology can be applied to other types
of data also. To achieve our goal we first fuzzify the given observed
parameters in a form of synthetic DNA sequence which is called fuzzy DNA and
which handles the vague concept of human reasoning.



In sentence modeling and classification, convolutional neural network
approaches have recently achieved state-of-the-art results, but all such
efforts process word vectors sequentially and neglect long-distance
dependencies. To exploit both deep learning and linguistic structures, we
propose a tree-based convolutional neural network model which exploit various
long-distance relationships between words. Our model improves the sequential
baselines on all three sentiment and question classification tasks, and
achieves the highest published accuracy on TREC.



It is now commonplace to observe that we are facing a deluge of online
information. Researchers have of course long acknowledged the potential value
of this information since digital traces make it possible to directly observe,
describe and analyze social facts, and above all the co-evolution of ideas and
communities over time. However, most online information is expressed through
text, which means it is not directly usable by machines, since computers
require structured, organized and typed information in order to be able to
manipulate it. Our goal is thus twofold: 1. Provide new natural language
processing techniques aiming at automatically extracting relevant information
from texts, especially in the context of social sciences, and connect these
pieces of information so as to obtain relevant socio-semantic networks; 2.
Provide new ways of exploring these socio-semantic networks, thanks to tools
allowing one to dynamically navigate these networks, de-construct and
re-construct them interactively, from different points of view following the
needs expressed by domain experts.



Research units in archaeology often manage large and precious archives
containing various documents, including reports on fieldwork, scholarly studies
and reference books. These archives are of course invaluable, recording decades
of work, but are generally hard to consult and access. In this context,
digitizing full text documents is not enough: information must be formalized,
structured and easy to access thanks to friendly user interfaces.



In this paper, we propose a different insight to analyze AdaBoost. This
analysis reveals that, beyond some preconceptions, AdaBoost can be directly
used as an asymmetric learning algorithm, preserving all its theoretical
properties. A novel class-conditional description of AdaBoost, which models the
actual asymmetric behavior of the algorithm, is presented.



A continuous-time Markov process is proposed to analyze how a group of humans
solves a complex task, consisting in the search of the optimal set of decisions
on a fitness landscape. Individuals change their opinions driven by two
different forces: (i) the self-interest, which pushes them to increase their
own fitness values, and (ii) the social interactions, which push individuals to
reduce the diversity of their opinions in order to reach consensus. Results
show that the performance of the group is strongly affected by the strength of
social interactions and by the level of knowledge of the individuals.
Increasing the strength of social interactions improves the performance of the
team. However, too strong social interactions slow down the search of the
optimal solution and worsen the performance of the group. In particular, we
find that the threshold value of the social interaction strength, which leads
to the emergence of a superior intelligence of the group, is just the critical
threshold at which the consensus among the members sets in. We also prove that
a moderate level of knowledge is already enough to guarantee high performance
of the group in making decisions.



Based on the use of different exponential bases to define class-dependent
error bounds, a new and highly efficient asymmetric boosting scheme, coined as
AdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical
derivation procedure, unlike most of the other approaches in the literature,
our algorithm preserves all the formal guarantees and properties of original
(cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive
AdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel
derivation scheme enables an extremely efficient conditional search procedure,
dramatically improving and simplifying the training phase of the algorithm.
Experiments, both over synthetic and real datasets, reveal that AdaBoostDB is
able to save over 99% training time with regard to Cost-Sensitive AdaBoost,
providing the same cost-sensitive results. This computational advantage of
AdaBoostDB can make a difference in problems managing huge pools of weak
classifiers in which boosting techniques are commonly used.



The current study examines how adequate coordination among different
cognitive processes including visual recognition, attention switching, action
preparation and generation can be developed via learning of robots by
introducing a novel model, the Visuo-Motor Deep Dynamic Neural Network (VMDNN).
The proposed model is built on coupling of a dynamic vision network, a motor
generation network, and a higher level network allocated on top of these two.
The simulation experiments using the iCub simulator were conducted for
cognitive tasks including visual object manipulation responding to human
gestures. The results showed that synergetic coordination can be developed via
iterative learning through the whole network when spatio-temporal hierarchy and
temporal one can be self-organized in the visual pathway and in the motor
pathway, respectively, such that the higher level can manipulate them with
abstraction.



Microarray is one of the essential technologies used by the biologist to
measure genome-wide expression levels of genes in a particular organism under
some particular conditions or stimuli. As microarrays technologies have become
more prevalent, the challenges of analyzing these data for getting better
insight about biological processes have essentially increased. Due to
availability of artificial intelligence based sophisticated computational
techniques, such as artificial neural networks, fuzzy logic, genetic
algorithms, and many other nature-inspired algorithms, it is possible to
analyse microarray gene expression data in more better way. Here, we reviewed
artificial intelligence based techniques for the analysis of microarray gene
expression data. Further, challenges in the field and future work direction
have also been suggested.



A wide variety of problems in machine learning, including exemplar
clustering, document summarization, and sensor placement, can be cast as
constrained submodular maximization problems. A lot of recent effort has been
devoted to developing distributed algorithms for these problems. However, these
results suffer from high number of rounds, suboptimal approximation ratios, or
both. We develop a framework for bringing existing algorithms in the sequential
setting to the distributed setting, achieving near optimal approximation ratios
for many settings in only a constant number of MapReduce rounds. Our techniques
also give a fast sequential algorithm for non-monotone maximization subject to
a matroid constraint.



Nicod's criterion states that observing a black raven is evidence for the
hypothesis H that all ravens are black. We show that Solomonoff induction does
not satisfy Nicod's criterion: there are time steps in which observing black
ravens decreases the belief in H. Moreover, while observing any computable
infinite string compatible with H, the belief in H decreases infinitely often
when using the unnormalized Solomonoff prior, but only finitely often when
using the normalized Solomonoff prior. We argue that the fault is not with
Solomonoff induction; instead we should reject Nicod's criterion.



Boosting algorithms have been widely used to tackle a plethora of problems.
In the last few years, a lot of approaches have been proposed to provide
standard AdaBoost with cost-sensitive capabilities, each with a different
focus. However, for the researcher, these algorithms shape a tangled set with
diffuse differences and properties, lacking a unifying analysis to jointly
compare, classify, evaluate and discuss those approaches on a common basis. In
this series of two papers we aim to revisit the various proposals, both from
theoretical (Part I) and practical (Part II) perspectives, in order to analyze
their specific properties and behavior, with the final goal of identifying the
algorithm providing the best and soundest results.



A lot of approaches, each following a different strategy, have been proposed
in the literature to provide AdaBoost with cost-sensitive properties. In the
first part of this series of two papers, we have presented these algorithms in
a homogeneous notational framework, proposed a clustering scheme for them and
performed a thorough theoretical analysis of those approaches with a fully
theoretical foundation. The present paper, in order to complete our analysis,
is focused on the empirical study of all the algorithms previously presented
over a wide range of heterogeneous classification problems. The results of our
experiments, confirming the theoretical conclusions, seem to reveal that the
simplest approach, just based on cost-sensitive weight initialization, is the
one showing the best and soundest results, despite having been recurrently
overlooked in the literature.



In dynamic epistemic logic, actions are described using action models. In
this paper we introduce a framework for studying learnability of action models
from observations. We present first results concerning propositional action
models. First we check two basic learnability criteria: finite identifiability
(conclusively inferring the appropriate action model in finite time) and
identifiability in the limit (inconclusive convergence to the right action
model). We show that deterministic actions are finitely identifiable, while
non-deterministic actions require more learning power-they are identifiable in
the limit. We then move on to a particular learning method, which proceeds via
restriction of a space of events within a learning-specific action model. This
way of learning closely resembles the well-known update method from dynamic
epistemic logic. We introduce several different learning methods suited for
finite identifiability of particular types of deterministic actions.



We present the first massively distributed architecture for deep
reinforcement learning. This architecture uses four main components: parallel
actors that generate new behaviour; parallel learners that are trained from
stored experience; a distributed neural network to represent the value function
or behaviour policy; and a distributed store of experience. We used our
architecture to implement the Deep Q-Network algorithm (DQN). Our distributed
algorithm was applied to 49 games from Atari 2600 games from the Arcade
Learning Environment, using identical hyperparameters. Our performance
surpassed non-distributed DQN in 41 of the 49 games and also reduced the
wall-time required to achieve these results by an order of magnitude on most
games.



We investigate the task of building open domain, conversational dialogue
systems based on large dialogue corpora using generative models. Generative
models produce system responses that are autonomously generated word-by-word,
opening up the possibility for realistic, flexible interactions. In support of
this goal, we extend the recently proposed hierarchical recurrent
encoder-decoder neural network to the dialogue domain, and demonstrate that
this model is competitive with state-of-the-art neural language models and
back-off n-gram models. We investigate the limitations of this and similar
approaches, and show how its performance can be improved by bootstrapping the
learning from a larger question-answer pair corpus and from pretrained word
embeddings.



The visualization of an image collection is the process of displaying a
collection of images on a screen under some specific layout requirements. This
paper focuses on an important problem that is not well addressed by the
previous methods: visualizing image collections into arbitrary layout shapes
while arranging images according to user-defined semantic or visual
correlations (e.g., color or object category). To this end, we first propose a
property-based tree construction scheme to organize images of a collection into
a tree structure according to user-defined properties. In this way, images can
be adaptively placed with the desired semantic or visual correlations in the
final visualization layout. Then, we design a two-step visualization
optimization scheme to further optimize image layouts. As a result, multiple
layout effects including layout shape and image overlap ratio can be
effectively controlled to guarantee a satisfactory visualization. Finally, we
also propose a tree-transfer scheme such that visualization layouts can be
adaptively changed when users select different "images of interest". We
demonstrate the effectiveness of our proposed approach through the comparisons
with state-of-the-art visualization techniques.



In regression problems, the use of TSK fuzzy systems is widely extended due
to the precision of the obtained models. Moreover, the use of simple linear TSK
models is a good choice in many real problems due to the easy understanding of
the relationship between the output and input variables. In this paper we
present FRULER, a new genetic fuzzy system for automatically learning accurate
and simple linguistic TSK fuzzy rule bases for regression problems. In order to
reduce the complexity of the learned models while keeping a high accuracy, the
algorithm consists of three stages: instance selection, multi-granularity fuzzy
discretization of the input variables, and the evolutionary learning of the
rule base that uses the Elastic Net regularization to obtain the consequents of
the rules. Each stage was validated using 28 real-world datasets and FRULER was
compared with three state of the art enetic fuzzy systems. Experimental results
show that FRULER achieves the most accurate and simple models compared even
with approximative approaches.



Disjunctive Answer Set Programming is a powerful declarative programming
paradigm with complexity beyond NP. Identifying classes of programs for which
the consistency problem is in NP is of interest from the theoretical standpoint
and can potentially lead to improvements in the design of answer set
programming solvers. One of such classes consists of dual-normal programs,
where the number of positive body atoms in proper rules is at most one. Unlike
other classes of programs, dual-normal programs have received little attention
so far. In this paper we study this class. We relate dual-normal programs to
propositional theories and to normal programs by presenting several
inter-translations. With the translation from dual-normal to normal programs at
hand, we introduce the novel class of body-cycle free programs, which are in
many respects dual to head-cycle free programs. We establish the expressive
power of dual-normal programs in terms of SE- and UE-models, and compare them
to normal programs. We also discuss the complexity of deciding whether
dual-normal programs are strongly and uniformly equivalent.



We propose a new algorithm for recommender systems with numeric ratings which
is based on Pattern Structures (RAPS). As the input the algorithm takes rating
matrix, e.g., such that it contains movies rated by users. For a target user,
the algorithm returns a rated list of items (movies) based on its previous
ratings and ratings of other users. We compare the results of the proposed
algorithm in terms of precision and recall measures with Slope One, one of the
state-of-the-art item-based algorithms, on Movie Lens dataset and RAPS
demonstrates the best or comparable quality.



Conflict of interest is the permanent companion of any population of agents
(computational or biological). For that reason, the ability to compromise is of
paramount importance, making voting a key element of societal mechanisms. One
of the voting procedures most often discussed in the literature and, due to its
intuitiveness, also conceptually quite appealing is Charles Dodgson's scoring
rule, basically using the respective closeness to being a Condorcet winner for
evaluating competing alternatives. In this paper, we offer insights on the
practical limits of algorithms computing the exact Dodgson scores from a number
of votes. While the problem itself is theoretically intractable, this work
proposes and analyses five different solutions which try distinct approaches to
practically solve the issue in an effective manner. Additionally, three of the
discussed procedures can be run in parallel which has the potential of
drastically reducing the problem size.



A major challenge in crowdsourcing evaluation tasks like labeling objects,
grading assignments in online courses, etc., is that of eliciting truthful
responses from agents in the absence of verifiability. In this paper, we
propose new reward mechanisms for such settings that, unlike many previously
studied mechanisms, impose minimal assumptions on the structure and knowledge
of the underlying generating model, can account for heterogeneity in the
agents' abilities, require no extraneous elicitation from them, and furthermore
allow their beliefs to be (almost) arbitrary. These mechanisms have the simple
and intuitive structure of an output agreement mechanism: an agent gets a
reward if her evaluation matches that of her peer, but unlike the classic
output agreement mechanism, this reward is not the same across evaluations, but
is inversely proportional to an appropriately defined popularity index of each
evaluation. The popularity indices are computed by leveraging the existence of
a large number of similar tasks, which is a typical characteristic of these
settings. Experiments performed on MTurk workers demonstrate higher efficacy
(with a $p$-value of $0.02$) of these mechanisms in inducing truthful behavior
compared to the state of the art.



The problem of autonomous navigation is one of the basic problems for
robotics. Although, in general, it may be challenging when an autonomous
vehicle is placed into partially observable domain. In this paper we consider
simplistic environment model and introduce a navigation algorithm based on
Learning Classifier System.



Paragraph Vectors has been recently proposed as an unsupervised method for
learning distributed representations for pieces of texts. In their work, the
authors showed that the method can learn an embedding of movie review texts
which can be leveraged for sentiment analysis. That proof of concept, while
encouraging, was rather narrow. Here we consider tasks other than sentiment
analysis, provide a more thorough comparison of Paragraph Vectors to other
document modelling algorithms such as Latent Dirichlet Allocation, and evaluate
performance of the method as we vary the dimensionality of the learned
representation. We benchmarked the models on two document similarity data sets,
one from Wikipedia, one from arXiv. We observe that the Paragraph Vector method
performs significantly better than other methods, and propose a simple
improvement to enhance embedding quality. Somewhat surprisingly, we also show
that much like word embeddings, vector operations on Paragraph Vectors can
perform useful semantic results.



Approximate Newton methods are a standard optimization tool which aim to
maintain the benefits of Newton's method, such as a fast rate of convergence,
whilst alleviating its drawbacks, such as computationally expensive calculation
or estimation of the inverse Hessian. In this work we investigate approximate
Newton methods for policy optimization in Markov Decision Processes (MDPs). We
first analyse the structure of the Hessian of the objective function for MDPs.
We show that, like the gradient, the Hessian exhibits useful structure in the
context of MDPs and we use this analysis to motivate two Gauss-Newton Methods
for MDPs. Like the Gauss-Newton method for non-linear least squares, these
methods involve approximating the Hessian by ignoring certain terms in the
Hessian which are difficult to estimate. The approximate Hessians possess
desirable properties, such as negative definiteness, and we demonstrate several
important performance guarantees including guaranteed ascent directions,
invariance to affine transformation of the parameter space, and convergence
guarantees. We finally provide a unifying perspective of key policy search
algorithms, demonstrating that our second Gauss-Newton algorithm is closely
related to both the EM-algorithm and natural gradient ascent applied to MDPs,
but performs significantly better in practice on a range of challenging
domains.



In this paper we provide a broad framework for describing learning agents in
general quantum environments. We analyze the types of classically specified
environments which allow for quantum enhancements in learning, by contrasting
environments to quantum oracles. We show that whether or not quantum
improvements are at all possible depends on the internal structure of the
quantum environment. If the environments are constructed and the internal
structure is appropriately chosen, or if the agent has limited capacities to
influence the internal states of the environment, we show that improvements in
learning times are possible in a broad range of scenarios. Such scenarios we
call luck-favoring settings. The case of constructed environments is
particularly relevant for the class of model-based learning agents, where our
results imply a near-generic improvement.



Motivated by vision-based reinforcement learning (RL) problems, in particular
Atari games from the recent benchmark Aracade Learning Environment (ALE), we
consider spatio-temporal prediction problems where future (image-)frames are
dependent on control variables or actions as well as previous frames. While not
composed of natural scenes, frames in Atari games are high-dimensional in size,
can involve tens of objects with one or more objects being controlled by the
actions directly and many other objects being influenced indirectly, can
involve entry and departure of objects, and can involve deep partial
observability. We propose and evaluate two deep neural network architectures
that consist of encoding, action-conditional transformation, and decoding
layers based on convolutional neural networks and recurrent neural networks.
Experimental results show that the proposed architectures are able to generate
visually-realistic frames that are also useful for control over approximately
100-step action-conditional futures in some games. To the best of our
knowledge, this paper is the first to make and evaluate long-term predictions
on high-dimensional video conditioned by control inputs.



Robots assisting humans in complex domains have to represent knowledge and
reason at both the sensorimotor level and the social level. The architecture
described in this paper couples the non-monotonic logical reasoning
capabilities of a declarative language with probabilistic belief revision,
enabling robots to represent and reason with qualitative and quantitative
descriptions of knowledge and degrees of belief. Specifically, incomplete
domain knowledge, including information that holds in all but a few exceptional
situations, is represented as a Answer Set Prolog (ASP) program. The answer set
obtained by solving this program is used for inference, planning, and for
jointly explaining (a) unexpected action outcomes due to exogenous actions and
(b) partial scene descriptions extracted from sensor input. For any given task,
each action in the plan contained in the answer set is executed
probabilistically. The subset of the domain relevant to the action is
identified automatically, and observations extracted from sensor inputs perform
incremental Bayesian updates to a belief distribution defined over this domain
subset, with highly probable beliefs being committed to the ASP program. The
architecture's capabilities are illustrated in simulation and on a mobile robot
in the context of a robot waiter operating in the dining room of a restaurant.



Real world problems always have different multiple solutions. For instance,
optical engineers need to tune the recording parameters to get as many optimal
solutions as possible for multiple trials in the varied-line-spacing
holographic grating design problem. Unfortunately, most traditional
optimization techniques focus on solving for a single optimal solution. They
need to be applied several times; yet all solutions are not guaranteed to be
found. Thus the multimodal optimization problem was proposed. In that problem,
we are interested in not only a single optimal point, but also the others. With
strong parallel search capability, evolutionary algorithms are shown to be
particularly effective in solving this type of problem. In particular, the
evolutionary algorithms for multimodal optimization usually not only locate
multiple optima in a single run, but also preserve their population diversity
throughout a run, resulting in their global optimization ability on multimodal
functions. In addition, the techniques for multimodal optimization are borrowed
as diversity maintenance techniques to other problems. In this chapter, we
describe and review the state-of-the-arts evolutionary algorithms for
multimodal optimization in terms of methodology, benchmarking, and application.



Purpose. Radiation therapy is a local treatment aimed at cells in and around
a tumor. The goal of this study is to develop an algorithmic solution for
predicting the position of a target in 3D in real time, aiming for the short
fixed calibration time for each patient at the beginning of the procedure.
Accurate predictions of lung tumor motion are expected to improve the precision
of radiation treatment by controlling the position of a couch or a beam in
order to compensate for respiratory motion during radiation treatment.
  Methods. For developing the algorithmic solution, data mining techniques are
used. A model form from the family of exponential smoothing is assumed, and the
model parameters are fitted by minimizing the absolute disposition error, and
the fluctuations of the prediction signal (jitter). The predictive performance
is evaluated retrospectively on clinical datasets capturing different behavior
(being quiet, talking, laughing), and validated in real-time on a prototype
system with respiratory motion imitation.
  Results. An algorithmic solution for respiratory motion prediction (called
ExSmi) is designed. ExSmi achieves good accuracy of prediction (error $4-9$
mm/s) with acceptable jitter values (5-7 mm/s), as tested on out-of-sample
data. The datasets, the code for algorithms and the experiments are openly
available for research purposes on a dedicated website.
  Conclusions. The developed algorithmic solution performs well to be
prototyped and deployed in applications of radiotherapy.



Presently, a very large number of public and private data sets are available
around the local governments. In most cases, they are not semantically
interoperable and a huge human effort is needed to create integrated ontologies
and knowledge base for smart city. Smart City ontology is not yet standardized,
and a lot of research work is needed to identify models that can easily support
the data reconciliation, the management of the complexity and reasoning. In
this paper, a system for data ingestion and reconciliation smart cities related
aspects as road graph, services available on the roads, traffic sensors etc.,
is proposed. The system allows managing a big volume of data coming from a
variety of sources considering both static and dynamic data. These data are
mapped to smart-city ontology and stored into an RDF-Store where they are
available for applications via SPARQL queries to provide new services to the
users. The paper presents the process adopted to produce the ontology and the
knowledge base and the mechanisms adopted for the verification, reconciliation
and validation. Some examples about the possible usage of the coherent
knowledge base produced are also offered and are accessible from the RDF-Store.



Presently, a very large number of public and private data sets are available
from local governments. In most cases, they are not semantically interoperable
and a huge human effort would be needed to create integrated ontologies and
knowledge base for smart city. Smart City ontology is not yet standardized, and
a lot of research work is needed to identify models that can easily support the
data reconciliation, the management of the complexity, to allow the data
reasoning. In this paper, a system for data ingestion and reconciliation of
smart cities related aspects as road graph, services available on the roads,
traffic sensors etc., is proposed. The system allows managing a big data volume
of data coming from a variety of sources considering both static and dynamic
data. These data are mapped to a smart-city ontology, called KM4City (Knowledge
Model for City), and stored into an RDF-Store where they are available for
applications via SPARQL queries to provide new services to the users via
specific applications of public administration and enterprises. The paper
presents the process adopted to produce the ontology and the big data
architecture for the knowledge base feeding on the basis of open and private
data, and the mechanisms adopted for the data verification, reconciliation and
validation. Some examples about the possible usage of the coherent big data
knowledge base produced are also offered and are accessible from the RDF-Store
and related services. The article also presented the work performed about
reconciliation algorithms and their comparative assessment and selection.



This report describes an initial replication study of the PRECISE system and
develops a clearer, more formal description of the approach. Based on our
evaluation, we conclude that the PRECISE results do not fully replicate.
However the formalization developed here suggests a road map to further enhance
and extend the approach pioneered by PRECISE.
  After a long, productive discussion with Ana-Maria Popescu (one of the
authors of PRECISE) we got more clarity on the PRECISE approach and how the
lexicon was authored for the GEO evaluation. Based on this we built a more
direct implementation over a repaired formalism. Although our new evaluation is
not yet complete, it is clear that the system is performing much better now. We
will continue developing our ideas and implementation and generate a future
report/publication that more accurately evaluates PRECISE like approaches.



The logic-based machine-understandable framework of the Semantic Web often
challenges naive users when they try to query ontology-based knowledge bases.
Existing research efforts have approached this problem by introducing Natural
Language (NL) interfaces to ontologies. These NL interfaces have the ability to
construct SPARQL queries based on NL user queries. However, most efforts were
restricted to queries expressed in English, and they often benefited from the
advancement of English NLP tools. However, little research has been done to
support querying the Arabic content on the Semantic Web by using NL queries.
This paper presents a domain-independent approach to translate Arabic NL
queries to SPARQL by leveraging linguistic analysis. Based on a special
consideration on Noun Phrases (NPs), our approach uses a language parser to
extract NPs and the relations from Arabic parse trees and match them to the
underlying ontology. It then utilizes knowledge in the ontology to group NPs
into triple-based representations. A SPARQL query is finally generated by
extracting targets and modifiers, and interpreting them into SPARQL. The
interpretation of advanced semantic features including negation, conjunctive
and disjunctive modifiers is also supported. The approach was evaluated by
using two datasets consisting of OWL test data and queries, and the obtained
results have confirmed its feasibility to translate Arabic NL queries to
SPARQL.



Currently the Dempster-Shafer based algorithm and Uniform Random Probability
based algorithm are the preferred method of resolving security games, in which
defenders are able to identify attackers and only strategy remained ambiguous.
However this model is inefficient in situations where resources are limited and
both the identity of the attackers and their strategies are ambiguous. The
intent of this study is to find a more effective algorithm to guide the
defenders in choosing which outside agents with which to cooperate given both
ambiguities. We designed an experiment where defenders were compelled to engage
with outside agents in order to maximize protection of their targets. We
introduced two important notions: the behavior of each agent in target
protection and the tolerance threshold in the target protection process. From
these, we proposed an algorithm that was applied by each defender to determine
the best potential assistant(s) with which to cooperate. Our results showed
that our proposed algorithm is safer than the Dempster-Shafer based algorithm.



This paper focuses on finding spatial and temporal criminal hotspots. It
analyses two different real-world crimes datasets for Denver, CO and Los
Angeles, CA and provides a comparison between the two datasets through a
statistical analysis supported by several graphs. Then, it clarifies how we
conducted Apriori algorithm to produce interesting frequent patterns for
criminal hotspots. In addition, the paper shows how we used Decision Tree
classifier and Naive Bayesian classifier in order to predict potential crime
types. To further analyse crimes datasets, the paper introduces an analysis
study by combining our findings of Denver crimes dataset with its demographics
information in order to capture the factors that might affect the safety of
neighborhoods. The results of this solution could be used to raise awareness
regarding the dangerous locations and to help agencies to predict future crimes
in a specific location within a particular time.



Deep compositional models of meaning acting on distributional representations
of words in order to produce vectors of larger text constituents are evolving
to a popular area of NLP research. We detail a compositional distributional
framework based on a rich form of word embeddings that aims at facilitating the
interactions between words in the context of a sentence. Embeddings and
composition layers are jointly learned against a generic objective that
enhances the vectors with syntactic information from the surrounding context.
Furthermore, each word is associated with a number of senses, the most
plausible of which is selected dynamically during the composition process. We
evaluate the produced vectors qualitatively and quantitatively with positive
results. At the sentence level, the effectiveness of the framework is
demonstrated on the MSRPar task, for which we report results within the
state-of-the-art range.



This paper addresses the problem of finding multiple near-optimal,
spatially-dissimilar paths that can be considered as alternatives in the
decision making process, for finding optimal corridors in which to construct a
new road. We further consider combinations of techniques for reducing the costs
associated with the computation and increasing the accuracy of the cost
formulation. Numerical results for five algorithms to solve the dissimilar
multipath problem show that a "bidirectional approach" yields the fastest
running times and the most robust algorithm. Further modifications of the
algorithms to reduce the running time were tested and it is shown that running
time can be reduced by an average of 56 percent without compromising the
quality of the results.



We explore methods for content selection and address the issue of coherence
in the context of the generation of multimedia artifacts. We use audio and
video to present two case studies: generation of film tributes, and
lecture-driven science talks. For content selection, we use centrality-based
and diversity-based summarization, along with topic analysis. To establish
coherence, we use the emotional content of music, for film tributes, and ensure
topic similarity between lectures and documentaries, for science talks.
Composition techniques for the production of multimedia artifacts are addressed
as a means of organizing content, in order to improve coherence. We discuss our
results considering the above aspects.



We present a general theory and corresponding declarative model for the
embodied grounding and natural language based analytical summarisation of
dynamic visuo-spatial imagery. The declarative model ---ecompassing
spatio-linguistic abstractions, image schemas, and a spatio-temporal feature
based language generator--- is modularly implemented within Constraint Logic
Programming (CLP). The implemented model is such that primitives of the theory,
e.g., pertaining to space and motion, image schemata, are available as
first-class objects with `deep semantics' suited for inference and query. We
demonstrate the model with select examples broadly motivated by areas such as
film, design, geography, smart environments where analytical natural language
based externalisations of the moving image are central from the viewpoint of
human interaction, evidence-based qualitative analysis, and sensemaking.
  Keywords: moving image, visual semantics and embodiment, visuo-spatial
cognition and computation, cognitive vision, computational models of narrative,
declarative spatial reasoning



Learning novel concepts and relations from relational databases is an
important problem with many applications in database systems and machine
learning. Relational learning algorithms learn the definition of a new relation
in terms of existing relations in the database. Nevertheless, the same data set
may be represented under different schemas for various reasons, such as
efficiency, data quality, and usability. Unfortunately, the output of current
relational learning algorithms tends to vary quite substantially over the
choice of schema, both in terms of learning accuracy and efficiency. This
variation complicates their off-the-shelf application. In this paper, we
introduce and formalize the property of schema independence of relational
learning algorithms, and study both the theoretical and empirical dependence of
existing algorithms on the common class of (de) composition schema
transformations. We study both sample-based learning algorithms, which learn
from sets of labeled examples, and query-based algorithms, which learn by
asking queries to an oracle. We prove that current relational learning
algorithms are generally not schema independent. For query-based learning
algorithms we show that the (de) composition transformations influence their
query complexity. We propose Castor, a sample-based relational learning
algorithm that achieves schema independence by leveraging data dependencies. We
support the theoretical results with an empirical study that demonstrates the
schema dependence/independence of several algorithms on existing benchmark and
real-world datasets under (de) compositions.



The paper focuses on composite multistage decision making problems which are
targeted to design a route/trajectory from an initial decision situation
(origin) to goal (destination) decision situation(s). Automobile routing
problem is considered as a basic physical metaphor. The problems are based on a
discrete (combinatorial) operations/states design/solving space (e.g.,
digraph). The described types of discrete decision making problems can be
considered as intelligent design of a route (trajectory, strategy) and can be
used in many domains: (a) education (planning of student educational
trajectory), (b) medicine (medical treatment), (c) economics (trajectory of
start-up development). Several types of the route decision making problems are
described: (i) basic route decision making, (ii) multi-goal route decision
making, (iii) multi-route decision making, (iv) multi-route decision making
with route/trajectory change(s), (v) composite multi-route decision making
(solution is a composition of several routes/trajectories at several
corresponding domains), and (vi) composite multi-route decision making with
coordinated routes/trajectories. In addition, problems of modeling and building
the design spaces are considered. Numerical examples illustrate the suggested
approach. Three applications are considered: educational trajectory
(orienteering problem), plan of start-up company (modular three-stage design),
and plan of medical treatment (planning over digraph with two-component
vertices).



This paper describes an architecture that combines the complementary
strengths of probabilistic graphical models and declarative programming to
enable robots to represent and reason with logic-based and probabilistic
descriptions of uncertainty and domain knowledge. An action language is
extended to support non-boolean fluents and non-deterministic causal laws. This
action language is used to describe tightly-coupled transition diagrams at two
levels of granularity, refining a coarse-resolution transition diagram of the
domain to obtain a fine-resolution transition diagram. The coarse-resolution
system description, and a history that includes (prioritized) defaults, are
translated into an Answer Set Prolog (ASP) program. For any given goal,
inference in the ASP program provides a plan of abstract actions. To implement
each such abstract action probabilistically, the part of the fine-resolution
transition diagram relevant to this action is identified, and a probabilistic
representation of the uncertainty in sensing and actuation is included and used
to construct a partially observable Markov decision process (POMDP). The policy
obtained by solving the POMDP is invoked repeatedly to implement the abstract
action as a sequence of concrete actions, with the corresponding observations
being recorded in the coarse-resolution history and used for subsequent
reasoning. The architecture is evaluated in simulation and on a mobile robot
moving objects in an indoor domain, to show that it supports reasoning with
violation of defaults, noisy observations and unreliable actions, in complex
domains.



SelectScript is an extendable, adaptable, and declarative domain-specific
language aimed at information retrieval from simulation environments and
robotic world models in an SQL-like manner. In this work we have extended the
language in two directions. First, we have implemented hierarchical queries;
second, we improve efficiency enabling manual design space exploration on
different "search" strategies. We demonstrate the applicability of such
extensions in two application problems; the basic language concepts are
explained by solving the classical problem of the Towers of Hanoi and then a
common path planning problem in a complex 3D environment is implemented.



We propose a distributed deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is based on the deep Q-network, a convolutional neural
network trained with a variant of Q-learning. Its input is raw pixels and its
output is a value function estimating future rewards from taking an action
given a system state. To distribute the deep Q-network training, we adapt the
DistBelief software framework to the context of efficiently training
reinforcement learning agents. As a result, the method is completely
asynchronous and scales well with the number of machines. We demonstrate that
the deep Q-network agent, receiving only the pixels and the game score as
inputs, was able to achieve reasonable success on a simple game with minimal
parameter tuning.



Many of the current state-of-the-art Large Vocabulary Continuous Speech
Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov
Models (HMMs). Most of these systems contain separate components that deal with
the acoustic modelling, language modelling and sequence decoding. We
investigate a more direct approach in which the HMM is replaced with a
Recurrent Neural Network (RNN) that performs sequence prediction directly at
the character level. Alignment between the input features and the desired
character sequence is learned automatically by an attention mechanism built
into the RNN. For each predicted character, the attention mechanism scans the
input sequence and chooses relevant frames. We propose two methods to speed up
this operation: limiting the scan to a subset of most promising frames and
pooling over time the information contained in neighboring frames, thereby
reducing source sequence length. Integrating an n-gram language model into the
decoding process yields recognition accuracies similar to other HMM-free
RNN-based approaches.



Discrete combinatorial optimization has a central role in many scientific
disciplines, however, for hard problems we lack linear time algorithms that
would allow us to solve very large instances. Moreover, it is still unclear
what are the key features that make a discrete combinatorial optimization
problem hard to solve. Here we study random K-satisfiability problems with
$K=3,4$, which are known to be very hard close to the SAT-UNSAT threshold,
where problems stop having solutions. We show that the backtracking survey
propagation algorithm, in a time practically linear in the problem size, is
able to find solutions very close to the threshold, in a region unreachable by
any other algorithm. All solutions found have no frozen variables, thus
supporting the conjecture that only unfrozen solutions can be found in linear
time, and that a problem becomes impossible to solve in linear time when all
solutions contain frozen variables.



We propose a method combining relational-logic representations with neural
network learning. A general lifted architecture, possibly reflecting some
background domain knowledge, is described through relational rules which may be
handcrafted or learned. The relational rule-set serves as a template for
unfolding possibly deep neural networks whose structures also reflect the
structures of given training or testing relational examples. Different networks
corresponding to different examples share their weights, which co-evolve during
training by stochastic gradient descent algorithm. The framework allows for
hierarchical relational modeling constructs and learning of latent relational
concepts through shared hidden layers weights corresponding to the rules.
Discovery of notable relational concepts and experiments on 78 relational
learning benchmarks demonstrate favorable performance of the method.



We propose Neural Reasoner, a framework for neural network-based reasoning
over natural language sentences. Given a question, Neural Reasoner can infer
over multiple supporting facts and find an answer to the question in specific
forms. Neural Reasoner has 1) a specific interaction-pooling mechanism,
allowing it to examine multiple facts, and 2) a deep architecture, allowing it
to model the complicated logical relations in reasoning tasks. Assuming no
particular structure exists in the question and facts, Neural Reasoner is able
to accommodate different types of reasoning and different forms of language
expressions. Despite the model complexity, Neural Reasoner can still be trained
effectively in an end-to-end manner. Our empirical studies show that Neural
Reasoner can outperform existing neural reasoning systems with remarkable
margins on two difficult artificial tasks (Positional Reasoning and Path
Finding) proposed in [8]. For example, it improves the accuracy on Path
Finding(10K) from 33.4% [6] to over 98%.



We consider the Max $K$-Armed Bandit problem, where a learning agent is faced
with several sources (arms) of items (rewards), and interested in finding the
best item overall. At each time step the agent chooses an arm, and obtains a
random real valued reward. The rewards of each arm are assumed to be i.i.d.,
with an unknown probability distribution that generally differs among the arms.
Under the PAC framework, we provide lower bounds on the sample complexity of
any $(\epsilon,\delta)$-correct algorithm, and propose algorithms that attain
this bound up to logarithmic factors. We compare the performance of this
multi-arm algorithms to the variant in which the arms are not distinguishable
by the agent and are chosen randomly at each stage. Interestingly, when the
maximal rewards of the arms happen to be similar, the latter approach may
provide better performance.



Entity resolution (ER), an important and common data cleaning problem, is
about detecting data duplicate representations for the same external entities,
and merging them into single representations. Relatively recently, declarative
rules called matching dependencies (MDs) have been proposed for specifying
similarity conditions under which attribute values in database records are
merged. In this work we show the process and the benefits of integrating three
components of ER: (a) Classifiers for duplicate/non-duplicate record pairs
built using machine learning (ML) techniques, (b) MDs for supporting both the
blocking phase of ML and the merge itself; and (c) The use of the declarative
language LogiQL -an extended form of Datalog supported by the LogicBlox
platform- for data processing, and the specification and enforcement of MDs.



In this paper, we propose a model-based clustering method (TVClust) that
robustly incorporates noisy side information as soft-constraints and aims to
seek a consensus between side information and the observed data. Our method is
based on a nonparametric Bayesian hierarchical model that combines the
probabilistic model for the data instance and the one for the side-information.
An efficient Gibbs sampling algorithm is proposed for posterior inference.
Using the small-variance asymptotics of our probabilistic model, we then derive
a new deterministic clustering algorithm (RDP-means). It can be viewed as an
extension of K-means that allows for the inclusion of side information and has
the additional property that the number of clusters does not need to be
specified a priori. Empirical studies have been carried out to compare our work
with many constrained clustering algorithms from the literature on both a
variety of data sets and under a variety of conditions such as using noisy side
information and erroneous k values. The results of our experiments show strong
results for our probabilistic and deterministic approaches under these
conditions when compared to other algorithms in the literature.



Online media provides opportunities for marketers through which they can
deliver effective brand messages to a wide range of audiences. Advertising
technology platforms enable advertisers to reach their target audience by
delivering ad impressions to online users in real time. In order to identify
the best marketing message for a user and to purchase impressions at the right
price, we rely heavily on bid prediction and optimization models. Even though
the bid prediction models are well studied in the literature, the equally
important subject of model evaluation is usually overlooked. Effective and
reliable evaluation of an online bidding model is crucial for making faster
model improvements as well as for utilizing the marketing budgets more
efficiently. In this paper, we present an experimentation framework for bid
prediction models where our focus is on the practical aspects of model
evaluation. Specifically, we outline the unique challenges we encounter in our
platform due to a variety of factors such as heterogeneous goal definitions,
varying budget requirements across different campaigns, high seasonality and
the auction-based environment for inventory purchasing. Then, we introduce
return on investment (ROI) as a unified model performance (i.e., success)
metric and explain its merits over more traditional metrics such as
click-through rate (CTR) or conversion rate (CVR). Most importantly, we discuss
commonly used evaluation and metric summarization approaches in detail and
propose a more accurate method for online evaluation of new experimental models
against the baseline. Our meta-analysis-based approach addresses various
shortcomings of other methods and yields statistically robust conclusions that
allow us to conclude experiments more quickly in a reliable manner. We
demonstrate the effectiveness of our evaluation strategy on real campaign data
through some experiments.



The problem of domain generalization is to take knowledge acquired from a
number of related domains where training data is available, and to then
successfully apply it to previously unseen domains. We propose a new feature
learning algorithm, Multi-Task Autoencoder (MTAE), that provides good
generalization performance for cross-domain object recognition.
  Our algorithm extends the standard denoising autoencoder framework by
substituting artificially induced corruption with naturally occurring
inter-domain variability in the appearance of objects. Instead of
reconstructing images from noisy versions, MTAE learns to transform the
original image into analogs in multiple related domains. It thereby learns
features that are robust to variations across domains. The learnt features are
then used as inputs to a classifier.
  We evaluated the performance of the algorithm on benchmark image recognition
datasets, where the task is to learn features from multiple datasets and to
then predict the image label from unseen datasets. We found that (denoising)
MTAE outperforms alternative autoencoder-based models as well as the current
state-of-the-art algorithms for domain generalization.



Web Usage Mining is the application of data mining techniques to web usage
log repositories in order to discover the usage patterns that can be used to
analyze the users navigational behavior. During the preprocessing stage, raw
web log data is transformed into a set of user profiles. Each user profile
captures a set of URLs representing a user session. Clustering can be applied
to this sessionized data in order to capture similar interests and trends among
users navigational patterns. Since the sessionized data may contain thousands
of user sessions and each user session may consist of hundreds of URL accesses,
dimensionality reduction is achieved by eliminating the low support URLs. Very
small sessions are also removed in order to filter out the noise from the data.
But direct elimination of low support URLs and small sized sessions may results
in loss of a significant amount of information especially when the count of low
support URLs and small sessions is large. We propose a fuzzy solution to deal
with this problem by assigning weights to URLs and user sessions based on a
fuzzy membership function. After assigning the weights we apply a Fuzzy c-Mean
Clustering algorithm to discover the clusters of user profiles. In this paper,
we describe our fuzzy set theoretic approach to perform feature selection (or
dimensionality reduction) and session weight assignment. Finally we compare our
soft computing based approach of dimensionality reduction with the traditional
approach of direct elimination of small sessions and low support count URLs.
Our results show that fuzzy feature evaluation and dimensionality reduction
results in better performance and validity indices for the discovered clusters.



We propose an end-to-end, domain-independent neural encoder-aligner-decoder
model for selective generation, i.e., the joint task of content selection and
surface realization. Our model first encodes a full set of over-determined
database event records via an LSTM-based recurrent neural network, then
utilizes a novel coarse-to-fine aligner to identify the small subset of salient
records to talk about, and finally employs a decoder to generate free-form
descriptions of the aligned, selected records. Our model achieves the best
selection and generation results reported to-date (with 59% relative
improvement in generation) on the benchmark WeatherGov dataset, despite using
no specialized features or linguistic resources. Using an improved k-nearest
neighbor beam filter helps further. We also perform a series of ablations and
visualizations to elucidate the contributions of our key model components.
Lastly, we evaluate the generalizability of our model on the RoboCup dataset,
and get results that are competitive with or better than the state-of-the-art,
despite being severely data-starved.



Propagating input uncertainty through non-linear Gaussian process (GP)
mappings is intractable. This hinders the task of training GPs using uncertain
and partially observed inputs. In this paper we refer to this task as
"semi-described learning". We then introduce a GP framework that solves both,
the semi-described and the semi-supervised learning problems (where missing
values occur in the outputs). Auto-regressive state space simulation is also
recognised as a special case of semi-described learning. To achieve our goal we
develop variational methods for handling semi-described inputs in GPs, and
couple them with algorithms that allow for imputing the missing values while
treating the uncertainty in a principled, Bayesian manner. Extensive
experiments on simulated and real-world data study the problems of iterative
forecasting and regression/classification with missing values. The results
suggest that the principled propagation of uncertainty stemming from our
framework can significantly improve performance in these tasks.



We propose a quantization based approach for fast approximate Maximum Inner
Product Search (MIPS). Each database vector is quantized in multiple subspaces
via a set of codebooks, learned directly by minimizing the inner product
quantization error. Then, the inner product of a query to a database vector is
approximated as the sum of inner products with the subspace quantizers.
Different from recently proposed LSH approaches to MIPS, the database vectors
and queries do not need to be augmented in a higher dimensional feature space.
We also provide a theoretical analysis of the proposed approach, consisting of
the concentration results under mild assumptions. Furthermore, if a small
sample of example queries is given at the training time, we propose a modified
codebook learning procedure which further improves the accuracy. Experimental
results on a variety of datasets including those arising from deep neural
networks show that the proposed approach significantly outperforms the existing
state-of-the-art.



This report presents Giraffe, a chess engine that uses self-play to discover
all its domain-specific knowledge, with minimal hand-crafted knowledge given by
the programmer. Unlike previous attempts using machine learning only to perform
parameter-tuning on hand-crafted evaluation functions, Giraffe's learning
system also performs automatic feature extraction and pattern recognition. The
trained evaluation function performs comparably to the evaluation functions of
state-of-the-art chess engines - all of which containing thousands of lines of
carefully hand-crafted pattern recognizers, tuned over many years by both
computer chess experts and human chess masters. Giraffe is the most successful
attempt thus far at using end-to-end machine learning to play chess.



Paper provides a method for solving the reverse Monge-Kantorovich transport
problem (TP). It allows to accumulate positive decision-taking experience made
by decision-taker in situations that can be presented in the form of TP. The
initial data for the solution of the inverse TP is the information on orders,
inventories and effective decisions take by decision-taker. The result of
solving the inverse TP contains evaluations of the TPs payoff matrix elements.
It can be used in new situations to select the solution corresponding to the
preferences of the decision-taker. The method allows to gain decision-taker
experience, so it can be used by others. The method allows to build the model
of decision-taker preferences in a specific application area. The model can be
updated regularly to ensure its relevance and adequacy to the decision-taker
system of preferences. This model is adaptive to the current preferences of the
decision taker.



The paper presents a new script classification method for the discrimination
of the South Slavic medieval labels. It consists in the textural analysis of
the script types. In the first step, each letter is coded by the equivalent
script type, which is defined by its typographical features. Obtained coded
text is subjected to the run-length statistical analysis and to the adjacent
local binary pattern analysis in order to extract the features. The result
shows a diversity between the extracted features of the scripts, which makes
the feature classification more effective. It is the basis for the
classification process of the script identification by using an extension of a
state-of-the-art approach for document clustering. The proposed method is
evaluated on an example of hand-engraved in stone and hand-printed in paper
labels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate
very positive results, which prove the effectiveness of the proposed method.



This paper proposes GProp, a deep reinforcement learning algorithm for
continuous policies with compatible function approximation. The algorithm is
based on two innovations. Firstly, we present a temporal-difference based
method for learning the gradient of the value-function. Secondly, we present
the deviator-actor-critic (DAC) model, which comprises three neural networks
that estimate the value function, its gradient, and determine the actor's
policy respectively. We evaluate GProp on two challenging tasks: a contextual
bandit problem constructed from nonparametric regression datasets that is
designed to probe the ability of reinforcement learning algorithms to
accurately estimate gradients; and the octopus arm, a challenging reinforcement
learning benchmark. GProp is competitive with fully supervised methods on the
bandit task and achieves the best performance to date on the octopus arm.



Successful applications of reinforcement learning in real-world problems
often require dealing with partially observable states. It is in general very
challenging to construct and infer hidden states as they often depend on the
agent's entire interaction history and may require substantial domain
knowledge. In this work, we investigate a deep-learning approach to learning
the representation of states in partially observable tasks, with minimal prior
knowledge of the domain. In particular, we propose a new family of hybrid
models that combines the strength of both supervised learning (SL) and
reinforcement learning (RL), trained in a joint fashion: The SL component can
be a recurrent neural networks (RNN) or its long short-term memory (LSTM)
version, which is equipped with the desired property of being able to capture
long-term dependency on history, thus providing an effective way of learning
the representation of hidden states. The RL component is a deep Q-network (DQN)
that learns to optimize the control for maximizing long-term rewards. Extensive
experiments in a direct mailing campaign problem demonstrate the effectiveness
and advantages of the proposed approach, which performs the best among a set of
previous state-of-the-art methods.



We consider the following problem in which a given number of items has to be
chosen from a predefined set. Each item is described by a vector of attributes
and for each attribute there is a desired distribution that the selected set
should have. We look for a set that fits as much as possible the desired
distributions on all attributes. Examples of applications include choosing
members of a representative committee, where candidates are described by
attributes such as sex, age and profession, and where we look for a committee
that for each attribute offers a certain representation, i.e., a single
committee that contains a certain number of young and old people, certain
number of men and women, certain number of people with different professions,
etc. With a single attribute the problem collapses to the apportionment problem
for party-list proportional representation systems (in such case the value of
the single attribute would be a political affiliation of a candidate). We study
the properties of the associated subset selection rules, as well as their
computation complexity.



Objective: Anemia is a frequent comorbidity in hemodialysis patients that can
be successfully treated by administering erythropoiesis-stimulating agents
(ESAs). ESAs dosing is currently based on clinical protocols that often do not
account for the high inter- and intra-individual variability in the patient's
response. As a result, the hemoglobin level of some patients oscillates around
the target range, which is associated with multiple risks and side-effects.
This work proposes a methodology based on reinforcement learning (RL) to
optimize ESA therapy.
  Methods: RL is a data-driven approach for solving sequential decision-making
problems that are formulated as Markov decision processes (MDPs). Computing
optimal drug administration strategies for chronic diseases is a sequential
decision-making problem in which the goal is to find the best sequence of drug
doses. MDPs are particularly suitable for modeling these problems due to their
ability to capture the uncertainty associated with the outcome of the treatment
and the stochastic nature of the underlying process. The RL algorithm employed
in the proposed methodology is fitted Q iteration, which stands out for its
ability to make an efficient use of data.
  Results: The experiments reported here are based on a computational model
that describes the effect of ESAs on the hemoglobin level. The performance of
the proposed method is evaluated and compared with the well-known Q-learning
algorithm and with a standard protocol. Simulation results show that the
performance of Q-learning is substantially lower than FQI and the protocol.
  Conclusion: Although prospective validation is required, promising results
demonstrate the potential of RL to become an alternative to current protocols.



With the impressive capability to capture visual content, deep convolutional
neural networks (CNN) have demon- strated promising performance in various
vision-based ap- plications, such as classification, recognition, and objec- t
detection. However, due to the intrinsic structure design of CNN, for images
with complex content, it achieves lim- ited capability on invariance to
translation, rotation, and re-sizing changes, which is strongly emphasized in
the s- cenario of content-based image retrieval. In this paper, to address this
problem, we proposed a new kernelized deep convolutional neural network. We
first discuss our motiva- tion by an experimental study to demonstrate the
sensitivi- ty of the global CNN feature to the basic geometric trans-
formations. Then, we propose to represent visual content with approximate
invariance to the above geometric trans- formations from a kernelized
perspective. We extract CNN features on the detected object-like patches and
aggregate these patch-level CNN features to form a vectorial repre- sentation
with the Fisher vector model. The effectiveness of our proposed algorithm is
demonstrated on image search application with three benchmark datasets.



In many human brain network studies, we do not have sufficient number (n) of
images relative to the number (p) of voxels due to the prohibitively expensive
cost of scanning enough subjects. Thus, brain network models usually suffer the
small-n large-p problem. Such a problem is often remedied by sparse network
models, which are usually solved numerically by optimizing L1-penalties.
Unfortunately, due to the computational bottleneck associated with optimizing
L1-penalties, it is not practical to apply such methods to construct
large-scale brain networks at the voxel-level. In this paper, we propose a new
scalable sparse network model using cross-correlations that bypass the
computational bottleneck. Our model can build sparse brain networks at the
voxel level with p > 25000. Instead of using a single sparse parameter that may
not be optimal in other studies and datasets, the computational speed gain
enables us to analyze the collection of networks at every possible sparse
parameter in a coherent mathematical framework via persistent homology. The
method is subsequently applied in determining the extent of heritability on a
functional brain network at the voxel-level for the first time using twin fMRI.



Anticipating the future actions of a human is a widely studied problem in
robotics that requires spatio-temporal reasoning. In this work we propose a
deep learning approach for anticipation in sensory-rich robotics applications.
We introduce a sensory-fusion architecture which jointly learns to anticipate
and fuse information from multiple sensory streams. Our architecture consists
of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM)
units to capture long temporal dependencies. We train our architecture in a
sequence-to-sequence prediction manner, and it explicitly learns to predict the
future given only a partial temporal context. We further introduce a novel loss
layer for anticipation which prevents over-fitting and encourages early
anticipation. We use our architecture to anticipate driving maneuvers several
seconds before they happen on a natural driving data set of 1180 miles. The
context for maneuver anticipation comes from multiple sensors installed on the
vehicle. Our approach shows significant improvement over the state-of-the-art
in maneuver anticipation by increasing the precision from 77.4% to 90.5% and
recall from 71.2% to 87.4%.



Real-time estimation of destination and travel time for taxis is of great
importance for existing electronic dispatch systems. We present an approach
based on trip matching and ensemble learning, in which we leverage the patterns
observed in a dataset of roughly 1.7 million taxi journeys to predict the
corresponding final destination and travel time for ongoing taxi trips, as a
solution for the ECML/PKDD Discovery Challenge 2015 competition. The results of
our empirical evaluation show that our approach is effective and very robust,
which led our team -- BlueTaxi -- to the 3rd and 7th position of the final
rankings for the trip time and destination prediction tasks, respectively.
Given the fact that the final rankings were computed using a very small test
set (with only 320 trips) we believe that our approach is one of the most
robust solutions for the challenge based on the consistency of our good results
across the test sets.



This paper presents a case study of a recommender system that can be used to
save energy in smart homes without lowering the comfort of the inhabitants. We
present an algorithm that uses consumer behavior data only and uses machine
learning to suggest actions for inhabitants to reduce the energy consumption of
their homes. The system mines for frequent and periodic patterns in the event
data provided by the Digitalstrom home automation system. These patterns are
converted into association rules, prioritized and compared with the current
behavior of the inhabitants. If the system detects an opportunities to save
energy without decreasing the comfort level it sends a recommendation to the
residents.



We approach the challenging problem of generating highlights from sports
broadcasts utilizing audio information only. A language-independent,
multi-stage classification approach is employed for detection of key acoustic
events which then act as a platform for summarization of highlight scenes.
Objective results and human experience indicate that our system is highly
efficient.



The `pet fish' phenomenon is often cited as a paradigm example of the
`non-compositionality' of human concept use. We show here how this phenomenon
is naturally accommodated within a compositional distributional model of
meaning. This model describes the meaning of a composite concept by accounting
for interaction between its constituents via their grammatical roles. We give
two illustrative examples to show how the qualitative phenomena are exhibited.
We go on to apply the model to experimental data, and finally discuss
extensions of the formalism.



While most approaches to automatically recognizing entailment relations have
used classifiers employing hand engineered features derived from complex
natural language processing pipelines, in practice their performance has been
only slightly better than bag-of-word pair classifiers using only lexical
similarity. The only attempt so far to build an end-to-end differentiable
neural network for entailment failed to outperform such a simple similarity
classifier. In this paper, we propose a neural model that reads two sentences
to determine entailment using long short-term memory units. We extend this
model with a word-by-word neural attention mechanism that encourages reasoning
over entailments of pairs of words and phrases. Furthermore, we present a
qualitative analysis of attention weights produced by this model, demonstrating
such reasoning capabilities. On a large entailment dataset this model
outperforms the previous best neural model and a classifier with engineered
features by a substantial margin. It is the first generic end-to-end
differentiable system that achieves state-of-the-art accuracy on a textual
entailment dataset.



Max-product Belief Propagation (BP) is a popular message-passing algorithm
for computing a Maximum-A-Posteriori (MAP) assignment over a distribution
represented by a Graphical Model (GM). It has been shown that BP can solve a
number of combinatorial optimization problems including minimum weight
matching, shortest path, network flow and vertex cover under the following
common assumption: the respective Linear Programming (LP) relaxation is tight,
i.e., no integrality gap is present. However, when LP shows an integrality gap,
no model has been known which can be solved systematically via sequential
applications of BP. In this paper, we develop the first such algorithm, coined
Blossom-BP, for solving the minimum weight matching problem over arbitrary
graphs. Each step of the sequential algorithm requires applying BP over a
modified graph constructed by contractions and expansions of blossoms, i.e.,
odd sets of vertices. Our scheme guarantees termination in O(n^2) of BP runs,
where n is the number of vertices in the original graph. In essence, the
Blossom-BP offers a distributed version of the celebrated Edmonds' Blossom
algorithm by jumping at once over many sub-steps with a single BP. Moreover,
our result provides an interpretation of the Edmonds' algorithm as a sequence
of LPs.



This study explores the design and control of the behaviour of agents and
robots using simple circuits of spiking neurons and Spike Timing Dependent
Plasticity (STDP) as a mechanism of associative and unsupervised learning.
Based on a "reward and punishment" classical conditioning, it is demonstrated
that these robots learnt to identify and avoid obstacles as well as to identify
and look for rewarding stimuli. Using the simulation and programming
environment NetLogo, a software engine for the Integrate and Fire model was
developed, which allowed us to monitor in discrete time steps the dynamics of
each single neuron, synapse and spike in the proposed neural networks. These
spiking neural networks (SNN) served as simple brains for the experimental
robots. The Lego Mindstorms robot kit was used for the embodiment of the
simulated agents. In this paper the topological building blocks are presented
as well as the neural parameters required to reproduce the experiments. This
paper summarizes the resulting behaviour as well as the observed dynamics of
the neural circuits. The Internet-link to the NetLogo code is included in the
annex.



In this paper, we evaluate convolutional neural network (CNN) features using
the AlexNet architecture and very deep convolutional network (VGGNet)
architecture. To date, most CNN researchers have employed the last layers
before output, which were extracted from the fully connected feature layers.
However, since it is unlikely that feature representation effectiveness is
dependent on the problem, this study evaluates additional convolutional layers
that are adjacent to fully connected layers, in addition to executing simple
tuning for feature concatenation (e.g., layer 3 + layer 5 + layer 7) and
transformation, using tools such as principal component analysis. In our
experiments, we carried out detection and classification tasks using the
Caltech 101 and Daimler Pedestrian Benchmark Datasets.



A robot operating in a real-world environment needs to perform reasoning over
a variety of sensor modalities such as vision, language and motion
trajectories. However, it is extremely challenging to manually design features
relating such disparate modalities. In this work, we introduce an algorithm
that learns to embed point-cloud, natural language, and manipulation trajectory
data into a shared embedding space with a deep neural network. To learn
semantically meaningful spaces throughout our network, we use a loss-based
margin to bring embeddings of relevant pairs closer together while driving
less-relevant cases from different modalities further apart. We use this both
to pre-train its lower layers and fine-tune our final embedding space, leading
to a more robust representation. We test our algorithm on the task of
manipulating novel objects and appliances based on prior experience with other
objects. On a large dataset, we achieve significant improvements in both
accuracy and inference time over the previous state of the art. We also perform
end-to-end experiments on a PR2 robot utilizing our learned embedding space.



Based on ideas of quantum theory of open systems and psychological dual
system theory we propose two novel versions of Non-Boolean logic. The first
version can be interpreted in our opinion as simplified description of
primitive (mythological) thinking and the second one as the toy model of
everyday human reasoning in which aside from logical deduction, heuristic
elements and beliefs also play the considerable role. Several arguments in
favor of the interpretations proposed are adduced and discussed in the paper as
well.



It is commonplace to encounter nonstationary data, of which the underlying
generating process may change over time or across domains. The nonstationarity
presents both challenges and opportunities for causal discovery. In this paper
we propose a principled framework to handle nonstationarity, and develop some
methods to address three important questions. First, we propose an enhanced
constraint-based method to detect variables whose local mechanisms are
nonstationary and recover the skeleton of the causal structure over observed
variables. Second, we present a way to determine some causal directions by
taking advantage of information carried by changing distributions. Third, we
develop a method for visualizing the nonstationarity of causal modules.
Experimental results on various synthetic and real-world data sets are
presented to demonstrate the efficacy of our methods.



Slow feature analysis (SFA) is an unsupervised learning algorithm that
extracts slowly varying features from a time series. Graph-based SFA (GSFA) is
a supervised extension that can solve regression problems if followed by a
post-processing regression algorithm. A training graph specifies arbitrary
connections between the training samples. The connections in current graphs,
however, only depend on the rank of the involved labels. Exploiting the exact
label values makes further improvements in estimation accuracy possible.
  In this article, we propose the exact label learning (ELL) method to create a
graph that codes the desired label explicitly, so that GSFA is able to extract
a normalized version of it directly. The ELL method is used for three tasks:
(1) We estimate gender from artificial images of human faces (regression) and
show the advantage of coding additional labels, particularly skin color. (2) We
analyze two existing graphs for regression. (3) We extract compact
discriminative features to classify traffic sign images. When the number of
output features is limited, a higher classification rate is obtained compared
to a graph equivalent to nonlinear Fisher discriminant analysis. The method is
versatile, directly supports multiple labels, and provides higher accuracy
compared to current graphs for the problems considered.



We propose a particularly structured Boltzmann machine, which we refer to as
a dynamic Boltzmann machine (DyBM), as a stochastic model of a
multi-dimensional time-series. The DyBM can have infinitely many layers of
units but allows exact and efficient inference and learning when its parameters
have a proposed structure. This proposed structure is motivated by postulates
and observations, from biological neural networks, that the synaptic weight is
strengthened or weakened, depending on the timing of spikes (i.e., spike-timing
dependent plasticity or STDP). We show that the learning rule of updating the
parameters of the DyBM in the direction of maximizing the likelihood of given
time-series can be interpreted as STDP with long term potentiation and long
term depression. The learning rule has a guarantee of convergence and can be
performed in a distributed matter (i.e., local in space) with limited memory
(i.e., local in time).



The multilingual nature of the world makes translation a crucial requirement
today. Parallel dictionaries constructed by humans are a widely-available
resource, but they are limited and do not provide enough coverage for good
quality translation purposes, due to out-of-vocabulary words and neologisms.
This motivates the use of statistical translation systems, which are
unfortunately dependent on the quantity and quality of training data. Such has
a very limited availability especially for some languages and very narrow text
domains. Is this research we present our improvements to Yalign mining
methodology by reimplementing the comparison algorithm, introducing a tuning
scripts and by improving performance using GPU computing acceleration. The
experiments are conducted on various text domains and bi-data is extracted from
the Wikipedia dumps.



Reasoning with ontologies is one of the core fields of research in
Description Logics. A variety of efficient reasoner with highly optimized
algorithms have been developed to allow inference tasks on expressive ontology
languages such as OWL(DL). However, reasoner reported computing times have
exceeded and sometimes fall behind the expected theoretical values. From an
empirical perspective, it is not yet well understood, which particular aspects
in the ontology are reasoner performance degrading factors. In this paper, we
conducted an investigation about state of art works that attempted to portray
potential correlation between reasoner empirical behaviour and particular
ontological features. These works were analysed and then broken down into
categories. Further, we proposed a set of ontology features covering a broad
range of structural and syntactic ontology characteristics. We claim that these
features are good indicators of the ontology hardness level against reasoning
tasks.



The mutual information is a core statistical quantity that has applications
in all areas of machine learning, whether this is in training of density models
over multiple data modalities, in maximising the efficiency of noisy
transmission channels, or when learning behaviour policies for exploration by
artificial agents. Most learning algorithms that involve optimisation of the
mutual information rely on the Blahut-Arimoto algorithm --- an enumerative
algorithm with exponential complexity that is not suitable for modern machine
learning applications. This paper provides a new approach for scalable
optimisation of the mutual information by merging techniques from variational
inference and deep learning. We develop our approach by focusing on the problem
of intrinsically-motivated learning, where the mutual information forms the
definition of a well-known internal drive known as empowerment. Using a
variational lower bound on the mutual information, combined with convolutional
networks for handling visual input streams, we develop a stochastic
optimisation algorithm that allows for scalable information maximisation and
empowerment-based reasoning directly from pixels to actions.



Humans can learn the use of language through physical interaction with their
environment and semiotic communication with other people. It is very important
to obtain a computational understanding of how humans can form a symbol system
and obtain semiotic skills through their autonomous mental development.
Recently, many studies have been conducted on the construction of robotic
systems and machine-learning methods that can learn the use of language through
embodied multimodal interaction with their environment and other systems.
Understanding human social interactions and developing a robot that can
smoothly communicate with human users in the long term, requires an
understanding of the dynamics of symbol systems and is crucially important. The
embodied cognition and social interaction of participants gradually change a
symbol system in a constructive manner. In this paper, we introduce a field of
research called symbol emergence in robotics (SER). SER is a constructive
approach towards an emergent symbol system. The emergent symbol system is
socially self-organized through both semiotic communications and physical
interactions with autonomous cognitive developmental agents, i.e., humans and
developmental robots. Specifically, we describe some state-of-art research
topics concerning SER, e.g., multimodal categorization, word discovery, and a
double articulation analysis, that enable a robot to obtain words and their
embodied meanings from raw sensory--motor information, including visual
information, haptic information, auditory information, and acoustic speech
signals, in a totally unsupervised manner. Finally, we suggest future
directions of research in SER.



We examine the effect of clamping variables for approximate inference in
undirected graphical models with pairwise relationships and discrete variables.
For any number of variable labels, we demonstrate that clamping and summing
approximate sub-partition functions can lead only to a decrease in the
partition function estimate for TRW, and an increase for the naive mean field
method, in each case guaranteeing an improvement in the approximation and
bound. We next focus on binary variables, add the Bethe approximation to
consideration and examine ways to choose good variables to clamp, introducing
new methods. We show the importance of identifying highly frustrated cycles,
and of checking the singleton entropy of a variable. We explore the value of
our methods by empirical analysis and draw lessons to guide practitioners.



In this paper, we propose an active perception method for recognizing object
categories based on the multimodal hierarchical Dirichlet process (MHDP). The
MHDP enables a robot to form object categories using multimodal information,
e.g., visual, auditory, and haptic information, which can be observed by
performing actions on an object. However, performing many actions on a target
object requires a long time. In a real-time scenario, i.e., when the time is
limited, the robot has to determine the set of actions that is most effective
for recognizing a target object. We propose an MHDP-based active perception
method that uses the information gain (IG) maximization criterion and lazy
greedy algorithm. We show that the IG maximization criterion is optimal in the
sense that the criterion is equivalent to a minimization of the expected
Kullback--Leibler divergence between a final recognition state and the
recognition state after the next set of actions. However, a straightforward
calculation of IG is practically impossible. Therefore, we derive an efficient
Monte Carlo approximation method for IG by making use of a property of the
MHDP. We also show that the IG has submodular and non-decreasing properties as
a set function because of the structure of the graphical model of the MHDP.
Therefore, the IG maximization problem is reduced to a submodular maximization
problem. This means that greedy and lazy greedy algorithms are effective and
have a theoretical justification for their performance. We conducted an
experiment using an upper-torso humanoid robot and a second one using synthetic
data. The experimental results show that the method enables the robot to select
a set of actions that allow it to recognize target objects quickly and
accurately. The results support our theoretical outcomes.



All solutions SAT (AllSAT for short) is a variant of propositional
satisfiability problem. Despite its significance, AllSAT has been relatively
unexplored compared to other variants. We thus survey and discuss major
techniques of AllSAT solvers. We faithfully implement them and conduct
comprehensive experiments using a large number of instances and various types
of solvers including one of the few public softwares. The experiments reveal
solver's characteristics. Our implemented solvers are made publicly available
so that other researchers can easily develop their solver by modifying our
codes and compare it with existing methods.



We present a data mining approach for profiling bank clients in order to
support the process of detection of anti-money laundering operations. We first
present the overall system architecture, and then focus on the relevant
component for this paper. We detail the experiments performed on real world
data from a financial institution, which allowed us to group clients in
clusters and then generate a set of classification rules. We discuss the
relevance of the founded client profiles and of the generated classification
rules. According to the defined overall agent-based architecture, these rules
will be incorporated in the knowledge base of the intelligent agents
responsible for the signaling of suspicious transactions.



This paper provides a general result on controlling local Rademacher
complexities, which captures in an elegant form to relate the complexities with
constraint on the expected norm to the corresponding ones with constraint on
the empirical norm. This result is convenient to apply in real applications and
could yield refined local Rademacher complexity bounds for function classes
satisfying general entropy conditions. We demonstrate the power of our
complexity bounds by applying them to derive effective generalization error
bounds.



This paper proposes a new general approach based on Bayesian networks to
model the human behaviour. This approach represents human behaviour
withprobabilistic cause-effect relations based not only on previous works, but
also with conditional probabilities coming either from expert knowledge or
deduced from observations. The approach has been used in the co-simulation of
building physics and human behaviour in order to assess the CO 2 concentration
in an office.



Data-efficient reinforcement learning (RL) in continuous state-action spaces
using very high-dimensional observations remains a key challenge in developing
fully autonomous systems. We consider a particularly important instance of this
challenge, the pixels-to-torques problem, where an RL agent learns a
closed-loop control policy ("torques") from pixel information only. We
introduce a data-efficient, model-based reinforcement learning algorithm that
learns such a closed-loop policy directly from pixel information. The key
ingredient is a deep dynamical model for learning a low-dimensional feature
embedding of images jointly with a predictive model in this low-dimensional
feature space. Joint learning is crucial for long-term predictions, which lie
at the core of the adaptive nonlinear model predictive control strategy that we
use for closed-loop control. Compared to state-of-the-art RL methods for
continuous states and actions, our approach learns quickly, scales to
high-dimensional state spaces, is lightweight and an important step toward
fully autonomous end-to-end learning from pixels to torques.



We propose an efficient Context-Aware clustering of Bandits (CAB) algorithm,
which can capture collaborative effects. CAB can be easily deployed in a
real-world recommendation system, where multi-armed bandits have been shown to
perform well in particular with respect to the cold-start problem. CAB utilizes
a context-aware clustering augmented by exploration-exploitation strategies.
CAB dynamically clusters the users based on the content universe under
consideration. We give a theoretical analysis in the standard stochastic
multi-armed bandits setting. We show the efficiency of our approach on
production and real-world datasets, demonstrate the scalability, and, more
importantly, the significant increased prediction performance against several
state-of-the-art methods.



This paper addresses classification tasks on a particular target domain in
which labeled training data are only available from source domains different
from (but related to) the target. Two closely related frameworks, domain
adaptation and domain generalization, are concerned with such tasks, where the
only difference between those frameworks is the availability of the unlabeled
target data: domain adaptation can leverage unlabeled target information, while
domain generalization cannot. We propose Scatter Component Analyis (SCA), a
fast representation learning algorithm that can be applied to both domain
adaptation and domain generalization. SCA is based on a simple geometrical
measure, i.e., scatter, which operates on reproducing kernel Hilbert space. SCA
finds a representation that trades between maximizing the separability of
classes, minimizing the mismatch between domains, and maximizing the
separability of data; each of which is quantified through scatter. The
optimization problem of SCA can be reduced to a generalized eigenvalue problem,
which results in a fast and exact solution. Comprehensive experiments on
benchmark cross-domain object recognition datasets verify that SCA performs
much faster than several state-of-the-art algorithms and also provides
state-of-the-art classification accuracy in both domain adaptation and domain
generalization. We also show that scatter can be used to establish a
theoretical generalization bound in the case of domain adaptation.



The increasing complexity of deep learning architectures is resulting in
training time requiring weeks or even months. This slow training is due in part
to vanishing gradients, in which the gradients used by back-propagation are
extremely large for weights connecting deep layers (layers near the output
layer), and extremely small for shallow layers (near the input layer); this
results in slow learning in the shallow layers. Additionally, it has also been
shown that in highly non-convex problems, such as deep neural networks, there
is a proliferation of high-error low curvature saddle points, which slows down
learning dramatically. In this paper, we attempt to overcome the two above
problems by proposing an optimization method for training deep neural networks
which uses learning rates which are both specific to each layer in the network
and adaptive to the curvature of the function, increasing the learning rate at
low curvature points. This enables us to speed up learning in the shallow
layers of the network and quickly escape high-error low curvature saddle
points. We test our method on standard image classification datasets such as
MNIST, CIFAR10 and ImageNet, and demonstrate that our method increases accuracy
as well as reduces the required training time over standard algorithms.



Learning embeddings of entities and relations is an efficient and versatile
method to perform machine learning on relational data such as knowledge graphs.
In this work, we propose holographic embeddings (HolE) to learn compositional
vector space representations of entire knowledge graphs. The proposed method is
related to holographic models of associative memory in that it employs circular
correlation to create compositional representations. By using correlation as
the compositional operator HolE can capture rich interactions but
simultaneously remains efficient to compute, easy to train, and scalable to
very large datasets. In extensive experiments we show that holographic
embeddings are able to outperform state-of-the-art methods for link prediction
in knowledge graphs and relational learning benchmark datasets.



We analyze the RI-TIMEXes in temporally annotated corpora and propose two
hypotheses regarding the normalization of RI-TIMEXes in the clinical narrative
domain: the anchor point hypothesis and the anchor relation hypothesis. We
annotate the RI-TIMEXes in three corpora to study the characteristics of
RI-TMEXes in different domains. This informed the design of our RI-TIMEX
normalization system for the clinical domain, which consists of an anchor point
classifier, an anchor relation classifier and a rule-based RI-TIMEX text span
parser. We experiment with different feature sets and perform error analysis
for each system component. The annotation confirmed the hypotheses that we can
simplify the RI-TIMEXes normalization task using two multi-label classifiers.
Our system achieves anchor point classification, anchor relation classification
and rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under
relaxed matching criteria) respectively on the held-out test set of the 2012
i2b2 temporal relation challenge. Experiments with feature sets reveals some
interesting findings such as the verbal tense feature does not inform the
anchor relation classification in clinical narratives as much as the tokens
near the RI-TIMEX. Error analysis shows that underrepresented anchor point and
anchor relation classes are difficult to detect. We formulate the RI-TIMEX
normalization problem as a pair of multi-label classification problems.
Considering only the RI-TIMEX extraction and normalization, the system achieves
statistically significant improvement over the RI-TIMEX results of the best
systems in the 2012 i2b2 challenge.



In many robotic domains such as flexible automated manufacturing or personal
assistance, a fundamental perception task is that of identifying and localizing
objects whose 3D models are known. Canonical approaches to this problem include
discriminative methods that find correspondences between feature descriptors
computed over the model and observed data. While these methods have been
employed successfully, they can be unreliable when the feature descriptors fail
to capture variations in observed data; a classic cause being occlusion. As a
step towards deliberative reasoning, we present PERCH: PErception via SeaRCH,
an algorithm that seeks to find the best explanation of the observed sensor
data by hypothesizing possible scenes in a generative fashion. Our
contributions are: i) formulating the multi-object recognition and localization
task as an optimization problem over the space of hypothesized scenes, ii)
exploiting structure in the optimization to cast it as a combinatorial search
problem on what we call the Monotone Scene Generation Tree, and iii) leveraging
parallelization and recent advances in multi-heuristic search in making
combinatorial search tractable. We prove that our system can guaranteedly
produce the best explanation of the scene under the chosen cost function, and
validate our claims on real world RGB-D test data. Our experimental results
show that we can identify and localize objects under heavy occlusion--cases
where state-of-the-art methods struggle.



Traditional fact checking by experts and analysts cannot keep pace with the
volume of newly created information. It is important and necessary, therefore,
to enhance our ability to computationally determine whether some statement of
fact is true or false. We view this problem as a link-prediction task in a
knowledge graph, and present a discriminative path-based method for fact
checking in knowledge graphs that incorporates connectivity, type information,
and predicate interactions. Given a statement S of the form (subject,
predicate, object), for example, (Chicago, capitalOf, Illinois), our approach
mines discriminative paths that alternatively define the generalized statement
(U.S. city, predicate, U.S. state) and uses the mined rules to evaluate the
veracity of statement S. We evaluate our approach by examining thousands of
claims related to history, geography, biology, and politics using a public,
million node knowledge graph extracted from Wikipedia and PubMedDB. Not only
does our approach significantly outperform related models, we also find that
the discriminative predicate path model is easily interpretable and provides
sensible reasons for the final determination.



Controller synthesis for hybrid systems that satisfy temporal specifications
expressing various system properties is a challenging problem that has drawn
the attention of many researchers. However, making the assumption that such
temporal properties are deterministic is far from the reality. For example,
many of the properties the controller has to satisfy are learned through
machine learning techniques based on sensor input data. In this paper, we
propose a new logic, Probabilistic Signal Temporal Logic (PrSTL), as an
expressive language to define the stochastic properties, and enforce
probabilistic guarantees on them. We further show how to synthesize safe
controllers using this logic for cyber-physical systems under the assumption
that the stochastic properties are based on a set of Gaussian random variables.
One of the key distinguishing features of PrSTL is that the encoded logic is
adaptive and changes as the system encounters additional data and updates its
beliefs about the latent random variables that define the safety properties. We
demonstrate our approach by synthesizing safe controllers under the PrSTL
specifications for multiple case studies including control of quadrotors and
autonomous vehicles in dynamic environments.



Bayesian nonparametric models, such as Gaussian processes, provide a
compelling framework for automatic statistical modelling: these models have a
high degree of flexibility, and automatically calibrated complexity. However,
automating human expertise remains elusive; for example, Gaussian processes
with standard kernels struggle on function extrapolation problems that are
trivial for human learners. In this paper, we create function extrapolation
problems and acquire human responses, and then design a kernel learning
framework to reverse engineer the inductive biases of human learners across a
set of behavioral experiments. We use the learned kernels to gain psychological
insights and to extrapolate in human-like ways that go beyond traditional
stationary and polynomial kernels. Finally, we investigate Occam's razor in
human and Gaussian process based function learning.



In this paper we explore deep learning models with memory component or
attention mechanism for question answering task. We combine and compare three
models, Neural Machine Translation, Neural Turing Machine, and Memory Networks
for a simulated QA data set. This paper is the first one that uses Neural
Machine Translation and Neural Turing Machines for solving QA tasks. Our
results suggest that the combination of attention and memory have potential to
solve certain QA problem.



In a conversation or a dialogue process, attention and intention play
intrinsic roles. This paper proposes a neural network based approach that
models the attention and intention processes. It essentially consists of three
recurrent networks. The encoder network is a word-level model representing
source side sentences. The intention network is a recurrent network that models
the dynamics of the intention process. The decoder network is a recurrent
network produces responses to the input from the source side. It is a language
model that is dependent on the intention and has an attention mechanism to
attend to particular source side words, when predicting a symbol in the
response. The model is trained end-to-end without labeling data. Experiments
show that this model generates natural responses to user inputs.



The first ever human vs. computer no-limit Texas hold 'em competition took
place from April 24-May 8, 2015 at River's Casino in Pittsburgh, PA. In this
article I present my thoughts on the competition design, agent architecture,
and lessons learned.



Recently, there has been significant progress in understanding reinforcement
learning in discounted infinite-horizon Markov decision processes (MDPs) by
deriving tight sample complexity bounds. However, in many real-world
applications, an interactive learning agent operates for a fixed or bounded
period of time, for example tutoring students for exams or handling customer
service requests. Such scenarios can often be better treated as episodic
fixed-horizon MDPs, for which only looser bounds on the sample complexity
exist. A natural notion of sample complexity in this setting is the number of
episodes required to guarantee a certain performance with high probability (PAC
guarantee). In this paper, we derive an upper PAC bound $\tilde
O(\frac{|\mathcal S|^2 |\mathcal A| H^2}{\epsilon^2} \ln\frac 1 \delta)$ and a
lower PAC bound $\tilde \Omega(\frac{|\mathcal S| |\mathcal A| H^2}{\epsilon^2}
\ln \frac 1 {\delta + c})$ that match up to log-terms and an additional linear
dependency on the number of states $|\mathcal S|$. The lower bound is the first
of its kind for this setting. Our upper bound leverages Bernstein's inequality
to improve on previous bounds for episodic finite-horizon MDPs which have a
time-horizon dependency of at least $H^3$.



Matrix rank minimization problem is in general NP-hard. The nuclear norm is
used to substitute the rank function in many recent studies. Nevertheless, the
nuclear norm approximation adds all singular values together and the
approximation error may depend heavily on the magnitudes of singular values.
This might restrict its capability in dealing with many practical problems. In
this paper, an arctangent function is used as a tighter approximation to the
rank function. We use it on the challenging subspace clustering problem. For
this nonconvex minimization problem, we develop an effective optimization
procedure based on a type of augmented Lagrange multipliers (ALM) method.
Extensive experiments on face clustering and motion segmentation show that the
proposed method is effective for rank approximation.



In this paper, we extend the deep long short-term memory (DLSTM) recurrent
neural networks by introducing gated direct connections between memory cells in
adjacent layers. These direct links, called highway connections, enable
unimpeded information flow across different layers and thus alleviate the
gradient vanishing problem when building deeper LSTMs. We further introduce the
latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole
history while keeping the latency under control. Efficient algorithms are
proposed to train these novel networks using both frame and sequence
discriminative criteria. Experiments on the AMI distant speech recognition
(DSR) task indicate that we can train deeper LSTMs and achieve better
improvement from sequence training with highway LSTMs (HLSTMs). Our novel model
obtains $43.9/47.7\%$ WER on AMI (SDM) dev and eval sets, outperforming all
previous works. It beats the strong DNN and DLSTM baselines with $15.7\%$ and
$5.3\%$ relative improvement respectively.



In many statistical problems, a more coarse-grained model may be suitable for
population-level behaviour, whereas a more detailed model is appropriate for
accurate modelling of individual behaviour. This raises the question of how to
integrate both types of models. Methods such as posterior regularization follow
the idea of generalized moment matching, in that they allow matching
expectations between two models, but sometimes both models are most
conveniently expressed as latent variable models. We propose latent Bayesian
melding, which is motivated by averaging the distributions over populations
statistics of both the individual-level and the population-level models under a
logarithmic opinion pool framework. In a case study on electricity
disaggregation, which is a type of single-channel blind source separation
problem, we show that latent Bayesian melding leads to significantly more
accurate predictions than an approach based solely on generalized moment
matching.



Characterizing relationships between people is fundamental for the
understanding of narratives. In this work, we address the problem of inferring
the polarity of relationships between people in narrative summaries. We
formulate the problem as a joint structured prediction for each narrative, and
present a model that combines evidence from linguistic and semantic features,
as well as features based on the structure of the social community in the text.
We also provide a clustering-based approach that can exploit regularities in
narrative types. e.g., learn an affinity for love-triangles in romantic
stories. On a dataset of movie summaries from Wikipedia, our structured models
provide more than a 30% error-reduction over a competitive baseline that
considers pairs of characters in isolation.



Artificial neural networks are powerful models, which have been widely
applied into many aspects of machine translation, such as language modeling and
translation modeling. Though notable improvements have been made in these
areas, the reordering problem still remains a challenge in statistical machine
translations. In this paper, we present a novel neural reordering model that
directly models word pairs and alignment. By utilizing LSTM recurrent neural
networks, much longer context could be learned for reordering prediction.
Experimental results on NIST OpenMT12 Arabic-English and Chinese-English
1000-best rescoring task show that our LSTM neural reordering feature is robust
and achieves significant improvements over various baseline systems.



In the context of embodied artificial intelligence, morphological computation
refers to processes which are conducted by the body (and environment) that
otherwise would have to be performed by the brain. Exploiting environmental and
morphological properties is an important feature of embodied systems. The main
reason is that it allows to significantly reduce the controller complexity. An
important aspect of morphological computation is that it cannot be assigned to
an embodied system per se, but that it is, as we show, behavior- and
state-dependent. In this work, we evaluate two different measures of
morphological computation that can be applied in robotic systems and in
computer simulations of biological movement. As an example, these measures were
evaluated on muscle and DC-motor driven hopping models. We show that a
state-dependent analysis of the hopping behaviors provides additional insights
that cannot be gained from the averaged measures alone. This work includes
algorithms and computer code for the measures.



This paper investigates a novel problem of generating images from visual
attributes. We model the image as a composite of foreground and background and
develop a layered generative model with disentangled latent variables that can
be learned end-to-end using a variational auto-encoder. We experiment with
natural images of faces and birds and demonstrate that the proposed models are
capable of generating realistic and diverse samples with disentangled latent
representations. We use a general energy minimization algorithm for posterior
inference of latent variables given novel images. Therefore, the learned
generative models show excellent quantitative and visual results in the tasks
of attribute-conditioned image reconstruction and completion.



To accomplish tasks in human-centric indoor environments, robots need to
represent and understand the world in terms of objects and their attributes. We
refer to this attribute-based representation as a world model, and consider how
to acquire it via noisy perception and maintain it over time, as objects are
added, changed, and removed in the world. Previous work has framed this as
multiple-target tracking problem, where objects are potentially in motion at
all times. Although this approach is general, it is computationally expensive.
We argue that such generality is not needed in typical world modeling tasks,
where objects only change state occasionally. More efficient approaches are
enabled by restricting ourselves to such semi-static environments.
  We consider a previously-proposed clustering-based world modeling approach
that assumed static environments, and extend it to semi-static domains by
applying a dependent Dirichlet-process (DDP) mixture model. We derive a novel
MAP inference algorithm under this model, subject to data association
constraints. We demonstrate our approach improves computational performance in
semi-static environments.



The human face constantly conveys information, both consciously and
subconsciously. However, as basic as it is for humans to visually interpret
this information, it is quite a big challenge for machines. Conventional
semantic facial feature recognition and analysis techniques are already in use
and are based on physiological heuristics, but they suffer from lack of
robustness and high computation time. This thesis aims to explore ways for
machines to learn to interpret semantic information available in faces in an
automated manner without requiring manual design of feature detectors, using
the approach of Deep Learning. This thesis provides a study of the effects of
various factors and hyper-parameters of deep neural networks in the process of
determining an optimal network configuration for the task of semantic facial
feature recognition. This thesis explores the effectiveness of the system to
recognize the various semantic features (like emotions, age, gender, ethnicity
etc.) present in faces. Furthermore, the relation between the effect of
high-level concepts on low level features is explored through an analysis of
the similarities in low-level descriptors of different semantic features. This
thesis also demonstrates a novel idea of using a deep network to generate 3-D
Active Appearance Models of faces from real-world 2-D images.
  For a more detailed report on this work, please see [arXiv:1512.00743v1].



We proposed Neural Enquirer as a neural network architecture to execute a
natural language (NL) query on a knowledge-base (KB) for answers. Basically,
Neural Enquirer finds the distributed representation of a query and then
executes it on knowledge-base tables to obtain the answer as one of the values
in the tables. Unlike similar efforts in end-to-end training of semantic
parsers, Neural Enquirer is fully "neuralized": it not only gives
distributional representation of the query and the knowledge-base, but also
realizes the execution of compositional queries as a series of differentiable
operations, with intermediate results (consisting of annotations of the tables
at different levels) saved on multiple layers of memory. Neural Enquirer can be
trained with gradient descent, with which not only the parameters of the
controlling components and semantic parsing component, but also the embeddings
of the tables and query words can be learned from scratch. The training can be
done in an end-to-end fashion, but it can take stronger guidance, e.g., the
step-by-step supervision for complicated queries, and benefit from it. Neural
Enquirer is one step towards building neural network systems which seek to
understand language by executing it on real-world. Our experiments show that
Neural Enquirer can learn to execute fairly complicated NL queries on tables
with rich structures.



In this paper we present, by way of case studies, a proof of concept, based
on a prototype working on a automotive data set, aimed at showing the potential
usefulness of using formulas of {\L}ukasiewicz propositional logic to query
databases in a fuzzy way. Our approach distinguishes itself for its stress on
the purely linguistic, contraposed with numeric, formulations of queries. Our
queries are expressed in the pure language of logic, and when we use (integer)
numbers, these stand for shortenings of formulas on the syntactic level, and
serve as linguistic hedges on the semantic one. Our case-study queries aim
first at showing that each numeric-threshold fuzzy query is simulated by a
{\L}ukasiewicz formula. Then they focus on the expressing power of
{\L}ukasiewicz logic which easily allows for updating queries by clauses and
for modifying them through a potentially infinite variety of linguistic hedges
implemented with a uniform syntactic mechanism. Finally we shall hint how,
already at propositional level, {\L}ukasiewicz natural semantics enjoys a
degree of reflection, allowing to write syntactically simple queries that
semantically work as meta-queries weighing the contribution of simpler ones.



Bayesian matrix completion has been studied based on a low-rank matrix
factorization formulation with promising results. However, little work has been
done on Bayesian matrix completion based on the more direct spectral
regularization formulation. We fill this gap by presenting a novel Bayesian
matrix completion method based on spectral regularization. In order to
circumvent the difficulties of dealing with the orthonormality constraints of
singular vectors, we derive a new equivalent form with relaxed constraints,
which then leads us to design an adaptive version of spectral regularization
feasible for Bayesian inference. Our Bayesian method requires no parameter
tuning and can infer the number of latent factors automatically. Experiments on
synthetic and real datasets demonstrate encouraging results on rank recovery
and collaborative filtering, with notably good results for very sparse
matrices.



Many real-world problems come with action spaces represented as feature
vectors. Although high-dimensional control is a largely unsolved problem, there
has recently been progress for modest dimensionalities. Here we report on a
successful attempt at addressing problems of dimensionality as high as $2000$,
of a particular form. Motivated by important applications such as
recommendation systems that do not fit the standard reinforcement learning
frameworks, we introduce Slate Markov Decision Processes (slate-MDPs). A
Slate-MDP is an MDP with a combinatorial action space consisting of slates
(tuples) of primitive actions of which one is executed in an underlying MDP.
The agent does not control the choice of this executed action and the action
might not even be from the slate, e.g., for recommendation systems for which
all recommendations can be ignored. We use deep Q-learning based on feature
representations of both the state and action to learn the value of whole
slates. Unlike existing methods, we optimize for both the combinatorial and
sequential aspects of our tasks. The new agent's superiority over agents that
either ignore the combinatorial or sequential long-term value aspect is
demonstrated on a range of environments with dynamics from a real-world
recommendation system. Further, we use deep deterministic policy gradients to
learn a policy that for each position of the slate, guides attention towards
the part of the action space in which the value is the highest and we only
evaluate actions in this area. The attention is used within a sequentially
greedy procedure leveraging submodularity. Finally, we show how introducing
risk-seeking can dramatically improve the agents performance and ability to
discover more far reaching strategies.



We present a new perspective on neural knowledge base (KB) embeddings, from
which we build a framework that can model symbolic knowledge in the KB together
with its learning process. We show that this framework well regularizes
previous neural KB embedding model for superior performance in reasoning tasks,
while having the capabilities of dealing with unseen entities, that is, to
learn their embeddings from natural language descriptions, which is very like
human's behavior of learning semantic concepts.



There is a widespread need for statistical methods that can analyze
high-dimensional datasets with- out imposing restrictive or opaque modeling
assumptions. This paper describes a domain-general data analysis method called
CrossCat. CrossCat infers multiple non-overlapping views of the data, each
consisting of a subset of the variables, and uses a separate nonparametric
mixture to model each view. CrossCat is based on approximately Bayesian
inference in a hierarchical, nonparamet- ric model for data tables. This model
consists of a Dirichlet process mixture over the columns of a data table in
which each mixture component is itself an independent Dirichlet process mixture
over the rows; the inner mixture components are simple parametric models whose
form depends on the types of data in the table. CrossCat combines strengths of
mixture modeling and Bayesian net- work structure learning. Like mixture
modeling, CrossCat can model a broad class of distributions by positing latent
variables, and produces representations that can be efficiently conditioned and
sampled from for prediction. Like Bayesian networks, CrossCat represents the
dependencies and independencies between variables, and thus remains accurate
when there are multiple statistical signals. Inference is done via a scalable
Gibbs sampling scheme; this paper shows that it works well in practice. This
paper also includes empirical results on heterogeneous tabular data of up to 10
million cells, such as hospital cost and quality measures, voting records,
unemployment rates, gene expression measurements, and images of handwritten
digits. CrossCat infers structure that is consistent with accepted findings and
common-sense knowledge in multiple domains and yields predictive accuracy
competitive with generative, discriminative, and model-free alternatives.



The Ramsey number is of vital importance in Ramsey's theorem. This paper
proposed a novel methodology for constructing Ramsey graphs about R(3,10),
which uses Artificial Bee Colony optimization(ABC) to raise the lower bound of
Ramsey number R(3,10). The r(3,10)-graph contains two limitations, that is,
neither complete graphs of order 3 nor independent sets of order 10. To resolve
these limitations, a special mathematical model is put in the paradigm to
convert the problems into discrete optimization whose smaller minimizers are
correspondent to bigger lower bound as approximation of inf R(3,10). To
demonstrate the potential of the proposed method, simulations are done to to
minimize the amount of these two types of graphs. For the first time, four
r(3,9,39) graphs with best approximation for inf R(3,10) are reported in
simulations to support the current lower bound for R(3,10). The experiments'
results show that the proposed paradigm for Ramsey number's calculation driven
by ABC is a successful method with the advantages of high precision and
robustness.



In many sequential decision-making problems one is interested in minimizing
an expected cumulative cost while taking into account \emph{risk}, i.e.,
increased awareness of events of small probability and high consequences.
Accordingly, the objective of this paper is to present efficient reinforcement
learning algorithms for risk-constrained Markov decision processes (MDPs),
where risk is represented via a chance constraint or a constraint on the
conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer
to such problems as percentile risk-constrained MDPs.
  Specifically, we first derive a formula for computing the gradient of the
Lagrangian function for percentile risk-constrained MDPs. Then, we devise
policy gradient and actor-critic algorithms that (1) estimate such gradient,
(2) update the policy in the descent direction, and (3) update the Lagrange
multiplier in the ascent direction. For these algorithms we prove convergence
to locally optimal policies. Finally, we demonstrate the effectiveness of our
algorithms in an optimal stopping problem and an online marketing application.



In this dissertation, we analyze the computational properties of
game-theoretic centrality measures. The key idea behind game-theoretic approach
to network analysis is to treat nodes as players in a cooperative game, where
the value of each coalition of nodes is determined by certain graph properties.
Next, the centrality of any individual node is determined by a chosen
game-theoretic solution concept (notably, the Shapley value) in the same way as
the payoff of a player in a cooperative game. On one hand, the advantage of
game-theoretic centrality measures is that nodes are ranked not only according
to their individual roles but also according to how they contribute to the role
played by all possible subsets of nodes. On the other hand, the disadvantage is
that the game-theoretic solution concepts are typically computationally
challenging. The main contribution of this dissertation is that we show that a
wide variety of game-theoretic solution concepts on networks can be computed in
polynomial time. Our focus is on centralities based on the Shapley value and
its various extensions, such as the Semivalues and Coalitional Semivalues.
Furthermore, we prove #P-hardness of computing the Shapley value in
connectivity games and propose an algorithm to compute it. Finally, we analyse
computational properties of generalized version of cooperative games in which
order of player matters. We propose a new representation for such games, called
generalized marginal contribution networks, that allows for polynomial
computation in the size of the representation of two dedicated extensions of
the Shapley value to this class of games.



While emerging deep-learning systems have outclassed knowledge-based
approaches in many tasks, their application to detection tasks for autonomous
technologies remains an open field for scientific exploration. Broadly, there
are two major developmental bottlenecks: the unavailability of comprehensively
labeled datasets and of expressive evaluation strategies. Approaches for
labeling datasets have relied on intensive hand-engineering, and strategies for
evaluating learning systems have been unable to identify failure-case
scenarios. Human intelligence offers an untapped approach for breaking through
these bottlenecks. This paper introduces Driverseat, a technology for embedding
crowds around learning systems for autonomous driving. Driverseat utilizes
crowd contributions for (a) collecting complex 3D labels and (b) tagging
diverse scenarios for ready evaluation of learning systems. We demonstrate how
Driverseat can crowdstrap a convolutional neural network on the lane-detection
task. More generally, crowdstrapping introduces a valuable paradigm for any
technology that can benefit from leveraging the powerful combination of human
and computer intelligence.



Humans routinely confront the following key question which could be viewed as
a probabilistic variant of the controllability problem: While faced with an
uncertain environment governed by causal structures, how should they practice
their autonomy by intervening on driver variables, in order to increase (or
decrease) the probability of attaining their desired (or undesired) state for
some target variable? In this paper, for the first time, the problem of
probabilistic controllability in Causal Bayesian Networks (CBNs) is studied.
More specifically, the aim of this paper is two-fold: (i) to introduce and
formalize the problem of probabilistic structural controllability in CBNs, and
(ii) to identify a sufficient set of driver variables for the purpose of
probabilistic structural controllability of a generic CBN. We also elaborate on
the nature of minimality the identified set of driver variables satisfies. In
this context, the term "structural" signifies the condition wherein solely the
structure of the CBN is known.



There exists a theory of a single general-purpose learning algorithm which
could explain the principles its operation. It assumes the initial rough
architecture, a small library of simple innate circuits which are prewired at
birth. and proposes that all significant mental algorithms are learned. Given
current understanding and observations, this paper reviews and lists the
ingredients of such an algorithm from architectural and functional
perspectives.



In the literature of game theory, the information sets of extensive form
games have different interpretations, which may lead to confusions and
paradoxical cases. We argue that the problem lies in the mix-up of two
interpretations of the extensive form game structures: game rules or game runs
which do not always coincide. In this paper, we try to separate and connect
these two views by proposing a dynamic epistemic framework in which we can
compute the runs step by step from the game rules plus the given assumptions of
the players. We propose a modal logic to describe players' knowledge and its
change during the plays, and provide a complete axiomatization. We also show
that, under certain conditions, the mix-up of the rules and the runs is not
harmful due to the structural similarity of the two.



Many scientific datasets are of high dimension, and the analysis usually
requires visual manipulation by retaining the most important structures of
data. Principal curve is a widely used approach for this purpose. However, many
existing methods work only for data with structures that are not
self-intersected, which is quite restrictive for real applications. A few
methods can overcome the above problem, but they either require complicated
human-made rules for a specific task with lack of convergence guarantee and
adaption flexibility to different tasks, or cannot obtain explicit structures
of data. To address these issues, we develop a new regularized principal graph
learning framework that captures the local information of the underlying graph
structure based on reversed graph embedding. As showcases, models that can
learn a spanning tree or a weighted undirected $\ell_1$ graph are proposed, and
a new learning algorithm is developed that learns a set of principal points and
a graph structure from data, simultaneously. The new algorithm is simple with
guaranteed convergence. We then extend the proposed framework to deal with
large-scale data. Experimental results on various synthetic and six real world
datasets show that the proposed method compares favorably with baselines and
can uncover the underlying structure correctly.



The proliferation of contextualized knowledge in the Semantic Web (SW) has
led to the popularity of knowledge formats such as \emph{quads} in the SW
community. A quad is an extension of an RDF triple with contextual information
of the triple. In this paper, we study the problem of query answering over
quads augmented with forall-existential bridge rules that enable
interoperability of reasoning between triples in various contexts. We call a
set of quads together with such expressive bridge rules, a quad-system. Query
answering over quad-systems is undecidable, in general. We derive decidable
classes of quad-systems, for which query answering can be done using forward
chaining. Sound, complete and terminating procedures, which are adaptations of
the well known chase algorithm, are provided for these classes for deciding
query entailment. Safe, msafe, and csafe class of quad-systems restrict the
structure of blank nodes generated during the chase computation process to be
directed acyclic graphs (DAGs) of bounded depth. RR and restricted RR classes
do not allow the generation of blank nodes during the chase computation
process. Both data and combined complexity of query entailment has been
established for the classes derived. We further show that quad-systems are
equivalent to forall-existential rules whose predicates are restricted to
ternary arity, modulo polynomial time translations. We subsequently show that
the technique of safety, strictly subsumes in expressivity, some of the well
known and expressive techniques, such as joint acyclicity and model faithful
acyclicity, used for decidability guarantees in the realm of forall-existential
rules.



An ever increasing number of computer vision and image/video processing
challenges are being approached using deep convolutional neural networks,
obtaining state-of-the-art results in object recognition and detection,
semantic segmentation, action recognition, optical flow and superresolution.
Hardware acceleration of these algorithms is essential to adopt these
improvements in embedded and mobile computer vision systems. We present a new
architecture, design and implementation as well as the first reported silicon
measurements of such an accelerator, outperforming previous work in terms of
power-, area- and I/O-efficiency. The manufactured device provides up to 196
GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power
efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it
the first architecture scalable to TOp/s performance.



Most of Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC)
algorithms in existing probabilistic programming systems suboptimally use only
model priors as proposal distributions. In this work, we describe an approach
for training a discriminative model, namely a neural network, in order to
approximate the optimal proposal by using posterior estimates from previous
runs of inference. We show an example that incorporates a data-driven proposal
for use in a non-parametric model in the Anglican probabilistic programming
system. Our results show that data-driven proposals can significantly improve
inference performance so that considerably fewer particles are necessary to
perform a good posterior estimation.



Distributional semantic models provide vector representations for words by
gathering co-occurrence frequencies from corpora of text. Compositional
distributional models extend these representations from words to phrases and
sentences. In categorical compositional distributional semantics these
representations are built in such a manner that meanings of phrases and
sentences are functions of their grammatical structure and the meanings of the
words therein. These models have been applied to reasoning about phrase and
sentence level similarity. In this paper, we argue for and prove that these
models can also be used to reason about phrase and sentence level entailment.
We provide preliminary experimental results on a toy entailment dataset.



Reverse engineering the brain is proving difficult, perhaps impossible. While
many believe that this is just a matter of time and effort, a different
approach might help. Here, we describe a very simple idea which explains the
power of the brain as well as its structure, exploiting complex dynamics rather
than abstracting it away. Just as a Turing Machine is a Universal Digital
Computer operating in a world of symbols, we propose that the brain is a
Universal Dynamical Systems Modeller, evolved bottom-up (itself using nested
networks of interconnected, self-organised dynamical systems) to prosper in a
world of dynamical systems.
  Recent progress in Applied Mathematics has produced startling evidence of
what happens when abstract Dynamical Systems interact. Key latent information
describing system A can be extracted by system B from very simple signals, and
signals can be used by one system to control and manipulate others. Using these
facts, we show how a region of the neocortex uses its dynamics to intrinsically
"compute" about the external and internal world.
  Building on an existing "static" model of cortical computation (Hawkins'
Hierarchical Temporal Memory - HTM), we describe how a region of neocortex can
be viewed as a network of components which together form a Dynamical Systems
modelling module, connected via sensory and motor pathways to the external
world, and forming part of a larger dynamical network in the brain.
  Empirical modelling and simulations of Dynamical HTM are possible with simple
extensions and combinations of currently existing open source software. We list
a number of relevant projects.



Good predictors of ICU Mortality have the potential to identify high-risk
patients earlier, improve ICU resource allocation, or create more accurate
population-level risk models. Machine learning practitioners typically make
choices about how to represent features in a particular model, but these
choices are seldom evaluated quantitatively. This study compares the
performance of different representations of clinical event data from MIMIC II
in a logistic regression model to predict 36-hour ICU mortality. The most
common representations are linear (normalized counts) and binary (yes/no).
These, along with a new representation termed "hill", are compared using both
L1 and L2 regularization. Results indicate that the introduced "hill"
representation outperforms both the binary and linear representations, the hill
representation thus has the potential to improve existing models of ICU
mortality.



We present a framework for representing and modeling data on graphs. Based on
this framework, we study three typical classes of graph signals: smooth graph
signals, piecewise-constant graph signals, and piecewise-smooth graph signals.
For each class, we provide an explicit definition of the graph signals and
construct a corresponding graph dictionary with desirable properties. We then
study how such graph dictionary works in two standard tasks: approximation and
sampling followed with recovery, both from theoretical as well as algorithmic
perspectives. Finally, for each class, we present a case study of a real-world
problem by using the proposed methodology.



This paper explores the performance of fitted neural Q iteration for
reinforcement learning in several partially observable environments, using
three recurrent neural network architectures: Long Short-Term Memory, Gated
Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of
several thousands candidate architectures. A variant of fitted Q iteration,
based on Advantage values instead of Q values, is also explored. The results
show that GRU performs significantly better than LSTM and MUT1 for most of the
problems considered, requiring less training episodes and less CPU time before
learning a very good policy. Advantage learning also tends to produce better
results.



Gaussian Processes (GPs) are widely used tools in statistics, machine
learning, robotics, computer vision, and scientific computation. However,
despite their popularity, they can be difficult to apply; all but the simplest
classification or regression applications require specification and inference
over complex covariance functions that do not admit simple analytical
posteriors. This paper shows how to embed Gaussian processes in any
higher-order probabilistic programming language, using an idiom based on
memoization, and demonstrates its utility by implementing and extending classic
and state-of-the-art GP applications. The interface to Gaussian processes,
called gpmem, takes an arbitrary real-valued computational process as input and
returns a statistical emulator that automatically improve as the original
process is invoked and its input-output behavior is recorded. The flexibility
of gpmem is illustrated via three applications: (i) robust GP regression with
hierarchical hyper-parameter learning, (ii) discovering symbolic expressions
from time-series data by fully Bayesian structure learning over kernels
generated by a stochastic grammar, and (iii) a bandit formulation of Bayesian
optimization with automatic inference and action selection. All applications
share a single 50-line Python library and require fewer than 20 lines of
probabilistic code each.



Convolutional neural networks demonstrated outstanding empirical results in
computer vision and speech recognition tasks where labeled training data is
abundant. In medical imaging, there is a huge variety of possible imaging
modalities and contrasts, where annotated data is usually very scarce. We
present two approaches to deal with this challenge. A network pretrained in a
different domain with abundant data is used as a feature extractor, while a
subsequent classifier is trained on a small target dataset; and a deep
architecture trained with heavy augmentation and equipped with sophisticated
regularization methods. We test the approaches on a corpus of X-ray images to
design an anatomy detection system.



The paper focuses on a new class of combinatorial problems which consists in
restructuring of solutions (as sets/structures) in combinatorial optimization.
Two main features of the restructuring process are examined: (i) a cost of the
restructuring, (ii) a closeness to a goal solution. Three types of the
restructuring problems are under study: (a) one-stage structuring, (b)
multi-stage structuring, and (c) structuring over changed element set.
One-criterion and multicriteria problem formulations can be considered. The
restructuring problems correspond to redesign (improvement, upgrade) of modular
systems or solutions. The restructuring approach is described and illustrated
(problem statements, solving schemes, examples) for the following combinatorial
optimization problems: knapsack problem, multiple choice problem, assignment
problem, spanning tree problems, clustering problem, multicriteria ranking
(sorting) problem, morphological clique problem. Numerical examples illustrate
the restructuring problems and solving schemes.



Bounded rationality, that is, decision-making and planning under resource
limitations, is widely regarded as an important open problem in artificial
intelligence, reinforcement learning, computational neuroscience and economics.
This paper offers a consolidated presentation of a theory of bounded
rationality based on information-theoretic ideas. We provide a conceptual
justification for using the free energy functional as the objective function
for characterizing bounded-rational decisions. This functional possesses three
crucial properties: it controls the size of the solution space; it has Monte
Carlo planners that are exact, yet bypass the need for exhaustive search; and
it captures model uncertainty arising from lack of evidence or from interacting
with other agents having unknown intentions. We discuss the single-step
decision-making case, and show how to extend it to sequential decisions using
equivalence transformations. This extension yields a very general class of
decision problems that encompass classical decision rules (e.g. EXPECTIMAX and
MINIMAX) as limit cases, as well as trust- and risk-sensitive planning.



Online reviews are often our first port of call when considering products and
purchases online. When evaluating a potential purchase, we may have a specific
query in mind, e.g. `will this baby seat fit in the overhead compartment of a
747?' or `will I like this album if I liked Taylor Swift's 1989?'. To answer
such questions we must either wade through huge volumes of consumer reviews
hoping to find one that is relevant, or otherwise pose our question directly to
the community via a Q/A system.
  In this paper we hope to fuse these two paradigms: given a large volume of
previously answered queries about products, we hope to automatically learn
whether a review of a product is relevant to a given query. We formulate this
as a machine learning problem using a mixture-of-experts-type framework---here
each review is an `expert' that gets to vote on the response to a particular
query; simultaneously we learn a relevance function such that `relevant'
reviews are those that vote correctly. At test time this learned relevance
function allows us to surface reviews that are relevant to new queries
on-demand. We evaluate our system, Moqa, on a novel corpus of 1.4 million
questions (and answers) and 13 million reviews. We show quantitatively that it
is effective at addressing both binary and open-ended queries, and
qualitatively that it surfaces reviews that human evaluators consider to be
relevant.



Hypothetical Datalog is based on an intuitionistic semantics rather than on a
classical logic semantics, and embedded implications are allowed in rule
bodies. While the usual implication (i.e., the neck of a Horn clause) stands
for inferring facts, an embedded implication plays the role of assuming its
premise for deriving its consequence. A former work introduced both a formal
framework and a goal-oriented tabled implementation, allowing negation in rule
bodies. While in that work positive assumptions for both facts and rules can
occur in the premise, negative assumptions are not allowed. In this work, we
cover this subject by introducing a new concept: a restricted predicate, which
allows negative assumptions by pruning the usual semantics of a predicate. This
new setting has been implemented in the deductive system DES.



Approaches to signal representation and coding theory have traditionally
focused on how to best represent signals using parsimonious representations
that incur the lowest possible distortion. Classical examples include linear
and non-linear approximations, sparse representations, and rate-distortion
theory. Very often, however, the goal of processing is to extract specific
information from the signal, and the distortion should be measured on the
extracted information. The corresponding representation should, therefore,
represent that information as parsimoniously as possible, without necessarily
accurately representing the signal itself.
  In this paper, we examine the problem of encoding signals such that
sufficient information is preserved about their pairwise distances and their
inner products. For that goal, we consider randomized embeddings as an encoding
mechanism and provide a framework to analyze their performance. We also
demonstrate that it is possible to design the embedding such that it represents
different ranges of distances with different precision. These embeddings also
allow the computation of kernel inner products with control on their inner
product-preserving properties. Our results provide a broad framework to design
and analyze embeddins, and generalize existing results in this area, such as
random Fourier kernels and universal embeddings.



We consider the Max $K$-Armed Bandit problem, where a learning agent is faced
with several stochastic arms, each a source of i.i.d. rewards of unknown
distribution. At each time step the agent chooses an arm, and observes the
reward of the obtained sample. Each sample is considered here as a separate
item with the reward designating its value, and the goal is to find an item
with the highest possible value. Our basic assumption is a known lower bound on
the {\em tail function} of the reward distributions. Under the PAC framework,
we provide a lower bound on the sample complexity of any
$(\epsilon,\delta)$-correct algorithm, and propose an algorithm that attains
this bound up to logarithmic factors. We analyze the robustness of the proposed
algorithm and in addition, we compare the performance of this algorithm to the
variant in which the arms are not distinguishable by the agent and are chosen
randomly at each stage. Interestingly, when the maximal rewards of the arms
happen to be similar, the latter approach may provide better performance.



Being able to reason in an environment with a large number of discrete
actions is essential to bringing reinforcement learning to a larger class of
problems. Recommender systems, industrial plants and language models are only
some of the many real-world tasks involving large numbers of discrete actions
for which current methods are difficult or even often impossible to apply. An
ability to generalize over the set of actions as well as sub-linear complexity
relative to the size of the set are both necessary to handle such tasks.
Current approaches are not able to provide both of these, which motivates the
work in this paper. Our proposed approach leverages prior information about the
actions to embed them in a continuous space upon which it can generalize.
Additionally, approximate nearest-neighbor methods allow for logarithmic-time
lookup complexity relative to the number of actions, which is necessary for
time-wise tractable training. This combined approach allows reinforcement
learning methods to be applied to large-scale learning problems previously
intractable with current methods. We demonstrate our algorithm's abilities on a
series of tasks having up to one million actions.



Several algorithms and tools have been developed to (semi) automate the
process of glycan identification by interpreting Mass Spectrometric data.
However, each has limitations when annotating MSn data with thousands of MS
spectra using uncurated public databases. Moreover, the existing tools are not
designed to manage MSn data where n > 2. We propose a novel software package to
automate the annotation of tandem MS data. This software consists of two major
components. The first, is a free, semi-automated MSn data interpreter called
the Glycomic Elucidation and Annotation Tool (GELATO). This tool extends and
automates the functionality of existing open source projects, namely,
GlycoWorkbench (GWB) and GlycomeDB. The second is a machine learning model
called Smart Anotation Enhancement Graph (SAGE), which learns the behavior of
glycoanalysts to select annotations generated by GELATO that emulate human
interpretation of the spectra.



Natural language inference (NLI) is a fundamentally important task in natural
language processing that has many applications. The recently released Stanford
Natural Language Inference (SNLI) corpus has made it possible to develop and
evaluate learning-centered methods such as deep neural networks for natural
language inference (NLI). In this paper, we propose a special long short-term
memory (LSTM) architecture for NLI. Our model builds on top of a recently
proposed neural attention model for NLI but is based on a significantly
different idea. Instead of deriving sentence embeddings for the premise and the
hypothesis to be used for classification, our solution uses a match-LSTM to
perform word-by-word matching of the hypothesis with the premise. This LSTM is
able to place more emphasis on important word-level matching results. In
particular, we observe that this LSTM remembers important mismatches that are
critical for predicting the contradiction or the neutral relationship label. On
the SNLI corpus, our model achieves an accuracy of 86.1%, outperforming the
state of the art.



We consider effort allocation in crowdsourcing, where we wish to assign
labeling tasks to imperfect homogeneous crowd workers to maximize overall
accuracy in a continuous-time Bayesian setting, subject to budget and time
constraints. The Bayes-optimal policy for this problem is the solution to a
partially observable Markov decision process, but the curse of dimensionality
renders the computation infeasible. Based on the Lagrangian Relaxation
technique in Adelman & Mersereau (2008), we provide a computationally tractable
instance-specific upper bound on the value of this Bayes-optimal policy, which
can in turn be used to bound the optimality gap of any other sub-optimal
policy. In an approach similar in spirit to the Whittle index for restless
multiarmed bandits, we provide an index policy for effort allocation in
crowdsourcing and demonstrate numerically that it outperforms other stateof-
arts and performs close to optimal solution.



We investigate the 3-architecture Connected Facility Location Problem arising
in the design of urban telecommunication access networks. We propose an
original optimization model for the problem that includes additional variables
and constraints to take into account wireless signal coverage. Since the
problem can prove challenging even for modern state-of-the art optimization
solvers, we propose to solve it by an original primal heuristic which combines
a probabilistic fixing procedure, guided by peculiar Linear Programming
relaxations, with an exact MIP heuristic, based on a very large neighborhood
search. Computational experiments on a set of realistic instances show that our
heuristic can find solutions associated with much lower optimality gaps than a
state-of-the-art solver.



This paper presents HEALER, a software agent that recommends sequential
intervention plans for use by homeless shelters, who organize these
interventions to raise awareness about HIV among homeless youth. HEALER's
sequential plans (built using knowledge of social networks of homeless youth)
choose intervention participants strategically to maximize influence spread,
while reasoning about uncertainties in the network. While previous work
presents influence maximizing techniques to choose intervention participants,
they do not address three real-world issues: (i) they completely fail to scale
up to real-world sizes; (ii) they do not handle deviations in execution of
intervention plans; (iii) constructing real-world social networks is an
expensive process. HEALER handles these issues via four major contributions:
(i) HEALER casts this influence maximization problem as a POMDP and solves it
using a novel planner which scales up to previously unsolvable real-world
sizes; (ii) HEALER allows shelter officials to modify its recommendations, and
updates its future plans in a deviation-tolerant manner; (iii) HEALER
constructs social networks of homeless youth at low cost, using a Facebook
application. Finally, (iv) we show hardness results for the problem that HEALER
solves. HEALER will be deployed in the real world in early Spring 2016 and is
currently undergoing testing at a homeless shelter.



In this work we propose a new deep learning tool called deep dictionary
learning. Multi-level dictionaries are learnt in a greedy fashion, one layer at
a time. This requires solving a simple (shallow) dictionary learning problem,
the solution to this is well known. We apply the proposed technique on some
benchmark deep learning datasets. We compare our results with other deep
learning tools like stacked autoencoder and deep belief network; and state of
the art supervised dictionary learning tools like discriminative KSVD and label
consistent KSVD. Our method yields better results than all.



In this paper, we propose a novel unsupervised learning method for the
lexical acquisition of words related to places visited by robots, from human
continuous speech signals. We address the problem of learning novel words by a
robot that has no prior knowledge of these words except for a primitive
acoustic model. Further, we propose a method that allows a robot to effectively
use the learned words and their meanings for self-localization tasks. The
proposed method is nonparametric Bayesian spatial concept acquisition method
(SpCoA) that integrates the generative model for self-localization and the
unsupervised word segmentation in uttered sentences via latent variables
related to the spatial concept. We implemented the proposed method SpCoA on
SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile
robot in a real environment. Further, we conducted experiments for evaluating
the performance of SpCoA. The experimental results showed that SpCoA enabled
the robot to acquire the names of places from speech sentences. They also
revealed that the robot could effectively utilize the acquired spatial concepts
and reduce the uncertainty in self-localization.



Categorical compositional distributional semantics is a model of natural
language; it combines the statistical vector space models of words with the
compositional models of grammar. We formalise in this model the generalised
quantifier theory of natural language, due to Barwise and Cooper. The
underlying setting is a compact closed category with bialgebras. We start from
a generative grammar formalisation and develop an abstract categorical
compositional semantics for it, then instantiate the abstract setting to sets
and relations and to finite dimensional vector spaces and linear maps. We prove
the equivalence of the relational instantiation to the truth theoretic
semantics of generalised quantifiers. The vector space instantiation formalises
the statistical usages of words and enables us to, for the first time, reason
about quantified phrases and sentences compositionally in distributional
semantics.



The current paper proposes a novel neural network model for recognizing
visually perceived human actions. The proposed multiple spatio-temporal scales
recurrent neural network (MSTRNN) model is derived by introducing multiple
timescale recurrent dynamics to the conventional convolutional neural network
model. One of the essential characteristics of the MSTRNN is that its
architecture imposes both spatial and temporal constraints simultaneously on
the neural activity which vary in multiple scales among different layers. As
suggested by the principle of the upward and downward causation, it is assumed
that the network can develop meaningful structures such as functional hierarchy
by taking advantage of such constraints during the course of learning. To
evaluate the characteristics of the model, the current study uses three types
of human action video dataset consisting of different types of primitive
actions and different levels of compositionality on them. The performance of
the MSTRNN in testing with these dataset is compared with the ones by other
representative deep learning models used in the field. The analysis of the
internal representation obtained through the learning with the dataset
clarifies what sorts of functional hierarchy can be developed by extracting the
essential compositionality underlying the dataset.



Performing efficient inference on Bayesian Networks (BNs), with large numbers
of densely connected variables is challenging. With exact inference methods,
such as the Junction Tree algorithm, clustering complexity can grow
exponentially with the number of nodes and so computation becomes intractable.
This paper presents a general purpose approximate inference algorithm called
Triplet Region Construction (TRC) that reduces the clustering complexity for
factorized models from worst case exponential to polynomial. We employ graph
factorization to reduce connection complexity and produce clusters of limited
size. Unlike MCMC algorithms TRC is guaranteed to converge and we present
experiments that show that TRC achieves accurate results when compared with
exact solutions.



In this paper, we extend the Maximum Satisfiability (MaxSAT) problem to
{\L}ukasiewicz logic. The MaxSAT problem for a set of formulae {\Phi} is the
problem of finding an assignment to the variables in {\Phi} that satisfies the
maximum number of formulae. Three possible solutions (encodings) are proposed
to the new problem: (1) Disjunctive Linear Relations (DLRs), (2) Mixed Integer
Linear Programming (MILP) and (3) Weighted Constraint Satisfaction Problem
(WCSP). Like its Boolean counterpart, the extended fuzzy MaxSAT will have
numerous applications in optimization problems that involve vagueness.



The information age has brought a deluge of data. Much of this is in text
form, insurmountable in scope for humans and incomprehensible in structure for
computers. Text mining is an expanding field of research that seeks to utilize
the information contained in vast document collections. General data mining
methods based on machine learning face challenges with the scale of text data,
posing a need for scalable text mining methods.
  This thesis proposes a solution to scalable text mining: generative models
combined with sparse computation. A unifying formalization for generative text
models is defined, bringing together research traditions that have used
formally equivalent models, but ignored parallel developments. This framework
allows the use of methods developed in different processing tasks such as
retrieval and classification, yielding effective solutions across different
text mining tasks. Sparse computation using inverted indices is proposed for
inference on probabilistic models. This reduces the computational complexity of
the common text mining operations according to sparsity, yielding probabilistic
models with the scalability of modern search engines.
  The proposed combination provides sparse generative models: a solution for
text mining that is general, effective, and scalable. Extensive experimentation
on text classification and ranked retrieval datasets are conducted, showing
that the proposed solution matches or outperforms the leading task-specific
methods in effectiveness, with a order of magnitude decrease in classification
times for Wikipedia article categorization with a million classes. The
developed methods were further applied in two 2014 Kaggle data mining prize
competitions with over a hundred competing teams, earning first and second
places.



Entity resolution (ER), an important and common data cleaning problem, is
about detecting data duplicate representations for the same external entities,
and merging them into single representations. Relatively recently, declarative
rules called "matching dependencies" (MDs) have been proposed for specifying
similarity conditions under which attribute values in database records are
merged. In this work we show the process and the benefits of integrating four
components of ER: (a) Building a classifier for duplicate/non-duplicate record
pairs built using machine learning (ML) techniques; (b) Use of MDs for
supporting the blocking phase of ML; (c) Record merging on the basis of the
classifier results; and (d) The use of the declarative language "LogiQL" -an
extended form of Datalog supported by the "LogicBlox" platform- for all
activities related to data processing, and the specification and enforcement of
MDs.



In recent years there is a growing interest in using deep representations for
reinforcement learning. In this paper, we present a methodology and tools to
analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a
new model, the Semi Aggregated Markov Decision Process (SAMDP), and an
algorithm that learns it automatically. The SAMDP model allows us to identify
spatio-temporal abstractions directly from features and may be used as a
sub-goal detector in future work. Using our tools we reveal that the features
learned by DQNs aggregate the state space in a hierarchical fashion, explaining
its success. Moreover, we are able to understand and describe the policies
learned by DQNs for three different Atari2600 games and suggest ways to
interpret, debug and optimize deep neural networks in reinforcement learning.



In clinical data sets we often find static information (e.g. patient gender,
blood type, etc.) combined with sequences of data that are recorded during
multiple hospital visits (e.g. medications prescribed, tests performed, etc.).
Recurrent Neural Networks (RNNs) have proven to be very successful for
modelling sequences of data in many areas of Machine Learning. In this work we
present an approach based on RNNs, specifically designed for the clinical
domain, that combines static and dynamic information in order to predict future
events. We work with a database collected in the Charit\'{e} Hospital in Berlin
that contains complete information concerning patients that underwent a kidney
transplantation. After the transplantation three main endpoints can occur:
rejection of the kidney, loss of the kidney and death of the patient. Our goal
is to predict, based on information recorded in the Electronic Health Record of
each patient, whether any of those endpoints will occur within the next six or
twelve months after each visit to the clinic. We compared different types of
RNNs that we developed for this work, with a model based on a Feedforward
Neural Network and a Logistic Regression model. We found that the RNN that we
developed based on Gated Recurrent Units provides the best performance for this
task. We also used the same models for a second task, i.e., next event
prediction, and found that here the model based on a Feedforward Neural Network
outperformed the other models. Our hypothesis is that long-term dependencies
are not as relevant in this task.



We introduce a problem set-up we call the Iterated Matching Pennies (IMP)
game and show that it is a powerful framework for the study of three problems:
adversarial learnability, conventional (i.e., non-adversarial) learnability and
approximability. Using it, we are able to derive the following theorems. (1) It
is possible to learn by example all of $\Sigma^0_1 \cup \Pi^0_1$ as well as
some supersets; (2) in adversarial learning (which we describe as a
pursuit-evasion game), the pursuer has a winning strategy (in other words,
$\Sigma^0_1$ can be learned adversarially, but $\Pi^0_1$ not); (3) some
languages in $\Pi^0_1$ cannot be approximated by any language in $\Sigma^0_1$.
  We show corresponding results also for $\Sigma^0_i$ and $\Pi^0_i$ for
arbitrary $i$.



We introduce the value iteration network (VIN): a fully differentiable neural
network with a `planning module' embedded within. VINs can learn to plan, and
are suitable for predicting outcomes that involve planning-based reasoning,
such as policies for reinforcement learning. Key to our approach is a novel
differentiable approximation of the value-iteration algorithm, which can be
represented as a convolutional neural network, and trained end-to-end using
standard backpropagation. We evaluate VIN based policies on discrete and
continuous path-planning domains, and on a natural-language based search task.
We show that by learning an explicit planning computation, VIN policies
generalize better to new, unseen domains.



We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that
(1) learns skills (i.e., temporally extended actions or options) as well as (2)
where to apply them. We believe that both (1) and (2) are necessary for a truly
general skill learning framework, which is a key building block needed to scale
up to lifelong learning agents. The ASAP framework can also solve related new
tasks simply by adapting where it applies its existing learned skills. We prove
that ASAP converges to a local optimum under natural conditions. Finally, our
experimental results, which include a RoboCup domain, demonstrate the ability
of ASAP to learn where to reuse skills as well as solve multiple tasks with
considerably less experience than solving each task from scratch.



In order to distribute the best arm identification task as close as possible
to the user's devices, on the edge of the Radio Access Network, we propose a
new problem setting, where distributed players collaborate to find the best
arm. This architecture guarantees privacy to end-users since no events are
stored. The only thing that can be observed by an adversary through the core
network is aggregated information across users. We provide a first algorithm,
Distributed Median Elimination, which is optimal in term of number of
transmitted bits and near optimal in term of speed-up factor with respect to an
optimal algorithm run independently on each player. In practice, this first
algorithm cannot handle the trade-off between the communication cost and the
speed-up factor, and requires some knowledge about the distribution of players.
Extended Distributed Median Elimination overcomes these limitations, by playing
in parallel different instances of Distributed Median Elimination and selecting
the best one. Experiments illustrate and complete the analysis. According to
the analysis, in comparison to Median Elimination performed on each player, the
proposed algorithm shows significant practical improvements.



Collaborative human activities are grounded in social and moral norms, which
humans consciously and subconsciously use to guide and constrain their
decision-making and behavior, thereby strengthening their interactions and
preventing emotional and physical harm. This type of norm-based processing is
also critical for robots in many human-robot interaction scenarios (e.g., when
helping elderly and disabled persons in assisted living facilities, or
assisting humans in assembly tasks in factories or even the space station). In
this position paper, we will briefly describe how several components in an
integrated cognitive architecture can be used to implement processes that are
required for normative human-robot interactions, especially in collaborative
tasks where actions and situations could potentially be perceived as
threatening and thus need a change in course of action to mitigate the
perceived threats.



Whether in groups of humans or groups of computer agents, collaboration is
most effective between individuals who have the ability to coordinate on a
joint strategy for collective action. However, in general a rational actor will
only intend to coordinate if that actor believes the other group members have
the same intention. This circular dependence makes rational coordination
difficult in uncertain environments if communication between actors is
unreliable and no prior agreements have been made. An important normative
question with regard to coordination in these ad hoc settings is therefore how
one can come to believe that other actors will coordinate, and with regard to
systems involving humans, an important empirical question is how humans arrive
at these expectations. We introduce an exact algorithm for computing the
infinitely recursive hierarchy of graded beliefs required for rational
coordination in uncertain environments, and we introduce a novel mechanism for
multiagent coordination that uses it. Our algorithm is valid in any environment
with a finite state space, and extensions to certain countably infinite state
spaces are likely possible. We test our mechanism for multiagent coordination
as a model for human decisions in a simple coordination game using existing
experimental data. We then explore via simulations whether modeling humans in
this way may improve human-agent collaboration.



We study a problem of allocating divisible jobs, arriving online, to workers
in a crowdsourcing setting which involves learning two parameters of
strategically behaving workers. Each job is split into a certain number of
tasks that are then allocated to workers. Each arriving job has to be completed
within a deadline and each task has to be completed satisfying an upper bound
on probability of failure. The job population is homogeneous while the workers
are heterogeneous in terms of costs, completion times, and times to failure.
The job completion time and time to failure of each worker are stochastic with
fixed but unknown means. The requester is faced with the challenge of learning
two separate parameters of each (strategically behaving) worker simultaneously,
namely, the mean job completion time and the mean time to failure. The time to
failure of a worker depends on the duration of the task handled by the worker.
Assuming non-strategic workers to start with, we solve this biparameter
learning problem by applying the Robust UCB algorithm. Then, we non-trivially
extend this algorithm to the setting where the workers are strategic about
their costs. Our proposed mechanism is dominant strategy incentive compatible
and ex-post individually rational with asymptotically optimal regret
performance.



Sum-Product Networks (SPNs) are a class of expressive yet tractable
hierarchical graphical models. LearnSPN is a structure learning algorithm for
SPNs that uses hierarchical co-clustering to simultaneously identifying similar
entities and similar features. The original LearnSPN algorithm assumes that all
the variables are discrete and there is no missing data. We introduce a
practical, simplified version of LearnSPN, MiniSPN, that runs faster and can
handle missing data and heterogeneous features common in real applications. We
demonstrate the performance of MiniSPN on standard benchmark datasets and on
two datasets from Google's Knowledge Graph exhibiting high missingness rates
and a mix of discrete and continuous features.



In this paper we construct a learning architecture for high dimensional time
series sampled by sensor arrangements. Using a redundant wavelet decomposition
on a graph constructed over the sensor locations, our algorithm is able to
construct discriminative features that exploit the mutual information between
the sensors. The algorithm then applies scattering networks to the time series
graphs to create the feature space. We demonstrate our method on a machine
olfaction problem, where one needs to classify the gas type and the location
where it originates from data sampled by an array of sensors. Our experimental
results clearly demonstrate that our method outperforms classical machine
learning techniques used in previous studies.



We provide a solution for elementary science test using instructional
materials. We posit that there is a hidden structure that explains the
correctness of an answer given the question and instructional materials and
present a unified max-margin framework that learns to find these hidden
structures (given a corpus of question-answer pairs and instructional
materials), and uses what it learns to answer novel elementary science
questions. Our evaluation shows that our framework outperforms several strong
baselines.



Concept drift has potential in smart grid analysis because the socio-economic
behaviour of consumers is not governed by the laws of physics. Likewise there
are also applications in wind power forecasting. In this paper we present
decision tree ensemble classification method based on the Random Forest
algorithm for concept drift. The weighted majority voting ensemble aggregation
rule is employed based on the ideas of Accuracy Weighted Ensemble (AWE) method.
Base learner weight in our case is computed for each sample evaluation using
base learners accuracy and intrinsic proximity measure of Random Forest. Our
algorithm exploits both temporal weighting of samples and ensemble pruning as a
forgetting strategy. We present results of empirical comparison of our method
with original random forest with incorporated "replace-the-looser" forgetting
andother state-of-the-art concept-drfit classifiers like AWE2.



Efficient exploration in complex environments remains a major challenge for
reinforcement learning. We propose bootstrapped DQN, a simple algorithm that
explores in a computationally and statistically efficient manner through use of
randomized value functions. Unlike dithering strategies such as epsilon-greedy
exploration, bootstrapped DQN carries out temporally-extended (or deep)
exploration; this can lead to exponentially faster learning. We demonstrate
these benefits in complex stochastic MDPs and in the large-scale Arcade
Learning Environment. Bootstrapped DQN substantially improves learning times
and performance across most Atari games.



The exploration of social conversations for addressing patient's needs is an
important analytical task in which many scholarly publications are contributing
to fill the knowledge gap in this area. The main difficulty remains the
inability to turn such contributions into pragmatic processes the
pharmaceutical industry can leverage in order to generate insight from social
media data, which can be considered as one of the most challenging source of
information available today due to its sheer volume and noise. This study is
based on the work by Scott Spangler and Jeffrey Kreulen and applies it to
identify structure in social media through the extraction of a topical taxonomy
able to capture the latent knowledge in social conversations in health-related
sites. The mechanism for automatically identifying and generating a taxonomy
from social conversations is developed and pressured tested using public data
from media sites focused on the needs of cancer patients and their families.
Moreover, a novel method for generating the category's label and the
determination of an optimal number of categories is presented which extends
Scott and Jeffrey's research in a meaningful way. We assume the reader is
familiar with taxonomies, what they are and how they are used.



This paper addresses the problem of detecting coherent motions in crowd
scenes and presents its two applications in crowd scene understanding: semantic
region detection and recurrent activity mining. It processes input motion
fields (e.g., optical flow fields) and produces a coherent motion filed, named
as thermal energy field. The thermal energy field is able to capture both
motion correlation among particles and the motion trends of individual
particles which are helpful to discover coherency among them. We further
introduce a two-step clustering process to construct stable semantic regions
from the extracted time-varying coherent motions. These semantic regions can be
used to recognize pre-defined activities in crowd scenes. Finally, we introduce
a cluster-and-merge process which automatically discovers recurrent activities
in crowd scenes by clustering and merging the extracted coherent motions.
Experiments on various videos demonstrate the effectiveness of our approach.



Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.



We propose and analyze an alternate approach to off-policy multi-step
temporal difference learning, in which off-policy returns are corrected with
the current Q-function in terms of rewards, rather than with the target policy
in terms of transition probabilities. We prove that such approximate
corrections are sufficient for off-policy convergence both in policy evaluation
and control, provided certain conditions. These conditions relate the distance
between the target and behavior policies, the eligibility trace parameter and
the discount factor, and formalize an underlying tradeoff in off-policy
TD($\lambda$). We illustrate this theoretical relationship empirically on a
continuous-state control task.



Recent sequential pattern mining methods have used the minimum description
length (MDL) principle to define an encoding scheme which describes an
algorithm for mining the most compressing patterns in a database. We present a
novel subsequence interleaving model based on a probabilistic model of the
sequence database, which allows us to search for the most compressing set of
patterns without designing a specific encoding scheme. Our proposed algorithm
is able to efficiently mine the most relevant sequential patterns and rank them
using an associated measure of interestingness. The efficient inference in our
model is a direct result of our use of a structural expectation-maximization
framework, in which the expectation-step takes the form of a submodular
optimization problem subject to a coverage constraint. We show on both
synthetic and real world datasets that our model mines a set of sequential
patterns with low spuriousness and redundancy, high interpretability and
usefulness in real-world applications. Furthermore, we demonstrate that the
quality of the patterns from our approach is comparable to, if not better than,
existing state of the art sequential pattern mining algorithms.



It was shown before that the NP-hard problem of deterministic finite automata
(DFA) identification can be effectively translated to Boolean satisfiability
(SAT). Modern SAT-solvers can tackle hard DFA identification instances
efficiently. We present a technique to reduce the problem search space by
enforcing an enumeration of DFA states in depth-first search (DFS) or
breadth-first search (BFS) order. We propose symmetry breaking predicates,
which can be added to Boolean formulae representing various DFA identification
problems. We show how to apply this technique to DFA identification from both
noiseless and noisy data. Also we propose a method to identify all automata of
the desired size. The proposed approach outperforms the current
state-of-the-art DFASAT method for DFA identification from noiseless data. A
big advantage of the proposed approach is that it allows to determine exactly
the existence or non-existence of a solution of the noisy DFA identification
problem unlike metaheuristic approaches such as genetic algorithms.



Most data for evaluating and training recommender systems is subject to
selection biases, either through self-selection by the users or through the
actions of the recommendation system itself. In this paper, we provide a
principled approach to handling selection biases, adapting models and
estimation techniques from causal inference. The approach leads to unbiased
performance estimators despite biased data, and to a matrix factorization
method that provides substantially improved prediction performance on
real-world data. We theoretically and empirically characterize the robustness
of the approach, finding that it is highly practical and scalable.



Inverse reinforcement learning (IRL) has become a useful tool for learning
behavioral models from demonstration data. However, IRL remains mostly
unexplored for multi-agent systems. In this paper, we show how the principle of
IRL can be extended to homogeneous large-scale problems, inspired by the
collective swarming behavior of natural systems. In particular, we make the
following contributions to the field: 1) We introduce the swarMDP framework, a
sub-class of decentralized partially observable Markov decision processes
endowed with a swarm characterization. 2) Exploiting the inherent homogeneity
of this framework, we reduce the resulting multi-agent IRL problem to a
single-agent one by proving that the agent-specific value functions in this
model coincide. 3) To solve the corresponding control problem, we propose a
novel heterogeneous learning scheme that is particularly tailored to the swarm
setting. Results on two example systems demonstrate that our framework is able
to produce meaningful local reward models from which we can replicate the
observed global system dynamics.



Deep generative models parameterized by neural networks have recently
achieved state-of-the-art performance in unsupervised and semi-supervised
learning. We extend deep generative models with auxiliary variables which
improves the variational approximation. The auxiliary variables leave the
generative model unchanged but make the variational distribution more
expressive. Inspired by the structure of the auxiliary variable we also propose
a model with two stochastic layers and skip connections. Our findings suggest
that more expressive and properly specified deep generative models converge
faster with better results. We show state-of-the-art performance within
semi-supervised learning on MNIST, SVHN and NORB datasets.



In this paper, we introduce a notion of backdoors to Reiter's propositional
default logic and study structural properties of it. Also we consider the
problems of backdoor detection (parameterised by the solution size) as well as
backdoor evaluation (parameterised by the size of the given backdoor), for
various kinds of target classes (cnf, horn, krom, monotone, identity). We show
that backdoor detection is fixed-parameter tractable for the considered target
classes, and backdoor evaluation is either fixed-parameter tractable, in
para-DP2 , or in para-NP, depending on the target class.



Complex network topologies and hyperbolic geometry seem specularly connected,
and one of the most fascinating and challenging problems of recent complex
network theory is to map a given network to its hyperbolic space. The
Popularity Similarity Optimization (PSO) model represents - at the moment - the
climax of this theory. It suggests that the trade-off between node popularity
and similarity is a mechanism to explain how complex network topologies emerge
- as discrete samples - from the continuous world of hyperbolic geometry. The
hyperbolic space seems appropriate to represent real complex networks. In fact,
it preserves many of their fundamental topological properties, and can be
exploited for real applications such as, among others, link prediction and
community detection. Here, we observe for the first time that a
topological-based machine learning class of algorithms - for nonlinear
unsupervised dimensionality reduction - can directly approximate the network's
node angular coordinates of the hyperbolic model into a two-dimensional space,
according to a similar topological organization that we named angular
coalescence. On the basis of this phenomenon, we propose a new class of
algorithms that offers fast and accurate coalescent embedding of networks in
the hyperbolic space even for graphs with thousands of nodes.



Storytelling algorithms aim to 'connect the dots' between disparate documents
by linking starting and ending documents through a series of intermediate
documents. Existing storytelling algorithms are based on notions of coherence
and connectivity, and thus the primary way by which users can steer the story
construction is via design of suitable similarity functions. We present an
alternative approach to storytelling wherein the user can interactively and
iteratively provide 'must use' constraints to preferentially support the
construction of some stories over others. The three innovations in our approach
are distance measures based on (inferred) topic distributions, the use of
constraints to define sets of linear inequalities over paths, and the
introduction of slack and surplus variables to condition the topic distribution
to preferentially emphasize desired terms over others. We describe experimental
results to illustrate the effectiveness of our interactive storytelling
approach over multiple text datasets.



Although RNNs have been shown to be powerful tools for processing sequential
data, finding architectures or optimization strategies that allow them to model
very long term dependencies is still an active area of research. In this work,
we carefully analyze two synthetic datasets originally outlined in (Hochreiter
and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store
information over many time steps. We explicitly construct RNN solutions to
these problems, and using these constructions, illuminate both the problems
themselves and the way in which RNNs store different types of information in
their hidden states. These constructions furthermore explain the success of
recent methods that specify unitary initializations or constraints on the
transition matrices.



This paper presents a strategy to guide a mobile ground robot equipped with a
camera or depth sensor, in order to autonomously map the visible part of a
bounded three-dimensional structure. We describe motion planning algorithms
that determine appropriate successive viewpoints and attempt to fill holes
automatically in a point cloud produced by the sensing and perception layer.
The emphasis is on accurately reconstructing a 3D model of a structure of
moderate size rather than mapping large open environments, with applications
for example in architecture, construction and inspection. The proposed
algorithms do not require any initialization in the form of a mesh model or a
bounding box, and the paths generated are well adapted to situations where the
vision sensor is used simultaneously for mapping and for localizing the robot,
in the absence of additional absolute positioning system. We analyze the
coverage properties of our policy, and compare its performance to the classic
frontier based exploration algorithm. We illustrate its efficacy for different
structure sizes, levels of localization accuracy and range of the depth sensor,
and validate our design on a real-world experiment.



From smart homes that prepare coffee when we wake, to phones that know not to
interrupt us during important conversations, our collective visions of HCI
imagine a future in which computers understand a broad range of human
behaviors. Today our systems fall short of these visions, however, because this
range of behaviors is too large for designers or programmers to capture
manually. In this paper, we instead demonstrate it is possible to mine a broad
knowledge base of human behavior by analyzing more than one billion words of
modern fiction. Our resulting knowledge base, Augur, trains vector models that
can predict many thousands of user activities from surrounding objects in
modern contexts: for example, whether a user may be eating food, meeting with a
friend, or taking a selfie. Augur uses these predictions to identify actions
that people commonly take on objects in the world and estimate a user's future
activities given their current situation. We demonstrate Augur-powered,
activity-based systems such as a phone that silences itself when the odds of
you answering it are low, and a dynamic music player that adjusts to your
present activity. A field deployment of an Augur-powered wearable camera
resulted in 96% recall and 71% precision on its unsupervised predictions of
common daily activities. A second evaluation where human judges rated the
system's predictions over a broad set of input images found that 94% were rated
sensible.



The present complexity in designing web applications makes software security
a difficult goal to achieve. An attacker can explore a deployed service on the
web and attack at his/her own leisure. Moving Target Defense (MTD) in web
applications is an effective mechanism to nullify this advantage of their
reconnaissance but the framework demands a good switching strategy when
switching between multiple configurations for its web-stack. To address this
issue, we propose modeling of a real-world MTD web application as a repeated
Bayesian game. We then formulate an optimization problem that generates an
effective switching strategy while considering the cost of switching between
different web-stack configurations. To incorporate this model into a developed
MTD system, we develop an automated system for generating attack sets of Common
Vulnerabilities and Exposures (CVEs) for input attacker types with predefined
capabilities. Our framework obtains realistic reward values for the players
(defenders and attackers) in this game by using security domain expertise on
CVEs obtained from the National Vulnerability Database (NVD). We also address
the issue of prioritizing vulnerabilities that when fixed, improves the
security of the MTD system. Lastly, we demonstrate the robustness of our
proposed model by evaluating its performance when there is uncertainty about
input attacker information.



Students in online courses generate large amounts of data that can be used to
personalize the learning process and improve quality of education. In this
paper, we present the Latent Skill Embedding (LSE), a probabilistic model of
students and educational content that can be used to recommend personalized
sequences of lessons with the goal of helping students prepare for specific
assessments. Akin to collaborative filtering for recommender systems, the
algorithm does not require students or content to be described by features, but
it learns a representation using access traces. We formulate this problem as a
regularized maximum-likelihood embedding of students, lessons, and assessments
from historical student-content interactions. An empirical evaluation on
large-scale data from Knewton, an adaptive learning technology company, shows
that this approach predicts assessment results competitively with benchmark
models and is able to discriminate between lesson sequences that lead to
mastery and failure.



Online media offers opportunities to marketers to deliver brand messages to a
large audience. Advertising technology platforms enables the advertisers to
find the proper group of audiences and deliver ad impressions to them in real
time. The recent growth of the real time bidding has posed a significant
challenge on monitoring such a complicated system. With so many components we
need a reliable system that detects the possible changes in the system and
alerts the engineering team. In this paper we describe the mechanism that we
invented for recovering the representative metrics and detecting the change in
their behavior. We show that this mechanism is able to detect the possible
problems in time by describing some incident cases.



Most learning algorithms are not invariant to the scale of the function that
is being approximated. We propose to adaptively normalize the targets used in
learning. This is useful in value-based reinforcement learning, where the
magnitude of appropriate value approximations can change over time when we
update the policy of behavior. Our main motivation is prior work on learning to
play Atari games, where the rewards were all clipped to a predetermined range.
This clipping facilitates learning across many different games with a single
learning algorithm, but a clipped reward function can result in qualitatively
different behavior. Using the adaptive normalization we can remove this
domain-specific heuristic without diminishing overall performance.



The importance of accurate recommender systems has been widely recognized by
academia and industry. However, the recommendation quality is still rather low.
Recently, a linear sparse and low-rank representation of the user-item matrix
has been applied to produce Top-N recommendations. This approach uses the
nuclear norm as a convex relaxation for the rank function and has achieved
better recommendation accuracy than the state-of-the-art methods. In the past
several years, solving rank minimization problems by leveraging nonconvex
relaxations has received increasing attention. Some empirical results
demonstrate that it can provide a better approximation to original problems
than convex relaxation. In this paper, we propose a novel rank approximation to
enhance the performance of Top-N recommendation systems, where the
approximation error is controllable. Experimental results on real data show
that the proposed rank approximation improves the Top-$N$ recommendation
accuracy substantially.



Submodular function maximization finds application in a variety of real-world
decision-making problems. However, most existing methods, based on greedy
maximization, assume it is computationally feasible to evaluate F, the function
being maximized. Unfortunately, in many realistic settings F is too expensive
to evaluate exactly even once. We present probably approximately correct greedy
maximization, which requires access only to cheap anytime confidence bounds on
F and uses them to prune elements. We show that, with high probability, our
method returns an approximately optimal set. We propose novel, cheap confidence
bounds for conditional entropy, which appears in many common choices of F and
for which it is difficult to find unbiased or bounded estimates. Finally,
results on a real-world dataset from a multi-camera tracking system in a
shopping mall demonstrate that our approach performs comparably to existing
methods, but at a fraction of the computational cost.



We present weight normalization: a reparameterization of the weight vectors
in a neural network that decouples the length of those weight vectors from
their direction. By reparameterizing the weights in this way we improve the
conditioning of the optimization problem and we speed up convergence of
stochastic gradient descent. Our reparameterization is inspired by batch
normalization but does not introduce any dependencies between the examples in a
minibatch. This means that our method can also be applied successfully to
recurrent models such as LSTMs and to noise-sensitive applications such as deep
reinforcement learning or generative models, for which batch normalization is
less well suited. Although our method is much simpler, it still provides much
of the speed-up of full batch normalization. In addition, the computational
overhead of our method is lower, permitting more optimization steps to be taken
in the same amount of time. We demonstrate the usefulness of our method on
applications in supervised image recognition, generative modelling, and deep
reinforcement learning.



We discuss a variant of Thompson sampling for nonparametric reinforcement
learning in a countable classes of general stochastic environments. These
environments can be non-Markov, non-ergodic, and partially observable. We show
that Thompson sampling learns the environment class in the sense that (1)
asymptotically its value converges to the optimal value in mean and (2) given a
recoverability assumption regret is sublinear.



Ordinal peer grading has been proposed as a simple and scalable solution for
computing reliable information about student performance in massive open online
courses. The idea is to outsource the grading task to the students themselves
as follows. After the end of an exam, each student is asked to rank ---in terms
of quality--- a bundle of exam papers by fellow students. An aggregation rule
will then combine the individual rankings into a global one that contains all
students. We define a broad class of simple aggregation rules and present a
theoretical framework for assessing their effectiveness. When statistical
information about the grading behaviour of students is available, the framework
can be used to compute the optimal rule from this class with respect to a
series of performance objectives. For example, a natural rule known as Borda is
proved to be optimal when students grade correctly. In addition, we present
extensive simulations and a field experiment that validate our theory and prove
it to be extremely accurate in predicting the performance of aggregation rules
even when only rough information about grading behaviour is available.



Learning models of artificial intelligence can nowadays perform very well on
a large variety of tasks. However, in practice different task environments are
best handled by different learning models, rather than a single, universal,
approach. Most non-trivial models thus require the adjustment of several to
many learning parameters, which is often done on a case-by-case basis by an
external party. Meta-learning refers to the ability of an agent to autonomously
and dynamically adjust its own learning parameters, or meta-parameters. In this
work we show how projective simulation, a recently developed model of
artificial intelligence, can naturally be extended to account for meta-learning
in reinforcement learning settings. The projective simulation approach is based
on a random walk process over a network of clips. The suggested meta-learning
scheme builds upon the same design and employs clip networks to monitor the
agent's performance and to adjust its meta-parameters "on the fly". We
distinguish between "reflexive adaptation" and "adaptation through learning",
and show the utility of both approaches. In addition, a trade-off between
flexibility and learning-time is addressed. The extended model is examined on
three different kinds of reinforcement learning tasks, in which the agent has
different optimal values of the meta-parameters, and is shown to perform well,
reaching near-optimal to optimal success rates in all of them, without ever
needing to manually adjust any meta-parameter.



Decision making is a vital function in the age of machine learning and
artificial intelligence; however, its physical realizations and their
theoretical fundamentals are not yet known. In our former study, we
demonstrated that single photons can be used to make decisions in uncertain,
dynamically changing environments. The multi-armed bandit problem was
successfully solved using the dual probabilistic and particle attributes of
single photons. Herein, we revolutionize how decision making is comprehended
via a category theoretic viewpoint; we present the category theoretic
foundation of the single-photon-based decision making, including quantitative
analysis that agrees well with the experimental results. The category theoretic
model unveils complex interdependencies of the entities of the subject matter
in the most simplified manner, including a dynamically changing environment. In
particular, the octahedral structure and the braid structure in triangulated
categories provide clear understandings and quantitative metrics of the
underlying mechanisms for the single-photon decision maker. This is the first
demonstration of a category theoretic interpretation of decision making, and
provides a solid understanding and a design fundamental for machine learning
and artificial intelligence.



Bounded rational decision-makers transform sensory input into motor output
under limited computational resources. Mathematically, such decision-makers can
be modeled as information-theoretic channels with limited transmission rate.
Here, we apply this formalism for the first time to multilayer feedforward
neural networks. We derive synaptic weight update rules for two scenarios,
where either each neuron is considered as a bounded rational decision-maker or
the network as a whole. In the update rules, bounded rationality translates
into information-theoretically motivated types of regularization in weight
space. In experiments on the MNIST benchmark classification task for
handwritten digits, we show that such information-theoretic regularization
successfully prevents overfitting across different architectures and attains
results that are competitive with other recent techniques like dropout,
dropconnect and Bayes by backprop, for both ordinary and convolutional neural
networks.



Following the recent trend in explicit neural memory structures, we present a
new design of an external memory, wherein memories are stored in an Euclidean
key space $\mathbb R^n$. An LSTM controller performs read and write via
specialized read and write heads. It can move a head by either providing a new
address in the key space (aka random access) or moving from its previous
position via a Lie group action (aka Lie access). In this way, the "L" and "R"
instructions of a traditional Turing Machine are generalized to arbitrary
elements of a fixed Lie group action. For this reason, we name this new model
the Lie Access Neural Turing Machine, or LANTM.
  We tested two different configurations of LANTM against an LSTM baseline in
several basic experiments. We found the right configuration of LANTM to
outperform the baseline in all of our experiments. In particular, we trained
LANTM on addition of $k$-digit numbers for $2 \le k \le 16$, but it was able to
generalize almost perfectly to $17 \le k \le 32$, all with the number of
parameters 2 orders of magnitude below the LSTM baseline.



Off-policy reinforcement learning has many applications including: learning
from demonstration, learning multiple goal seeking policies in parallel, and
representing predictive knowledge. Recently there has been an proliferation of
new policy-evaluation algorithms that fill a longstanding algorithmic void in
reinforcement learning: combining robustness to off-policy sampling, function
approximation, linear complexity, and temporal difference (TD) updates. This
paper contains two main contributions. First, we derive two new hybrid TD
policy-evaluation algorithms, which fill a gap in this collection of
algorithms. Second, we perform an empirical comparison to elicit which of these
new linear TD methods should be preferred in different situations, and make
concrete suggestions about practical use.



A key problem in reinforcement learning for control with general function
approximators (such as deep neural networks and other nonlinear functions) is
that, for many algorithms employed in practice, updates to the policy or
$Q$-function may fail to improve performance---or worse, actually cause the
policy performance to degrade. Prior work has addressed this for policy
iteration by deriving tight policy improvement bounds; by optimizing the lower
bound on policy improvement, a better policy is guaranteed. However, existing
approaches suffer from bounds that are hard to optimize in practice because
they include sup norm terms which cannot be efficiently estimated or
differentiated. In this work, we derive a better policy improvement bound where
the sup norm of the policy divergence has been replaced with an average
divergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that
generates sequences of policies with guaranteed non-decreasing returns and is
easy to implement in a sample-based framework.



Recursive neural networks (RNN) and their recently proposed extension
recursive long short term memory networks (RLSTM) are models that compute
representations for sentences, by recursively combining word embeddings
according to an externally provided parse tree. Both models thus, unlike
recurrent networks, explicitly make use of the hierarchical structure of a
sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the
vanishing gradient and long distance dependency problem, and that RLSTMs
greatly improve over RNN's on these problems. We present an artificial learning
task that allows us to quantify the severity of these problems for both models.
We further show that a ratio of gradients (at the root node and a focal leaf
node) is highly indicative of the success of backpropagation at optimizing the
relevant weights low in the tree. This paper thus provides an explanation for
existing, superior results of RLSTMs on tasks such as sentiment analysis, and
suggests that the benefits of including hierarchical structure and of including
LSTM-style gating are complementary.



Reinforcement learning can acquire complex behaviors from high-level
specifications. However, defining a cost function that can be optimized
effectively and encodes the correct task is challenging in practice. We explore
how inverse optimal control (IOC) can be used to learn behaviors from
demonstrations, with applications to torque control of high-dimensional robotic
systems. Our method addresses two key challenges in inverse optimal control:
first, the need for informative features and effective regularization to impose
structure on the cost, and second, the difficulty of learning the cost function
under unknown dynamics for high-dimensional continuous systems. To address the
former challenge, we present an algorithm capable of learning arbitrary
nonlinear cost functions, such as neural networks, without meticulous feature
engineering. To address the latter challenge, we formulate an efficient
sample-based approximation for MaxEnt IOC. We evaluate our method on a series
of simulated tasks and real-world robotic manipulation problems, demonstrating
substantial improvement over prior methods both in terms of task complexity and
sample efficiency.



Model-free reinforcement learning has been successfully applied to a range of
challenging problems, and has recently been extended to handle large neural
network policies and value functions. However, the sample complexity of
model-free algorithms, particularly when using high-dimensional function
approximators, tends to limit their applicability to physical systems. In this
paper, we explore algorithms and representations to reduce the sample
complexity of deep reinforcement learning for continuous control tasks. We
propose two complementary techniques for improving the efficiency of such
algorithms. First, we derive a continuous variant of the Q-learning algorithm,
which we call normalized adantage functions (NAF), as an alternative to the
more commonly used policy gradient and actor-critic methods. NAF representation
allows us to apply Q-learning with experience replay to continuous tasks, and
substantially improves performance on a set of simulated robotic control tasks.
To further improve the efficiency of our approach, we explore the use of
learned models for accelerating model-free reinforcement learning. We show that
iteratively refitted local linear models are especially effective for this, and
demonstrate substantially faster learning on domains where such models are
applicable.



Probabilistic modeling is iterative. A scientist posits a simple model, fits
it to her data, refines it according to her analysis, and repeats. However,
fitting complex models to large data is a bottleneck in this process. Deriving
algorithms for new models can be both mathematically and computationally
challenging, which makes it difficult to efficiently cycle through the steps.
To this end, we develop automatic differentiation variational inference (ADVI).
Using our method, the scientist only provides a probabilistic model and a
dataset, nothing else. ADVI automatically derives an efficient variational
inference algorithm, freeing the scientist to refine and explore many models.
ADVI supports a broad class of models-no conjugacy assumptions are required. We
study ADVI across ten different models and apply it to a dataset with millions
of observations. ADVI is integrated into Stan, a probabilistic programming
system; it is available for immediate use.



Collaborative Filtering aims at exploiting the feedback of users to provide
personalised recommendations. Such algorithms look for latent variables in a
large sparse matrix of ratings. They can be enhanced by adding side information
to tackle the well-known cold start problem. While Neu-ral Networks have
tremendous success in image and speech recognition, they have received less
attention in Collaborative Filtering. This is all the more surprising that
Neural Networks are able to discover latent variables in large and
heterogeneous datasets. In this paper, we introduce a Collaborative Filtering
Neural network architecture aka CFN which computes a non-linear Matrix
Factorization from sparse rating inputs and side information. We show
experimentally on the MovieLens and Douban dataset that CFN outper-forms the
state of the art and benefits from side information. We provide an
implementation of the algorithm as a reusable plugin for Torch, a popular
Neural Network framework.



We represent the sequence of fMRI (Functional Magnetic Resonance Imaging)
brain volumes recorded during a cognitive stimulus by a graph which consists of
a set of local meshes. The corresponding cognitive process, encoded in the
brain, is then represented by these meshes each of which is estimated assuming
a linear relationship among the voxel time series in a predefined locality.
First, we define the concept of locality in two neighborhood systems, namely,
the spatial and functional neighborhoods. Then, we construct spatially and
functionally local meshes around each voxel, called seed voxel, by connecting
it either to its spatial or functional p-nearest neighbors. The mesh formed
around a voxel is a directed sub-graph with a star topology, where the
direction of the edges is taken towards the seed voxel at the center of the
mesh. We represent the time series recorded at each seed voxel in terms of
linear combination of the time series of its p-nearest neighbors in the mesh.
The relationships between a seed voxel and its neighbors are represented by the
edge weights of each mesh, and are estimated by solving a linear regression
equation. The estimated mesh edge weights lead to a better representation of
information in the brain for encoding and decoding of the cognitive tasks. We
test our model on a visual object recognition and emotional memory retrieval
experiments using Support Vector Machines that are trained using the mesh edge
weights as features. In the experimental analysis, we observe that the edge
weights of the spatial and functional meshes perform better than the
state-of-the-art brain decoding models.



Many real-world applications can be described as large-scale games of
imperfect information. To deal with these challenging domains, prior work has
focused on computing Nash equilibria in a handcrafted abstraction of the
domain. In this paper we introduce the first scalable end-to-end approach to
learning approximate Nash equilibria without prior domain knowledge. Our method
combines fictitious self-play with deep reinforcement learning. When applied to
Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium,
whereas common reinforcement learning methods diverged. In Limit Texas Holdem,
a poker game of real-world scale, NFSP learnt a strategy that approached the
performance of state-of-the-art, superhuman algorithms based on significant
domain expertise.



In many common interactive scenarios, participants lack information about
other participants, and specifically about the preferences of other
participants. In this work, we model an extreme case of incomplete information,
which we term games with type ambiguity, where a participant lacks even
information enabling him to form a belief on the preferences of others. Under
type ambiguity, one cannot analyze the scenario using the commonly used
Bayesian framework, and therefore he needs to model the participants using a
different decision model.
  In this work, we present the ${\rm MINthenMAX}$ decision model under
ambiguity. This model is a refinement of Wald's MiniMax principle, which we
show to be too coarse for games with type ambiguity. We characterize ${\rm
MINthenMAX}$ as the finest refinement of the MiniMax principle that satisfies
three properties we claim are necessary for games with type ambiguity. This
prior-less approach we present her also follows the common practice in computer
science of worst-case analysis.
  Finally, we define and analyze the corresponding equilibrium concept assuming
all players follow ${\rm MINthenMAX}$. We demonstrate this equilibrium by
applying it to two common economic scenarios: coordination games and bilateral
trade. We show that in both scenarios, an equilibrium in pure strategies always
exists and we analyze the equilibria.



The power grid is a complex and vital system that necessitates careful
reliability management. Managing the grid is a difficult problem with multiple
time scales of decision making and stochastic behavior due to renewable energy
generations, variable demand and unplanned outages. Solving this problem in the
face of uncertainty requires a new methodology with tractable algorithms. In
this work, we introduce a new model for hierarchical decision making in complex
systems. We apply reinforcement learning (RL) methods to learn a proxy, i.e., a
level of abstraction, for real-time power grid reliability. We devise an
algorithm that alternates between slow time-scale policy improvement, and fast
time-scale value function approximation. We compare our results to prevailing
heuristics, and show the strength of our method.



Probabilistic inference procedures are usually coded painstakingly from
scratch, for each target model and each inference algorithm. We reduce this
effort by generating inference procedures from models automatically. We make
this code generation modular by decomposing inference algorithms into reusable
program-to-program transformations. These transformations perform exact
inference as well as generate probabilistic programs that compute expectations,
densities, and MCMC samples. The resulting inference procedures are about as
accurate and fast as other probabilistic programming systems on real-world
problems.



We address the robot grasp optimization problem of unknown objects
considering uncertainty in the input space. Grasping unknown objects can be
achieved by using a trial and error exploration strategy. Bayesian optimization
is a sample efficient optimization algorithm that is especially suitable for
this setups as it actively reduces the number of trials for learning about the
function to optimize. In fact, this active object exploration is the same
strategy that infants do to learn optimal grasps. One problem that arises while
learning grasping policies is that some configurations of grasp parameters may
be very sensitive to error in the relative pose between the object and robot
end-effector. We call these configurations unsafe because small errors during
grasp execution may turn good grasps into bad grasps. Therefore, to reduce the
risk of grasp failure, grasps should be planned in safe areas. We propose a new
algorithm, Unscented Bayesian optimization that is able to perform sample
efficient optimization while taking into consideration input noise to find safe
optima. The contribution of Unscented Bayesian optimization is twofold as if
provides a new decision process that drives exploration to safe regions and a
new selection procedure that chooses the optimal in terms of its safety without
extra analysis or computational cost. Both contributions are rooted on the
strong theory behind the unscented transformation, a popular nonlinear
approximation method. We show its advantages with respect to the classical
Bayesian optimization both in synthetic problems and in realistic robot grasp
simulations. The results highlights that our method achieves optimal and robust
grasping policies after few trials while the selected grasps remain in safe
regions.



We describe a learning-based approach to hand-eye coordination for robotic
grasping from monocular images. To learn hand-eye coordination for grasping, we
trained a large convolutional neural network to predict the probability that
task-space motion of the gripper will result in successful grasps, using only
monocular camera images and independently of camera calibration or the current
robot pose. This requires the network to observe the spatial relationship
between the gripper and objects in the scene, thus learning hand-eye
coordination. We then use this network to servo the gripper in real time to
achieve successful grasps. To train our network, we collected over 800,000
grasp attempts over the course of two months, using between 6 and 14 robotic
manipulators at any given time, with differences in camera placement and
hardware. Our experimental evaluation demonstrates that our method achieves
effective real-time control, can successfully grasp novel objects, and corrects
mistakes by continuous servoing.



Additive utility function models are widely used in multiple criteria
decision analysis. In such models, a numerical value is associated to each
alternative involved in the decision problem. It is computed by aggregating the
scores of the alternative on the different criteria of the decision problem.
The score of an alternative is determined by a marginal value function that
evolves monotonically as a function of the performance of the alternative on
this criterion. Determining the shape of the marginals is not easy for a
decision maker. It is easier for him/her to make statements such as
"alternative $a$ is preferred to $b$". In order to help the decision maker, UTA
disaggregation procedures use linear programming to approximate the marginals
by piecewise linear functions based only on such statements. In this paper, we
propose to infer polynomials and splines instead of piecewise linear functions
for the marginals. In this aim, we use semidefinite programming instead of
linear programming. We illustrate this new elicitation method and present some
experimental results.



Without discourse connectives, classifying implicit discourse relations is a
challenging task and a bottleneck for building a practical discourse parser.
Previous research usually makes use of one kind of discourse framework such as
PDTB or RST to improve the classification performance on discourse relations.
Actually, under different discourse annotation frameworks, there exist multiple
corpora which have internal connections. To exploit the combination of
different discourse corpora, we design related discourse classification tasks
specific to a corpus, and propose a novel Convolutional Neural Network embedded
multi-task learning system to synthesize these tasks by learning both unique
and shared representations for each task. The experimental results on the PDTB
implicit discourse relation classification task demonstrate that our model
achieves significant gains over baseline systems.



Recognizing the activities of daily living plays an important role in
healthcare. It is necessary to use an adapted model to simulate the human
behavior in a domestic space to monitor the patient harmonically and to
intervene in the necessary time. In this paper, we tackle this problem using
the hierarchical hidden Markov model for representing and recognizing complex
indoor activities. We propose a new grammar, called "Home By Room Activities
Language", to facilitate the complexity of human scenarios and consider the
abnormal activities.



In this paper, we consider the problem of actively learning a linear
classifier through query synthesis where the learner can construct artificial
queries in order to estimate the true decision boundaries. This problem has
recently gained a lot of interest in automated science and adversarial reverse
engineering for which only heuristic algorithms are known. In such
applications, queries can be constructed de novo to elicit information (e.g.,
automated science) or to evade detection with minimal cost (e.g., adversarial
reverse engineering). We develop a general framework, called dimension coupling
(DC), that 1) reduces a d-dimensional learning problem to d-1 low dimensional
sub-problems, 2) solves each sub-problem efficiently, 3) appropriately
aggregates the results and outputs a linear classifier, and 4) provides a
theoretical guarantee for all possible schemes of aggregation. The proposed
method is proved resilient to noise. We show that the DC framework avoids the
curse of dimensionality: its computational complexity scales linearly with the
dimension. Moreover, we show that the query complexity of DC is near optimal
(within a constant factor of the optimum algorithm). To further support our
theoretical analysis, we compare the performance of DC with the existing work.
We observe that DC consistently outperforms the prior arts in terms of query
complexity while often running orders of magnitude faster.



Game balancing is an important part of the (computer) game design process, in
which designers adapt a game prototype so that the resulting gameplay is as
entertaining as possible. In industry, the evaluation of a game is often based
on costly playtests with human players. It suggests itself to automate this
process using surrogate models for the prediction of gameplay and outcome. In
this paper, the feasibility of automatic balancing using simulation- and
deck-based objectives is investigated for the card game top trumps.
Additionally, the necessity of a multi-objective approach is asserted by a
comparison with the only known (single-objective) method. We apply a
multi-objective evolutionary algorithm to obtain decks that optimise
objectives, e.g. win rate and average number of tricks, developed to express
the fairness and the excitement of a game of top trumps. The results are
compared with decks from published top trumps decks using simulation-based
objectives. The possibility to generate decks better or at least as good as
decks from published top trumps decks in terms of these objectives is
demonstrated. Our results indicate that automatic balancing with the presented
approach is feasible even for more complex games such as real-time strategy
games.



The Maximum Satisfiability (MaxSAT) problem is the problem of finding a truth
assignment that maximizes the number of satisfied clauses of a given Boolean
formula in Conjunctive Normal Form (CNF). Many exact solvers for MaxSAT have
been developed during recent years, and many of them were presented in the
well-known SAT conference. Algorithms for MaxSAT generally fall into two
categories: (1) branch and bound algorithms and (2) algorithms that use
successive calls to a SAT solver (SAT- based), which this paper in on. In
practical problems, SAT-based algorithms have been shown to be more efficient.
This paper provides an experimental investigation to compare the performance of
recent SAT-based and branch and bound algorithms on the benchmarks of the
MaxSAT Evaluations.



Robots assisting the disabled or elderly must perform complex manipulation
tasks and must adapt to the home environment and preferences of their user.
Learning from demonstration is a promising choice, that would allow the
non-technical user to teach the robot different tasks. However, collecting
demonstrations in the home environment of a disabled user is time consuming,
disruptive to the comfort of the user, and presents safety challenges. It would
be desirable to perform the demonstrations in a virtual environment. In this
paper we describe a solution to the challenging problem of behavior transfer
from virtual demonstration to a physical robot. The virtual demonstrations are
used to train a deep neural network based controller, which is using a Long
Short Term Memory (LSTM) recurrent neural network to generate trajectories. The
training process uses a Mixture Density Network (MDN) to calculate an error
signal suitable for the multimodal nature of demonstrations. The controller
learned in the virtual environment is transferred to a physical robot (a
Rethink Robotics Baxter). An off-the-shelf vision component is used to
substitute for geometric knowledge available in the simulation and an inverse
kinematics module is used to allow the Baxter to enact the trajectory. Our
experimental studies validate the three contributions of the paper: (1) the
controller learned from virtual demonstrations can be used to successfully
perform the manipulation tasks on a physical robot, (2) the LSTM+MDN
architectural choice outperforms other choices, such as the use of feedforward
networks and mean-squared error based training signals and (3) allowing
imperfect demonstrations in the training set also allows the controller to
learn how to correct its manipulation mistakes.



Single Index Models (SIMs) are simple yet flexible semi-parametric models for
machine learning, where the response variable is modeled as a monotonic
function of a linear combination of features. Estimation in this context
requires learning both the feature weights and the nonlinear function that
relates features to observations. While methods have been described to learn
SIMs in the low dimensional regime, a method that can efficiently learn SIMs in
high dimensions, and under general structural assumptions, has not been
forthcoming. In this paper, we propose computationally efficient algorithms for
SIM inference in high dimensions with structural constraints. Our general
approach specializes to sparsity, group sparsity, and low-rank assumptions
among others. Experiments show that the proposed method enjoys superior
predictive performance when compared to generalized linear models, and achieves
results comparable to or better than single layer feedforward neural networks
with significantly less computational cost.



In this paper we model the problem of learning preferences of a population as
an active learning problem. We propose an algorithm can adaptively choose pairs
of items to show to users coming from a heterogeneous population, and use the
obtained reward to decide which pair of items to show next. We provide
computationally efficient algorithms with provable sample complexity guarantees
for this problem in both the noiseless and noisy cases. In the process of
establishing sample complexity guarantees for our algorithms, we establish new
results using a Nystr{\"o}m-like method which can be of independent interest.
We supplement our theoretical results with experimental comparisons.



High-dimensional observations and complex real-world dynamics present major
challenges in reinforcement learning for both function approximation and
exploration. We address both of these challenges with two complementary
techniques: First, we develop a gradient-boosting style, non-parametric
function approximator for learning on $Q$-function residuals. And second, we
propose an exploration strategy inspired by the principles of state abstraction
and information acquisition under uncertainty. We demonstrate the empirical
effectiveness of these techniques, first, as a preliminary check, on two
standard tasks (Blackjack and $n$-Chain), and then on two much larger and more
realistic tasks with high-dimensional observation spaces. Specifically, we
introduce two benchmarks built within the game Minecraft where the observations
are pixel arrays of the agent's visual field. A combination of our two
algorithmic techniques performs competitively on the standard
reinforcement-learning tasks while consistently and substantially outperforming
baselines on the two tasks with high-dimensional observation spaces. The new
function approximator, exploration strategy, and evaluation benchmarks are each
of independent interest in the pursuit of reinforcement-learning methods that
scale to real-world domains.



Many Collaborative Filtering (CF) algorithms are item-based in the sense that
they analyze item-item relations in order to produce item similarities.
Recently, several works in the field of Natural Language Processing (NLP)
suggested to learn a latent representation of words using neural embedding
algorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known
as word2vec, was shown to provide state-of-the-art results on various
linguistics tasks. In this paper, we show that item-based CF can be cast in the
same framework of neural word embedding. Inspired by SGNS, we describe a method
we name item2vec for item-based CF that produces embedding for items in a
latent space. The method is capable of inferring item-item relations even when
user information is not available. We present experimental results that
demonstrate the effectiveness of the item2vec method and show it is competitive
with SVD.



Learning the influence structure of multiple time series data is of great
interest to many disciplines. This paper studies the problem of recovering the
causal structure in network of multivariate linear Hawkes processes. In such
processes, the occurrence of an event in one process affects the probability of
occurrence of new events in some other processes. Thus, a natural notion of
causality exists between such processes captured by the support of the
excitation matrix. We show that the resulting causal influence network is
equivalent to the Directed Information graph (DIG) of the processes, which
encodes the causal factorization of the joint distribution of the processes.
Furthermore, we present an algorithm for learning the support of excitation
matrix (or equivalently the DIG). The performance of the algorithm is evaluated
on synthesized multivariate Hawkes networks as well as a stock market and
MemeTracker real-world dataset.



Domain adaptation algorithms are useful when the distributions of the
training and the test data are different. In this paper, we focus on the
problem of instrumental variation and time-varying drift in the field of
sensors and measurement, which can be viewed as discrete and continuous
distributional change in the feature space. We propose maximum independence
domain adaptation (MIDA) and semi-supervised MIDA (SMIDA) to address this
problem. Domain features are first defined to describe the background
information of a sample, such as the device label and acquisition time. Then,
MIDA learns a subspace which has maximum independence with the domain features,
so as to reduce the inter-domain discrepancy in distributions. A feature
augmentation strategy is also designed to project samples according to their
backgrounds so as to improve the adaptation. The proposed algorithms are
flexible and fast. Their effectiveness is verified by experiments on synthetic
datasets and four real-world ones on sensors, measurement, and computer vision.
They can greatly enhance the practicability of sensor systems, as well as
extend the application scope of existing domain adaptation algorithms by
uniformly handling different kinds of distributional change.



Sequential decision making under uncertainty is studied in a mixed
observability domain. The goal is to maximize the amount of information
obtained on a partially observable stochastic process under constraints imposed
by a fully observable internal state. An upper bound for the optimal value
function is derived by relaxing constraints. We identify conditions under which
the relaxed problem is a multi-armed bandit whose optimal policy is easily
computable. The upper bound is applied to prune the search space in the
original problem, and the effect on solution quality is assessed via simulation
experiments. Empirical results show effective pruning of the search space in a
target monitoring domain.



Humans have an impressive ability to reason about new concepts and
experiences from just a single example. In particular, humans have an ability
for one-shot generalization: an ability to encounter a new concept, understand
its structure, and then be able to generate compelling alternative variations
of the concept. We develop machine learning systems with this important
capacity by developing new deep generative models, models that combine the
representational power of deep learning with the inferential power of Bayesian
reasoning. We develop a class of sequential generative models that are built on
the principles of feedback and attention. These two characteristics lead to
generative models that are among the state-of-the art in density estimation and
image generation. We demonstrate the one-shot generalization ability of our
models using three tasks: unconditional sampling, generating new exemplars of a
given concept, and generating new exemplars of a family of concepts. In all
cases our models are able to generate compelling and diverse samples---having
seen new examples just once---providing an important class of general-purpose
models for one-shot machine learning.



Many deep Convolutional Neural Networks (CNN) make incorrect predictions on
adversarial samples obtained by imperceptible perturbations of clean samples.
We hypothesize that this is caused by a failure to suppress unusual signals
within network layers. As remedy we propose the use of Symmetric Activation
Functions (SAF) in non-linear signal transducer units. These units suppress
signals of exceptional magnitude. We prove that SAF networks can perform
classification tasks to arbitrary precision in a simplified situation. In
practice, rather than use SAFs alone, we add them into CNNs to improve their
robustness. The modified CNNs can be easily trained using popular strategies
with the moderate training load. Our experiments on MNIST and CIFAR-10 show
that the modified CNNs perform similarly to plain ones on clean samples, and
are remarkably more robust against adversarial and nonsense samples.



This paper proposes a new method for an optimized mapping of temporal
variables, describing a temporal stream data, into the recently proposed
NeuCube spiking neural network architecture. This optimized mapping extends the
use of the NeuCube, which was initially designed for spatiotemporal brain data,
to work on arbitrary stream data and to achieve a better accuracy of temporal
pattern recognition, a better and earlier event prediction and a better
understanding of complex temporal stream data through visualization of the
NeuCube connectivity. The effect of the new mapping is demonstrated on three
bench mark problems. The first one is early prediction of patient sleep stage
event from temporal physiological data. The second one is pattern recognition
of dynamic temporal patterns of traffic in the Bay Area of California and the
last one is the Challenge 2012 contest data set. In all cases the use of the
proposed mapping leads to an improved accuracy of pattern recognition and event
prediction and a better understanding of the data when compared to traditional
machine learning techniques or spiking neural network reservoirs with arbitrary
mapping of the variables.



There has been an explosion of work in the vision & language community during
the past few years from image captioning to video transcription, and answering
questions about images. These tasks have focused on literal descriptions of the
image. To move beyond the literal, we choose to explore how questions about an
image are often directed at commonsense inference and the abstract events
evoked by objects in the image. In this paper, we introduce the novel task of
Visual Question Generation (VQG), where the system is tasked with asking a
natural and engaging question when shown an image. We provide three datasets
which cover a variety of images from object-centric to event-centric, with
considerably more abstract training data than provided to state-of-the-art
captioning systems thus far. We train and test several generative and retrieval
models to tackle the task of VQG. Evaluation results show that while such
models ask reasonable questions for a variety of images, there is still a wide
gap with human performance which motivates further work on connecting images
with commonsense knowledge and pragmatics. Our proposed task offers a new
challenge to the community which we hope furthers interest in exploring deeper
connections between vision & language.



We review the task of Sentence Pair Scoring, popular in the literature in
various forms - viewed as Answer Sentence Selection, Semantic Text Scoring,
Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a
component of Memory Networks.
  We argue that all such tasks are similar from the model perspective and
propose new baselines by comparing the performance of common IR metrics and
popular convolutional, recurrent and attention-based neural models across many
Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating
randomized models, propose a statistically grounded methodology, and attempt to
improve comparisons by releasing new datasets that are much harder than some of
the currently used well explored benchmarks. We introduce a unified open source
software framework with easily pluggable models and tasks, which enables us to
experiment with multi-task reusability of trained sentence model. We set a new
state-of-art in performance on the Ubuntu Dialogue dataset.



We present a method for automatically generating repair feedback for syntax
errors for introductory programming problems. Syntax errors constitute one of
the largest classes of errors (34%) in our dataset of student submissions
obtained from a MOOC course on edX. The previous techniques for generating
automated feed- back on programming assignments have focused on functional
correctness and style considerations of student programs. These techniques
analyze the program AST of the program and then perform some dynamic and
symbolic analyses to compute repair feedback. Unfortunately, it is not possible
to generate ASTs for student pro- grams with syntax errors and therefore the
previous feedback techniques are not applicable in repairing syntax errors.
  We present a technique for providing feedback on syntax errors that uses
Recurrent neural networks (RNNs) to model syntactically valid token sequences.
Our approach is inspired from the recent work on learning language models from
Big Code (large code corpus). For a given programming assignment, we first
learn an RNN to model all valid token sequences using the set of syntactically
correct student submissions. Then, for a student submission with syntax errors,
we query the learnt RNN model with the prefix to- ken sequence to predict token
sequences that can fix the error by either replacing or inserting the predicted
token sequence at the error location. We evaluate our technique on over 14, 000
student submissions with syntax errors. Our technique can completely re- pair
31.69% (4501/14203) of submissions with syntax errors and in addition partially
correct 6.39% (908/14203) of the submissions.



As the field of data science continues to grow, there will be an
ever-increasing demand for tools that make machine learning accessible to
non-experts. In this paper, we introduce the concept of tree-based pipeline
optimization for automating one of the most tedious parts of machine
learning---pipeline design. We implement an open source Tree-based Pipeline
Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a
series of simulated and real-world benchmark data sets. In particular, we show
that TPOT can design machine learning pipelines that provide a significant
improvement over a basic machine learning analysis while requiring little to no
input nor prior knowledge from the user. We also address the tendency for TPOT
to design overly complex pipelines by integrating Pareto optimization, which
produces compact pipelines without sacrificing classification accuracy. As
such, this work represents an important step toward fully automating machine
learning pipeline design.



The subpath planning problem is a branch of the path planning problem, which
has widespread applications in automated manufacturing process as well as
vehicle and robot navigation. This problem is to find the shortest path or tour
subject for travelling a set of given subpaths. The current approaches for
dealing with the subpath planning problem are all based on meta-heuristic
approaches. It is well-known that meta-heuristic based approaches have several
deficiencies. To address them, we propose a novel approximation algorithm in
the O(n^3) time complexity class, which guarantees to solve any subpath
planning problem instance with the fixed ratio bound of 2. Also, the formal
proofs of the claims, our empirical evaluation shows that our approximation
method acts much better than a state-of-the-art method, both in result and
execution time.



In many scientific and engineering applications, we are tasked with the
optimisation of an expensive to evaluate black box function $f$. Traditional
settings for this problem assume just the availability of this single function.
However, in many cases, cheap approximations to $f$ may be obtainable. For
example, the expensive real world behaviour of a robot can be approximated by a
cheap computer simulation. We can use these approximations to eliminate low
function value regions cheaply and use the expensive evaluations of $f$ in a
small but promising region and speedily identify the optimum. We formalise this
task as a \emph{multi-fidelity} bandit problem where the target function and
its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a
novel method based on upper confidence bound techniques. In our theoretical
analysis we demonstrate that it exhibits precisely the above behaviour, and
achieves better regret than strategies which ignore multi-fidelity information.
Empirically, MF-GP-UCB outperforms such naive strategies and other
multi-fidelity methods on several synthetic and real experiments.



Combining deep neural networks with structured logic rules is desirable to
harness flexibility and reduce uninterpretability of the neural models. We
propose a general framework capable of enhancing various types of neural
networks (e.g., CNNs and RNNs) with declarative first-order logic rules.
Specifically, we develop an iterative distillation method that transfers the
structured information of logic rules into the weights of neural networks. We
deploy the framework on a CNN for sentiment analysis, and an RNN for named
entity recognition. With a few highly intuitive rules, we obtain substantial
improvements and achieve state-of-the-art or comparable results to previous
best-performing systems.



We address an important problem in sequence-to-sequence (Seq2Seq) learning
referred to as copying, in which certain segments in the input sequence are
selectively replicated in the output sequence. A similar phenomenon is
observable in human language communication. For example, humans tend to repeat
entity names or even long phrases in conversation. The challenge with regard to
copying in Seq2Seq is that new machinery is needed to decide when to perform
the operation. In this paper, we incorporate copying into neural network-based
Seq2Seq learning and propose a new model called CopyNet with encoder-decoder
structure. CopyNet can nicely integrate the regular way of word generation in
the decoder with the new copying mechanism which can choose sub-sequences in
the input sequence and put them at proper places in the output sequence. Our
empirical study on both synthetic data sets and real world data sets
demonstrates the efficacy of CopyNet. For example, CopyNet can outperform
regular RNN-based model with remarkable margins on text summarization tasks.



This paper presents a system which creates and visualizes probabilistic
semantic links between concepts in a thesaurus and classes in a classification
system. For creating the links, we build on the Polylingual Labeled Topic Model
(PLL-TM). PLL-TM identifies probable thesaurus descriptors for each class in
the classification system by using information from the natural language text
of documents, their assigned thesaurus descriptors and their designated
classes. The links are then presented to users of the system in an interactive
visualization, providing them with an automatically generated overview of the
relations between the thesaurus and the classification system.



Most recent work focused on affect from facial expressions, and not as much
on body. This work focuses on body affect analysis. Affect does not occur in
isolation. Humans usually couple affect with an action in natural interactions;
for example, a person could be talking and smiling. Recognizing body affect in
sequences requires efficient algorithms to capture both the micro movements
that differentiate between happy and sad and the macro variations between
different actions. We depart from traditional approaches for time-series data
analytics by proposing a multi-task learning model that learns a shared
representation that is well-suited for action-affect classification as well as
generation. For this paper we choose Conditional Restricted Boltzmann Machines
to be our building block. We propose a new model that enhances the CRBM model
with a factored multi-task component to become Multi-Task Conditional
Restricted Boltzmann Machines (MTCRBMs). We evaluate our approach on two
publicly available datasets, the Body Affect dataset and the Tower Game
dataset, and show superior classification performance improvement over the
state-of-the-art, as well as the generative abilities of our model.



Over the past decade, large-scale supervised learning corpora have enabled
machine learning researchers to make substantial advances. However, to this
date, there are no large-scale question-answer corpora available. In this paper
we present the 30M Factoid Question-Answer Corpus, an enormous question answer
pair corpus produced by applying a novel neural network architecture on the
knowledge base Freebase to transduce facts into natural language questions. The
produced question answer pairs are evaluated both by human evaluators and using
automatic evaluation metrics, including well-established machine translation
and sentence similarity metrics. Across all evaluation criteria the
question-generation model outperforms the competing template-based baseline.
Furthermore, when presented to human evaluators, the generated questions appear
comparable in quality to real human-generated questions.



Real-world problems are very difficult to optimize. However, many researchers
have been solving benchmark problems that have been extensively investigated
for the last decades even if they have very few direct applications. The
Traveling Thief Problem (TTP) is a NP-hard optimization problem that aims to
provide a more realistic model. TTP targets particularly routing problem under
packing/loading constraints which can be found in supply chain management and
transportation. In this paper, TTP is presented and formulated mathematically.
A combined local search algorithm is proposed and compared with Random Local
Search (RLS) and Evolutionary Algorithm (EA). The obtained results are quite
promising since new better solutions were found.



Unlike traditional programs (such as operating systems or word processors)
which have large amounts of code, machine learning tasks use programs with
relatively small amounts of code (written in machine learning libraries), but
voluminous amounts of data. Just like developers of traditional programs debug
errors in their code, developers of machine learning tasks debug and fix errors
in their data. However, algorithms and tools for debugging and fixing errors in
data are less common, when compared to their counterparts for detecting and
fixing errors in code. In this paper, we consider classification tasks where
errors in training data lead to misclassifications in test points, and propose
an automated method to find the root causes of such misclassifications. Our
root cause analysis is based on Pearl's theory of causation, and uses Pearl's
PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,
encodes the computation of PS as a probabilistic program, and uses recent work
on probabilistic programs and transformations on probabilistic programs (along
with gray-box models of machine learning algorithms) to efficiently compute PS.
Psi is able to identify root causes of data errors in interesting data sets.



Bayesian inference has great promise for the privacy-preserving analysis of
sensitive data, as posterior sampling automatically preserves differential
privacy, an algorithmic notion of data privacy, under certain conditions
(Dimitrakakis et al., 2014; Wang et al., 2015). While this one posterior sample
(OPS) approach elegantly provides privacy "for free," it is data inefficient in
the sense of asymptotic relative efficiency (ARE). We show that a simple
alternative based on the Laplace mechanism, the workhorse of differential
privacy, is as asymptotically efficient as non-private posterior inference,
under general assumptions. This technique also has practical advantages
including efficient use of the privacy budget for MCMC. We demonstrate the
practicality of our approach on a time-series analysis of sensitive military
records from the Afghanistan and Iraq wars disclosed by the Wikileaks
organization.



In this paper, we propose a new deep learning approach, called neural
association model (NAM), for probabilistic reasoning in artificial
intelligence. We propose to use neural networks to model association between
any two events in a domain. Neural networks take one event as input and compute
a conditional probability of the other event to model how likely these two
events are to be associated. The actual meaning of the conditional
probabilities varies between applications and depends on how the models are
trained. In this work, as two case studies, we have investigated two NAM
structures, namely deep neural networks (DNN) and relation-modulated neural
nets (RMNN), on several probabilistic reasoning tasks in AI, including
recognizing textual entailment, triple classification in multi-relational
knowledge bases and commonsense reasoning. Experimental results on several
popular datasets derived from WordNet, FreeBase and ConceptNet have all
demonstrated that both DNNs and RMNNs perform equally well and they can
significantly outperform the conventional methods available for these reasoning
tasks. Moreover, compared with DNNs, RMNNs are superior in knowledge transfer,
where a pre-trained model can be quickly extended to an unseen relation after
observing only a few training samples. To further prove the effectiveness of
the proposed models, in this work, we have applied NAMs to solving challenging
Winograd Schema (WS) problems. Experiments conducted on a set of WS problems
prove that the proposed models have the potential for commonsense reasoning.



What makes images similar? To measure the similarity between images, they are
typically embedded in a feature-vector space, in which their distance preserve
the relative dissimilarity. However, when learning such similarity embeddings
the simplifying assumption is commonly made that images are only compared to
one unique measure of similarity. A main reason for this is that contradicting
notions of similarities cannot be captured in a single space. To address this
shortcoming, we propose Conditional Similarity Networks (CSNs) that learn
embeddings differentiated into semantically distinct subspaces that capture the
different notions of similarities. CSNs jointly learn a disentangled embedding
where features for different similarities are encoded in separate dimensions as
well as masks that select and reweight relevant dimensions to induce a subspace
that encodes a specific similarity notion. We show that our approach learns
interpretable image representations with visually relevant semantic subspaces.
Further, when evaluating on triplet questions from multiple similarity notions
our model even outperforms the accuracy obtained by training individual
specialized networks for each notion separately.



Integration between biology and information science benefits both fields.
Many related models have been proposed, such as computational visual cognition
models, computational motor control models, integrations of both and so on. In
general, the robustness and precision of recognition is one of the key problems
for object recognition models.
  In this paper, inspired by features of human recognition process and their
biological mechanisms, a new integrated and dynamic framework is proposed to
mimic the semantic extraction, concept formation and feature re-selection in
human visual processing. The main contributions of the proposed model are as
follows:
  (1) Semantic feature extraction: Local semantic features are learnt from
episodic features that are extracted from raw images through a deep neural
network;
  (2) Integrated concept formation: Concepts are formed with local semantic
information and structural information learnt through network.
  (3) Feature re-selection: When ambiguity is detected during recognition
process, distinctive features according to the difference between ambiguous
candidates are re-selected for recognition.
  Experimental results on hand-written digits and facial shape dataset show
that, compared with other methods, the new proposed model exhibits higher
robustness and precision for visual recognition, especially in the condition
when input samples are smantic ambiguous. Meanwhile, the introduced biological
mechanisms further strengthen the interaction between neuroscience and
information science.



We investigate evaluation metrics for dialogue response generation systems
where supervised labels, such as task completion, are not available. Recent
works in response generation have adopted metrics from machine translation to
compare a model's generated response to a single target response. We show that
these metrics correlate very weakly with human judgements in the non-technical
Twitter domain, and not at all in the technical Ubuntu domain. We provide
quantitative and qualitative results highlighting specific weaknesses in
existing metrics, and provide recommendations for future development of better
automatic evaluation metrics for dialogue systems.

