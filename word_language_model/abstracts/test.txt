

Understanding language goes hand in hand with the ability to integrate
complex contextual information obtained via perception. In this work, we
present a novel task for grounded language understanding: disambiguating a
sentence given a visual scene which depicts one of the possible interpretations
of that sentence. To this end, we introduce a new multimodal corpus containing
ambiguous sentences, representing a wide range of syntactic, semantic and
discourse ambiguities, coupled with videos that visualize the different
interpretations for each sentence. We address this task by extending a vision
model which determines if a sentence is depicted by a video. We demonstrate how
such a model can be adjusted to recognize different interpretations of the same
underlying sentence, allowing to disambiguate sentences in a unified fashion
across the different ambiguity types.



There exists a theory of a single general-purpose learning algorithm which
could explain the principles of its operation. This theory assumes that the
brain has some initial rough architecture, a small library of simple innate
circuits which are prewired at birth and proposes that all significant mental
algorithms can be learned. Given current understanding and observations, this
paper reviews and lists the ingredients of such an algorithm from both
architectural and functional perspectives.



Clearly explaining a rationale for a classification decision to an end-user
can be as important as the decision itself. Existing approaches for deep visual
recognition are generally opaque and do not output any justification text;
contemporary vision-language models can describe image content but fail to take
into account class-discriminative image aspects which justify visual
predictions. We propose a new model that focuses on the discriminating
properties of the visible object, jointly predicts a class label, and explains
why the predicted label is appropriate for the image. We propose a novel loss
function based on sampling and reinforcement learning that learns to generate
sentences that realize a global sentence property, such as class specificity.
Our results on a fine-grained bird species classification dataset show that our
model is able to generate explanations which are not only consistent with an
image but also more discriminative than descriptions produced by existing
captioning methods.



In this paper, we present an approach for learning a visual representation
from the raw spatiotemporal signals in videos. Our representation is learned
without supervision from semantic labels. We formulate our method as an
unsupervised sequential verification task, i.e., we determine whether a
sequence of frames from a video is in the correct temporal order. With this
simple task and no semantic labels, we learn a powerful visual representation
using a Convolutional Neural Network (CNN). The representation contains
complementary information to that learned from supervised image datasets like
ImageNet. Qualitative results show that our method captures information that is
temporally varying, such as human pose. When used as pre-training for action
recognition, our method gives significant gains over learning without external
data on benchmark datasets like UCF101 and HMDB51. To demonstrate its
sensitivity to human pose, we show results for pose estimation on the FLIC and
MPII datasets that are competitive, or better than approaches using
significantly more supervision. Our method can be combined with supervised
representations to provide an additional boost in accuracy.



COCO is a platform for Comparing Continuous Optimizers in a black-box
setting. It aims at automatizing the tedious and repetitive task of
benchmarking numerical optimization algorithms to the greatest possible extent.
We present the rationals behind the development of the platform as a general
proposition for a guideline towards better benchmarking. We detail underlying
fundamental concepts of COCO such as its definition of a problem, the idea of
instances, the relevance of target values, and runtime as central performance
measure. Finally, we give a quick overview of the basic code structure and the
available test suites.



The most well known and ubiquitous clustering problem encountered in nearly
every branch of science is undoubtedly $k$-means: given a set of data points
and a parameter $k$, select $k$ centres and partition the data points into $k$
clusters around these centres so that the sum of squares of distances of the
points to their cluster centre is minimized. Typically these data points lie
$\mathbb{R}^d$ for some $d\geq 2$.
  $k$-means and the first algorithms for it were introduced in the 1950's.
Since then, hundreds of papers have studied this problem and many algorithms
have been proposed for it. The most commonly used algorithm is known as
Lloyd-Forgy, which is also referred to as "the" $k$-means algorithm, and
various extensions of it often work very well in practice. However, they may
produce solutions whose cost is arbitrarily large compared to the optimum
solution. Kanungo et al. [2004] analyzed a simple local search heuristic to get
a polynomial-time algorithm with approximation ratio $9+\epsilon$ for any fixed
$\epsilon>0$ for $k$-means in Euclidean space.
  Finding an algorithm with a better approximation guarantee has remained one
of the biggest open questions in this area, in particular whether one can get a
true PTAS for fixed dimension Euclidean space. We settle this problem by
showing that a simple local search algorithm provides a PTAS for $k$-means in
$\mathbb{R}^d$ for any fixed $d$. More precisely, for any error parameter
$\epsilon>0$, the local search algorithm that considers swaps of up to
$\rho=d^{O(d)}\cdot{\epsilon}^{-O(d/\epsilon)}$ centres at a time finds a
solution using exactly $k$ centres whose cost is at most a
$(1+\epsilon)$-factor greater than the optimum.
  Finally, we provide the first demonstration that local search yields a PTAS
for the uncapacitated facility location problem and $k$-median with non-uniform
opening costs in doubling metrics.



Joint state and parameter estimation is a core problem for dynamic Bayesian
networks. Although modern probabilistic inference toolkits make it relatively
easy to specify large and practically relevant probabilistic models, the silver
bullet---an efficient and general online inference algorithm for such
problems---remains elusive, forcing users to write special-purpose code for
each application. We propose a novel blackbox algorithm -- a hybrid of particle
filtering for state variables and assumed density filtering for parameter
variables. It has following advantages: (a) it is efficient due to its online
nature, and (b) it is applicable to both discrete and continuous parameter
spaces . On a variety of toy and real models, our system is able to generate
more accurate results within a fixed computation budget. This preliminary
evidence indicates that the proposed approach is likely to be of practical use.



We study the worst-case adaptive optimization problem with budget constraint
that is useful for modeling various practical applications in artificial
intelligence and machine learning. We investigate the near-optimality of greedy
algorithms for this problem with both modular and non-modular cost functions.
In both cases, we prove that two simple greedy algorithms are not near-optimal
but the best between them is near-optimal if the utility function satisfies
pointwise submodularity and pointwise cost-sensitive submodularity
respectively. This implies a combined algorithm that is near-optimal with
respect to the optimal algorithm that uses half of the budget. We discuss
applications of our theoretical results and also report experiments comparing
the greedy algorithms on the active learning problem.



Neural network based approaches for sentence relation modeling automatically
generate hidden matching features from raw sentence pairs. However, the quality
of matching feature representation may not be satisfied due to complex semantic
relations such as entailment or contradiction. To address this challenge, we
propose a new deep neural network architecture that jointly leverage
pre-trained word embedding and auxiliary character embedding to learn sentence
meanings. The two kinds of word sequence representations as inputs into
multi-layer bidirectional LSTM to learn enhanced sentence representation. After
that, we construct matching features followed by another temporal CNN to learn
high-level hidden matching feature representations. Experimental results
demonstrate that our approach consistently outperforms the existing methods on
standard evaluation datasets.



Understanding physical phenomena is a key competence that enables humans and
animals to act and interact under uncertain perception in previously unseen
environments containing novel object and their configurations. Developmental
psychology has shown that such skills are acquired by infants from observations
at a very early stage.
  In this paper, we contrast a more traditional approach of taking a
model-based route with explicit 3D representations and physical simulation by
an end-to-end approach that directly predicts stability and related quantities
from appearance. We ask the question if and to what extent and quality such a
skill can directly be acquired in a data-driven way bypassing the need for an
explicit simulation.
  We present a learning-based approach based on simulated data that predicts
stability of towers comprised of wooden blocks under different conditions and
quantities related to the potential fall of the towers. The evaluation is
carried out on synthetic data and compared to human judgments on the same
stimuli.



Modern NLP models rely heavily on engineered features, which often combine
word and contextual information into complex lexical features. Such combination
results in large numbers of features, which can lead to over-fitting. We
present a new model that represents complex lexical features---comprised of
parts for words, contextual information and labels---in a tensor that captures
conjunction information among these parts. We apply low-rank tensor
approximations to the corresponding parameter tensors to reduce the parameter
space and improve prediction speed. Furthermore, we investigate two methods for
handling features that include $n$-grams of mixed lengths. Our model achieves
state-of-the-art results on tasks in relation extraction, PP-attachment, and
preposition disambiguation.



Learning from multiple-relational data which contains noise, ambiguities, or
duplicate entities is essential to a wide range of applications such as
statistical inference based on Web Linked Data, recommender systems,
computational biology, and natural language processing. These tasks usually
require working with very large and complex datasets - e.g., the Web graph -
however, current approaches to multi-relational learning are not practical for
such scenarios due to their high computational complexity and poor scalability
on large data.
  In this paper, we propose a novel and scalable approach for multi-relational
factorization based on consensus optimization. Our model, called ConsMRF, is
based on the Alternating Direction Method of Multipliers (ADMM) framework,
which enables us to optimize each target relation using a smaller set of
parameters than the state-of-the-art competitors in this task.
  Due to ADMM's nature, ConsMRF can be easily parallelized which makes it
suitable for large multi-relational data. Experiments on large Web datasets -
derived from DBpedia, Wikipedia and YAGO - show the efficiency and performance
improvement of ConsMRF over strong competitors. In addition, ConsMRF
near-linear scalability indicates great potential to tackle Web-scale problem
sizes.



We show that a character-level encoder-decoder framework can be successfully
applied to question answering with a structured knowledge base. We use our
model for single-relation question answering and demonstrate the effectiveness
of our approach on the SimpleQuestions dataset (Bordes et al., 2015), where we
improve state-of-the-art accuracy from 63.9% to 70.9%, without use of
ensembles. Importantly, our character-level model has 16x fewer parameters than
an equivalent word-level model, can be learned with significantly less data
compared to previous work, which relies on data augmentation, and is robust to
new entities in testing.



Despite the advances made in artificial intelligence, software agents, and
robotics, there is little we see today that we can truly call a fully
autonomous system. We conjecture that the main inhibitor for advancing autonomy
is lack of trust. Trusted autonomy is the scientific and engineering field to
establish the foundations and ground work for developing trusted autonomous
systems (robotics and software agents) that can be used in our daily life, and
can be integrated with humans seamlessly, naturally and efficiently.
  In this paper, we review this literature to reveal opportunities for
researchers and practitioners to work on topics that can create a leap forward
in advancing the field of trusted autonomy. We focus the paper on the `trust'
component as the uniting technology between humans and machines. Our inquiry
into this topic revolves around three sub-topics: (1) reviewing and positioning
the trust modelling literature for the purpose of trusted autonomy; (2)
reviewing a critical subset of sensor technologies that allow a machine to
sense human states; and (3) distilling some critical questions for advancing
the field of trusted autonomy. The inquiry is augmented with conceptual models
that we propose along the way by recompiling and reshaping the literature into
forms that enables trusted autonomous systems to become a reality. The paper
offers a vision for a Trusted Cyborg Swarm, an extension of our previous
Cognitive Cyber Symbiosis concept, whereby humans and machines meld together in
a harmonious, seamless, and coordinated manner.



Residual learning has recently surfaced as an effective means of constructing
very deep neural networks for object recognition. However, current incarnations
of residual networks do not allow for the modeling and integration of complex
relations between closely coupled recognition tasks or across domains. Such
problems are often encountered in multimedia applications involving large-scale
content recognition. We propose a novel extension of residual learning for deep
networks that enables intuitive learning across multiple related tasks using
cross-connections called cross-residuals. These cross-residuals connections can
be viewed as a form of in-network regularization and enables greater network
generalization. We show how cross-residual learning (CRL) can be integrated in
multitask networks to jointly train and detect visual concepts across several
tasks. We present a single multitask cross-residual network with >40% less
parameters that is able to achieve competitive, or even better, detection
performance on a visual sentiment concept detection problem normally requiring
multiple specialized single-task networks. The resulting multitask
cross-residual network also achieves better detection performance by about
10.4% over a standard multitask residual network without cross-residuals with
even a small amount of cross-task weighting.



What is the right supervisory signal to train visual representations? Current
approaches in computer vision use category labels from datasets such as
ImageNet to train ConvNets. However, in case of biological agents, visual
representation learning does not require millions of semantic labels. We argue
that biological agents use physical interactions with the world to learn visual
representations unlike current vision systems which just use passive
observations (images and videos downloaded from web). For example, babies push
objects, poke them, put them in their mouth and throw them to learn
representations. Towards this goal, we build one of the first systems on a
Baxter platform that pushes, pokes, grasps and observes objects in a tabletop
environment. It uses four different types of physical interactions to collect
more than 130K datapoints, with each datapoint providing supervision to a
shared ConvNet architecture allowing us to learn visual representations. We
show the quality of learned representations by observing neuron activations and
performing nearest neighbor retrieval on this learned representation.
Quantitatively, we evaluate our learned ConvNet on image classification tasks
and show improvements compared to learning without external data. Finally, on
the task of instance retrieval, our network outperforms the ImageNet network on
recall@1 by 3%



The uniform one-dimensional fragment of first-order logic, U1, is a recently
introduced formalism that extends two-variable logic in a natural way to
contexts with relations of all arities. We survey properties of U1 and
investigate its relationship to description logics designed to accommodate
higher arity relations, with particular attention given to DLR_reg. We also
define a description logic version of a variant of U1 and prove a range of new
results concerning the expressivity of U1 and related logics.



Machine learning techniques are often used in computer vision due to their
ability to leverage large amounts of training data to improve performance.
Unfortunately, most generic object trackers are still trained from scratch
online and do not benefit from the large number of videos that are readily
available for offline training. We propose a method for offline training of
neural networks that can track novel objects at test-time at 100 fps. Our
tracker is significantly faster than previous methods that use neural networks
for tracking, which are typically very slow to run and not practical for
real-time applications. Our tracker uses a simple feed-forward network with no
online training required. The tracker learns a generic relationship between
object motion and appearance and can be used to track novel objects that do not
appear in the training set. We test our network on a standard tracking
benchmark to demonstrate our tracker's state-of-the-art performance. Further,
our performance improves as we add more videos to our offline training set. To
the best of our knowledge, our tracker is the first neural-network tracker that
learns to track generic objects at 100 fps.



The emergence of "big data" offers unprecedented opportunities for not only
accelerating scientific advances but also enabling new modes of discovery.
Scientific progress in many disciplines is increasingly enabled by our ability
to examine natural phenomena through the computational lens, i.e., using
algorithmic or information processing abstractions of the underlying processes;
and our ability to acquire, share, integrate and analyze disparate types of
data. However, there is a huge gap between our ability to acquire, store, and
process data and our ability to make effective use of the data to advance
discovery. Despite successful automation of routine aspects of data management
and analytics, most elements of the scientific process currently require
considerable human expertise and effort. Accelerating science to keep pace with
the rate of data acquisition and data processing calls for the development of
algorithmic or information processing abstractions, coupled with formal methods
and tools for modeling and simulation of natural processes as well as major
innovations in cognitive tools for scientists, i.e., computational tools that
leverage and extend the reach of human intellect, and partner with humans on a
broad range of tasks in scientific discovery (e.g., identifying, prioritizing
formulating questions, designing, prioritizing and executing experiments
designed to answer a chosen question, drawing inferences and evaluating the
results, and formulating new questions, in a closed-loop fashion). This calls
for concerted research agenda aimed at: Development, analysis, integration,
sharing, and simulation of algorithmic or information processing abstractions
of natural processes, coupled with formal methods and tools for their analyses
and simulation; Innovations in cognitive tools that augment and extend human
intellect and partner with humans in all aspects of science.



Almost all of the work in graphical models for game theory has mirrored
previous work in probabilistic graphical models. Our work considers the
opposite direction: Taking advantage of recent advances in equilibrium
computation for probabilistic inference. We present formulations of inference
problems in Markov random fields (MRFs) as computation of equilibria in a
certain class of game-theoretic graphical models. We concretely establishes the
precise connection between variational probabilistic inference in MRFs and
correlated equilibria. No previous work exploits recent theoretical and
empirical results from the literature on algorithmic and computational game
theory on the tractable, polynomial-time computation of exact or approximate
correlated equilibria in graphical games with arbitrary, loopy graph structure.
We discuss how to design new algorithms with equally tractable guarantees for
the computation of approximate variational inference in MRFs. Also, inspired by
a previously stated game-theoretic view of state-of-the-art tree-reweighed
(TRW) message-passing techniques for belief inference as zero-sum game, we
propose a different, general-sum potential game to design approximate
fictitious-play techniques. We perform synthetic experiments evaluating our
proposed approximation algorithms with standard methods and TRW on several
classes of classical Ising models (i.e., with binary random variables). We also
evaluate the algorithms using Ising models learned from the MNIST dataset. Our
experiments show that our global approach is competitive, particularly shinning
in a class of Ising models with constant, "highly attractive" edge-weights, in
which it is often better than all other alternatives we evaluated. With a
notable exception, our more local approach was not as effective. Yet, in
fairness, almost all of the alternatives are often no better than a simple
baseline: estimate 0.5.



We study the complexity of the problem of searching for a set of patterns
that separate two given sets of strings. This problem has applications in a
wide variety of areas, most notably in data mining, computational biology, and
in understanding the complexity of genetic algorithms. We show that the basic
problem of finding a small set of patterns that match one set of strings but do
not match any string in a second set is difficult (NP-complete, W[2]-hard when
parameterized by the size of the pattern set, and APX-hard). We then perform a
detailed parameterized analysis of the problem, separating tractable and
intractable variants. In particular we show that parameterizing by the size of
pattern set and the number of strings, and the size of the alphabet and the
number of strings give FPT results, amongst others.



Many analyses of resource-allocation problems employ simplistic models of the
population. Using the example of a resource-allocation problem of Marecek et
al. [arXiv:1406.7639], we introduce rather a general behavioural model, where
the evolution of a heterogeneous population of agents is governed by a Markov
chain. Still, we are able to show that the distribution of agents across
resources converges in distribution, for suitable means of information
provision, under certain assumptions. The model and proof techniques may have
wider applicability.



Peer review, evaluation, and selection is a fundamental aspect of modern
science. Funding bodies the world over employ experts to review and select the
best proposals of those submitted for funding. The problem of peer selection,
however, is much more general: a professional society may want to give a subset
of its members awards based on the opinions of all members; an instructor for a
MOOC or online course may want to crowdsource grading; or a marketing company
may select ideas from group brainstorming sessions based on peer evaluation. We
make three fundamental contributions to the study of procedures or mechanisms
for peer selection, a specific type of group decision-making problem, studied
in computer science, economics, and political science. First, we propose a
novel mechanism that is strategyproof, i.e., agents cannot benefit by reporting
insincere valuations. Second, we demonstrate the effectiveness of our mechanism
by a comprehensive simulation-based comparison with a suite of mechanisms found
in the literature. Finally, our mechanism employs a randomized rounding
technique that is of independent interest, as it solves the apportionment
problem that arises in various settings where discrete resources such as
parliamentary representation slots need to be divided proportionally.



We consider the well-studied cake cutting problem in which the goal is to
find an envy-free allocation based on queries from $n$ agents. The problem has
received attention in computer science, mathematics, and economics. It has been
a major open problem whether there exists a discrete and bounded envy-free
protocol. We resolve the problem by proposing a discrete and bounded envy-free
protocol for any number of agents. The maximum number of queries required by
the protocol is $n^{n^{n^{n^{n^n}}}}$. We additionally show that even if we do
not run our protocol to completion, it can find in at most $n^3{(n^2)}^n$
queries a partial allocation of the cake that achieves proportionality (each
agent gets at least $1/n$ of the value of the whole cake) and envy-freeness.
Finally we show that an envy-free partial allocation can be computed in at most
$n^3{(n^2)}^n$ queries such that each agent gets a connected piece that gives
the agent at least $1/(3n)$ of the value of the whole cake.



In this paper, we present an approach for robot learning of social affordance
from human activity videos. We consider the problem in the context of
human-robot interaction: Our approach learns structural representations of
human-human (and human-object-human) interactions, describing how body-parts of
each agent move with respect to each other and what spatial relations they
should maintain to complete each sub-event (i.e., sub-goal). This enables the
robot to infer its own movement in reaction to the human body motion, allowing
it to naturally replicate such interactions.
  We introduce the representation of social affordance and propose a generative
model for its weakly supervised learning from human demonstration videos. Our
approach discovers critical steps (i.e., latent sub-events) in an interaction
and the typical motion associated with them, learning what body-parts should be
involved and how. The experimental results demonstrate that our Markov Chain
Monte Carlo (MCMC) based learning algorithm automatically discovers
semantically meaningful interactive affordance from RGB-D videos, which allows
us to generate appropriate full body motion for an agent.



Non-negative matrix factorization models based on a hierarchical
Gamma-Poisson structure capture user and item behavior effectively in extremely
sparse data sets, making them the ideal choice for collaborative filtering
applications. Hierarchical Poisson factorization (HPF) in particular has proved
successful for scalable recommendation systems with extreme sparsity. HPF,
however, suffers from a tight coupling of sparsity model (absence of a rating)
and response model (the value of the rating), which limits the expressiveness
of the latter. Here, we introduce hierarchical compound Poisson factorization
(HCPF) that has the favorable Gamma-Poisson structure and scalability of HPF to
high-dimensional extremely sparse matrices. More importantly, HCPF decouples
the sparsity model from the response model, allowing us to choose the most
suitable distribution for the response. HCPF can capture binary, non-negative
discrete, non-negative continuous, and zero-inflated continuous responses. We
compare HCPF with HPF on nine discrete and three continuous data sets and
conclude that HCPF captures the relationship between sparsity and response
better than HPF.



Inverse Reinforcement Learning (IRL) describes the problem of learning an
unknown reward function of a Markov Decision Process (MDP) from observed
behavior of an agent. Since the agent's behavior originates in its policy and
MDP policies depend on both the stochastic system dynamics as well as the
reward function, the solution of the inverse problem is significantly
influenced by both. Current IRL approaches assume that if the transition model
is unknown, additional samples from the system's dynamics are accessible, or
the observed behavior provides enough samples of the system's dynamics to solve
the inverse problem accurately. These assumptions are often not satisfied. To
overcome this, we present a gradient-based IRL approach that simultaneously
estimates the system's dynamics. By solving the combined optimization problem,
our approach takes into account the bias of the demonstrations, which stems
from the generating policy. The evaluation on a synthetic MDP and a transfer
learning task shows improvements regarding the sample efficiency as well as the
accuracy of the estimated reward functions and transition models.



We introduce the first dataset for sequential vision-to-language, and explore
how this data may be used for the task of visual storytelling. The first
release of this dataset, SIND v.1, includes 81,743 unique photos in 20,211
sequences, aligned to both descriptive (caption) and story language. We
establish several strong baselines for the storytelling task, and motivate an
automatic metric to benchmark progress. Modelling concrete description as well
as figurative and social language, as provided in this dataset and the
storytelling task, has the potential to move artificial intelligence from basic
understandings of typical visual scenes towards more and more human-like
understanding of grounded event structure and subjective expression.



Bat algorithm is a population metaheuristic proposed in 2010 which is based
on the echolocation or bio-sonar characteristics of microbats. Since its first
implementation, the bat algorithm has been used in a wide range of fields. In
this paper, we present a discrete version of the bat algorithm to solve the
well-known symmetric and asymmetric traveling salesman problems. In addition,
we propose an improvement in the basic structure of the classic bat algorithm.
To prove that our proposal is a promising approximation method, we have
compared its performance in 37 instances with the results obtained by five
different techniques: evolutionary simulated annealing, genetic algorithm, an
island based distributed genetic algorithm, a discrete firefly algorithm and an
imperialist competitive algorithm. In order to obtain fair and rigorous
comparisons, we have conducted three different statistical tests along the
paper: the Student's $t$-test, the Holm's test, and the Friedman test. We have
also compared the convergence behaviour shown by our proposal with the ones
shown by the evolutionary simulated annealing, and the discrete firefly
algorithm. The experimentation carried out in this study has shown that the
presented improved bat algorithm outperforms significantly all the other
alternatives in most of the cases.



A real-world newspaper distribution problem with recycling policy is tackled
in this work. In order to meet all the complex restrictions contained in such a
problem, it has been modeled as a rich vehicle routing problem, which can be
more specifically considered as an asymmetric and clustered vehicle routing
problem with simultaneous pickup and deliveries, variable costs and forbidden
paths (AC-VRP-SPDVCFP). This is the first study of such a problem in the
literature. For this reason, a benchmark composed by 15 instances has been also
proposed. In the design of this benchmark, real geographical positions have
been used, located in the province of Bizkaia, Spain. For the proper treatment
of this AC-VRP-SPDVCFP, a discrete firefly algorithm (DFA) has been developed.
This application is the first application of the firefly algorithm to any rich
vehicle routing problem. To prove that the proposed DFA is a promising
technique, its performance has been compared with two other well-known
techniques: an evolutionary algorithm and an evolutionary simulated annealing.
Our results have shown that the DFA has outperformed these two classic
meta-heuristics.



Existing open-domain human-computer conversation systems are typically
passive: they either synthesize or retrieve a reply provided a human-issued
utterance. It is generally presumed that humans should take the role to lead
the conversation and introduce new content when a stalemate occurs, and that
the computer only needs to "respond." In this paper, we propose
StalemateBreaker, a conversation system that can proactively introduce new
content when appropriate. We design a pipeline to determine when, what, and how
to introduce new content during human-computer conversation. We further propose
a novel reranking algorithm Bi-PageRank-HITS to enable rich interaction between
conversation context and candidate replies. Experiments show that both the
content-introducing approach and the reranking algorithm are effective. Our
full StalemateBreaker model outperforms a state-of-the-practice conversation
system by +14.4% p@1 when a stalemate occurs.



Semantic matching, which aims to determine the matching degree between two
texts, is a fundamental problem for many NLP applications. Recently, deep
learning approach has been applied to this problem and significant improvements
have been achieved. In this paper, we propose to view the generation of the
global interaction between two texts as a recursive process: i.e. the
interaction of two texts at each position is a composition of the interactions
between their prefixes as well as the word level interaction at the current
position. Based on this idea, we propose a novel deep architecture, namely
Match-SRNN, to model the recursive matching structure. Firstly, a tensor is
constructed to capture the word level interactions. Then a spatial RNN is
applied to integrate the local interactions recursively, with importance
determined by four types of gates. Finally, the matching score is calculated
based on the global interaction. We show that, after degenerated to the exact
matching scenario, Match-SRNN can approximate the dynamic programming process
of longest common subsequence. Thus, there exists a clear interpretation for
Match-SRNN. Our experiments on two semantic matching tasks showed the
effectiveness of Match-SRNN, and its ability of visualizing the learned
matching structure.



Earlier techniques of text mining included algorithms like k-means, Naive
Bayes, SVM which classify and cluster the text document for mining relevant
information about the documents. The need for improving the mining techniques
has us searching for techniques using the available algorithms. This paper
proposes one technique which uses the auxiliary information that is present
inside the text documents to improve the mining. This auxiliary information can
be a description to the content. This information can be either useful or
completely useless for mining. The user should assess the worth of the
auxiliary information before considering this technique for text mining. In
this paper, a combination of classical clustering algorithms is used to mine
the datasets. The algorithm runs in two stages which carry out mining at
different levels of abstraction. The clustered documents would then be
classified based on the necessary groups. The proposed technique is aimed at
improved results of document clustering.



Teaching machines to accomplish tasks by conversing naturally with humans is
challenging. Currently, developing task-oriented dialogue systems requires
creating multiple components and typically this involves either a large amount
of handcrafting, or acquiring costly labelled datasets to solve a statistical
learning problem for each component. In this work we introduce a neural
network-based text-in, text-out end-to-end trainable goal-oriented dialogue
system along with a new way of collecting dialogue data based on a novel
pipe-lined Wizard-of-Oz framework. This approach allows us to develop dialogue
systems easily and without making too many assumptions about the task at hand.
The results show that the model can converse with human subjects naturally
whilst helping them to accomplish tasks in a restaurant search domain.



Nowadays, there is increasing interest in the development of teamwork skills
in the educational context. This growing interest is motivated by its
pedagogical effectiveness and the fact that, in labour contexts, enterprises
organize their employees in teams to carry out complex projects. Despite its
crucial importance in the classroom and industry, there is a lack of support
for the team formation process. Not only do many factors influence team
performance, but the problem becomes exponentially costly if teams are to be
optimized. In this article, we propose a tool whose aim it is to cover such a
gap. It combines artificial intelligence techniques such as coalition structure
generation, Bayesian learning, and Belbin's role theory to facilitate the
generation of working groups in an educational context. This tool improves
current state of the art proposals in three ways: i) it takes into account the
feedback of other teammates in order to establish the most predominant role of
a student instead of self-perception questionnaires; ii) it handles uncertainty
with regard to each student's predominant team role; iii) it is iterative since
it considers information from several interactions in order to improve the
estimation of role assignments. We tested the performance of the proposed tool
in an experiment involving students that took part in three different team
activities. The experiments suggest that the proposed tool is able to improve
different teamwork aspects such as team dynamics and student satisfaction.



A negotiation team is a set of agents with common and possibly also
conflicting preferences that forms one of the parties of a negotiation. A
negotiation team is involved in two decision making processes simultaneously, a
negotiation with the opponents, and an intra-team process to decide on the
moves to make in the negotiation. This article focuses on negotiation team
decision making for circumstances that require unanimity of team decisions.
Existing agent-based approaches only guarantee unanimity in teams negotiating
in domains exclusively composed of predictable and compatible issues. This
article presents a model for negotiation teams that guarantees unanimous team
decisions in domains consisting of predictable and compatible, and also
unpredictable issues. Moreover, the article explores the influence of using
opponent, and team member models in the proposing strategies that team members
use. Experimental results show that the team benefits if team members employ
Bayesian learning to model their teammates' preferences.



In this article, an agent-based negotiation model for negotiation teams that
negotiate a deal with an opponent is presented. Agent-based negotiation teams
are groups of agents that join together as a single negotiation party because
they share an interest that is related to the negotiation process. The model
relies on a trusted mediator that coordinates and helps team members in the
decisions that they have to take during the negotiation process: which offer is
sent to the opponent, and whether the offers received from the opponent are
accepted. The main strength of the proposed negotiation model is the fact that
it guarantees unanimity within team decisions since decisions report a utility
to team members that is greater than or equal to their aspiration levels at
each negotiation round. This work analyzes how unanimous decisions are taken
within the team and the robustness of the model against different types of
manipulations. An empirical evaluation is also performed to study the impact of
the different parameters of the model.



Ambient Intelligence aims to offer personalized services and easier ways of
interaction between people and systems. Since several users and systems may
coexist in these environments, it is quite possible that entities with opposing
preferences need to cooperate to reach their respective goals. Automated
negotiation is pointed as one of the mechanisms that may provide a solution to
this kind of problems. In this article, a multi-issue bilateral bargaining
model for Ambient Intelligence domains is presented where it is assumed that
agents have computational bounded resources and do not know their opponents'
preferences. The main goal of this work is to provide negotiation models that
obtain efficient agreements while maintaining the computational cost low. A
niching genetic algorithm is used before the negotiation process to sample
one's own utility function (self-sampling). During the negotiation process,
genetic operators are applied over the opponent's and one's own offers in order
to sample new offers that are interesting for both parties. Results show that
the proposed model is capable of outperforming similarity heuristics which only
sample before the negotiation process and of obtaining similar results to
similarity heuristics which have access to all of the possible offers.



Under some circumstances, a group of individuals may need to negotiate
together as a negotiation team against another party. Unlike bilateral
negotiation between two individuals, this type of negotiations entails to adopt
an intra-team strategy for negotiation teams in order to make team decisions
and accordingly negotiate with the opponent. It is crucial to be able to
negotiate successfully with heterogeneous opponents since opponents'
negotiation strategy and behavior may vary in an open environment. While one
opponent might collaborate and concede over time, another may not be inclined
to concede. This paper analyzes the performance of recently proposed intra-team
strategies for negotiation teams against different categories of opponents:
competitors, matchers, and conceders. Furthermore, it provides an extension of
the negotiation tool Genius for negotiation teams in bilateral settings.
Consequently, this work facilitates research in negotiation teams.



In crowdsourcing when there is a lack of verification for contributed
answers, output agreement mechanisms are often used to incentivize participants
to provide truthful answers when the correct answer is hold by the majority. In
this paper, we focus on using output agreement mechanisms to elicit effort, in
addition to eliciting truthful answers, from a population of workers. We
consider a setting where workers have heterogeneous cost of effort exertion and
examine the data requester's problem of deciding the reward level in output
agreement for optimal elicitation. In particular, when the requester knows the
cost distribution, we derive the optimal reward level for output agreement
mechanisms. This is achieved by first characterizing Bayesian Nash equilibria
of output agreement mechanisms for a given reward level. When the requester
does not know the cost distribution, we develop sequential mechanisms that
combine learning the cost distribution with incentivizing effort exertion to
approximately determine the optimal reward level.



Existential rules, also known as data dependencies in Databases, have been
recently rediscovered as a promising family of languages for Ontology-based
Query Answering. In this paper, we prove that disjunctive embedded dependencies
exactly capture the class of recursively enumerable ontologies in
Ontology-based Conjunctive Query Answering (OCQA). Our expressive completeness
result does not rely on any built-in linear order on the database. To establish
the expressive completeness, we introduce a novel semantic definition for OCQA
ontologies. We also show that neither the class of disjunctive tuple-generating
dependencies nor the class of embedded dependencies is expressively complete
for recursively enumerable OCQA ontologies.



Social norms are powerful formalism in coordinating autonomous agents'
behaviour to achieve certain objectives. In this paper, we propose a dynamic
normative system to enable the reasoning of the changes of norms under
different circumstances, which cannot be done in the existing static normative
systems. We study two important problems (norm synthesis and norm recognition)
related to the autonomy of the entire system and the agents, and characterise
the computational complexities of solving these problems.



There is a consensus that human and non-human subjects experience temporal
distortions in many stages of their perceptual and decision-making systems.
Similarly, intertemporal choice research has shown that decision-makers
undervalue future outcomes relative to immediate ones. Here we combine
techniques from information theory and artificial intelligence to show how both
temporal distortions and intertemporal choice preferences can be explained as a
consequence of the coding efficiency of sensorimotor representation. In
particular, the model implies that interactions that constrain future behavior
are perceived as being both longer in duration and more valuable. Furthermore,
using simulations of artificial agents, we investigate how memory constraints
enforce a renormalization of the perceived timescales. Our results show that
qualitatively different discount functions, such as exponential and hyperbolic
discounting, arise as a consequence of an agent's probabilistic model of the
world.



This paper describes a new mechanism that might help with defining pattern
sequences, by the fact that it can produce an upper bound on the ensemble value
that can persistently oscillate with the actual values produced from each
pattern. With every firing event, a node also receives an on/off feedback
switch. If the node fires, then it sends a feedback result depending on the
input signal strength. If the input signal is positive or larger, it can store
an 'on' switch feedback for the next iteration. If the signal is negative or
smaller, it can store an 'off' switch feedback for the next iteration. If the
node does not fire, then it does not affect the current feedback situation and
receives the switch command produced by the last active pattern event for the
same neuron. The upper bound therefore also represents the largest or most
enclosing pattern set and the lower value is for the actual set of firing
patterns. If the pattern sequence repeats, it will oscillate between the two
values, allowing them to be recognised and measured more easily, over time.
Tests show that changing the sequence ordering produces different value sets,
which can also be measured.



Eliciting the preferences of a set of agents over a set of alternatives is a
problem of fundamental importance in social choice theory. Prior work on this
problem has studied the query complexity of preference elicitation for the
unrestricted domain and for the domain of single peaked preferences. In this
paper, we consider the domain of single crossing preference profiles and study
the query complexity of preference elicitation under various settings. We
consider two distinct situations: when an ordering of the voters with respect
to which the profile is single crossing is known versus when it is unknown. We
also consider different access models: when the votes can be accessed at
random, as opposed to when they are coming in a pre-defined sequence. In the
sequential access model, we distinguish two cases when the ordering is known:
the first is that sequence in which the votes appear is also a single-crossing
order, versus when it is not.
  The main contribution of our work is to provide polynomial time algorithms
with low query complexity for preference elicitation in all the above six
cases. Further, we show that the query complexities of our algorithms are
optimal up to constant factors for all but one of the above six cases. We then
present preference elicitation algorithms for profiles which are close to being
single crossing under various notions of closeness, for example, single
crossing width, minimum number of candidates | voters whose deletion makes a
profile single crossing.



We study the problem of predicting the results of computations that are too
expensive to run, via the observation of the results of smaller computations.
We model this as an online learning problem with delayed feedback, where the
length of the delay is unbounded, which we study mainly in a stochastic
setting. We show that in this setting, consistency is not possible in general,
and that optimal forecasters might not have average regret going to zero.
However, it is still possible to give algorithms that converge asymptotically
to Bayes-optimal predictions, by evaluating forecasters on specific sparse
independent subsequences of their predictions. We give an algorithm that does
this, which converges asymptotically on good behavior, and give very weak
bounds on how long it takes to converge. We then relate our results back to the
problem of predicting large computations in a deterministic setting.



While probability theory is normally applied to external environments, there
has been some recent interest in probabilistic modeling of the outputs of
computations that are too expensive to run. Since mathematical logic is a
powerful tool for reasoning about computer programs, we consider this problem
from the perspective of integrating probability and logic. Recent work on
assigning probabilities to mathematical statements has used the concept of
coherent distributions, which satisfy logical constraints such as the
probability of a sentence and its negation summing to one. Although there are
algorithms which converge to a coherent probability distribution in the limit,
this yields only weak guarantees about finite approximations of these
distributions. In our setting, this is a significant limitation: Coherent
distributions assign probability one to all statements provable in a specific
logical theory, such as Peano Arithmetic, which can prove what the output of
any terminating computation is; thus, a coherent distribution must assign
probability one to the output of any terminating computation. To model
uncertainty about computations, we propose to work with approximations to
coherent distributions. We introduce inductive coherence, a strengthening of
coherence that provides appropriate constraints on finite approximations, and
propose an algorithm which satisfies this criterion.



Online user reviews describing various products and services are now abundant
on the web. While the information conveyed through review texts and ratings is
easily comprehensible, there is a wealth of hidden information in them that is
not immediately obvious. In this study, we unlock this hidden value behind user
reviews to understand the various dimensions along which users rate products.
We learn a set of users that represent each of these dimensions and use their
ratings to predict product ratings. Specifically, we work with restaurant
reviews to identify users whose ratings are influenced by dimensions like
'Service', 'Atmosphere' etc. in order to predict restaurant ratings and
understand the variation in rating behaviour across different cuisines. While
previous approaches to obtaining product ratings require either a large number
of user ratings or a few review texts, we show that it is possible to predict
ratings with few user ratings and no review text. Our experiments show that our
approach outperforms other conventional methods by 16-27% in terms of RMSE.



It is important to have multi-agent robotic system specifications that ensure
correctness properties of safety and liveness. As these systems have
concurrency, and often have dynamic environment, the formal specification and
verification of these systems along with step-wise refinement from abstract to
concrete concepts play a major role in system correctness. Formal verification
is used for exhaustive investigation of the system space thus ensuring that
undetected failures in the behavior are excluded. We construct the system
incrementally from subcomponents, based on software architecture. The challenge
is to develop a safe multi-agent robotic system, more specifically to ensure
the correctness properties of safety and liveness. Formal specifications based
on model-checking are flexible, have a concrete syntax, and play vital role in
correctness of a multi-agent robotic system. To formally verify safety and
liveness of such systems is important because they have high concurrency and in
most of the cases have dynamic environment. We have considered a case-study of
a multi-agent robotic system for the transport of stock between storehouses to
exemplify our formal approach. Our proposed development approach allows for
formal verification during specification definition. The development process
has been classified in to four major phases of requirement specifications,
verification specifications, architecture specifications and implementation.



Two important requirements when aggregating the preferences of multiple
agents are that the outcome should be economically efficient and the
aggregation mechanism should not be manipulable. In this paper, we provide a
computer-aided proof of a sweeping impossibility using these two conditions for
randomized aggregation mechanisms. More precisely, we show that every efficient
aggregation mechanism can be manipulated for all expected utility
representations of the agents' preferences. This settles an open problem and
strengthens a number of existing theorems, including statements that were shown
within the special domain of assignment. Our proof is obtained by formulating
the claim as a satisfiability problem over predicates from real-valued
arithmetic, which is then checked using an SMT (satisfiability modulo theories)
solver. In order to verify the correctness of the result, a minimal
unsatisfiable set of constraints returned by the SMT solver was translated back
into a proof in higher-order logic, which was automatically verified by an
interactive theorem prover. To the best of our knowledge, this is the first
application of SMT solvers in computational social choice.



This paper presents a novel approach to procedural generation of urban maps
for First Person Shooter (FPS) games. A multi-agent evolutionary system is
employed to place streets, buildings and other items inside the Unity3D game
engine, resulting in playable video game levels. A computational agent is
trained using machine learning techniques to capture the intent of the game
designer as part of the multi-agent system, and to enable a semi-automated
aesthetic selection for the underlying genetic algorithm.



Embedding-based Knowledge Base Completion models have so far mostly combined
distributed representations of individual entities or relations to compute
truth scores of missing links. Facts can however also be represented using
pairwise embeddings, i.e. embeddings for pairs of entities and relations. In
this paper we explore such bigram embeddings with a flexible Factorization
Machine model and several ablations from it. We investigate the relevance of
various bigram types on the fb15k237 dataset and find relative improvements
compared to a compositional model.



Humans have an impressive ability to solve complex coordination problems in a
fully distributed manner. This ability, if learned as a set of distributed
multirobot coordination strategies, can enable programming large groups of
robots to collaborate towards complex coordination objectives in a way similar
to humans. Such strategies would offer robustness, adaptability,
fault-tolerance, and, importantly, distributed decision-making. To that end, we
have designed a networked gaming platform to investigate human group behavior,
specifically in solving complex collaborative coordinated tasks. Through this
platform, we are able to limit the communication, sensing, and actuation
capabilities provided to the players. With the aim of learning coordination
algorithms for robots in mind, we define these capabilities to mimic those of a
simple ground robot.



Restricted Boltzmann Machines (RBMs) and models derived from them have been
successfully used as basic building blocks in deep artificial neural networks
for automatic features extraction, unsupervised weights initialization, but
also as density estimators. Thus, their generative and discriminative
capabilities, but also their computational time are instrumental to a wide
range of applications. Our main contribution is to look at RBMs from a
topological perspective, bringing insights from network science. Firstly, here
we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which
naturally have a small-world topology. Secondly, we demonstrate both on
synthetic and real-world datasets that by constraining RBMs and GRBMs to a
scale-free topology (while still considering local neighborhoods and data
distribution), we reduce the number of weights that need to be computed by a
few orders of magnitude, at virtually no loss in generative performance.
Thirdly, we show that, for a fixed number of weights, our proposed sparse
models (which by design have a higher number of hidden neurons) achieve better
generative capabilities than standard fully connected RBMs and GRBMs (which by
design have a smaller number of hidden neurons), at no additional computational
costs.



In this paper we propose an approach to preference elicitation that is
suitable to large configuration spaces beyond the reach of existing
state-of-the-art approaches. Our setwise max-margin method can be viewed as a
generalization of max-margin learning to sets, and can produce a set of
"diverse" items that can be used to ask informative queries to the user.
Moreover, the approach can encourage sparsity in the parameter space, in order
to favor the assessment of utility towards combinations of weights that
concentrate on just few features. We present a mixed integer linear programming
formulation and show how our approach compares favourably with Bayesian
preference elicitation alternatives and easily scales to realistic datasets.



Recently, the long short-term memory neural network (LSTM) has attracted wide
interest due to its success in many tasks. LSTM architecture consists of a
memory cell and three gates, which looks similar to the neuronal networks in
the brain. However, there still lacks the evidence of the cognitive
plausibility of LSTM architecture as well as its working mechanism. In this
paper, we study the cognitive plausibility of LSTM by aligning its internal
architecture with the brain activity observed via fMRI when the subjects read a
story. Experiment results show that the artificial memory vector in LSTM can
accurately predict the observed sequential brain activities, indicating the
correlation between LSTM architecture and the cognitive process of story
reading.



We develop a natural language interface for human robot interaction that
implements reasoning about deep semantics in natural language. To realize the
required deep analysis, we employ methods from cognitive linguistics, namely
the modular and compositional framework of Embodied Construction Grammar (ECG)
[Feldman, 2009]. Using ECG, robots are able to solve fine-grained reference
resolution problems and other issues related to deep semantics and
compositionality of natural language. This also includes verbal interaction
with humans to clarify commands and queries that are too ambiguous to be
executed safely. We implement our NLU framework as a ROS package and present
proof-of-concept scenarios with different robots, as well as a survey on the
state of the art.



Recently, researchers have made significant progress combining the advances
in deep learning for learning feature representations with reinforcement
learning. Some notable examples include training agents to play Atari games
based on raw pixel data and to acquire advanced manipulation skills using raw
sensory inputs. However, it has been difficult to quantify progress in the
domain of continuous control due to the lack of a commonly adopted benchmark.
In this work, we present a benchmark suite of continuous control tasks,
including classic tasks like cart-pole swing-up, tasks with very high state and
action dimensionality such as 3D humanoid locomotion, tasks with partial
observations, and tasks with hierarchical structure. We report novel findings
based on the systematic evaluation of a range of implemented reinforcement
learning algorithms. Both the benchmark and reference implementations are
released at https://github.com/rllab/rllab in order to facilitate experimental
reproducibility and to encourage adoption by other researchers.



Clustering explores meaningful patterns in the non-labeled data sets. Cluster
Ensemble Selection (CES) is a new approach, which can combine individual
clustering results for increasing the performance of the final results.
Although CES can achieve better final results in comparison with individual
clustering algorithms and cluster ensemble methods, its performance can be
dramatically affected by its consensus diversity metric and thresholding
procedure. There are two problems in CES: 1) most of the diversity metrics is
based on heuristic Shannon's entropy and 2) estimating threshold values are
really hard in practice. The main goal of this paper is proposing a robust
approach for solving the above mentioned problems. Accordingly, this paper
develops a novel framework for clustering problems, which is called Weighted
Spectral Cluster Ensemble (WSCE), by exploiting some concepts from community
detection arena and graph based clustering. Under this framework, a new version
of spectral clustering, which is called Two Kernels Spectral Clustering, is
used for generating graphs based individual clustering results. Further, by
using modularity, which is a famous metric in the community detection, on the
transformed graph representation of individual clustering results, our approach
provides an effective diversity estimation for individual clustering results.
Moreover, this paper introduces a new approach for combining the evaluated
individual clustering results without the procedure of thresholding.
Experimental study on varied data sets demonstrates that the prosed approach
achieves superior performance to state-of-the-art methods.



We present an unsupervised visual feature learning algorithm driven by
context-based pixel prediction. By analogy with auto-encoders, we propose
Context Encoders -- a convolutional neural network trained to generate the
contents of an arbitrary image region conditioned on its surroundings. In order
to succeed at this task, context encoders need to both understand the content
of the entire image, as well as produce a plausible hypothesis for the missing
part(s). When training context encoders, we have experimented with both a
standard pixel-wise reconstruction loss, as well as a reconstruction plus an
adversarial loss. The latter produces much sharper results because it can
better handle multiple modes in the output. We found that a context encoder
learns a representation that captures not just appearance but also the
semantics of visual structures. We quantitatively demonstrate the effectiveness
of our learned features for CNN pre-training on classification, detection, and
segmentation tasks. Furthermore, context encoders can be used for semantic
inpainting tasks, either stand-alone or as initialization for non-parametric
methods.



We provide two distributed confidence ball algorithms for solving linear
bandit problems in peer to peer networks with limited communication
capabilities. For the first, we assume that all the peers are solving the same
linear bandit problem, and prove that our algorithm achieves the optimal
asymptotic regret rate of any centralised algorithm that can instantly
communicate information between the peers. For the second, we assume that there
are clusters of peers solving the same bandit problem within each cluster, and
we prove that our algorithm discovers these clusters, while achieving the
optimal asymptotic regret rate within each one. Through experiments on several
real-world datasets, we demonstrate the performance of proposed algorithms
compared to the state-of-the-art.



Tensor factorization is a powerful tool to analyse multi-way data. Compared
with traditional multi-linear methods, nonlinear tensor factorization models
are capable of capturing more complex relationships in the data. However, they
are computationally expensive and may suffer severe learning bias in case of
extreme data sparsity. To overcome these limitations, in this paper we propose
a distributed, flexible nonlinear tensor factorization model. Our model can
effectively avoid the expensive computations and structural restrictions of the
Kronecker-product in existing TGP formulations, allowing an arbitrary subset of
tensorial entries to be selected to contribute to the training. At the same
time, we derive a tractable and tight variational evidence lower bound (ELBO)
that enables highly decoupled, parallel computations and high-quality
inference. Based on the new bound, we develop a distributed inference algorithm
in the MapReduce framework, which is key-value-free and can fully exploit the
memory cache mechanism in fast MapReduce systems such as SPARK. Experimental
results fully demonstrate the advantages of our method over several
state-of-the-art approaches, in terms of both predictive performance and
computational efficiency. Moreover, our approach shows a promising potential in
the application of Click-Through-Rate (CTR) prediction for online advertising.



In this paper, we discuss software design issues related to the development
of parallel computational intelligence algorithms on multi-core CPUs, using the
new Java 8 functional programming features. In particular, we focus on
probabilistic graphical models (PGMs) and present the parallelisation of a
collection of algorithms that deal with inference and learning of PGMs from
data. Namely, maximum likelihood estimation, importance sampling, and greedy
search for solving combinatorial optimisation problems. Through these concrete
examples, we tackle the problem of defining efficient data structures for PGMs
and parallel processing of same-size batches of data sets using Java 8
features. We also provide straightforward techniques to code parallel
algorithms that seamlessly exploit multi-core processors. The experimental
analysis, carried out using our open source AMIDST (Analysis of MassIve Data
STreams) Java toolbox, shows the merits of the proposed solutions.



In this paper we combine one method for hierarchical reinforcement learning -
the options framework - with deep Q-networks (DQNs) through the use of
different "option heads" on the policy network, and a supervisory network for
choosing between the different options. We utilise our setup to investigate the
effects of architectural constraints in subtasks with positive and negative
transfer, across a range of network capacities. We empirically show that our
augmented DQN has lower sample complexity when simultaneously learning subtasks
with negative transfer, without degrading performance when learning subtasks
with positive transfer.



We present a data mining approach for reducing the search space of local
search algorithms in a class of binary integer programs including the set
covering and partitioning problems. The quality of locally optimal solutions
typically improves if a larger neighborhood is used, while the computation time
of searching the neighborhood increases exponentially. To overcome this, we
extract variable associations from the instance to be solved in order to
identify promising pairs of flipping variables in the neighborhood search.
Based on this, we develop a 4-flip neighborhood local search algorithm that
incorporates an efficient incremental evaluation of solutions and an adaptive
control of penalty weights. Computational results show that the proposed method
improves the performance of the local search algorithm for large-scale set
covering and partitioning problems.



Despite being the standard loss function to train multi-class neural
networks, the log-softmax has two potential limitations. First, it involves
computations that scale linearly with the number of output classes, which can
restrict the size of problems we are able to tackle with current hardware.
Second, it remains unclear how close it matches the task loss such as the top-k
error rate or other non-differentiable evaluation metrics which we aim to
optimize ultimately. In this paper, we introduce an alternative classification
loss function, the Z-loss, which is designed to address these two issues.
Unlike the log-softmax, it has the desirable property of belonging to the
spherical loss family (Vincent et al., 2015), a class of loss functions for
which training can be performed very efficiently with a complexity independent
of the number of output classes. We show experimentally that it significantly
outperforms the other spherical loss functions previously investigated.
Furthermore, we show on a word language modeling task that it also outperforms
the log-softmax with respect to certain ranking scores, such as top-k scores,
suggesting that the Z-loss has the flexibility to better match the task loss.
These qualities thus makes the Z-loss an appealing candidate to train very
efficiently large output networks such as word-language models or other extreme
classification problems. On the One Billion Word (Chelba et al., 2014) dataset,
we are able to train a model with the Z-loss 40 times faster than the
log-softmax and more than 4 times faster than the hierarchical softmax.



Human activity recognition (HAR) in ubiquitous computing is beginning to
adopt deep learning to substitute for well-established analysis techniques that
rely on hand-crafted feature extraction and classification techniques. From
these isolated applications of custom deep architectures it is, however,
difficult to gain an overview of their suitability for problems ranging from
the recognition of manipulative gestures to the segmentation and identification
of physical activities like running or ascending stairs. In this paper we
rigorously explore deep, convolutional, and recurrent approaches across three
representative datasets that contain movement data captured with wearable
sensors. We describe how to train recurrent approaches in this setting,
introduce a novel regularisation approach, and illustrate how they outperform
the state-of-the-art on a large benchmark dataset. Across thousands of
recognition experiments with randomly sampled model configurations we
investigate the suitability of each model for different tasks in HAR, explore
the impact of hyperparameters using the fANOVA framework, and provide
guidelines for the practitioner who wants to apply deep learning in their
problem setting.



Clustering is an underspecified task: there are no universal criteria for
what makes a good clustering. This is especially true for relational data,
where similarity can be based on the features of individuals, the relationships
between them, or a mix of both. Existing methods for relational clustering have
strong and often implicit biases in this respect. In this paper, we introduce a
novel similarity measure for relational data. It is the first measure to
incorporate a wide variety of types of similarity, including similarity of
attributes, similarity of relational context, and proximity in a hypergraph. We
experimentally evaluate how using this similarity affects the quality of
clustering on very different types of datasets. The experiments demonstrate
that (a) using this similarity in standard clustering methods consistently
gives good results, whereas other measures work well only on datasets that
match their bias; and (b) on most datasets, the novel similarity outperforms
even the best among the existing ones.



Novelty detection in news events has long been a difficult problem. A number
of models performed well on specific data streams but certain issues are far
from being solved, particularly in large data streams from the WWW where
unpredictability of new terms requires adaptation in the vector space model. We
present a novel event detection system based on the Incremental Term
Frequency-Inverse Document Frequency (TF-IDF) weighting incorporated with
Locality Sensitive Hashing (LSH). Our system could efficiently and effectively
adapt to the changes within the data streams of any new terms with continual
updates to the vector space model. Regarding miss probability, our proposed
novelty detection framework outperforms a recognised baseline system by
approximately 16% when evaluating a benchmark dataset from Google News.



Visual recognition systems mounted on autonomous moving agents face the
challenge of unconstrained data, but simultaneously have the opportunity to
improve their performance by moving to acquire new views of test data. In this
work, we first show how a recurrent neural network-based system may be trained
to perform end-to-end learning of motion policies suited for this "active
recognition" setting. Further, we hypothesize that active vision requires an
agent to have the capacity to reason about the effects of its motions on its
view of the world. To verify this hypothesis, we attempt to induce this
capacity in our active recognition pipeline, by simultaneously learning to
forecast the effects of the agent's motions on its internal representation of
the environment conditional on all past views. Results across two challenging
datasets confirm both that our end-to-end system successfully learns meaningful
policies for active category recognition, and that "learning to look ahead"
further boosts recognition performance.



We investigate an efficient context-dependent clustering technique for
recommender systems based on exploration-exploitation strategies through
multi-armed bandits over multiple users. Our algorithm dynamically groups users
based on their observed behavioral similarity during a sequence of logged
activities. In doing so, the algorithm reacts to the currently served user by
shaping clusters around him/her but, at the same time, it explores the
generation of clusters over users which are not currently engaged. We motivate
the effectiveness of this clustering policy, and provide an extensive empirical
analysis on real-world datasets, showing scalability and improved prediction
performance over state-of-the-art methods for sequential clustering of users in
multi-armed bandit scenarios.



Probabilistic Boolean networks (PBNs) is an important mathematical framework
widely used for modelling and analysing biological systems. PBNs are suited for
modelling large biological systems, which more and more often arise in systems
biology. However, the large system size poses a~significant challenge to the
analysis of PBNs, in particular, to the crucial analysis of their steady-state
behaviour. Numerical methods for performing steady-state analyses suffer from
the state-space explosion problem, which makes the utilisation of statistical
methods the only viable approach. However, such methods require long
simulations of PBNs, rendering the simulation speed a crucial efficiency
factor. For large PBNs and high estimation precision requirements, a slow
simulation speed becomes an obstacle. In this paper, we propose a
structure-based method for fast simulation of PBNs. This method first performs
a network reduction operation and then divides nodes into groups for parallel
simulation. Experimental results show that our method can lead to an
approximately 10 times speedup for computing steady-state probabilities of a
real-life biological network.



Humans demonstrate remarkable abilities to predict physical events in complex
scenes. Two classes of models for physical scene understanding have recently
been proposed: "Intuitive Physics Engines", or IPEs, which posit that people
make predictions by running approximate probabilistic simulations in causal
mental models similar in nature to video-game physics engines, and memory-based
models, which make judgments based on analogies to stored experiences of
previously encountered scenes and physical outcomes. Versions of the latter
have recently been instantiated in convolutional neural network (CNN)
architectures. Here we report four experiments that, to our knowledge, are the
first rigorous comparisons of simulation-based and CNN-based models, where both
approaches are concretely instantiated in algorithms that can run on raw image
inputs and produce as outputs physical judgments such as whether a stack of
blocks will fall. Both approaches can achieve super-human accuracy levels and
can quantitatively predict human judgments to a similar degree, but only the
simulation-based models generalize to novel situations in ways that people do,
and are qualitatively consistent with systematic perceptual illusions and
judgment asymmetries that people show.



We give solutions to two fundamental computational problems in ontology-based
data access with the W3C standard ontology language OWL 2 QL: the succinctness
problem for first-order rewritings of ontology-mediated queries (OMQs), and the
complexity problem for OMQ answering. We classify OMQs according to the shape
of their conjunctive queries (treewidth, the number of leaves) and the
existential depth of their ontologies. For each of these classes, we determine
the combined complexity of OMQ answering, and whether all OMQs in the class
have polynomial-size first-order, positive existential, and nonrecursive
datalog rewritings. We obtain the succinctness results using hypergraph
programs, a new computational model for Boolean functions, which makes it
possible to connect the size of OMQ rewritings and circuit complexity.



In these notes we propose a setting for fuzzy computing in a framework
similar to that of well-established theories of computation: boolean, and
quantum computing. Our efforts have been directed towards stressing the formal
similarities: there is a common pattern underlying these three theories. We
tried to conform our approach, as much as possible, to this pattern. This work
was part of a project jointly with Professor Vittorio Cafagna. Professor
Cafagna passed away unexpectedly in 2007. His intellectual breadth and
inspiring passion for mathematics is still very well alive.



We show how to adjust the coefficient of determination ($R^2$) when used for
measuring predictive accuracy via leave-one-out cross-validation.



Energy is a limited resource which has to be managed wisely, taking into
account both supply-demand matching and capacity constraints in the
distribution grid. One aspect of the smart energy management at the building
level is given by the problem of real-time detection of flexible demand
available. In this paper we propose the use of energy disaggregation techniques
to perform this task. Firstly, we investigate the use of existing
classification methods to perform energy disaggregation. A comparison is
performed between four classifiers, namely Naive Bayes, k-Nearest Neighbors,
Support Vector Machine and AdaBoost. Secondly, we propose the use of Restricted
Boltzmann Machine to automatically perform feature extraction. The extracted
features are then used as inputs to the four classifiers and consequently shown
to improve their accuracy. The efficiency of our approach is demonstrated on a
real database consisting of detailed appliance-level measurements with high
temporal resolution, which has been used for energy disaggregation in previous
studies, namely the REDD. The results show robustness and good generalization
capabilities to newly presented buildings with at least 96% accuracy.



The generalized belief propagation (GBP), introduced by Yedidia et al., is an
extension of the belief propagation (BP) algorithm, which is widely used in
different problems involved in calculating exact or approximate marginals of
probability distributions. In many problems, it has been observed that the
accuracy of GBP considerably outperforms that of BP. However, because in
general the computational complexity of GBP is higher than BP, its application
is limited in practice.
  In this paper, we introduce a stochastic version of GBP called stochastic
generalized belief propagation (SGBP) that can be considered as an extension to
the stochastic BP (SBP) algorithm introduced by Noorshams et al. They have
shown that SBP reduces the complexity per iteration of BP by an order of
magnitude in alphabet size. In contrast to SBP, SGBP can reduce the computation
complexity if certain topological conditions are met by the region graph
associated to a graphical model. However, this reduction can be larger than
only one order of magnitude in alphabet size. In this paper, we characterize
these conditions and the amount of computation gain that we can obtain by using
SGBP. Finally, using similar proof techniques employed by Noorshams et al., for
general graphical models satisfy contraction conditions, we prove the
asymptotic convergence of SGBP to the unique GBP fixed point, as well as
providing non-asymptotic upper bounds on the mean square error and on the high
probability error.



The recent advances in deep neural networks have led to effective
vision-based reinforcement learning methods that have been employed to obtain
human-level controllers in Atari 2600 games from pixel data. Atari 2600 games,
however, do not resemble real-world tasks since they involve non-realistic 2D
environments and the third-person perspective. Here, we propose a novel
test-bed platform for reinforcement learning research from raw visual
information which employs the first-person perspective in a semi-realistic 3D
world. The software, called ViZDoom, is based on the classical first-person
shooter video game, Doom. It allows developing bots that play the game using
the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a
convenient mechanism of user scenarios. In the experimental part, we test the
environment by trying to learn bots for two scenarios: a basic move-and-shoot
task and a more complex maze-navigation problem. Using convolutional deep
neural networks with Q-learning and experience replay, for both scenarios, we
were able to train competent bots, which exhibit human-like behaviors. The
results confirm the utility of ViZDoom as an AI research platform and imply
that visual reinforcement learning in 3D realistic first-person perspective
environments is feasible.



The Dialog State Tracking Challenge 4 (DSTC 4) proposes several pilot tasks.
In this paper, we focus on the spoken language understanding pilot task, which
consists of tagging a given utterance with speech acts and semantic slots. We
compare different classifiers: the best system obtains 0.52 and 0.67 F1-scores
on the test set for speech act recognition for the tourist and the guide
respectively, and 0.52 F1-score for semantic tagging for both the guide and the
tourist.



The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from the
previous three editions as follows: the number of slot-value pairs present in
the ontology is much larger, no spoken language understanding output is given,
and utterances are labeled at the subdialog level. This paper describes a novel
dialog state tracking method designed to work robustly under these conditions,
using elaborate string matching, coreference resolution tailored for dialogs
and a few other improvements. The method can correctly identify many values
that are not explicitly present in the utterance. On the final evaluation, our
method came in first among 7 competing teams and 24 entries. The F1-score
achieved by our method was 9 and 7 percentage points higher than that of the
runner-up for the utterance-level evaluation and for the subdialog-level
evaluation, respectively.



Acoustic event detection is essential for content analysis and description of
multimedia recordings. The majority of current literature on the topic learns
the detectors through fully-supervised techniques employing strongly labeled
data. However, the labels available for majority of multimedia data are
generally weak and do not provide sufficient detail for such methods to be
employed. In this paper we propose a framework for learning acoustic event
detectors using only weakly labeled data. We first show that audio event
detection using weak labels can be formulated as an Multiple Instance Learning
problem. We then suggest two frameworks for solving multiple-instance learning,
one based on support vector machines, and the other on neural networks. The
proposed methods can help in removing the time consuming and expensive process
of manually annotating data to facilitate fully supervised learning. Moreover,
it can not only detect events in a recording but can also provide temporal
locations of events in the recording. This helps in obtaining a complete
description of the recording and is notable since temporal information was
never known in the first place in weakly labeled data.



Computerized Evaluation of English Essays is performed using Machine learning
techniques like Latent Semantic Analysis (LSA), Generalized LSA, Bilingual
Evaluation Understudy and Maximum Entropy. Ontology, a concept map of domain
knowledge, can enhance the performance of these techniques. Use of Ontology
makes the evaluation process holistic as presence of keywords, synonyms, the
right word combination and coverage of concepts can be checked. In this paper,
the above mentioned techniques are implemented both with and without Ontology
and tested on common input data consisting of technical answers of Computer
Science. Domain Ontology of Computer Graphics is designed and developed. The
software used for implementation includes Java Programming Language and tools
such as MATLAB, Prot\'eg\'e, etc. Ten questions from Computer Graphics with
sixty answers for each question are used for testing. The results are analyzed
and it is concluded that the results are more accurate with use of Ontology.



We address a question answering task on real-world images that is set up as a
Visual Turing Test. By combining latest advances in image representation and
natural language processing, we propose Ask Your Neurons, a scalable, jointly
trained, end-to-end formulation to this problem.
  In contrast to previous efforts, we are facing a multi-modal problem where
the language output (answer) is conditioned on visual and natural language
inputs (image and question). We provide additional insights into the problem by
analyzing how much information is contained only in the language part for which
we provide a new human baseline. To study human consensus, which is related to
the ambiguities inherent in this challenging task, we propose two novel metrics
and collect additional answers which extend the original DAQUAR dataset to
DAQUAR-Consensus.
  Moreover, we also extend our analysis to VQA, a large-scale question
answering about images dataset, where we investigate some particular design
choices and show the importance of stronger visual models. At the same time, we
achieve strong performance of our model that still uses a global image
representation. Finally, based on such analysis, we refine our Ask Your Neurons
on DAQUAR, which also leads to a better performance on this challenging task.



Identifying context-specific entity networks from aggregated data is an
important task, arising often in bioinformatics and neuroimaging.
Computationally, this task can be formulated as jointly estimating multiple
different, but related, sparse Undirected Graphical Models (UGM) from
aggregated samples across several contexts. Previous joint-UGM studies have
mostly focused on sparse Gaussian Graphical Models (sGGMs) and can't identify
context-specific edge patterns directly. We, therefore, propose a novel
approach, SIMULE (detecting Shared and Individual parts of MULtiple graphs
Explicitly) to learn multi-UGM via a constrained L1 minimization. SIMULE
automatically infers both specific edge patterns that are unique to each
context and shared interactions preserved among all the contexts. Through the
L1 constrained formulation, this problem is cast as multiple independent
subtasks of linear programming that can be solved efficiently in parallel. In
addition to Gaussian data, SIMULE can also handle multivariate Nonparanormal
data that greatly relaxes the normality assumption that many real-world
applications do not follow. We provide a novel theoretical proof showing that
SIMULE achieves a consistent result at the rate O(log(Kp)/n_{tot}). On multiple
synthetic datasets and two biomedical datasets, SIMULE shows significant
improvement over state-of-the-art multi-sGGM and single-UGM baselines.



Observational studies are rising in importance due to the widespread
accumulation of data in fields such as healthcare, education, employment and
ecology. We consider the task of answering counterfactual questions such as,
"Would this patient have lower blood sugar had she received a different
medication?". We propose a new algorithmic framework for counterfactual
inference which brings together ideas from domain adaptation and representation
learning. In addition to a theoretical justification, we perform an empirical
comparison with previous approaches to causal inference from observational
data. Our deep learning algorithm significantly outperforms the previous
state-of-the-art.



Cross-domain visual data matching is one of the fundamental problems in many
real-world vision tasks, e.g., matching persons across ID photos and
surveillance videos. Conventional approaches to this problem usually involves
two steps: i) projecting samples from different domains into a common space,
and ii) computing (dis-)similarity in this space based on a certain distance.
In this paper, we present a novel pairwise similarity measure that advances
existing models by i) expanding traditional linear projections into affine
transformations and ii) fusing affine Mahalanobis distance and Cosine
similarity by a data-driven combination. Moreover, we unify our similarity
measure with feature representation learning via deep convolutional neural
networks. Specifically, we incorporate the similarity measure matrix into the
deep architecture, enabling an end-to-end way of model optimization. We
extensively evaluate our generalized similarity model in several challenging
cross-domain matching tasks: person re-identification under different views and
face verification over different modalities (i.e., faces from still images and
videos, older and younger faces, and sketch and photo portraits). The
experimental results demonstrate superior performance of our model over other
state-of-the-art methods.



Since the late 1990s when speech companies began providing their
customer-service software in the market, people have gotten used to speaking to
machines. As people interact more often with voice and gesture controlled
machines, they expect the machines to recognize different emotions, and
understand other high level communication features such as humor, sarcasm and
intention. In order to make such communication possible, the machines need an
empathy module in them which can extract emotions from human speech and
behavior and can decide the correct response of the robot. Although research on
empathetic robots is still in the early stage, we described our approach using
signal processing techniques, sentiment analysis and machine learning
algorithms to make robots that can "understand" human emotion. We propose Zara
the Supergirl as a prototype system of empathetic robots. It is a software
based virtual android, with an animated cartoon character to present itself on
the screen. She will get "smarter" and more empathetic through its deep
learning algorithms, and by gathering more data and learning from it. In this
paper, we present our work so far in the areas of deep learning of emotion and
sentiment recognition, as well as humor recognition. We hope to explore the
future direction of android development and how it can help improve people's
lives.



The Wisdom of Crowds is a phenomenon described in social science that
suggests four criteria applicable to groups of people. It is claimed that, if
these criteria are satisfied, then the aggregate decisions made by a group will
often be better than those of its individual members. Inspired by this concept,
we present a novel feedback framework for the cluster ensemble problem, which
we call Wisdom of Crowds Cluster Ensemble (WOCCE). Although many conventional
cluster ensemble methods focusing on diversity have recently been proposed,
WOCCE analyzes the conditions necessary for a crowd to exhibit this collective
wisdom. These include decentralization criteria for generating primary results,
independence criteria for the base algorithms, and diversity criteria for the
ensemble members. We suggest appropriate procedures for evaluating these
measures, and propose a new measure to assess the diversity. We evaluate the
performance of WOCCE against some other traditional base algorithms as well as
state-of-the-art ensemble methods. The results demonstrate the efficiency of
WOCCE's aggregate decision-making compared to other algorithms.



This paper is a reflexion on the computability of natural language semantics.
It does not contain a new model or new results in the formal semantics of
natural language: it is rather a computational analysis of the logical models
and algorithms currently used in natural language semantics, defined as the
mapping of a statement to logical formulas - formulas, because a statement can
be ambiguous. We argue that as long as possible world semantics is left out,
one can compute the semantic representation(s) of a given statement, including
aspects of lexical meaning. We also discuss the algorithmic complexity of this
process.



The estimation of class prevalence, i.e., the fraction of a population that
belongs to a certain class, is a very useful tool in data analytics and
learning, and finds applications in many domains such as sentiment analysis,
epidemiology, etc. For example, in sentiment analysis, the objective is often
not to estimate whether a specific text conveys a positive or a negative
sentiment, but rather estimate the overall distribution of positive and
negative sentiments during an event window. A popular way of performing the
above task, often dubbed quantification, is to use supervised learning to train
a prevalence estimator from labeled data.
  Contemporary literature cites several performance measures used to measure
the success of such prevalence estimators. In this paper we propose the first
online stochastic algorithms for directly optimizing these
quantification-specific performance measures. We also provide algorithms that
optimize hybrid performance measures that seek to balance quantification and
classification performance. Our algorithms present a significant advancement in
the theory of multivariate optimization and we show, by a rigorous theoretical
analysis, that they exhibit optimal convergence. We also report extensive
experiments on benchmark and real data sets which demonstrate that our methods
significantly outperform existing optimization techniques used for these
performance measures.



Learning the true ordering between objects by aggregating a set of expert
opinion rank order lists is an important and ubiquitous problem in many
applications ranging from social choice theory to natural language processing
and search aggregation. We study the problem of unsupervised rank aggregation
where no ground truth ordering information in available, neither about the true
preference ordering between any set of objects nor about the quality of
individual rank lists. Aggregating the often inconsistent and poor quality rank
lists in such an unsupervised manner is a highly challenging problem, and
standard consensus-based methods are often ill-defined, and difficult to solve.
In this manuscript we propose a novel framework to bypass these issues by using
object attributes to augment the standard rank aggregation framework. We design
algorithms that learn joint models on both rank lists and object features to
obtain an aggregated rank ordering that is more accurate and robust, and also
helps weed out rank lists of dubious validity. We validate our techniques on
synthetic datasets where our algorithm is able to estimate the true rank
ordering even when the rank lists are corrupted. Experiments on three real
datasets, MQ2008, MQ2008 and OHSUMED, show that using object features can
result in significant improvement in performance over existing rank aggregation
methods that do not use object information. Furthermore, when at least some of
the rank lists are of high quality, our methods are able to effectively exploit
their high expertise to output an aggregated rank ordering of great accuracy.



Databases in domains such as healthcare are routinely released to the public
in aggregated form. Unfortunately, naive modeling with aggregated data may
significantly diminish the accuracy of inferences at the individual level. This
paper addresses the scenario where features are provided at the individual
level, but the target variables are only available as histogram aggregates or
order statistics. We consider a limiting case of generalized linear modeling
when the target variables are only known up to permutation, and explore how
this relates to permutation testing; a standard technique for assessing
statistical dependency. Based on this relationship, we propose a simple
algorithm to estimate the model parameters and individual level inferences via
alternating imputation and standard generalized linear model fitting. Our
results suggest the effectiveness of the proposed approach when, in the
original data, permutation testing accurately ascertains the veracity of the
linear relationship. The framework is extended to general histogram data with
larger bins - with order statistics such as the median as a limiting case. Our
experimental results on simulated data and aggregated healthcare data suggest a
diminishing returns property with respect to the granularity of the histogram -
when a linear relationship holds in the original data, the targets can be
predicted accurately given relatively coarse histograms.



Link prediction in large knowledge graphs has received a lot of attention
recently because of its importance for inferring missing relations and for
completing and improving noisily extracted knowledge graphs. Over the years a
number of machine learning researchers have presented various models for
predicting the presence of missing relations in a knowledge base. Although all
the previous methods are presented with empirical results that show high
performance on select datasets, there is almost no previous work on
understanding the connection between properties of a knowledge base and the
performance of a model. In this paper we analyze the RESCAL method and prove
that it can not encode asymmetric transitive relations in knowledge bases.



This paper studies the evaluation of policies that recommend an ordered set
of items (e.g., a ranking) based on some context---a common scenario in web
search, ads, and recommendation. We build on techniques from combinatorial
bandits to introduce a new practical estimator that uses logged data to
estimate a policy's performance. A thorough empirical evaluation on real-world
data reveals that our estimator is accurate in a variety of settings, including
as a subroutine in a learning-to-rank task, where it achieves competitive
performance. We derive conditions under which our estimator is unbiased---these
conditions are weaker than prior heuristics for slate evaluation---and
experimentally demonstrate a smaller bias than parametric approaches, even when
these conditions are violated. Finally, our theory and experiments also show
exponential savings in the amount of required data compared with general
unbiased estimators.



Time-frequency methods for vibration-based gearbox faults detection have been
considered the most efficient method. Among these methods, continuous wavelet
transform (CWT) as one of the best time-frequency method has been used for both
stationary and transitory signals. Some deficiencies of CWT are problem of
overlapping and distortion ofsignals. In this condition, a large amount of
redundant information exists so that it may cause false alarm or
misinterpretation of the operator. In this paper a modified method called Exact
Wavelet Analysis is used to minimize the effects of overlapping and distortion
in case of gearbox faults. To implement exact wavelet analysis, Particle Swarm
Optimization (PSO) algorithm has been used for this purpose. This method have
been implemented for the acceleration signals from 2D acceleration sensor
acquired by Advantech PCI-1710 card from a gearbox test setup in Amirkabir
University of Technology. Gearbox has been considered in both healthy and
chipped tooth gears conditions. Kernelized Support Vector Machine (SVM) with
radial basis functions has used the extracted features from exact wavelet
analysis for classification. The efficiency of this classifier is then
evaluated with the other signals acquired from the setup test. The results show
that in comparison of CWT, PSO Exact Wavelet Transform has better ability in
feature extraction in price of more computational effort. In addition, PSO
exact wavelet has better speed comparing to Genetic Algorithm (GA) exact
wavelet in condition of equal population because of factoring mutation and
crossover in PSO algorithm. SVM classifier with the extracted features in
gearbox shows very good results and its ability has been proved.



There is an ever growing number of users with accounts on multiple social
media and networking sites. Consequently, there is increasing interest in
matching user accounts and profiles across different social networks in order
to create aggregate profiles of users. In this paper, we present models for
Digital Stylometry, which is a method for matching users through stylometry
inspired techniques. We experimented with linguistic, temporal, and combined
temporal-linguistic models for matching user accounts, using standard and novel
techniques. Using publicly available data, our best model, a combined
temporal-linguistic one, was able to correctly match the accounts of 31% of
5,612 distinct users across Twitter and Facebook.



The rise in popularity and ubiquity of Twitter has made sentiment analysis of
tweets an important and well-covered area of research. However, the 140
character limit imposed on tweets makes it hard to use standard linguistic
methods for sentiment classification. On the other hand, what tweets lack in
structure they make up with sheer volume and rich metadata. This metadata
includes geolocation, temporal and author information. We hypothesize that
sentiment is dependent on all these contextual factors. Different locations,
times and authors have different emotional valences. In this paper, we explored
this hypothesis by utilizing distant supervision to collect millions of
labelled tweets from different locations, times and authors. We used this data
to analyse the variation of tweet sentiments across different authors, times
and locations. Once we explored and understood the relationship between these
variables and sentiment, we used a Bayesian approach to combine these variables
with more standard linguistic features such as n-grams to create a Twitter
sentiment classifier. This combined classifier outperforms the purely
linguistic classifier, showing that integrating the rich contextual information
available on Twitter into sentiment classification is a promising direction of
research.



Numerous important problems can be framed as learning from graph data. We
propose a framework for learning convolutional neural networks for arbitrary
graphs. These graphs may be undirected, directed, and with both discrete and
continuous node and edge attributes. Analogous to image-based convolutional
networks that operate on locally connected regions of the input, we present a
general approach to extracting locally connected regions from graphs. Using
established benchmark data sets, we demonstrate that the learned feature
representations are competitive with state of the art graph kernels and that
their computation is highly efficient.



This paper introduces an automated skill acquisition framework in
reinforcement learning which involves identifying a hierarchical description of
the given task in terms of abstract states and extended actions between
abstract states. Identifying such structures present in the task provides ways
to simplify and speed up reinforcement learning algorithms. These structures
also help to generalize such algorithms over multiple tasks without relearning
policies from scratch. We use ideas from dynamical systems to find metastable
regions in the state space and associate them with abstract states. The
spectral clustering algorithm PCCA+ is used to identify suitable abstractions
aligned to the underlying structure. Skills are defined in terms of the
sequence of actions that lead to transitions between such abstract states. The
connectivity information from PCCA+ is used to generate these skills or
options. These skills are independent of the learning task and can be
efficiently reused across a variety of tasks defined over the same model. This
approach works well even without the exact model of the environment by using
sample trajectories to construct an approximate estimate. We also present our
approach to scaling the skill acquisition framework to complex tasks with large
state spaces for which we perform state aggregation using the representation
learned from an action conditional video prediction network and use the skill
acquisition framework on the aggregated state space.



Deep Reinforcement Learning methods have achieved state of the art
performance in learning control policies for the games in the Atari 2600
domain. One of the important parameters in the Arcade Learning Environment
(ALE) is the frame skip rate. It decides the granularity at which agents can
control game play. A frame skip value of $k$ allows the agent to repeat a
selected action $k$ number of times. The current state of the art architectures
like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of
a framework with a static frame skip rate, where the action output from the
network is repeated for a fixed number of frames regardless of the current
state. In this paper, we propose a new architecture, Dynamic Frame skip Deep
Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable
parameter. This allows us to choose the number of times an action is to be
repeated based on the current state. We show empirically that such a setting
improves the performance on relatively harder games like Seaquest.



Utilities face the challenge of responding to power outages due to storms and
ice damage, but most power grids are not equipped with sensors to pinpoint the
precise location of the faults causing the outage. Instead, utilities have to
depend primarily on phone calls (trouble calls) from customers who have lost
power to guide the dispatching of utility trucks. In this paper, we develop a
policy that routes a utility truck to restore outages in the power grid as
quickly as possible, using phone calls to create beliefs about outages, but
also using utility trucks as a mechanism for collecting additional information.
This means that routing decisions change not only the physical state of the
truck (as it moves from one location to another) and the grid (as the truck
performs repairs), but also our belief about the network, creating the first
stochastic vehicle routing problem that explicitly models information
collection and belief modeling. We address the problem of managing a single
utility truck, which we start by formulating as a sequential stochastic
optimization model which captures our belief about the state of the grid. We
propose a stochastic lookahead policy, and use Monte Carlo tree search (MCTS)
to produce a practical policy that is asymptotically optimal. Simulation
results show that the developed policy restores the power grid much faster
compared to standard industry heuristics.



Sequential data often possesses a hierarchical structure with complex
dependencies between subsequences, such as found between the utterances in a
dialogue. In an effort to model this kind of generative process, we propose a
neural network-based generative architecture, with latent stochastic variables
that span a variable number of time steps. We apply the proposed model to the
task of dialogue response generation and compare it with recent neural network
architectures. We evaluate the model performance through automatic evaluation
metrics and by carrying out a human evaluation. The experiments demonstrate
that our model improves upon recently proposed models and that the latent
variables facilitate the generation of long outputs and maintain the context.



Variational inference provides approximations to the computationally
intractable posterior distribution in Bayesian networks. A prominent medical
application of noisy-or Bayesian network is to infer potential diseases given
observed symptoms. Previous studies focus on approximating a handful of
complicated pathological cases using variational transformation. Our goal is to
use variational transformation as part of a novel hybridized inference for
serving reliable and real time diagnosis at web scale. We propose a hybridized
inference that allows variational parameters to be estimated without disease
posteriors or priors, making the inference faster and much of its computation
recyclable. In addition, we propose a transformation ranking algorithm that is
very stable to large variances in network prior probabilities, a common issue
that arises in medical applications of Bayesian networks. In experiments, we
perform comparative study on a large real life medical network and scalability
study on a much larger (36,000x) synthesized network.



We consider the problem of strongly-convex online optimization in presence of
adversarial delays; in a T-iteration online game, the feedback of the player's
query at time t is arbitrarily delayed by an adversary for d_t rounds and
delivered before the game ends, at iteration t+d_t-1. Specifically for
\algo{online-gradient-descent} algorithm we show it has a simple regret bound
of \Oh{\sum_{t=1}^T \log (1+ \frac{d_t}{t})}. This gives a clear and simple
bound without resorting any distributional and limiting assumptions on the
delays. We further show how this result encompasses and generalizes several of
the existing known results in the literature. Specifically it matches the
celebrated logarithmic regret \Oh{\log T} when there are no delays (i.e. d_t =
1) and regret bound of \Oh{\tau \log T} for constant delays d_t = \tau.



In this work we propose a novel interpretation of residual networks showing
that they can be seen as a collection of many paths of differing length.
Moreover, residual networks seem to enable very deep networks by leveraging
only the short paths during training. To support this observation, we rewrite
residual networks as an explicit collection of paths. Unlike traditional
models, paths through residual networks vary in length. Further, a lesion study
reveals that these paths show ensemble-like behavior in the sense that they do
not strongly depend on each other. Finally, and most surprising, most paths are
shorter than one might expect, and only the short paths are needed during
training, as longer paths do not contribute any gradient. For example, most of
the gradient in a residual network with 110 layers comes from paths that are
only 10-34 layers deep. Our results reveal one of the key characteristics that
seem to enable the training of very deep networks: Residual networks avoid the
vanishing gradient problem by introducing short paths which can carry gradient
throughout the extent of very deep networks.



One way to approach end-to-end autonomous driving is to learn a policy
function that maps from a sensory input, such as an image frame from a
front-facing camera, to a driving action, by imitating an expert driver, or a
reference policy. This can be done by supervised learning, where a policy
function is tuned to minimize the difference between the predicted and
ground-truth actions. A policy function trained in this way however is known to
suffer from unexpected behaviours due to the mismatch between the states
reachable by the reference policy and trained policy functions. More advanced
algorithms for imitation learning, such as DAgger, addresses this issue by
iteratively collecting training examples from both reference and trained
policies. These algorithms often requires a large number of queries to a
reference policy, which is undesirable as the reference policy is often
expensive. In this paper, we propose an extension of the DAgger, called
SafeDAgger, that is query-efficient and more suitable for end-to-end autonomous
driving. We evaluate the proposed SafeDAgger in a car racing simulator and show
that it indeed requires less queries to a reference policy. We observe a
significant speed up in convergence, which we conjecture to be due to the
effect of automated curriculum learning.



Large knowledge bases (KBs) are useful in many tasks, but it is unclear how
to integrate this sort of knowledge into "deep" gradient-based learning
systems. To address this problem, we describe a probabilistic deductive
database, called TensorLog, in which reasoning uses a differentiable process.
In TensorLog, each clause in a logical theory is first converted into certain
type of factor graph. Then, for each type of query to the factor graph, the
message-passing steps required to perform belief propagation (BP) are
"unrolled" into a function, which is differentiable. We show that these
functions can be composed recursively to perform inference in non-trivial
logical theories containing multiple interrelated clauses and predicates. Both
compilation and inference in TensorLog are efficient: compilation is linear in
theory size and proof depth, and inference is linear in database size and the
number of message-passing steps used in BP. We also present experimental
results with TensorLog and discuss its relationship to other first-order
probabilistic logics.



Given that in practice training data is scarce for all but a small set of
problems, a core question is how to incorporate prior knowledge into a model.
In this paper, we consider the case of prior procedural knowledge for neural
networks, such as knowing how a program should traverse a sequence, but not
what local actions should be performed at each step. To this end, we present an
end-to-end differentiable interpreter for the programming language Forth which
enables programmers to write program sketches with slots that can be filled
with behaviour trained from program input-output data. We can optimise this
behaviour directly through gradient descent techniques on user-specified
objectives, and also integrate the program into any larger neural computation
graph. We show empirically that our interpreter is able to effectively leverage
different levels of prior program structure and learn complex behaviours such
as sequence sorting and addition. When connected to outputs of an LSTM and
trained jointly, our interpreter achieves state-of-the-art accuracy for
end-to-end reasoning about quantities expressed in natural language stories.



We consider the problem of multiple agents sensing and acting in environments
with the goal of maximising their shared utility. In these environments, agents
must learn communication protocols in order to share information that is needed
to solve the tasks. By embracing deep neural networks, we are able to
demonstrate end-to-end learning of protocols in complex environments inspired
by communication riddles and multi-agent computer vision problems with partial
observability. We propose two approaches for learning in these domains:
Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning
(DIAL). The former uses deep Q-learning, while the latter exploits the fact
that, during learning, agents can backpropagate error derivatives through
(noisy) communication channels. Hence, this approach uses centralised learning
but decentralised execution. Our experiments introduce new environments for
studying the learning of communication protocols and present a set of
engineering innovations that are essential for success in these domains.



Recent advances in deep learning have enabled the extraction of high-level
features from raw sensor data which has opened up new possibilities in many
different fields, including computer generated choreography. In this paper we
present a system chor-rnn for generating novel choreographic material in the
nuanced choreographic language and style of an individual choreographer. It
also shows promising results in producing a higher level compositional
cohesion, rather than just generating sequences of movement. At the core of
chor-rnn is a deep recurrent neural network trained on raw motion capture data
and that can generate new dance sequences for a solo dancer. Chor-rnn can be
used for collaborative human-machine choreography or as a creative catalyst,
serving as inspiration for a choreographer.



Bayesian optimization has become a successful tool for hyperparameter
optimization of machine learning algorithms, such as support vector machines or
deep neural networks. Despite its success, for large datasets, training and
validating a single configuration often takes hours, days, or even weeks, which
limits the achievable performance. To accelerate hyperparameter optimization,
we propose a generative model for the validation error as a function of
training set size, which is learned during the optimization process and allows
exploration of preliminary configurations on small subsets, by extrapolating to
the full dataset. We construct a Bayesian optimization procedure, dubbed
Fabolas, which models loss and training time as a function of dataset size and
automatically trades off high information gain about the global optimum against
computational cost. Experiments optimizing support vector machines and deep
neural networks show that Fabolas often finds high-quality solutions 10 to 100
times faster than other state-of-the-art Bayesian optimization methods or the
recently proposed bandit strategy Hyperband.



Each human genome is a 3 billion base pair set of encoding instructions.
Decoding the genome using deep learning fundamentally differs from most tasks,
as we do not know the full structure of the data and therefore cannot design
architectures to suit it. As such, architectures that fit the structure of
genomics should be learned not prescribed. Here, we develop a novel search
algorithm, applicable across domains, that discovers an optimal architecture
which simultaneously learns general genomic patterns and identifies the most
important sequence motifs in predicting functional genomic outcomes. The
architectures we find using this algorithm succeed at using only RNA expression
data to predict gene regulatory structure, learn human-interpretable
visualizations of key sequence motifs, and surpass state-of-the-art results on
benchmark genomics challenges.



A core challenge for an agent learning to interact with the world is to
predict how its actions affect objects in its environment. Many existing
methods for learning the dynamics of physical interactions require labeled
object information. However, to scale real-world interaction learning to a
variety of scenes and objects, acquiring labeled data becomes increasingly
impractical. To learn about physical object motion without labels, we develop
an action-conditioned video prediction model that explicitly models pixel
motion, by predicting a distribution over pixel motion from previous frames.
Because our model explicitly predicts motion, it is partially invariant to
object appearance, enabling it to generalize to previously unseen objects. To
explore video prediction for real-world interactive agents, we also introduce a
dataset of 59,000 robot interactions involving pushing motions, including a
test set with novel objects. In this dataset, accurate prediction of videos
conditioned on the robot's future actions amounts to learning a "visual
imagination" of different futures based on different courses of action. Our
experiments show that our proposed method produces more accurate video
predictions both quantitatively and qualitatively, when compared to prior
methods.



The alternating direction method of multipliers (ADMM) is a versatile tool
for solving a wide range of constrained optimization problems, with
differentiable or non-differentiable objective functions. Unfortunately, its
performance is highly sensitive to a penalty parameter, which makes ADMM often
unreliable and hard to automate for a non-expert user. We tackle this weakness
of ADMM by proposing a method to adaptively tune the penalty parameters to
achieve fast convergence. The resulting adaptive ADMM (AADMM) algorithm,
inspired by the successful Barzilai-Borwein spectral method for gradient
descent, yields fast convergence and relative insensitivity to the initial
stepsize and problem scaling.



Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.



Iteratively reweighted least squares (IRLS) is a widely-used method in
machine learning to estimate the parameters in the generalised linear models.
In particular, IRLS for L1 minimisation under the linear model provides a
closed-form solution in each step, which is a simple multiplication between the
inverse of the weighted second moment matrix and the weighted first moment
vector. When dealing with privacy sensitive data, however, developing a privacy
preserving IRLS algorithm faces two challenges. First, due to the inversion of
the second moment matrix, the usual sensitivity analysis in differential
privacy incorporating a single datapoint perturbation gets complicated and
often requires unrealistic assumptions. Second, due to its iterative nature, a
significant cumulative privacy loss occurs. However, adding a high level of
noise to compensate for the privacy loss hinders from getting accurate
estimates. Here, we develop a practical algorithm that overcomes these
challenges and outputs privatised and accurate IRLS solutions. In our method,
we analyse the sensitivity of each moments separately and treat the matrix
inversion and multiplication as a post-processing step, which simplifies the
sensitivity analysis. Furthermore, we apply the {\it{concentrated differential
privacy}} formalism, a more relaxed version of differential privacy, which
requires adding a significantly less amount of noise for the same level of
privacy guarantee, compared to the conventional and advanced compositions of
differentially private mechanisms.



Probabilistic modeling is cyclical: we specify a model, infer its posterior,
and evaluate its performance. Evaluation drives the cycle, as we revise our
model based on how it performs. This requires a metric. Traditionally,
predictive accuracy prevails. Yet, predictive accuracy does not tell the whole
story. We propose to evaluate a model through posterior dispersion. The idea is
to analyze how each datapoint fares in relation to posterior uncertainty around
the hidden structure. We propose a family of posterior dispersion indices (PDI)
that capture this idea. A PDI identifies rich patterns of model mismatch in
three real data examples: voting preferences, supermarket shopping, and
population genetics.



Nutrient-based meal recommendations have the potential to help individuals
prevent or manage conditions such as diabetes and obesity. However, learning
people's food preferences and making recommendations that simultaneously appeal
to their palate and satisfy nutritional expectations are challenging. Existing
approaches either only learn high-level preferences or require a prolonged
learning period. We propose Yum-me, a personalized nutrient-based meal
recommender system designed to meet individuals' nutritional expectations,
dietary restrictions, and fine-grained food preferences. Yum-me enables a
simple and accurate food preference profiling procedure via a visual quiz-based
user interface, and projects the learned profile into the domain of
nutritionally appropriate food options to find ones that will appeal to the
user. We present the design and implementation of Yum-me, and further describe
and evaluate two innovative contributions. The first contriution is an open
source state-of-the-art food image analysis model, named FoodDist. We
demonstrate FoodDist's superior performance through careful benchmarking and
discuss its applicability across a wide array of dietary applications. The
second contribution is a novel online learning framework that learns food
preference from item-wise and pairwise image comparisons. We evaluate the
framework in a field study of 227 anonymous users and demonstrate that it
outperforms other baselines by a significant margin. We further conducted an
end-to-end validation of the feasibility and effectiveness of Yum-me through a
60-person user study, in which Yum-me improves the recommendation acceptance
rate by 42.63%.



Large labeled training sets are the critical building blocks of supervised
learning methods and are key enablers of deep learning techniques. For some
applications, creating labeled training sets is the most time-consuming and
expensive part of applying machine learning. We therefore propose a paradigm
for the programmatic creation of training sets called data programming in which
users express weak supervision strategies or domain heuristics as labeling
functions, which are programs that label subsets of the data, but that are
noisy and may conflict. We show that by explicitly representing this training
set labeling process as a generative model, we can "denoise" the generated
training set, and establish theoretically that we can recover the parameters of
these generative models in a handful of settings. We then show how to modify a
discriminative loss function to make it noise-aware, and demonstrate our method
over a range of discriminative models including logistic regression and LSTMs.
Experimentally, on the 2014 TAC-KBP Slot Filling challenge, we show that data
programming would have led to a new winning score, and also show that applying
data programming to an LSTM model leads to a TAC-KBP score almost 6 F1 points
over a state-of-the-art LSTM baseline (and into second place in the
competition). Additionally, in initial user studies we observed that data
programming may be an easier way for non-experts to create machine learning
models when training data is limited or unavailable.



Using top-ranked documents in response to a query has been shown to be an
effective approach to improve the quality of query translation in
dictionary-based cross-language information retrieval. In this paper, we
propose a new method for dictionary-based query translation based on dimension
projection of embedded vectors from the pseudo-relevant documents in the source
language to their equivalents in the target language. To this end, first we
learn low-dimensional vectors of the words in the pseudo-relevant collections
separately and then aim to find a query-dependent transformation matrix between
the vectors of translation pairs appeared in the collections. At the next step,
representation of each query term is projected to the target language and then,
after using a softmax function, a query-dependent translation model is built.
Finally, the model is used for query translation. Our experiments on four CLEF
collections in French, Spanish, German, and Italian demonstrate that the
proposed method outperforms a word embedding baseline based on bilingual
shuffling and a further number of competitive baselines. The proposed method
reaches up to 87% performance of machine translation (MT) in short queries and
considerable improvements in verbose queries.



Automatic extraction of cause-effect relationships from natural language
texts is a challenging open problem in Artificial Intelligence. Most of the
early attempts at its solution used manually constructed linguistic and
syntactic rules on small and domain-specific data sets. However, with the
advent of big data, the availability of affordable computing power and the
recent popularization of machine learning, the paradigm to tackle this problem
has slowly shifted. Machines are now expected to learn generic causal
extraction rules from labelled data with minimal supervision, in a domain
independent-manner. In this paper, we provide a comprehensive survey of causal
relation extraction techniques from both paradigms, and analyse their relative
strengths and weaknesses, with recommendations for future work.



Previous studies in Open Information Extraction (Open IE) are mainly based on
extraction patterns. They manually define patterns or automatically learn them
from a large corpus. However, these approaches are limited when grasping the
context of a sentence, and they fail to capture implicit relations. In this
paper, we address this problem with the following methods. First, we exploit
long short-term memory (LSTM) networks to extract higher-level features along
the shortest dependency paths, connecting headwords of relations and arguments.
The path-level features from LSTM networks provide useful clues regarding
contextual information and the validity of arguments. Second, we constructed
samples to train LSTM networks without the need for manual labeling. In
particular, feedback negative sampling picks highly negative samples among
non-positive samples through a model trained with positive samples. The
experimental results show that our approach produces more precise and abundant
extractions than state-of-the-art open IE systems. To the best of our
knowledge, this is the first work to apply deep learning to Open IE.



Machines, not humans, are the world's dominant knowledge accumulators but
humans remain the dominant decision makers. Interpreting and disseminating the
knowledge accumulated by machines requires expertise, time, and is prone to
failure. The problem of how best to convey accumulated knowledge from computers
to humans is a critical bottleneck in the broader application of machine
learning. We propose an approach based on human teaching where the problem is
formalized as selecting a small subset of the data that will, with high
probability, lead the human user to the correct inference. This approach,
though successful for modeling human learning in simple laboratory experiments,
has failed to achieve broader relevance due to challenges in formulating
general and scalable algorithms. We propose general-purpose teaching via
pseudo-marginal sampling and demonstrate the algorithm by teaching topic
models. Simulation results show our sampling-based approach: effectively
approximates the probability where ground-truth is possible via enumeration,
results in data that are markedly different from those expected by random
sampling, and speeds learning especially for small amounts of data. Application
to movie synopsis data illustrates differences between teaching and random
sampling for teaching distributions and specific topics, and demonstrates gains
in scalability and applicability to real-world problems.



Many interesting real world domains involve reinforcement learning (RL) in
partially observable environments. Efficient learning in such domains is
important, but existing sample complexity bounds for partially observable RL
are at least exponential in the episode length. We give, to our knowledge, the
first partially observable RL algorithm with a polynomial bound on the number
of episodes on which the algorithm may not achieve near-optimal performance.
Our algorithm is suitable for an important class of episodic POMDPs. Our
approach builds on recent advances in method of moments for latent variable
model estimation.



A recent trend in probabilistic inference emphasizes the codification of
models in a formal syntax, with suitable high-level features such as
individuals, relations, and connectives, enabling descriptive clarity,
succinctness and circumventing the need for the modeler to engineer a custom
solver. Unfortunately, bringing these linguistic and pragmatic benefits to
numerical optimization has proven surprisingly challenging. In this paper, we
turn to these challenges: we introduce a rich modeling language, for which an
interior-point method computes approximate solutions in a generic way. While
logical features easily complicates the underlying model, often yielding
intricate dependencies, we exploit and cache local structure using algebraic
decision diagrams (ADDs). Indeed, standard matrix-vector algebra is efficiently
realizable in ADDs, but we argue and show that well-known optimization methods
are not ideal for ADDs. Our engine, therefore, invokes a sophisticated
matrix-free approach. We demonstrate the flexibility of the resulting
symbolic-numeric optimizer on decision making and compressed sensing tasks with
millions of non-zero entries.



Determinantal Point Processes (DPPs) are probabilistic models over all
subsets a ground set of $N$ items. They have recently gained prominence in
several applications that rely on "diverse" subsets. However, their
applicability to large problems is still limited due to the $\mathcal O(N^3)$
complexity of core tasks such as sampling and learning. We enable efficient
sampling and learning for DPPs by introducing KronDPP, a DPP model whose kernel
matrix decomposes as a tensor product of multiple smaller kernel matrices. This
decomposition immediately enables fast exact sampling. But contrary to what one
may expect, leveraging the Kronecker product structure for speeding up DPP
learning turns out to be more difficult. We overcome this challenge, and derive
batch and stochastic optimization algorithms for efficiently learning the
parameters of a KronDPP.



We describe CITlab's recognition system for the HTRtS competition attached to
the 13. International Conference on Document Analysis and Recognition, ICDAR
2015. The task comprises the recognition of historical handwritten documents.
The core algorithms of our system are based on multi-dimensional recurrent
neural networks (MDRNN) and connectionist temporal classification (CTC). The
software modules behind that as well as the basic utility technologies are
essentially powered by PLANET's ARGUS framework for intelligent text
recognition and image processing.



We propose a low cost and effective way to combine a free simulation software
and free CAD models for modeling human-object interaction in order to improve
human & object segmentation. It is intended for research scenarios related to
safe human-robot collaboration (SHRC) and interaction (SHRI) in the industrial
domain. The task of human and object modeling has been used for detecting
activity, and for inferring and predicting actions, different from those works,
we do human and object modeling in order to learn interactions in RGB-D data
for improving segmentation. For this purpose, we define a novel density
function to model a three dimensional (3D) scene in a virtual environment
(VREP). This density function takes into account various possible
configurations of human-object and object-object relationships and interactions
governed by their affordances. Using this function, we synthesize a large,
realistic and highly varied synthetic RGB-D dataset that we use for training.
We train a random forest classifier, and the pixelwise predictions obtained is
integrated as a unary term in a pairwise conditional random fields (CRF). Our
evaluation shows that modeling these interactions improves segmentation
performance by ~7\% in mean average precision and recall over state-of-the-art
methods that ignore these interactions in real-world data. Our approach is
computationally efficient, robust and can run real-time on consumer hardware.



Unsupervised learning of probabilistic models is a central yet challenging
problem in machine learning. Specifically, designing models with tractable
learning, sampling, inference and evaluation is crucial in solving this task.
We extend the space of such models using real-valued non-volume preserving
(real NVP) transformations, a set of powerful invertible and learnable
transformations, resulting in an unsupervised learning algorithm with exact
log-likelihood computation, exact sampling, exact inference of latent
variables, and an interpretable latent space. We demonstrate its ability to
model natural images on four datasets through sampling, log-likelihood
evaluation and latent variable manipulations.



Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most
popular algorithms for computational inference in Graphical Models (GM). In
principle, MCMC is an exact probabilistic method which, however, often suffers
from exponentially slow mixing. In contrast, BP is a deterministic method,
which is typically fast, empirically very successful, however in general
lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC
algorithms correcting the approximation error of BP, i.e., we provide a way to
compensate for BP errors via a consecutive BP-aware MCMC. Our framework is
based on the Loop Calculus (LC) approach which allows to express the BP error
as a sum of weighted generalized loops. Although the full series is
computationally intractable, it is known that a truncated series, summing up
all 2-regular loops, is computable in polynomial-time for planar pair-wise
binary GMs and it also provides a highly accurate approximation empirically.
Motivated by this, we first propose a polynomial-time approximation MCMC scheme
for the truncated series of general (non-planar) pair-wise binary models. Our
main idea here is to use the Worm algorithm, known to provide fast mixing in
other (related) problems, and then design an appropriate rejection scheme to
sample 2-regular loops. Furthermore, we also design an efficient rejection-free
MCMC scheme for approximating the full series. The main novelty underlying our
design is in utilizing the concept of cycle basis, which provides an efficient
decomposition of the generalized loops. In essence, the proposed MCMC schemes
run on transformed GM built upon the non-trivial BP solution, and our
experiments show that this synthesis of BP and MCMC outperforms both direct
MCMC and bare BP schemes.



In this paper, we introduce a new set of reinforcement learning (RL) tasks in
Minecraft (a flexible 3D world). We then use these tasks to systematically
compare and contrast existing deep reinforcement learning (DRL) architectures
with our new memory-based DRL architectures. These tasks are designed to
emphasize, in a controllable manner, issues that pose challenges for RL methods
including partial observability (due to first-person visual observations),
delayed rewards, high-dimensional visual observations, and the need to use
active perception in a correct manner so as to perform well in the tasks. While
these tasks are conceptually simple to describe, by virtue of having all of
these challenges simultaneously they are difficult for current DRL
architectures. Additionally, we evaluate the generalization performance of the
architectures on environments not used during training. The experimental
results show that our new architectures generalize to unseen environments
better than existing DRL architectures.



We propose a new internal guidance method for automated theorem provers based
on the given-clause algorithm. Our method influences the choice of unprocessed
clauses using positive and negative examples from previous proofs. To this end,
we present an efficient scheme for Naive Bayesian classification by
generalising label occurrences to types with monoid structure. This makes it
possible to extend existing fast classifiers, which consider only positive
examples, with negative ones. We implement the method in the higher-order logic
prover Satallax, where we modify the delay with which propositions are
processed. We evaluated our method on a simply-typed higher-order logic version
of the Flyspeck project, where it solves 26% more problems than Satallax
without internal guidance.



Deep neural networks (DNNs) have demonstrated state-of-the-art results on
many pattern recognition tasks, especially vision classification problems.
Understanding the inner workings of such computational brains is both
fascinating basic science that is interesting in its own right - similar to why
we study the human brain - and will enable researchers to further improve DNNs.
One path to understanding how a neural network functions internally is to study
what each of its neurons has learned to detect. One such method is called
activation maximization (AM), which synthesizes an input (e.g. an image) that
highly activates a neuron. Here we dramatically improve the qualitative state
of the art of activation maximization by harnessing a powerful, learned prior:
a deep generator network (DGN). The algorithm (1) generates qualitatively
state-of-the-art synthetic images that look almost real, (2) reveals the
features learned by each neuron in an interpretable way, (3) generalizes well
to new datasets and somewhat well to different network architectures without
requiring the prior to be relearned, and (4) can be considered as a
high-quality generative method (in this case, by generating novel, creative,
interesting, recognizable images).



We show that the climate phenomena of El Nino and La Nina arise naturally as
states of macro-variables when our recent causal feature learning framework
(Chalupka 2015, Chalupka 2016) is applied to micro-level measures of zonal wind
(ZW) and sea surface temperatures (SST) taken over the equatorial band of the
Pacific Ocean. The method identifies these unusual climate states on the basis
of the relation between ZW and SST patterns without any input about past
occurrences of El Nino or La Nina. The simpler alternatives of (i) clustering
the SST fields while disregarding their relationship with ZW patterns, or (ii)
clustering the joint ZW-SST patterns, do not discover El Nino. We discuss the
degree to which our method supports a causal interpretation and use a
low-dimensional toy example to explain its success over other clustering
approaches. Finally, we propose a new robust and scalable alternative to our
original algorithm (Chalupka 2016), which circumvents the need for
high-dimensional density learning.



We propose a model of interdependent scheduling games in which each player
controls a set of services that they schedule independently. A player is free
to schedule his own services at any time; however, each of these services only
begins to accrue reward for the player when all predecessor services, which may
or may not be controlled by the same player, have been activated. This model,
where players have interdependent services, is motivated by the problems faced
in planning and coordinating large-scale infrastructures, e.g., restoring
electricity and gas to residents after a natural disaster or providing medical
care in a crisis when different agencies are responsible for the delivery of
staff, equipment, and medicine. We undertake a game-theoretic analysis of this
setting and in particular consider the issues of welfare maximization,
computing best responses, Nash dynamics, and existence and computation of Nash
equilibria.



Specialized dictionaries are used to understand concepts in specific domains,
especially where those concepts are not part of the general vocabulary, or
having meanings that differ from ordinary languages. The first step in creating
a specialized dictionary involves detecting the characteristic vocabulary of
the domain in question. Classical methods for detecting this vocabulary involve
gathering a domain corpus, calculating statistics on the terms found there, and
then comparing these statistics to a background or general language corpus.
Terms which are found significantly more often in the specialized corpus than
in the background corpus are candidates for the characteristic vocabulary of
the domain. Here we present two tools, a directed crawler, and a distributional
semantics package, that can be used together, circumventing the need of a
background corpus. Both tools are available on the web.



Adaptive learning rate algorithms such as RMSProp are widely used for
training deep neural networks. RMSProp offers efficient training since it uses
first order gradients to approximate Hessian-based preconditioning. However,
since the first order gradients include noise caused by stochastic
optimization, the approximation may be inaccurate. In this paper, we propose a
novel adaptive learning rate algorithm called SDProp. Its key idea is effective
handling of the noise by preconditioning based on covariance matrix. For
various neural networks, our approach is more efficient and effective than
RMSProp and its variant.



Scalable and effective exploration remains a key challenge in reinforcement
learning (RL). While there are methods with optimality guarantees in the
setting of discrete state and action spaces, these methods cannot be applied in
high-dimensional deep RL scenarios. As such, most contemporary RL relies on
simple heuristics such as epsilon-greedy exploration or adding Gaussian noise
to the controls. This paper introduces Variational Information Maximizing
Exploration (VIME), an exploration strategy based on maximization of
information gain about the agent's belief of environment dynamics. We propose a
practical implementation, using variational inference in Bayesian neural
networks which efficiently handles continuous state and action spaces. VIME
modifies the MDP reward function, and can be applied with several different
underlying RL algorithms. We demonstrate that VIME achieves significantly
better performance compared to heuristic exploration methods across a variety
of continuous control tasks and algorithms, including tasks with very sparse
rewards.



Reinforcement learning for embodied agents is a challenging problem. The
accumulated reward to be optimized is often a very rugged function, and
gradient methods are impaired by many local optimizers. We demonstrate, in an
experimental setting, that incorporating an intrinsic reward can smoothen the
optimization landscape while preserving the global optimizers of interest. We
show that policy gradient optimization for locomotion in a complex morphology
is significantly improved when supplementing the extrinsic reward by an
intrinsic reward defined in terms of the mutual information of time consecutive
sensor readings.



The maturity of deep learning techniques has led in recent years to a
breakthrough in object recognition in visual media. While for some specific
benchmarks, neural techniques seem to match if not outperform human judgement,
challenges are still open for detecting arbitrary concepts in arbitrary videos.
In this paper, we propose a system that combines neural techniques, a large
scale visual concepts ontology, and an active learning loop, to provide on the
fly model learning of arbitrary concepts. We give an overview of the system as
a whole, and focus on the central role of the ontology for guiding and
bootstrapping the learning of new concepts, improving the recall of concept
detection, and, on the user end, providing semantic search on a library of
annotated videos.



This paper presents a Directed Controller Synthesis (DCS) technique for
discrete event systems. The DCS method explores the solution space for reactive
controllers guided by a domain-independent heuristic. The heuristic is derived
from an efficient abstraction of the environment based on the componentized way
in which complex environments are described. Then by building the composition
of the components on-the-fly DCS obtains a solution by exploring a reduced
portion of the state space. This work focuses on untimed discrete event systems
with safety and co-safety (i.e. reachability) goals. An evaluation for the
technique is presented comparing it to other well-known approaches to
controller synthesis (based on symbolic representation and compositional
analyses).



This paper introduces a new technique for quantifying the approximation error
of a broad class of probabilistic inference programs, including ones based on
both variational and Monte Carlo approaches. The key idea is to derive a
subjective bound on the symmetrized KL divergence between the distribution
achieved by an approximate inference program and its true target distribution.
The bound's validity (and subjectivity) rests on the accuracy of two auxiliary
probabilistic programs: (i) a "reference" inference program that defines a gold
standard of accuracy and (ii) a "meta-inference" program that answers the
question "what internal random choices did the original approximate inference
program probably make given that it produced a particular result?" The paper
includes empirical results on inference problems drawn from linear regression,
Dirichlet process mixture modeling, HMMs, and Bayesian networks. The
experiments show that the technique is robust to the quality of the reference
inference program and that it can detect implementation bugs that are not
apparent from predictive performance.



With the rapid growth of knowledge bases (KBs) on the web, how to take full
advantage of them becomes increasingly important. Knowledge base-based question
answering (KB-QA) is one of the most promising approaches to access the
substantial knowledge. Meantime, as the neural network-based (NN-based) methods
develop, NN-based KB-QA has already achieved impressive results. However,
previous work did not put emphasis on question representation, and the question
is converted into a fixed vector regardless of its candidate answers. This
simple representation strategy is unable to express the proper information of
the question. Hence, we present a neural attention-based model to represent the
questions dynamically according to the different focuses of various candidate
answer aspects. In addition, we leverage the global knowledge inside the
underlying KB, aiming at integrating the rich KB information into the
representation of the answers. And it also alleviates the out of vocabulary
(OOV) problem, which helps the attention model to represent the question more
precisely. The experimental results on WEBQUESTIONS demonstrate the
effectiveness of the proposed approach.



The Schatten-p quasi-norm $(0<p<1)$ is usually used to replace the standard
nuclear norm in order to approximate the rank function more accurately.
However, existing Schatten-p quasi-norm minimization algorithms involve
singular value decomposition (SVD) or eigenvalue decomposition (EVD) in each
iteration, and thus may become very slow and impractical for large-scale
problems. In this paper, we first define two tractable Schatten quasi-norms,
i.e., the Frobenius/nuclear hybrid and bi-nuclear quasi-norms, and then prove
that they are in essence the Schatten-2/3 and 1/2 quasi-norms, respectively,
which lead to the design of very efficient algorithms that only need to update
two much smaller factor matrices. We also design two efficient proximal
alternating linearized minimization algorithms for solving representative
matrix completion problems. Finally, we provide the global convergence and
performance guarantees for our algorithms, which have better convergence
properties than existing algorithms. Experimental results on synthetic and
real-world data show that our algorithms are more accurate than the
state-of-the-art methods, and are orders of magnitude faster.



This paper presents a model for end-to-end learning of task-oriented dialog
systems. The main component of the model is a recurrent neural network (an
LSTM), which maps from raw dialog history directly to a distribution over
system actions. The LSTM automatically infers a representation of dialog
history, which relieves the system developer of much of the manual feature
engineering of dialog state. In addition, the developer can provide software
that expresses business rules and provides access to programmatic APIs,
enabling the LSTM to take actions in the real world on behalf of the user. The
LSTM can be optimized using supervised learning (SL), where a domain expert
provides example dialogs which the LSTM should imitate; or using reinforcement
learning (RL), where the system improves by interacting directly with end
users. Experiments show that SL and RL are complementary: SL alone can derive a
reasonable initial policy from a small number of training dialogs; and starting
RL optimization with a policy trained with SL substantially accelerates the
learning rate of RL.



Real-world multi-agent planning problems cannot be solved using
decision-theoretic planning methods due to the exponential complexity. We
approximate firefighting in rescue simulation as a spatially distributed task
and model with multi-agent Markov decision process. We use recent approximation
methods for spatial task problems to reduce the model complexity. Our
approximations are single-agent, static task, shortest path pruning, dynamic
planning horizon, and task clustering. We create scenarios from RoboCup Rescue
Simulation maps and evaluate our methods on these graph worlds. The results
show that our approach is faster and better than comparable methods and has
negligible performance loss compared to the optimal policy. We also show that
our method has a similar performance as DCOP methods on example RCRS scenarios.



The ability to reason with natural language is a fundamental prerequisite for
many NLP tasks such as information extraction, machine translation and question
answering. To quantify this ability, systems are commonly tested whether they
can recognize textual entailment, i.e., whether one sentence can be inferred
from another one. However, in most NLP applications only single source
sentences instead of sentence pairs are available. Hence, we propose a new task
that measures how well a model can generate an entailed sentence from a source
sentence. We take entailment-pairs of the Stanford Natural Language Inference
corpus and train an LSTM with attention. On a manually annotated test set we
found that 82% of generated sentences are correct, an improvement of 10.3% over
an LSTM baseline. A qualitative analysis shows that this model is not only
capable of shortening input sentences, but also inferring new statements via
paraphrasing and phrase entailment. We then apply this model recursively to
input-output pairs, thereby generating natural language inference chains that
can be used to automatically construct an entailment graph from source
sentences. Finally, by swapping source and target sentences we can also train a
model that given an input sentence invents additional information to generate a
new sentence.



Motivated by the search for a counterexample to the Poincar\'e conjecture in
three and four dimensions, the Andrews-Curtis conjecture was proposed in 1965.
It is now generally suspected that the Andrews-Curtis conjecture is false, but
small potential counterexamples are not so numerous, and previous work has
attempted to eliminate some via combinatorial search. Progress has however been
limited, with the most successful approach (breadth-first-search using
secondary storage) being neither scalable nor heuristically-informed. A
previous empirical analysis of problem structure examined several heuristic
measures of search progress and determined that none of them provided any
useful guidance for search. In this article, we induce new quality measures
directly from the problem structure and combine them to produce a more
effective search driver via ensemble machine learning. By this means, we
eliminate 19 potential counterexamples, the status of which had been unknown
for some years.



An open problem with categorical compositional distributional semantics is
the representation of words that are considered semantically vacuous from a
distributional perspective, such as determiners, prepositions, relative
pronouns or coordinators. This paper deals with the topic of coordination
between identical syntactic types, which accounts for the majority of
coordination cases in language. By exploiting the compact closed structure of
the underlying category and Frobenius operators canonically induced over the
fixed basis of finite-dimensional vector spaces, we provide a morphism as
representation of a coordinator tensor, and we show how it lifts from atomic
types to compound types. Linguistic intuitions are provided, and the importance
of the Frobenius operators as an addition to the compact closed setting with
regard to language is discussed.



Modeling textual or visual information with vector representations trained
from large language or visual datasets has been successfully explored in recent
years. However, tasks such as visual question answering require combining these
vector representations with each other. Approaches to multimodal pooling
include element-wise product or sum, as well as concatenation of the visual and
textual representations. We hypothesize that these methods are not as
expressive as an outer product of the visual and textual vectors. As the outer
product is typically infeasible due to its high dimensionality, we instead
propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and
expressively combine multimodal features. We extensively evaluate MCB on the
visual question answering and grounding tasks. We consistently show the benefit
of MCB over ablations without MCB. For visual question answering, we present an
architecture which uses MCB twice, once for predicting attention over spatial
features and again to combine the attended representation with the question
representation. This model outperforms the state-of-the-art on the Visual7W
dataset and the VQA challenge.



Algorithm design is a laborious process and often requires many iterations of
ideation and validation. In this paper, we explore automating algorithm design
and present a method to learn an optimization algorithm, which we believe to be
the first method that can automatically discover a better algorithm. We
approach this problem from a reinforcement learning perspective and represent
any particular optimization algorithm as a policy. We learn an optimization
algorithm using guided policy search and demonstrate that the resulting
algorithm outperforms existing hand-engineered algorithms in terms of
convergence speed and/or the final objective value.



We propose and investigate a semantics for "peer data exchange systems" where
different peers are related by data exchange constraints and trust
relationships. These two elements plus the data at the peers' sites and their
local integrity constraints are made compatible via a semantics that
characterizes sets of "solution instances" for the peers. They are the intended
-possibly virtual- instances for a peer that are obtained through a data repair
semantics that we introduce and investigate. The semantically correct answers
from a peer to a query, the so-called "peer consistent answers", are defined as
those answers that are invariant under all its different solution instances. We
show that solution instances can be specified as the models of logic programs
with a stable model semantics. The repair semantics is based on null values as
used in SQL databases, and is also of independent interest for repairs of
single databases with respect to integrity constraints.



Prior to seeking professional medical care it is increasingly common for
patients to use online resources such as automated symptom checkers. Many such
systems attempt to provide a differential diagnosis based on the symptoms
elucidated from the user, which may lead to anxiety if life or limb-threatening
conditions are part of the list, a phenomenon termed 'cyberchondria' [1].
Systems that provide advice on where to seek help, rather than a diagnosis, are
equally popular, and in our view provide the most useful information. In this
technical report we describe how such a triage system can be modelled
computationally, how medical insights can be translated into triage flows, and
how such systems can be validated and tested. We present babylon check, our
commercially deployed automated triage system, as a case study, and illustrate
its performance in a large, semi-naturalistic deployment study.



We introduce a novel playlist generation algorithm that focuses on the
quality of transitions using a recurrent neural network (RNN). The proposed
model assumes that optimal transitions between tracks can be modelled and
predicted by internal transitions within music tracks. We introduce modelling
sequences of high-level music descriptors using RNNs and discuss an experiment
involving different similarity functions, where the sequences are provided by a
musical structural analysis algorithm. Qualitative observations show that the
proposed approach can effectively model transitions of music tracks in
playlists.



Continual Learning in artificial neural networks suffers from interference
and forgetting when different tasks are learned sequentially. This paper
introduces the Active Long Term Memory Networks (A-LTM), a model of sequential
multi-task deep learning that is able to maintain previously learned
association between sensory input and behavioral output while acquiring knew
knowledge. A-LTM exploits the non-convex nature of deep neural networks and
actively maintains knowledge of previously learned, inactive tasks using a
distillation loss. Distortions of the learned input-output map are penalized
but hidden layers are free to transverse towards new local optima that are more
favorable for the multi-task objective. We re-frame the McClelland's seminal
Hippocampal theory with respect to Catastrophic Inference (CI) behavior
exhibited by modern deep architectures trained with back-propagation and
inhomogeneous sampling of latent factors across epochs. We present empirical
results of non-trivial CI during continual learning in Deep Linear Networks
trained on the same task, in Convolutional Neural Networks when the task shifts
from predicting semantic to graphical factors and during domain adaptation from
simple to complex environments. We present results of the A-LTM model's ability
to maintain viewpoint recognition learned in the highly controlled iLab-20M
dataset with 10 object categories and 88 camera viewpoints, while adapting to
the unstructured domain of Imagenet with 1,000 object categories.



We introduce SE3-Nets, which are deep neural networks designed to model and
learn rigid body motion from raw point cloud data. Based only on sequences of
depth images along with action vectors and point wise data associations,
SE3-Nets learn to segment effected object parts and predict their motion
resulting from the applied force. Rather than learning point wise flow vectors,
SE3-Nets predict SE3 transformations for different parts of the scene. Using
simulated depth data of a table top scene and a robot manipulator, we show that
the structure underlying SE3-Nets enables them to generate a far more
consistent prediction of object motion than traditional flow based networks.
Additional experiments with a depth camera observing a Baxter robot pushing
objects on a table show that SE3-Nets also work well on real data.



Learning robust value functions given raw observations and rewards is now
possible with model-free and model-based deep reinforcement learning
algorithms. There is a third alternative, called Successor Representations
(SR), which decomposes the value function into two components -- a reward
predictor and a successor map. The successor map represents the expected future
state occupancy from any given state and the reward predictor maps states to
scalar rewards. The value function of a state can be computed as the inner
product between the successor map and the reward weights. In this paper, we
present DSR, which generalizes SR within an end-to-end deep reinforcement
learning framework. DSR has several appealing properties including: increased
sensitivity to distal reward changes due to factorization of reward and world
dynamics, and the ability to extract bottleneck states (subgoals) given
successor maps trained under a random policy. We show the efficacy of our
approach on two diverse environments given raw pixel observations -- simple
grid-world domains (MazeBase) and the Doom game engine.



We derive a relationship between network representation in energy-efficient
neuromorphic architectures and block Toplitz convolutional matrices. Inspired
by this connection, we develop deep convolutional networks using a family of
structured convolutional matrices and achieve state-of-the-art trade-off
between energy efficiency and classification accuracy for well-known image
recognition tasks. We also put forward a novel method to train binary
convolutional networks by utilising an existing connection between
noisy-rectified linear units and binary activations.



As robots enter human environments, they will be expected to accomplish a
tremendous range of tasks. It is not feasible for robot designers to
pre-program these behaviors or know them in advance, so one way to address this
is through end-user programming, such as via learning from demonstration (LfD).
While significant work has been done on the mechanics of enabling robot
learning from human teachers, one unexplored aspect is enabling mutual feedback
between both the human teacher and robot during the learning process, i.e.,
implicit learning. In this paper, we explore one aspect of this mutual
understanding, grounding sequences, where both a human and robot provide
non-verbal feedback to signify their mutual understanding during interaction.
We conducted a study where people taught an autonomous humanoid robot a dance,
and performed gesture analysis to measure people's responses to the robot
during correct and incorrect demonstrations.



This paper presents an end-to-end framework for task-oriented dialog systems
using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to
interface with a relational database and jointly learn policies for both
language understanding and dialog strategy. Moreover, we propose a hybrid
algorithm that combines the strength of reinforcement learning and supervised
learning to achieve faster learning speed. We evaluated the proposed model on a
20 Question Game conversational game simulator. Results show that the proposed
method outperforms the modular-based baseline and learns a distributed
representation of the latent dialog state.



Concerns over the risks associated with advances in Artificial Intelligence
have prompted calls for greater efforts toward robust and beneficial AI,
including machine ethics. Recently, roboticists have responded by initiating
the development of so-called ethical robots. These robots would, ideally,
evaluate the consequences of their actions and morally justify their choices.
This emerging field promises to develop extensively over the next years.
However, in this paper, we point out an inherent limitation of the emerging
field of ethical robots. We show that building ethical robots also necessarily
facilitates the construction of unethical robots. In three experiments, we show
that it is remarkably easy to modify an ethical robot so that it behaves
competitively, or even aggressively. The reason for this is that the specific
AI, required to make an ethical robot, can always be exploited to make
unethical robots. Hence, the development of ethical robots will not guarantee
the responsible deployment of AI. While advocating for ethical robots, we
conclude that preventing the misuse of robots is beyond the scope of
engineering, and requires instead governance frameworks underpinned by
legislation. Without this, the development of ethical robots will serve to
increase the risks of robotic malpractice instead of diminishing it.



In this work, we take a fresh look at some old and new algorithms for
off-policy, return-based reinforcement learning. Expressing these in a common
form, we derive a novel algorithm, Retrace($\lambda$), with three desired
properties: (1) it has low variance; (2) it safely uses samples collected from
any behaviour policy, whatever its degree of "off-policyness"; and (3) it is
efficient as it makes the best use of samples collected from near on-policy
behaviour policies. We analyze the contractive nature of the related operator
under both off-policy policy evaluation and control settings and derive online
sample-based algorithms. We believe this is the first return-based off-policy
control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy
in the Limit with Infinite Exploration). As a corollary, we prove the
convergence of Watkins' Q($\lambda$), which was an open problem since 1989. We
illustrate the benefits of Retrace($\lambda$) on a standard suite of Atari 2600
games.



Recurrent neural networks such as the GRU and LSTM found wide adoption in
natural language processing and achieve state-of-the-art results for many
tasks. These models are characterized by a memory state that can be written to
and read from by applying gated composition operations to the current input and
the previous state. However, they only cover a small subset of potentially
useful compositions. We propose Multi-Function Recurrent Units (MuFuRUs) that
allow for arbitrary differentiable functions as composition operations.
Furthermore, MuFuRUs allow for an input- and state-dependent choice of these
composition operations that is learned. Our experiments demonstrate that the
additional functionality helps in different sequence modeling tasks, including
the evaluation of propositional logic formulae, language modeling and sentiment
analysis.



Building systems that possess the sensitivity and intelligence to identify
and describe high-level attributes in music audio signals continues to be an
elusive goal, but one that surely has broad and deep implications for a wide
variety of applications. Hundreds of papers have so far been published toward
this goal, and great progress appears to have been made. Some systems produce
remarkable accuracies at recognising high-level semantic concepts, such as
music style, genre and mood. However, it might be that these numbers do not
mean what they seem. In this paper, we take a state-of-the-art music content
analysis system and investigate what causes it to achieve exceptionally high
performance in a benchmark music audio dataset. We dissect the system to
understand its operation, determine its sensitivities and limitations, and
predict the kinds of knowledge it could and could not possess about music. We
perform a series of experiments to illuminate what the system has actually
learned to do, and to what extent it is performing the intended music listening
task. Our results demonstrate how the initial manifestation of music
intelligence in this state-of-the-art can be deceptive. Our work provides
constructive directions toward developing music content analysis systems that
can address the music information and creation needs of real-world users.



A complex nature of big data resources demands new methods for structuring
especially for textual content. WordNet is a good knowledge source for
comprehensive abstraction of natural language as its good implementations exist
for many languages. Since WordNet embeds natural language in the form of a
complex network, a transformation mechanism WordNet2Vec is proposed in the
paper. It creates vectors for each word from WordNet. These vectors encapsulate
general position - role of a given word towards all other words in the natural
language. Any list or set of such vectors contains knowledge about the context
of its component within the whole language. Such word representation can be
easily applied to many analytic tasks like classification or clustering. The
usefulness of the WordNet2Vec method was demonstrated in sentiment analysis,
i.e. classification with transfer learning for the real Amazon opinion textual
dataset.



Gibbs sampling is a Markov Chain Monte Carlo sampling technique that
iteratively samples variables from their conditional distributions. There are
two common scan orders for the variables: random scan and systematic scan. Due
to the benefits of locality in hardware, systematic scan is commonly used, even
though most statistical guarantees are only for random scan. While it has been
conjectured that the mixing times of random scan and systematic scan do not
differ by more than a logarithmic factor, we show by counterexample that this
is not the case, and we prove that that the mixing times do not differ by more
than a polynomial factor under mild conditions. To prove these relative bounds,
we introduce a method of augmenting the state space to study systematic scan
using conductance.



Objective: Patient notes in electronic health records (EHRs) may contain
critical information for medical investigations. However, the vast majority of
medical investigators can only access de-identified notes, in order to protect
the confidentiality of patients. In the United States, the Health Insurance
Portability and Accountability Act (HIPAA) defines 18 types of protected health
information (PHI) that needs to be removed to de-identify patient notes. Manual
de-identification is impractical given the size of EHR databases, the limited
number of researchers with access to the non-de-identified notes, and the
frequent mistakes of human annotators. A reliable automated de-identification
system would consequently be of high value.
  Materials and Methods: We introduce the first de-identification system based
on artificial neural networks (ANNs), which requires no handcrafted features or
rules, unlike existing systems. We compare the performance of the system with
state-of-the-art systems on two datasets: the i2b2 2014 de-identification
challenge dataset, which is the largest publicly available de-identification
dataset, and the MIMIC de-identification dataset, which we assembled and is
twice as large as the i2b2 2014 dataset.
  Results: Our ANN model outperforms the state-of-the-art systems. It yields an
F1-score of 97.85 on the i2b2 2014 dataset, with a recall 97.38 and a precision
of 97.32, and an F1-score of 99.23 on the MIMIC de-identification dataset, with
a recall 99.25 and a precision of 99.06.
  Conclusion: Our findings support the use of ANNs for de-identification of
patient notes, as they show better performance than previously published
systems while requiring no feature engineering.



A backbone of a boolean formula $F$ is a collection $S$ of its variables for
which there is a unique partial assignment $a_S$ such that $F[a_S]$ is
satisfiable [MZK+99,WGS03]. This paper studies the nontransparency of
backbones. We show that, under the widely believed assumption that integer
factoring is hard, there exist sets of boolean formulas that have obvious,
nontrivial backbones yet finding the values, $a_S$, of those backbones is
intractable. We also show that, under the same assumption, there exist sets of
boolean formulas that obviously have large backbones yet producing such a
backbone $S$ is intractable. Further, we show that if integer factoring is not
merely worst-case hard but is frequently hard, as is widely believed, then the
frequency of hardness in our two results is not too much less than that
frequency.



We introduce an online popularity prediction and tracking task as a benchmark
task for reinforcement learning with a combinatorial, natural language action
space. A specified number of discussion threads predicted to be popular are
recommended, chosen from a fixed window of recent comments to track. Novel deep
reinforcement learning architectures are studied for effective modeling of the
value function associated with actions comprised of interdependent sub-actions.
The proposed model, which represents dependence between sub-actions through a
bi-directional LSTM, gives the best performance across different experimental
configurations and domains, and it also generalizes well with varying numbers
of recommendation requests.



One of the core components of modern spoken dialogue systems is the belief
tracker, which estimates the user's goal at every step of the dialogue.
However, most current approaches have difficulty scaling to larger, more
complex dialogue domains. This is due to their dependency on either: a) Spoken
Language Understanding models that require large amounts of annotated training
data; or b) hand-crafted lexicons for capturing some of the linguistic
variation in users' language. We propose a novel Neural Belief Tracking (NBT)
framework which overcomes these problems by building on recent advances in
representation learning. NBT models reason over pre-trained word vectors,
learning to compose them into distributed representations of user utterances
and dialogue context. Our evaluation on two datasets shows that this approach
surpasses past limitations, matching the performance of state-of-the-art models
which rely on hand-crafted semantic lexicons and outperforming them when such
lexicons are not provided.



Probabilistic models analyze data by relying on a set of assumptions. Data
that exhibit deviations from these assumptions can undermine inference and
prediction quality. Robust models offer protection against mismatch between a
model's assumptions and reality. We propose a way to systematically detect and
mitigate mismatch of a large class of probabilistic models. The idea is to
raise the likelihood of each observation to a weight and then to infer both the
latent variables and the weights from data. Inferring the weights allows a
model to identify observations that match its assumptions and down-weight
others. This enables robust inference and improves predictive accuracy. We
study four different forms of mismatch with reality, ranging from missing
latent groups to structure misspecification. A Poisson factorization analysis
of the Movielens 1M dataset shows the benefits of this approach in a practical
scenario.



Many important NLP problems can be posed as dual-sequence or
sequence-to-sequence modeling tasks. Recent advances in building end-to-end
neural architectures have been highly successful in solving such tasks. In this
work we propose a new architecture for dual-sequence modeling that is based on
associative memory. We derive AM-RNNs, a recurrent associative memory (AM)
which augments generic recurrent neural networks (RNN). This architecture is
extended to the Dual AM-RNN which operates on two AMs at once. Our models
achieve very competitive results on textual entailment. A qualitative analysis
demonstrates that long range dependencies between source and target-sequence
can be bridged effectively using Dual AM-RNNs. However, an initial experiment
on auto-encoding reveals that these benefits are not exploited by the system
when learning to solve sequence-to-sequence tasks which indicates that
additional supervision or regularization is needed.



This work introduces a probabilistic-based model for binary CSP that provides
a fine grained analysis of its internal structure. Assuming that a domain
modification could occur in the CSP, it shows how to express, in a predictive
way, the probability that a domain value becomes inconsistent, then it express
the expectation of the number of arc-inconsistent values in each domain of the
constraint network. Thus, it express the expectation of the number of
arc-inconsistent values for the whole constraint network. Next, it provides
bounds for each of these three probabilistic indicators. Finally, a polytime
algorithm, which propagates the probabilistic information, is presented.



There is intense interest in applying machine learning to problems of causal
inference in fields such as healthcare, economics and education. In particular,
individual-level causal inference has important applications such as precision
medicine. We give a new theoretical analysis and family of algorithms for
predicting individual treatment effect (ITE) from observational data, under the
assumption known as strong ignorability. The algorithms learn a "balanced"
representation such that the induced treated and control distributions look
similar. We give a novel, simple and intuitive generalization-error bound
showing that the expected ITE estimation error of a representation is bounded
by a sum of the standard generalization-error of that representation and the
distance between the treated and control distributions induced by the
representation. We use Integral Probability Metrics to measure distances
between distributions, deriving explicit bounds for the Wasserstein and Maximum
Mean Discrepancy (MMD) distances. Experiments on real and simulated data show
the new algorithms match or outperform the state-of-the-art.



In this paper, we introduce Entropy/IP: a system that discovers Internet
address structure based on analyses of a subset of IPv6 addresses known to be
active, i.e., training data, gleaned by readily available passive and active
means. The system is completely automated and employs a combination of
information-theoretic and machine learning techniques to probabilistically
model IPv6 addresses. We present results showing that our system is effective
in exposing structural characteristics of portions of the IPv6 Internet address
space populated by active client, service, and router addresses.
  In addition to visualizing the address structure for exploration, the system
uses its models to generate candidate target addresses for scanning. For each
of 15 evaluated datasets, we train on 1K addresses and generate 1M candidates
for scanning. We achieve some success in 14 datasets, finding up to 40% of the
generated addresses to be active. In 11 of these datasets, we find active
network identifiers (e.g., /64 prefixes or `subnets') not seen in training.
Thus, we provide the first evidence that it is practical to discover subnets
and hosts by scanning probabilistically selected areas of the IPv6 address
space not known to contain active hosts a priori.



In many applications of black-box optimization, one can evaluate multiple
points simultaneously, e.g. when evaluating the performances of several
different neural network architectures in a parallel computing environment. In
this paper, we develop a novel batch Bayesian optimization algorithm --- the
parallel knowledge gradient method. By construction, this method provides the
one-step Bayes optimal batch of points to sample. We provide an efficient
strategy for computing this Bayes-optimal batch of points, and we demonstrate
that the parallel knowledge gradient method finds global optima significantly
faster than previous batch Bayesian optimization algorithms on both synthetic
test functions and when tuning hyperparameters of practical machine learning
algorithms, especially when function evaluations are noisy.



We propose Logic Tensor Networks: a uniform framework for integrating
automatic learning and reasoning. A logic formalism called Real Logic is
defined on a first-order language whereby formulas have truth-value in the
interval [0,1] and semantics defined concretely on the domain of real numbers.
Logical constants are interpreted as feature vectors of real numbers. Real
Logic promotes a well-founded integration of deductive reasoning on a
knowledge-base and efficient data-driven relational machine learning. We show
how Real Logic can be implemented in deep Tensor Neural Networks with the use
of Google's tensorflow primitives. The paper concludes with experiments
applying Logic Tensor Networks on a simple but representative example of
knowledge completion.



We study the effectiveness of neural sequence models for premise selection in
automated theorem proving, one of the main bottlenecks in the formalization of
mathematics. We propose a two stage approach for this task that yields good
results for the premise selection task on the Mizar corpus while avoiding the
hand-engineered features of existing state-of-the-art models. To our knowledge,
this is the first time deep learning has been applied to theorem proving on a
large scale.



Deep reinforcement learning has been shown to be a powerful framework for
learning policies from complex high-dimensional sensory inputs to actions in
complex tasks, such as the Atari domain. In this paper, we explore output
representation modeling in the form of temporal abstraction to improve
convergence and reliability of deep reinforcement learning approaches. We
concentrate on macro-actions, and evaluate these on different Atari 2600 games,
where we show that they yield significant improvements in learning speed.
Additionally, we show that they can even achieve better scores than DQN. We
offer analysis and explanation for both convergence and final results,
revealing a problem deep RL approaches have with sparse reward signals.



In classical reinforcement learning, when exploring an environment, agents
accept arbitrary short term loss for long term gain. This is infeasible for
safety critical applications, such as robotics, where even a single unsafe
action may cause system failure. In this paper, we address the problem of
safely exploring finite Markov decision processes (MDP). We define safety in
terms of an, a priori unknown, safety constraint that depends on states and
actions. We aim to explore the MDP under this constraint, assuming that the
unknown function satisfies regularity conditions expressed via a Gaussian
process prior. We develop a novel algorithm for this task and prove that it is
able to completely explore the safely reachable part of the MDP without
violating the safety constraint. To achieve this, it cautiously explores safe
states and actions in order to gain statistical confidence about the safety of
unvisited state-action pairs from noisy observations collected while navigating
the environment. Moreover, the algorithm explicitly considers reachability when
exploring the MDP, ensuring that it does not get stuck in any state with no
safe way out. We demonstrate our method on digital terrain models for the task
of exploring an unknown map with a rover.



An increasing number of domains are providing us with detailed trace data on
human decisions in settings where we can evaluate the quality of these
decisions via an algorithm. Motivated by this development, an emerging line of
work has begun to consider whether we can characterize and predict the kinds of
decisions where people are likely to make errors.
  To investigate what a general framework for human error prediction might look
like, we focus on a model system with a rich history in the behavioral
sciences: the decisions made by chess players as they select moves in a game.
We carry out our analysis at a large scale, employing datasets with several
million recorded games, and using chess tablebases to acquire a form of ground
truth for a subset of chess positions that have been completely solved by
computers but remain challenging even for the best players in the world.
  We organize our analysis around three categories of features that we argue
are present in most settings where the analysis of human error is applicable:
the skill of the decision-maker, the time available to make the decision, and
the inherent difficulty of the decision. We identify rich structure in all
three of these categories of features, and find strong evidence that in our
domain, features describing the inherent difficulty of an instance are
significantly more powerful than features based on skill or time.



We develop a belief space planning (BSP) approach that advances the state of
the art by incorporating reasoning about data association (DA) within planning,
while considering additional sources of uncertainty. Existing BSP approaches
typically assume data association is given and perfect, an assumption that can
be harder to justify while operating, in the presence of localization
uncertainty, in ambiguous and perceptually aliased environments. In contrast,
our data association aware belief space planning (DA-BSP) approach explicitly
reasons about DA within belief evolution, and as such can better accommodate
these challenging real world scenarios. In particular, we show that due to
perceptual aliasing, the posterior belief becomes a mixture of probability
distribution functions, and design cost functions that measure the expected
level of ambiguity and posterior uncertainty. Using these and standard costs
(e.g.~control penalty, distance to goal) within the objective function, yields
a general framework that reliably represents action impact, and in particular,
capable of active disambiguation. Our approach is thus applicable to robust
active perception and autonomous navigation in perceptually aliased
environments. We demonstrate key aspects in basic and realistic simulations.



We show how to estimate a model's test error from unlabeled data, on
distributions very different from the training distribution, while assuming
only that certain conditional independencies are preserved between train and
test. We do not need to assume that the optimal predictor is the same between
train and test, or that the true distribution lies in any parametric family. We
can also efficiently differentiate the error estimate to perform unsupervised
discriminative learning. Our technical tool is the method of moments, which
allows us to exploit conditional independencies in the absence of a
fully-specified model. Our framework encompasses a large family of losses
including the log and exponential loss, and extends to structured output
settings such as hidden Markov models.



We propose a new approach to the problem of neural network expressivity,
which seeks to characterize how structural properties of a neural network
family affect the functions it is able to compute. Our approach is based on an
interrelated set of measures of expressivity, unified by the novel notion of
trajectory length, which measures how the output of a network changes as the
input sweeps along a one-dimensional path. Our findings can be summarized as
follows:
  (1) The complexity of the computed function grows exponentially with depth.
  (2) All weights are not equal: trained networks are more sensitive to their
lower (initial) layer weights.
  (3) Regularizing on trajectory length (trajectory regularization) is a
simpler alternative to batch normalization, with the same performance.



This volume of EPTCS contains the proceedings of the First Workshop on
Hammers for Type Theories (HaTT 2016), held on 1 July 2016 as part of the
International Joint Conference on Automated Reasoning (IJCAR 2016) in Coimbra,
Portugal. The proceedings contain four regular papers, as well as abstracts of
the two invited talks by Pierre Corbineau (Verimag, France) and Aleksy Schubert
(University of Warsaw, Poland).



Multi-label classification has received considerable interest in recent
years. Multi-label classifiers have to address many problems including:
handling large-scale datasets with many instances and a large set of labels,
compensating missing label assignments in the training set, considering
correlations between labels, as well as exploiting unlabeled data to improve
prediction performance. To tackle datasets with a large set of labels,
embedding-based methods have been proposed which seek to represent the label
assignments in a low-dimensional space. Many state-of-the-art embedding-based
methods use a linear dimensionality reduction to represent the label
assignments in a low-dimensional space. However, by doing so, these methods
actually neglect the tail labels - labels that are infrequently assigned to
instances. We propose an embedding-based method that non-linearly embeds the
label vectors using an stochastic approach, thereby predicting the tail labels
more accurately. Moreover, the proposed method have excellent mechanisms for
handling missing labels, dealing with large-scale datasets, as well as
exploiting unlabeled data. With the best of our knowledge, our proposed method
is the first multi-label classifier that simultaneously addresses all of the
mentioned challenges. Experiments on real-world datasets show that our method
outperforms stateof-the-art multi-label classifiers by a large margin, in terms
of prediction performance, as well as training time.



Students opting for Engineering as their discipline is increasing rapidly.
But due to various factors and inappropriate primary education in India,
failure rates are high. Students are unable to excel in core engineering
because of complex and mathematical subjects. Hence, they fail in such
subjects. With the help of data mining techniques, we can predict the
performance of students in terms of grades and failure in subjects. This paper
performs a comparative analysis of various classification techniques, such as
Na\"ive Bayes, LibSVM, J48, Random Forest, and JRip and tries to choose best
among these. Based on the results obtained, we found that Na\"ive Bayes is the
most accurate method in terms of students failure prediction and JRip is most
accurate in terms of students grade prediction. We also found that JRip
marginally differs from Na\"ive Bayes in terms of accuracy for students failure
prediction and gives us a set of rules from which we derive the key factors
influencing students performance. Finally, we suggest various ways to mitigate
these factors. This study is limited to Indian Education system scenarios.
However, the factors found can be helpful in other scenarios as well.



We introduce LAMBADA, a dataset to evaluate the capabilities of computational
models for text understanding by means of a word prediction task. LAMBADA is a
collection of narrative passages sharing the characteristic that human subjects
are able to guess their last word if they are exposed to the whole passage, but
not if they only see the last sentence preceding the target word. To succeed on
LAMBADA, computational models cannot simply rely on local context, but must be
able to keep track of information in the broader discourse. We show that
LAMBADA exemplifies a wide range of linguistic phenomena, and that none of
several state-of-the-art language models reaches accuracy above 1% on this
novel benchmark. We thus propose LAMBADA as a challenging test set, meant to
encourage the development of new models capable of genuine understanding of
broad context in natural language text.



Product classification is the task of automatically predicting a taxonomy
path for a product in a predefined taxonomy hierarchy given a textual product
description or title. For efficient product classification we require a
suitable representation for a document (the textual description of a product)
feature vector and efficient and fast algorithms for prediction. To address the
above challenges, we propose a new distributional semantics representation for
document vector formation. We also develop a new two-level ensemble approach
utilizing (with respect to the taxonomy tree) a path-wise, node-wise and
depth-wise classifiers for error reduction in the final product classification.
Our experiments show the effectiveness of the distributional representation and
the ensemble approach on data sets from a leading e-commerce platform and
achieve better results on various evaluation metrics compared to earlier
approaches.



For an autonomous agent, executing a poor policy may be costly or even
dangerous. For such agents, it is desirable to determine confidence interval
lower bounds on the performance of any given policy without executing said
policy. Current methods for exact high confidence off-policy evaluation that
use importance sampling require a substantial amount of data to achieve a tight
lower bound. Existing model-based methods only address the problem in discrete
state spaces. Since exact bounds are intractable for many domains we trade off
strict guarantees of safety for more data-efficient approximate bounds. In this
context, we propose two bootstrapping off-policy evaluation methods which use
learned MDP transition models in order to estimate lower confidence bounds on
policy performance with limited data in both continuous and discrete state
spaces. Since direct use of a model may introduce bias, we derive a theoretical
upper bound on model bias for when the model transition function is estimated
with i.i.d. trajectories. This bound broadens our understanding of the
conditions under which model-based methods have high bias. Finally, we
empirically evaluate our proposed methods and analyze the settings in which
different bootstrapping off-policy confidence interval methods succeed and
fail.



This paper addresses a question about music cognition: how do we derive
polymetric structures. A preference rule system is presented which is
implemented into a drum computer. The preference rule system allows inferring
local polymetric structures, like two-over-three and three-over-two. By
analyzing the micro-timing of West African percussion music a timing pattern
consisting of six pulses was discovered. It integrates binary and ternary
rhythmic feels. The presented drum computer integrates the discovered
superimposed polymetric swing (timing and velocity) appropriate to the rhythmic
sequence the user inputs. For binary sequences, the amount of binary swing is
increased and for ternary sequences, the ternary swing is increased.



This paper describes a simple new semantics for logic rules, founded
semantics, and its straightforward extension to another simple new semantics,
constraint semantics. The new semantics support unrestricted negation, as well
as unrestricted existential and universal quantifications. They are uniquely
expressive and intuitive by allowing assumptions about the predicates and rules
to be specified explicitly. They are completely declarative and easy to
understand and relate cleanly to prior semantics. In addition, founded
semantics can be computed in linear time in the size of the ground program.



In statistical relational learning, the link prediction problem is key to
automatically understand the structure of large knowledge bases. As in previous
studies, we propose to solve this problem through latent factorization.
However, here we make use of complex valued embeddings. The composition of
complex embeddings can handle a large variety of binary relations, among them
symmetric and antisymmetric relations. Compared to state-of-the-art models such
as Neural Tensor Network and Holographic Embeddings, our approach based on
complex embeddings is arguably simpler, as it only uses the Hermitian dot
product, the complex counterpart of the standard dot product between real
vectors. Our approach is scalable to large datasets as it remains linear in
both space and time, while consistently outperforming alternative approaches on
standard link prediction benchmarks.



Can we train a system that, on any new input, either says "don't know" or
makes a prediction that is guaranteed to be correct? We answer the question in
the affirmative provided our model family is well-specified. Specifically, we
introduce the unanimity principle: only predict when all models consistent with
the training data predict the same output. We operationalize this principle for
semantic parsing, the task of mapping utterances to logical forms. We develop a
simple, efficient method that reasons over the infinite set of all consistent
models by only checking two of the models. We prove that our method obtains
100% precision even with a modest amount of training data from a possibly
adversarial distribution. Empirically, we demonstrate the effectiveness of our
approach on the standard GeoQuery dataset.



This paper contributes a preliminary report on the advantages and
disadvantages of incorporating simultaneous human control and feedback signals
in the training of a reinforcement learning robotic agent. While robotic
human-machine interfaces have become increasingly complex in both form and
function, control remains challenging for users. This has resulted in an
increasing gap between user control approaches and the number of robotic motors
which can be controlled. One way to address this gap is to shift some autonomy
to the robot. Semi-autonomous actions of the robotic agent can then be shaped
by human feedback, simplifying user control. Most prior work on agent shaping
by humans has incorporated training with feedback, or has included indirect
control signals. By contrast, in this paper we explore how a human can provide
concurrent feedback signals and real-time myoelectric control signals to train
a robot's actor-critic reinforcement learning control system. Using both a
physical and a simulated robotic system, we compare training performance on a
simple movement task when reward is derived from the environment, when reward
is provided by the human, and combinations of these two approaches. Our results
indicate that some benefit can be gained with the inclusion of human generated
feedback.



Attack graphs provide compact representations of the attack paths that an
attacker can follow to compromise network resources by analysing network
vulnerabilities and topology. These representations are a powerful tool for
security risk assessment. Bayesian inference on attack graphs enables the
estimation of the risk of compromise to the system's components given their
vulnerabilities and interconnections, and accounts for multi-step attacks
spreading through the system. Whilst static analysis considers the risk posture
at rest, dynamic analysis also accounts for evidence of compromise, e.g. from
SIEM software or forensic investigation. However, in this context, exact
Bayesian inference techniques do not scale well. In this paper we show how
Loopy Belief Propagation - an approximate inference technique - can be applied
to attack graphs, and that it scales linearly in the number of nodes for both
static and dynamic analysis, making such analyses viable for larger networks.
We experiment with different topologies and network clustering on synthetic
Bayesian attack graphs with thousands of nodes to show that the algorithm's
accuracy is acceptable and converge to a stable solution. We compare sequential
and parallel versions of Loopy Belief Propagation with exact inference
techniques for both static and dynamic analysis, showing the advantages of
approximate inference techniques to scale to larger attack graphs.



Constraint-based causal discovery from limited data is a notoriously
difficult challenge due to the many borderline independence test decisions.
Several approaches to improve the reliability of the predictions by exploiting
redundancy in the independence information have been proposed recently. Though
promising, existing approaches can still be greatly improved in terms of
accuracy and scalability. We present a novel method that reduces the
combinatorial explosion of the search space by using a more coarse-grained
representation of causal information, drastically reducing computation time.
Additionally, we propose a method to score causal predictions based on their
confidence. Crucially, our implementation also allows one to easily combine
observational and interventional data and to incorporate various types of
available background knowledge. We prove soundness and asymptotic consistency
of our method and demonstrate that it can outperform the state-of-the-art on
synthetic data, achieving a speedup of several orders of magnitude. We
illustrate its practical feasibility by applying it on a challenging protein
data set.



Conversational agents ("bots") are beginning to be widely used in
conversational interfaces. To design a system that is capable of emulating
human-like interactions, a conversational layer that can serve as a fabric for
chat-like interaction with the agent is needed. In this paper, we introduce a
model that employs Information Retrieval by utilizing convolutional deep
structured semantic neural network-based features in the ranker to present
human-like responses in ongoing conversation with a user. In conversations,
accounting for context is critical to the retrieval model; we show that our
context-sensitive approach using a Convolutional Deep Structured Semantic Model
(cDSSM) with character trigrams significantly outperforms several conventional
baselines in terms of the relevance of responses retrieved.



We report on a project to use a theorem prover to find proofs of the theorems
in Tarskian geometry. These theorems start with fundamental properties of
betweenness, proceed through the derivations of several famous theorems due to
Gupta and end with the derivation from Tarski's axioms of Hilbert's 1899 axioms
for geometry. They include the four challenge problems left unsolved by Quaife,
who two decades ago found some \Otter proofs in Tarskian geometry (solving
challenges issued in Wos's 1998 book). There are 212 theorems in this
collection. We were able to find \Otter proofs of all these theorems. We
developed a methodology for the automated preparation and checking of the input
files for those theorems, to ensure that no human error has corrupted the
formal development of an entire theory as embodied in two hundred input files
and proofs. We distinguish between proofs that were found completely
mechanically (without reference to the steps of a book proof) and proofs that
were constructed by some technique that involved a human knowing the steps of a
book proof. Proofs of length 40--100, roughly speaking, are difficult exercises
for a human, and proofs of 100-250 steps belong in a Ph.D. thesis or
publication. 29 of the proofs in our collection are longer than 40 steps, and
ten are longer than 90 steps. We were able to derive completely mechanically
all but 26 of the 183 theorems that have "short" proofs (40 or fewer deduction
steps). We found proofs of the rest, as well as the 29 "hard" theorems, using a
method that requires consulting the book proof at the outset. Our "subformula
strategy" enabled us to prove four of the 29 hard theorems completely
mechanically. These are Ph.D. level proofs, of length up to 108.



We present a simple approach for automatically extracting the number of
subjects involved in randomised controlled trials (RCT). Our approach first
applies a set of rule-based techniques to extract candidate study sizes from
the abstracts of the articles. Supervised classification is then performed over
the candidates with support vector machines, using a small set of lexical,
structural, and contextual features. With only a small annotated training set
of 201 RCTs, we obtained an accuracy of 88\%. We believe that this system will
aid complex medical text processing tasks such as summarisation and question
answering.



In recent years online advertising has become increasingly ubiquitous and
effective. Advertisements shown to visitors fund sites and apps that publish
digital content, manage social networks, and operate e-mail services. Given
such large variety of internet resources, determining an appropriate type of
advertising for a given platform has become critical to financial success.
Native advertisements, namely ads that are similar in look and feel to content,
have had great success in news and social feeds. However, to date there has not
been a winning formula for ads in e-mail clients. In this paper we describe a
system that leverages user purchase history determined from e-mail receipts to
deliver highly personalized product ads to Yahoo Mail users. We propose to use
a novel neural language-based algorithm specifically tailored for delivering
effective product recommendations, which was evaluated against baselines that
included showing popular products and products predicted based on
co-occurrence. We conducted rigorous offline testing using a large-scale
product purchase data set, covering purchases of more than 29 million users
from 172 e-commerce websites. Ads in the form of product recommendations were
successfully tested on online traffic, where we observed a steady 9% lift in
click-through rates over other ad formats in mail, as well as comparable lift
in conversion rates. Following successful tests, the system was launched into
production during the holiday season of 2014.



Markov models lie at the interface between statistical independence in a
probability distribution and graph separation properties. We review model
selection and estimation in directed and undirected Markov models with Gaussian
parametrization, emphasizing the main similarities and differences. These two
models are similar but not equivalent, although they share a common
intersection. We present the existing results from a historical perspective,
taking into account the amount of literature existing from both the artificial
intelligence and statistics research communities, where these models were
originated. We also discuss how the Gaussian assumption can be relaxed. We
finally point out the main areas of application where these Markov models are
nowadays used.



The 15th Conference on Theoretical Aspects of Rationality and Knowledge
(TARK) took place in Carnegie Mellon University, Pittsburgh, USA from June 4 to
6, 2015.
  The mission of the TARK conferences is to bring together researchers from a
wide variety of fields, including Artificial Intelligence, Cryptography,
Distributed Computing, Economics and Game Theory, Linguistics, Philosophy, and
Psychology, in order to further our understanding of interdisciplinary issues
involving reasoning about rationality and knowledge.
  These proceedings consist of a subset of the papers / abstracts presented at
the TARK conference.



Recently, a number of deep-learning based models have been proposed for the
task of Visual Question Answering (VQA). The performance of most models is
clustered around 60-70%. In this paper we propose systematic methods to analyze
the behavior of these models as a first step towards recognizing their
strengths and weaknesses, and identifying the most fruitful directions for
progress. We analyze two models, one each from two major classes of VQA models
-- with-attention and without-attention and show the similarities and
differences in the behavior of these models. We also analyze the winning entry
of the VQA Challenge 2016.
  Our behavior analysis reveals that despite recent progress, today's VQA
models are "myopic" (tend to fail on sufficiently novel instances), often "jump
to conclusions" (converge on a predicted answer after 'listening' to just half
the question), and are "stubborn" (do not change their answers across images).



We investigate an experiential learning paradigm for acquiring an internal
model of intuitive physics. Our model is evaluated on a real-world robotic
manipulation task that requires displacing objects to target locations by
poking. The robot gathered over 400 hours of experience by executing more than
100K pokes on different objects. We propose a novel approach based on deep
neural networks for modeling the dynamics of robot's interactions directly from
images, by jointly estimating forward and inverse models of dynamics. The
inverse model objective provides supervision to construct informative visual
features, which the forward model can then predict and in turn regularize the
feature space for the inverse model. The interplay between these two objectives
creates useful, accurate models that can then be used for multi-step decision
making. This formulation has the additional benefit that it is possible to
learn forward models in an abstract feature space and thus alleviate the need
of predicting pixels. Our experiments show that this joint modeling approach
outperforms alternative methods.



Recurrent neural networks, and in particular long short-term memory (LSTM)
networks, are a remarkably effective tool for sequence modeling that learn a
dense black-box hidden representation of their sequential input. Researchers
interested in better understanding these models have studied the changes in
hidden state representations over time and noticed some interpretable patterns
but also significant noise. In this work, we present LSTMVIS, a visual analysis
tool for recurrent neural networks with a focus on understanding these hidden
state dynamics. The tool allows users to select a hypothesis input range to
focus on local state changes, to match these states changes to similar patterns
in a large data set, and to align these results with structural annotations
from their domain. We show several use cases of the tool for analyzing specific
hidden state properties on dataset containing nesting, phrase structure, and
chord progressions, and demonstrate how the tool can be used to isolate
patterns for further statistical analysis. We characterize the domain, the
different stakeholders, and their goals and tasks.



Levels are a key component of many different video games, and a large body of
work has been produced on how to procedurally generate game levels. Recently,
Machine Learning techniques have been applied to video game level generation
towards the purpose of automatically generating levels that have the properties
of the training corpus. Towards that end we have made available a corpora of
video game levels in an easy to parse format ideal for different machine
learning and other game AI research purposes.



Temporal common sense has applications in AI tasks such as QA, multi-document
summarization, and human-AI communication. We propose the task of sequencing --
given a jumbled set of aligned image-caption pairs that belong to a story, the
task is to sort them such that the output sequence forms a coherent story. We
present multiple approaches, via unary (position) and pairwise (order)
predictions, and their ensemble-based combinations, achieving strong results on
this task. We use both text-based and image-based features, which depict
complementary improvements. Using qualitative examples, we demonstrate that our
models have learnt interesting aspects of temporal common sense.



Extensive work has been conducted both in game theory and logic to model
strategic interaction. An important question is whether we can use these
theories to design agents for interacting with people? On the one hand, they
provide a formal design specification for agent strategies. On the other hand,
people do not necessarily adhere to playing in accordance with these
strategies, and their behavior is affected by a multitude of social and
psychological factors. In this paper we will consider the question of whether
strategies implied by theories of strategic behavior can be used by automated
agents that interact proficiently with people. We will focus on automated
agents that we built that need to interact with people in two negotiation
settings: bargaining and deliberation. For bargaining we will study game-theory
based equilibrium agents and for argumentation we will discuss logic-based
argumentation theory. We will also consider security games and persuasion games
and will discuss the benefits of using equilibrium based agents.



Distributed knowledge is the sum of the knowledge in a group; what someone
who is able to discern between two possible worlds whenever any member of the
group can discern between them, would know. Sometimes distributed knowledge is
referred to as the potential knowledge of a group, or the joint knowledge they
could obtain if they had unlimited means of communication. In epistemic logic,
the formula D_G{\phi} is intended to express the fact that group G has
distributed knowledge of {\phi}, that there is enough information in the group
to infer {\phi}. But this is not the same as reasoning about what happens if
the members of the group share their information. In this paper we introduce an
operator R_G, such that R_G{\phi} means that {\phi} is true after G have shared
all their information with each other - after G's distributed knowledge has
been resolved. The R_G operators are called resolution operators. Semantically,
we say that an expression R_G{\phi} is true iff {\phi} is true in what van
Benthem [11, p. 249] calls (G's) communication core; the model update obtained
by removing links to states for members of G that are not linked by all members
of G. We study logics with different combinations of resolution operators and
operators for common and distributed knowledge. Of particular interest is the
relationship between distributed and common knowledge. The main results are
sound and complete axiomatizations.



Gossip protocols aim at arriving, by means of point-to-point or group
communications, at a situation in which all the agents know each other's
secrets. We consider distributed gossip protocols which are expressed by means
of epistemic logic. We provide an operational semantics of such protocols and
set up an appropriate framework to argue about their correctness. Then we
analyze specific protocols for complete graphs and for directed rings.



Information delivery in a network of agents is a key issue for large, complex
systems that need to do so in a predictable, efficient manner. The delivery of
information in such multi-agent systems is typically implemented through
routing protocols that determine how information flows through the network.
Different routing protocols exist each with its own benefits, but it is
generally unclear which properties can be successfully combined within a given
algorithm. We approach this problem from the axiomatic point of view, i.e., we
try to establish what are the properties we would seek to see in such a system,
and examine the different properties which uniquely define common routing
algorithms used today.
  We examine several desirable properties, such as robustness, which ensures
adding nodes and edges does not change the routing in a radical, unpredictable
ways; and properties that depend on the operating environment, such as an
"economic model", where nodes choose their paths based on the cost they are
charged to pass information to the next node. We proceed to fully characterize
minimal spanning tree, shortest path, and weakest link routing algorithms,
showing a tight set of axioms for each.



The Knowledge of Preconditions principle (KoP) is proposed as a widely
applicable connection between knowledge and action in multi-agent systems.
Roughly speaking, it asserts that if some condition is a necessary condition
for performing a given action A, then knowing that this condition holds is also
a necessary condition for performing A. Since the specifications of tasks often
involve necessary conditions for actions, the KoP principle shows that such
specifications induce knowledge preconditions for the actions. Distributed
protocols or multi-agent plans that satisfy the specifications must ensure that
this knowledge be attained, and that it is detected by the agents as a
condition for action. The knowledge of preconditions principle is formalised in
the runs and systems framework, and is proven to hold in a wide class of
settings. Well-known connections between knowledge and coordinated action are
extended and shown to derive directly from the KoP principle: a "common
knowledge of preconditions" principle is established showing that common
knowledge is a necessary condition for performing simultaneous actions, and a
"nested knowledge of preconditions" principle is proven, showing that
coordinating actions to be performed in linear temporal order requires a
corresponding form of nested knowledge.



In this paper, we introduce a lightweight dynamic epistemic logical framework
for automated planning under initial uncertainty. We reduce plan verification
and conformant planning to model checking problems of our logic. We show that
the model checking problem of the iteration-free fragment is PSPACE-complete.
By using two non-standard (but equivalent) semantics, we give novel model
checking algorithms to the full language and the iteration-free language.



An agent who lacks preferences and instead makes decisions using criteria
that are costly to create should select efficient sets of criteria, where the
cost of making a given number of choice distinctions is minimized. Under mild
conditions, efficiency requires that binary criteria with only two categories
per criterion are chosen. When applied to the problem of determining the
optimal number of digits in an information storage device, this result implies
that binary digits (bits) are the efficient solution, even when the marginal
cost of using additional digits declines rapidly to 0. This short paper pays
particular attention to the symmetry conditions entailed when sets of criteria
are efficient.



Although several RDF knowledge bases are available through the LOD
initiative, the ontology schema of such linked datasets is not very rich. In
particular, they lack object properties. The problem of finding new object
properties (and their instances) between any two given classes has not been
investigated in detail in the context of Linked Data. In this paper, we present
DART (Detecting Arbitrary Relations for enriching T-Boxes of Linked Data) - an
unsupervised solution to enrich the LOD cloud with new object properties
between two given classes. DART exploits contextual similarity to identify text
patterns from the web corpus that can potentially represent relations between
individuals. These text patterns are then clustered by means of paraphrase
detection to capture the object properties between the two given LOD classes.
DART also performs fully automated mapping of the discovered relations to the
properties in the linked dataset. This serves many purposes such as
identification of completely new relations, elimination of irrelevant
relations, and generation of prospective property axioms. We have empirically
evaluated our approach on several pairs of classes and found that the system
can indeed be used for enriching the linked datasets with new object properties
and their instances. We compared DART with newOntExt system which is an
offshoot of the NELL (Never-Ending Language Learning) effort. Our experiments
reveal that DART gives better results than newOntExt with respect to both the
correctness, as well as the number of relations.



This paper presents a new model for word sense disambiguation formulated in
terms of evolutionary game theory, where each word to be disambiguated is
represented as a node on a graph whose edges represent word relations and
senses are represented as classes. The words simultaneously update their class
membership preferences according to the senses that neighboring words are
likely to choose. We use distributional information to weigh the influence that
each word has on the decisions of the others and semantic similarity
information to measure the strength of compatibility among the choices. With
this information we can formulate the word sense disambiguation problem as a
constraint satisfaction problem and solve it using tools derived from game
theory, maintaining the textual coherence. The model is based on two ideas:
similar words should be assigned to similar classes and the meaning of a word
does not depend on all the words in a text but just on some of them. The paper
provides an in-depth motivation of the idea of modeling the word sense
disambiguation problem in terms of game theory, which is illustrated by an
example. The conclusion presents an extensive analysis on the combination of
similarity measures to use in the framework and a comparison with
state-of-the-art systems. The results show that our model outperforms
state-of-the-art algorithms and can be applied to different tasks and in
different scenarios.



Recently, the next-item/basket recommendation system, which considers the
sequential relation between bought items, has drawn attention of researchers.
The utilization of sequential patterns has boosted performance on several kinds
of recommendation tasks. Inspired by natural language processing (NLP)
techniques, we propose a novel neural network (NN) based next-song recommender,
CNN-rec, in this paper. Then, we compare the proposed system with several NN
based and classic recommendation systems on the next-song recommendation task.
Verification results indicate the proposed system outperforms classic systems
and has comparable performance with the state-of-the-art system.



We present in this paper an efficient approach for acoustic scene
classification by exploring the structure of class labels. Given a set of class
labels, a category taxonomy is automatically learned by collectively optimizing
a clustering of the labels into multiple meta-classes in a tree structure. An
acoustic scene instance is then embedded into a low-dimensional feature
representation which consists of the likelihoods that it belongs to the
meta-classes. We demonstrate state-of-the-art results on two different datasets
for the acoustic scene classification task, including the DCASE 2013 and LITIS
Rouen datasets.



A true lie is a lie that becomes true when announced. In a logic of
announcements, where the announcing agent is not modelled, a true lie is a
formula (that is false and) that becomes true when announced. We investigate
true lies and other types of interaction between announced formulas, their
preconditions and their postconditions, in the setting of Gerbrandy's logic of
believed announcements, wherein agents may have or obtain incorrect beliefs.
Our results are on the satisfiability and validity of instantiations of these
semantically defined categories, on iterated announcements, including
arbitrarily often iterated announcements, and on syntactic characterization. We
close with results for iterated announcements in the logic of knowledge
(instead of belief), and for lying as private announcements (instead of public
announcements) to different agents. Detailed examples illustrate our lying
concepts.



Methods based on representation learning currently hold the state-of-the-art
in many natural language processing and knowledge base inference tasks. Yet, a
major challenge is how to efficiently incorporate commonsense knowledge into
such models. A recent approach regularizes relation and entity representations
by propositionalization of first-order logic rules. However,
propositionalization does not scale beyond domains with only few entities and
rules. In this paper we present a highly efficient method for incorporating
implication rules into distributed representations for automated knowledge base
construction. We map entity-tuple embeddings into an approximately Boolean
space and encourage a partial ordering over relation embeddings based on
implication rules mined from WordNet. Surprisingly, we find that the strong
restriction of the entity-tuple embedding space does not hurt the
expressiveness of the model and even acts as a regularizer that improves
generalization. By incorporating few commonsense rules, we achieve an increase
of 2 percentage points mean average precision over a matrix factorization
baseline, while observing a negligible increase in runtime.



A function $f: \mathbb{Z}_+^E \rightarrow \mathbb{R}_+$ is DR-submodular if
it satisfies $f(\bx + \chi_i) -f (\bx) \ge f(\by + \chi_i) - f(\by)$ for all
$\bx\le \by, i\in E$. Recently, the problem of maximizing a DR-submodular
function $f: \mathbb{Z}_+^E \rightarrow \mathbb{R}_+$ subject to a budget
constraint $\|\bx\|_1 \leq B$ as well as additional constraints has received
significant attention \cite{SKIK14,SY15,MYK15,SY16}.
  In this note, we give a generic reduction from the DR-submodular setting to
the submodular setting. The running time of the reduction and the size of the
resulting submodular instance depends only \emph{logarithmically} on $B$. Using
this reduction, one can translate the results for unconstrained and constrained
submodular maximization to the DR-submodular setting for many types of
constraints in a unified manner.



Relational logistic regression (RLR) is a representation of conditional
probability in terms of weighted formulae for modelling multi-relational data.
In this paper, we develop a learning algorithm for RLR models. Learning an RLR
model from data consists of two steps: 1- learning the set of formulae to be
used in the model (a.k.a. structure learning) and learning the weight of each
formula (a.k.a. parameter learning). For structure learning, we deploy Schmidt
and Murphy's hierarchical assumption: first we learn a model with simple
formulae, then more complex formulae are added iteratively only if all their
sub-formulae have proven effective in previous learned models. For parameter
learning, we convert the problem into a non-relational learning problem and use
an off-the-shelf logistic regression learning algorithm from Weka, an
open-source machine learning tool, to learn the weights. We also indicate how
hidden features about the individuals can be incorporated into RLR to boost the
learning performance. We compare our learning algorithm to other structure and
parameter learning algorithms in the literature, and compare the performance of
RLR models to standard logistic regression and RDN-Boost on a modified version
of the MovieLens data-set.



This paper presents a simple but effective density-based outlier detection
approach with the local kernel density estimation (KDE). A Relative
Density-based Outlier Score (RDOS) is introduced to measure the local
outlierness of objects, in which the density distribution at the location of an
object is estimated with a local KDE method based on extended nearest neighbors
of the object. Instead of using only $k$ nearest neighbors, we further consider
reverse nearest neighbors and shared nearest neighbors of an object for density
distribution estimation. Some theoretical properties of the proposed RDOS
including its expected value and false alarm probability are derived. A
comprehensive experimental study on both synthetic and real-life data sets
demonstrates that our approach is more effective than state-of-the-art outlier
detection methods.



One of the most basic functions of language is to refer to objects in a
shared scene. Modeling reference with continuous representations is challenging
because it requires individuation, i.e., tracking and distinguishing an
arbitrary number of referents. We introduce a neural network model that, given
a definite description and a set of objects represented by natural images,
points to the intended object if the expression has a unique referent, or
indicates a failure, if it does not. The model, directly trained on reference
acts, is competitive with a pipeline manually engineered to perform the same
task, both when referents are purely visual, and when they are characterized by
a combination of visual and linguistic properties.



There are many declarative frameworks that allow us to implement code
formatters relatively easily for any specific language, but constructing them
is cumbersome. The first problem is that "everybody" wants to format their code
differently, leading to either many formatter variants or a ridiculous number
of configuration options. Second, the size of each implementation scales with a
language's grammar size, leading to hundreds of rules.
  In this paper, we solve the formatter construction problem using a novel
approach, one that automatically derives formatters for any given language
without intervention from a language expert. We introduce a code formatter
called CodeBuff that uses machine learning to abstract formatting rules from a
representative corpus, using a carefully designed feature set. Our experiments
on Java, SQL, and ANTLR grammars show that CodeBuff is efficient, has excellent
accuracy, and is grammar invariant for a given language. It also generalizes to
a 4th language tested during manuscript preparation.



In this paper, we present subgraph2vec, a novel approach for learning latent
representations of rooted subgraphs from large graphs inspired by recent
advancements in Deep Learning and Graph Kernels. These latent representations
encode semantic substructure dependencies in a continuous vector space, which
is easily exploited by statistical models for tasks such as graph
classification, clustering, link prediction and community detection.
subgraph2vec leverages on local information obtained from neighbourhoods of
nodes to learn their latent representations in an unsupervised fashion. We
demonstrate that subgraph vectors learnt by our approach could be used in
conjunction with classifiers such as CNNs, SVMs and relational data clustering
algorithms to achieve significantly superior accuracies. Also, we show that the
subgraph vectors could be used for building a deep learning variant of
Weisfeiler-Lehman graph kernel. Our experiments on several benchmark and
large-scale real-world datasets reveal that subgraph2vec achieves significant
improvements in accuracies over existing graph kernels on both supervised and
unsupervised learning tasks. Specifically, on two realworld program analysis
tasks, namely, code clone and malware detection, subgraph2vec outperforms
state-of-the-art kernels by more than 17% and 4%, respectively.



We consider the problem of personalization of online services from the
viewpoint of ad targeting, where we seek to find the best ad categories to be
shown to each user, resulting in improved user experience and increased
advertisers' revenue. We propose to address this problem as a task of ranking
the ad categories depending on a user's preference, and introduce a novel label
ranking approach capable of efficiently learning non-linear, highly accurate
models in large-scale settings. Experiments on a real-world advertising data
set with more than 3.2 million users show that the proposed algorithm
outperforms the existing solutions in terms of both rank loss and top-K
retrieval performance, strongly suggesting the benefit of using the proposed
model on large-scale ranking problems.



Neural Machine Translation (NMT), like many other deep learning domains,
typically suffers from over-parameterization, resulting in large storage sizes.
This paper examines three simple magnitude-based pruning schemes to compress
NMT models, namely class-blind, class-uniform, and class-distribution, which
differ in terms of how pruning thresholds are computed for the different
classes of weights in the NMT architecture. We demonstrate the efficacy of
weight pruning as a compression technique for a state-of-the-art NMT system. We
show that an NMT model with over 200 million parameters can be pruned by 40%
with very little performance loss as measured on the WMT'14 English-German
translation task. This sheds light on the distribution of redundancy in the NMT
architecture. Our main result is that with retraining, we can recover and even
surpass the original performance with an 80%-pruned model.



Email classification is still a mostly manual task. Consequently, most Web
mail users never define a single folder. Recently however, automatic
classification offering the same categories to all users has started to appear
in some Web mail clients, such as AOL or Gmail. We adopt this approach, rather
than previous (unsuccessful) personalized approaches because of the change in
the nature of consumer email traffic, which is now dominated by (non-spam)
machine-generated email. We propose here a novel approach for (1) automatically
distinguishing between personal and machine-generated email and (2) classifying
messages into latent categories, without requiring users to have defined any
folder. We report how we have discovered that a set of 6 "latent" categories
(one for human- and the others for machine-generated messages) can explain a
significant portion of email traffic. We describe in details the steps involved
in building a Web-scale email categorization system, from the collection of
ground-truth labels, the selection of features to the training of models.
Experimental evaluation was performed on more than 500 billion messages
received during a period of six months by users of Yahoo mail service, who
elected to be part of such research studies. Our system achieved precision and
recall rates close to 90% and the latent categories we discovered were shown to
cover 70% of both email traffic and email search queries. We believe that these
results pave the way for a change of approach in the Web mail industry, and
could support the invention of new large-scale email discovery paradigms that
had not been possible before.



Disjunctive Answer Set Programming (ASP) is a powerful declarative
programming paradigm whose main decision problems are located on the second
level of the polynomial hierarchy. Identifying tractable fragments and
developing efficient algorithms for such fragments are thus important
objectives in order to complement the sophisticated ASP systems available to
date. Hard problems can become tractable if some problem parameter is bounded
by a fixed constant; such problems are then called fixed-parameter tractable
(FPT). While several FPT results for ASP exist, parameters that relate to
directed or signed graphs representing the program at hand have been neglected
so far. In this paper, we first give some negative observations showing that
directed width measures on the dependency graph of a program do not lead to FPT
results. We then consider the graph parameter of signed clique-width and
present a novel dynamic programming algorithm that is FPT w.r.t. this
parameter. Clique-width is more general than the well-known treewidth, and, to
the best of our knowledge, ours is the first FPT algorithm for bounded
clique-width for reasoning problems beyond SAT.



Areas where Artificial Intelligence (AI) & related fields are finding their
applications are increasing day by day, moving from core areas of computer
science they are finding their applications in various other domains.In recent
times Machine Learning i.e. a sub-domain of AI has been widely used in order to
assist medical experts and doctors in the prediction, diagnosis and prognosis
of various diseases and other medical disorders. In this manuscript the authors
applied various machine learning algorithms to a problem in the domain of
medical diagnosis and analyzed their efficiency in predicting the results. The
problem selected for the study is the diagnosis of the Chronic Kidney
Disease.The dataset used for the study consists of 400 instances and 24
attributes. The authors evaluated 12 classification techniques by applying them
to the Chronic Kidney Disease data. In order to calculate efficiency, results
of the prediction by candidate methods were compared with the actual medical
results of the subject.The various metrics used for performance evaluation are
predictive accuracy, precision, sensitivity and specificity. The results
indicate that decision-tree performed best with nearly the accuracy of 98.6%,
sensitivity of 0.9720, precision of 1 and specificity of 1.



As a general means of expression, audio analysis and recognition has
attracted much attentions for its wide applications in real-life world. Audio
emotion recognition (AER) attempts to understand emotional states of human with
the given utterance signals, and has been studied abroad for its further
development on friendly human-machine interfaces. Distinguish from other
existing works, the person-dependent patterns of audio emotions are conducted,
and fractal dimension features are calculated for acoustic feature extraction.
Furthermore, it is able to efficiently learn intrinsic characteristics of
auditory emotions, while the utterance features are learned from fractal
dimensions of each sub-bands. Experimental results show the proposed method is
able to provide comparative performance for audio emotion recognition.



In this paper, we examine the problem of missing data in high-dimensional
datasets by taking into consideration the Missing Completely at Random and
Missing at Random mechanisms, as well as theArbitrary missing pattern.
Additionally, this paper employs a methodology based on Deep Learning and Swarm
Intelligence algorithms in order to provide reliable estimates for missing
data. The deep learning technique is used to extract features from the input
data via an unsupervised learning approach by modeling the data distribution
based on the input. This deep learning technique is then used as part of the
objective function for the swarm intelligence technique in order to estimate
the missing data after a supervised fine-tuning phase by minimizing an error
function based on the interrelationship and correlation between features in the
dataset. The investigated methodology in this paper therefore has longer
running times, however, the promising potential outcomes justify the trade-off.
Also, basic knowledge of statistics is presumed.



Mechanical devices such as engines, vehicles, aircrafts, etc., are typically
instrumented with numerous sensors to capture the behavior and health of the
machine. However, there are often external factors or variables which are not
captured by sensors leading to time-series which are inherently unpredictable.
For instance, manual controls and/or unmonitored environmental conditions or
load may lead to inherently unpredictable time-series. Detecting anomalies in
such scenarios becomes challenging using standard approaches based on
mathematical models that rely on stationarity, or prediction models that
utilize prediction errors to detect anomalies. We propose a Long Short Term
Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD)
that learns to reconstruct 'normal' time-series behavior, and thereafter uses
reconstruction error to detect anomalies. We experiment with three publicly
available quasi predictable time-series datasets: power demand, space shuttle,
and ECG, and two real-world engine datasets with both predictive and
unpredictable behavior. We show that EncDec-AD is robust and can detect
anomalies from predictable, unpredictable, periodic, aperiodic, and
quasi-periodic time-series. Further, we show that EncDec-AD is able to detect
anomalies from short time-series (length as small as 30) as well as long
time-series (length as large as 500).



Computational results demonstrate that posterior sampling for reinforcement
learning (PSRL) dramatically outperforms algorithms driven by optimism, such as
UCRL2. We provide insight into the extent of this performance boost and the
phenomenon that drives it. We leverage this insight to establish an
$\tilde{O}(H\sqrt{SAT})$ Bayesian expected regret bound for PSRL in
finite-horizon episodic Markov decision processes, where $H$ is the horizon,
$S$ is the number of states, $A$ is the number of actions and $T$ is the time
elapsed. This improves upon the best previous bound of $\tilde{O}(H S
\sqrt{AT})$ for any reinforcement learning algorithm.



We propose a simple domain adaptation method for neural networks in a
supervised setting. Supervised domain adaptation is a way of improving the
generalization performance on the target domain by using the source domain
dataset, assuming that both of the datasets are labeled. Recently, recurrent
neural networks have been shown to be successful on a variety of NLP tasks such
as caption generation; however, the existing domain adaptation techniques are
limited to (1) tune the model parameters by the target dataset after the
training by the source dataset, or (2) design the network to have dual output,
one for the source domain and the other for the target domain. Reformulating
the idea of the domain adaptation technique proposed by Daume (2007), we
propose a simple domain adaptation method, which can be applied to neural
networks trained with a cross-entropy loss. On captioning datasets, we show
performance improvements over other domain adaptation methods.



We consider the task of KBP slot filling -- extracting relation information
from newswire documents for knowledge base construction. We present our
pipeline, which employs Relational Dependency Networks (RDNs) to learn
linguistic patterns for relation extraction. Additionally, we demonstrate how
several components such as weak supervision, word2vec features, joint learning
and the use of human advice, can be incorporated in this relational framework.
We evaluate the different components in the benchmark KBP 2015 task and show
that RDNs effectively model a diverse set of features and perform competitively
with current state-of-the-art relation extraction.



One of the main obstacles to broad application of reinforcement learning
methods is the parameter sensitivity of our core learning algorithms. In many
large-scale applications, online computation and function approximation
represent key strategies in scaling up reinforcement learning algorithms. In
this setting, we have effective and reasonably well understood algorithms for
adapting the learning-rate parameter, online during learning. Such
meta-learning approaches can improve robustness of learning and enable
specialization to current task, improving learning speed. For
temporal-difference learning algorithms which we study here, there is yet
another parameter, $\lambda$, that similarly impacts learning speed and
stability in practice. Unfortunately, unlike the learning-rate parameter,
$\lambda$ parametrizes the objective function that temporal-difference methods
optimize. Different choices of $\lambda$ produce different fixed-point
solutions, and thus adapting $\lambda$ online and characterizing the
optimization is substantially more complex than adapting the learning-rate
parameter. There are no meta-learning method for $\lambda$ that can achieve (1)
incremental updating, (2) compatibility with function approximation, and (3)
maintain stability of learning under both on and off-policy sampling. In this
paper we contribute a novel objective function for optimizing $\lambda$ as a
function of state rather than time. We derive a new incremental, linear
complexity $\lambda$-adaption algorithm that does not require offline batch
updating or access to a model of the world, and present a suite of experiments
illustrating the practicality of our new algorithm in three different settings.
Taken together, our contributions represent a concrete step towards black-box
application of temporal-difference learning methods in real world problems.



A neighborhood graph, which represents the instances as vertices and their
relations as weighted edges, is the basis of many semi-supervised and
relational models for node labeling and link prediction. Most methods employ a
sequential process to construct the neighborhood graph. This process often
consists of generating a candidate graph, pruning the candidate graph to make a
neighborhood graph, and then performing inference on the variables (i.e.,
nodes) in the neighborhood graph. In this paper, we propose a framework that
can dynamically adapt the neighborhood graph based on the states of variables
from intermediate inference results, as well as structural properties of the
relations connecting them. A key strength of our framework is its ability to
handle multi-relational data and employ varying amounts of relations for each
instance based on the intermediate inference results. We formulate the link
prediction task as inference on neighborhood graphs, and include preliminary
results illustrating the effects of different strategies in our proposed
framework.



Traditionally, researchers in decision making have focused on attempting to
reach Pareto Optimality using horizontal approaches, where optimality is
calculated taking into account every participant at the same time. Sometimes,
this may prove to be a difficult task (e.g., conflict, mistrust, no information
sharing, etc.). In this paper, we explore the possibility of achieving Pareto
Optimal outcomes in a group by using a bottom-up approach: discovering Pareto
optimal outcomes by interacting in subgroups. We analytically show that Pareto
optimal outcomes in a subgroup are also Pareto optimal in a supergroup of those
agents in the case of strict, transitive, and complete preferences. Then, we
empirically analyze the prospective usability and practicality of bottom-up
approaches in a variety of decision making domains.



In distributed, or privacy-preserving learning, we are often given a set of
probabilistic models estimated from different local repositories, and asked to
combine them into a single model that gives efficient statistical estimation. A
simple method is to linearly average the parameters of the local models, which,
however, tends to be degenerate or not applicable on non-convex models, or
models with different parameter dimensions. One more practical strategy is to
generate bootstrap samples from the local models, and then learn a joint model
based on the combined bootstrap set. Unfortunately, the bootstrap procedure
introduces additional noise and can significantly deteriorate the performance.
In this work, we propose two variance reduction methods to correct the
bootstrap noise, including a weighted M-estimator that is both statistically
efficient and practically powerful. Both theoretical and empirical analysis is
provided to demonstrate our methods.



Recommendation systems usually involve exploiting the relations among known
features and content that describe items (content-based filtering) or the
overlap of similar users who interacted with or rated the target item
(collaborative filtering). To combine these two filtering approaches, current
model-based hybrid recommendation systems typically require extensive feature
engineering to construct a user profile. Statistical Relational Learning (SRL)
provides a straightforward way to combine the two approaches. However, due to
the large scale of the data used in real world recommendation systems, little
research exists on applying SRL models to hybrid recommendation systems, and
essentially none of that research has been applied on real big-data-scale
systems. In this paper, we proposed a way to adapt the state-of-the-art in SRL
learning approaches to construct a real hybrid recommendation system.
Furthermore, in order to satisfy a common requirement in recommendation systems
(i.e. that false positives are more undesirable and therefore penalized more
harshly than false negatives), our approach can also allow tuning the trade-off
between the precision and recall of the system in a principled way. Our
experimental results demonstrate the efficiency of our proposed approach as
well as its improved performance on recommendation precision.



We present a novel form of interactive video object segmentation where a few
clicks by the user helps the system produce a full spatio-temporal segmentation
of the object of interest. Whereas conventional interactive pipelines take the
user's initialization as a starting point, we show the value in the system
taking the lead even in initialization. In particular, for a given video frame,
the system precomputes a ranked list of thousands of possible segmentation
hypotheses (also referred to as object region proposals) using image and motion
cues. Then, the user looks at the top ranked proposals, and clicks on the
object boundary to carve away erroneous ones. This process iterates (typically
2-3 times), and each time the system revises the top ranked proposal set, until
the user is satisfied with a resulting segmentation mask. Finally, the mask is
propagated across the video to produce a spatio-temporal object tube. On three
challenging datasets, we provide extensive comparisons with both existing work
and simpler alternative methods. In all, the proposed Click Carving approach
strikes an excellent balance of accuracy and human effort. It outperforms all
similarly fast methods, and is competitive or better than those requiring 2 to
12 times the effort.



This paper addresses an optimal control problem for a robot that has to find
and collect a finite number of objects and move them to a depot in minimum
time. The robot has fourth-order dynamics that change instantaneously at any
pick-up or drop-off of an object. The objects are modeled by point masses with
a-priori unknown locations in a bounded two-dimensional space that may contain
unknown obstacles. For this hybrid system, an Optimal Control Problem (OCP) is
approximately solved by a receding horizon scheme, where the derived lower
bound for the cost-to-go is evaluated for the worst and for a probabilistic
case, assuming a uniform distribution of the objects. First, a time-driven
approximate solution based on time and position space discretization and mixed
integer programming is presented. Due to the high computational cost of this
solution, an alternative event-driven approximate approach based on a suitable
motion parameterization and gradient-based optimization is proposed. The
solutions are compared in a numerical example, suggesting that the latter
approach offers a significant computational advantage while yielding similar
qualitative results compared to the former. The methods are particularly
relevant for various robotic applications like automated cleaning, search and
rescue, harvesting or manufacturing.



The present study provides the first evidence that illiteracy can be reliably
predicted from standard mobile phone logs. By deriving a broad set of mobile
phone indicators reflecting users financial, social and mobility patterns we
show how supervised machine learning can be used to predict individual
illiteracy in an Asian developing country, externally validated against a
large-scale survey. On average the model performs 10 times better than random
guessing with a 70% accuracy. Further we show how individual illiteracy can be
aggregated and mapped geographically at cell tower resolution. Geographical
mapping of illiteracy is crucial to know where the illiterate people are, and
where to put in resources. In underdeveloped countries such mappings are often
based on out-dated household surveys with low spatial and temporal resolution.
One in five people worldwide struggle with illiteracy, and it is estimated that
illiteracy costs the global economy more than 1 trillion dollars each year.
These results potentially enable costeffective, questionnaire-free
investigation of illiteracy-related questions on an unprecedented scale



In recent years, content recommendation systems in large websites (or
\emph{content providers}) capture an increased focus. While the type of content
varies, e.g.\ movies, articles, music, advertisements, etc., the high level
problem remains the same. Based on knowledge obtained so far on the user,
recommend the most desired content. In this paper we present a method to handle
the well known user-cold-start problem in recommendation systems. In this
scenario, a recommendation system encounters a new user and the objective is to
present items as relevant as possible with the hope of keeping the user's
session as long as possible. We formulate an optimization problem aimed to
maximize the length of this initial session, as this is believed to be the key
to have the user come back and perhaps register to the system. In particular,
our model captures the fact that a single round with low quality recommendation
is likely to terminate the session. In such a case, we do not proceed to the
next round as the user leaves the system, possibly never to seen again. We
denote this phenomenon a \emph{One-Shot Session}. Our optimization problem is
formulated as an MDP where the action space is of a combinatorial nature as we
recommend in each round, multiple items. This huge action space presents a
computational challenge making the straightforward solution intractable. We
analyze the structure of the MDP to prove monotone and submodular like
properties that allow a computationally efficient solution via a method denoted
by \emph{Greedy Value Iteration} (G-VI).



Choosing control inputs randomly can result in a reduced expected cost in
optimal control problems with stochastic constraints, such as stochastic model
predictive control (SMPC). We consider a controller with initial randomization,
meaning that the controller randomly chooses from K+1 control sequences at the
beginning (called K-randimization).It is known that, for a finite-state,
finite-action Markov Decision Process (MDP) with K constraints, K-randimization
is sufficient to achieve the minimum cost. We found that the same result holds
for stochastic optimal control problems with continuous state and action
spaces.Furthermore, we show the randomization of control input can result in
reduced cost when the optimization problem is nonconvex, and the cost reduction
is equal to the duality gap. We then provide the necessary and sufficient
conditions for the optimality of a randomized solution, and develop an
efficient solution method based on dual optimization. Furthermore, in a special
case with K=1 such as a joint chance-constrained problem, the dual optimization
can be solved even more efficiently by root finding. Finally, we test the
theories and demonstrate the solution method on multiple practical problems
ranging from path planning to the planning of entry, descent, and landing (EDL)
for future Mars missions.



Deep neural networks are able to learn powerful representations from large
quantities of labeled input data, however they cannot always generalize well
across changes in input distributions. Domain adaptation algorithms have been
proposed to compensate for the degradation in performance due to domain shift.
In this paper, we address the case when the target domain is unlabeled,
requiring unsupervised adaptation. CORAL is a "frustratingly easy" unsupervised
domain adaptation method that aligns the second-order statistics of the source
and target distributions with a linear transformation. Here, we extend CORAL to
learn a nonlinear transformation that aligns correlations of layer activations
in deep neural networks (Deep CORAL). Experiments on standard benchmark
datasets show state-of-the-art performance.



Sponsored search represents a major source of revenue for web search engines.
This popular advertising model brings a unique possibility for advertisers to
target users' immediate intent communicated through a search query, usually by
displaying their ads alongside organic search results for queries deemed
relevant to their products or services. However, due to a large number of
unique queries it is challenging for advertisers to identify all such relevant
queries. For this reason search engines often provide a service of advanced
matching, which automatically finds additional relevant queries for advertisers
to bid on. We present a novel advanced matching approach based on the idea of
semantic embeddings of queries and ads. The embeddings were learned using a
large data set of user search sessions, consisting of search queries, clicked
ads and search links, while utilizing contextual information such as dwell time
and skipped ads. To address the large-scale nature of our problem, both in
terms of data and vocabulary size, we propose a novel distributed algorithm for
training of the embeddings. Finally, we present an approach for overcoming a
cold-start problem associated with new ads and queries. We report results of
editorial evaluation and online tests on actual search traffic. The results
show that our approach significantly outperforms baselines in terms of
relevance, coverage, and incremental revenue. Lastly, we open-source learned
query embeddings to be used by researchers in computational advertising and
related fields.



Owing to the remarkable photometric precision of space observatories like
Kepler, stellar and planetary systems beyond our own are now being
characterized en masse for the first time. These characterizations are pivotal
for endeavors such as searching for Earth-like planets and solar twins,
understanding the mechanisms that govern stellar evolution, and tracing the
dynamics of our Galaxy. The volume of data that is becoming available, however,
brings with it the need to process this information accurately and rapidly.
While existing methods can constrain fundamental stellar parameters such as
ages, masses, and radii from these observations, they require substantial
computational efforts to do so.
  We develop a method based on machine learning for rapidly estimating
fundamental parameters of main-sequence solar-like stars from classical and
asteroseismic observations. We first demonstrate this method on a
hare-and-hound exercise and then apply it to the Sun, 16 Cyg A & B, and 34
planet-hosting candidates that have been observed by the Kepler spacecraft. We
find that our estimates and their associated uncertainties are comparable to
the results of other methods, but with the additional benefit of being able to
explore many more stellar parameters while using much less computation time. We
furthermore use this method to present evidence for an empirical diffusion-mass
relation. Our method is open source and freely available for the community to
use.
  The source code for all analyses and for all figures appearing in this
manuscript can be found electronically at
https://github.com/earlbellinger/asteroseismology



This report describes our submissions to Task2 and Task3 of the DCASE 2016
challenge. The systems aim at dealing with the detection of overlapping audio
events in continuous streams, where the detectors are based on random decision
forests. The proposed forests are jointly trained for classification and
regression simultaneously. Initially, the training is classification-oriented
to encourage the trees to select discriminative features from overlapping
mixtures to separate positive audio segments from the negative ones. The
regression phase is then carried out to let the positive audio segments vote
for the event onsets and offsets, and therefore model the temporal structure of
audio events. One random decision forest is specifically trained for each event
category of interest. Experimental results on the development data show that
our systems significantly outperform the baseline on the Task2 evaluation while
they are inferior to the baseline in the Task3 evaluation.



In this work we propose a game theoretic model for document clustering. Each
document to be clustered is represented as a player and each cluster as a
strategy. The players receive a reward interacting with other players that they
try to maximize choosing their best strategies. The geometry of the data is
modeled with a weighted graph that encodes the pairwise similarity among
documents, so that similar players are constrained to choose similar
strategies, updating their strategy preferences at each iteration of the games.
We used different approaches to find the prototypical elements of the clusters
and with this information we divided the players into two disjoint sets, one
collecting players with a definite strategy and the other one collecting
players that try to learn from others the correct strategy to play. The latter
set of players can be considered as new data points that have to be clustered
according to previous information. This representation is useful in scenarios
in which the data are streamed continuously. The evaluation of the system was
conducted on 13 document datasets using different settings. It shows that the
proposed method performs well compared to different document clustering
algorithms.



Deep convolutional neural networks (CNNs) have been actively adopted in the
field of music information retrieval, e.g. genre classification, mood
detection, and chord recognition. However, the process of learning and
prediction is little understood, particularly when it is applied to
spectrograms. We introduce auralisation of a CNN to understand its underlying
mechanism, which is based on a deconvolution procedure introduced in [2].
Auralisation of a CNN is converting the learned convolutional features that are
obtained from deconvolution into audio signals. In the experiments and
discussions, we explain trained features of a 5-layer CNN based on the
deconvolved spectrograms and auralised signals. The pairwise correlations per
layers with varying different musical attributes are also investigated to
understand the evolution of the learnt features. It is shown that in the deep
layers, the features are learnt to capture textures, the patterns of continuous
distributions, rather than shapes of lines.



We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural
Networks that replaces the softmax output layer by a log-linear output layer,
of which the softmax is a special case. This conceptually simple move has two
main advantages. First, it allows the learner to combat training data sparsity
by allowing it to model words (or more generally, output symbols) as complex
combinations of attributes without requiring that each combination is directly
observed in the training data (as the softmax does). Second, it permits the
inclusion of flexible prior knowledge in the form of a priori specified modular
features, where the neural network component learns to dynamically control the
weights of a log-linear distribution exploiting these features.
  We conduct experiments in the domain of language modelling of French, that
exploit morphological prior knowledge and show an important decrease in
perplexity relative to a baseline RNN.
  We provide other motivating iillustrations, and finally argue that the
log-linear and the neural-network components contribute complementary strengths
to the LL-RNN: the LL aspect allows the model to incorporate rich prior
knowledge, while the NN aspect, according to the "representation learning"
paradigm, allows the model to discover novel combination of characteristics.



Much of the worlds data is streaming, time-series data, where anomalies give
significant information in critical situations. Yet detecting anomalies in
streaming data is a difficult task, requiring detectors to process data in
real-time, and learn while simultaneously making predictions. We present a
novel anomaly detection technique based on an on-line sequence memory algorithm
called Hierarchical Temporal Memory (HTM). We show results from a live
application that detects anomalies in financial metrics in real-time. We also
test the algorithm on NAB, a published benchmark for real-time anomaly
detection, where our algorithm achieves best-in-class results.



In sentiment analysis, the polarities of the opinions expressed on an
object/feature are determined to assess the sentiment of a sentence or document
whether it is positive/negative/neutral. Naturally, the object/feature is a
noun representation which refers to a product or a component of a product, let
us say, the "lens" in a camera and opinions emanating on it are captured in
adjectives, verbs, adverbs and noun words themselves. Apart from such words,
other meta-information and diverse effective features are also going to play an
important role in influencing the sentiment polarity and contribute
significantly to the performance of the system. In this paper, some of the
associated information/meta-data are explored and investigated in the sentiment
text. Based on the analysis results presented here, there is scope for further
assessment and utilization of the meta-information as features in text
categorization, ranking text document, identification of spam documents and
polarity classification problems.



The aim of this research is development of rule based decision model for
emotion recognition. This research also proposes using the rules for augmenting
inter-corporal recognition accuracy in multimodal systems that use supervised
learning techniques. The classifiers for such learning based recognition
systems are susceptible to over fitting and only perform well on intra-corporal
data. To overcome the limitation this research proposes using rule based model
as an additional modality. The rules were developed using raw feature data from
visual channel, based on human annotator agreement and existing studies that
have attributed movement and postures to emotions. The outcome of the rule
evaluations was combined during the decision phase of emotion recognition
system. The results indicate rule based emotion recognition augment recognition
accuracy of learning based systems and also provide better recognition rate
across inter corpus emotion test data.



We study classification problems where features are corrupted by noise and
where the magnitude of the noise in each feature is influenced by the resources
allocated to its acquisition. This is the case, for example, when multiple
sensors share a common resource (power, bandwidth, attention, etc.). We develop
a method for computing the optimal resource allocation for a variety of
scenarios and derive theoretical bounds concerning the benefit that may arise
by non-uniform allocation. We further demonstrate the effectiveness of the
developed method in simulations.



Word embeddings have been shown to be useful across state-of-the-art systems
in many natural language processing tasks, ranging from question answering
systems to dependency parsing. (Herbelot and Vecchi, 2015) explored word
embeddings and their utility for modeling language semantics. In particular,
they presented an approach to automatically map a standard distributional
semantic space onto a set-theoretic model using partial least squares
regression. We show in this paper that a simple baseline achieves a +51%
relative improvement compared to their model on one of the two datasets they
used, and yields competitive results on the second dataset.



Real-world optimisation problems are often dynamic. Previously good solutions
must be updated or replaced due to changes in objectives and constraints. It is
often claimed that evolutionary algorithms are particularly suitable for
dynamic optimisation because a large population can contain different solutions
that may be useful in the future. However, rigorous theoretical demonstrations
for how populations in dynamic optimisation can be essential are sparse and
restricted to special cases.
  This paper provides theoretical explanations of how populations can be
essential in evolutionary dynamic optimisation in a general and natural
setting. We describe a natural class of dynamic optimisation problems where a
sufficiently large population is necessary to keep track of moving optima
reliably. We establish a relationship between the population-size and the
probability that the algorithm loses track of the optimum.



Strategy Logic (SL) is a logical formalism for strategic reasoning in
multi-agent systems. Its main feature is that it has variables for strategies
that are associated to specific agents with a binding operator. We introduce
Graded Strategy Logic (GradedSL), an extension of SL by graded quantifiers over
tuples of strategy variables, i.e., "there exist at least g different tuples
(x_1,...,x_n) of strategies" where g is a cardinal from the set N union
{aleph_0, aleph_1, 2^aleph_0}. We prove that the model-checking problem of
GradedSL is decidable. We then turn to the complexity of fragments of GradedSL.
When the g's are restricted to finite cardinals, written GradedNSL, the
complexity of model-checking is no harder than for SL, i.e., it is
non-elementary in the quantifier rank. We illustrate our formalism by showing
how to count the number of different strategy profiles that are Nash equilibria
(NE), or subgame-perfect equilibria (SPE). By analyzing the structure of the
specific formulas involved, we conclude that the important problems of checking
for the existence of a unique NE or SPE can both be solved in 2ExpTime, which
is not harder than merely checking for the existence of such equilibria.



In this paper, we propose a novel unsupervised domain adaptation algorithm
based on deep learning for visual object recognition. Specifically, we design a
new model called Deep Reconstruction-Classification Network (DRCN), which
jointly learns a shared encoding representation for two tasks: i) supervised
classification of labeled source data, and ii) unsupervised reconstruction of
unlabeled target data.In this way, the learnt representation not only preserves
discriminability, but also encodes useful information from the target domain.
Our new DRCN model can be optimized by using backpropagation similarly as the
standard neural networks.
  We evaluate the performance of DRCN on a series of cross-domain object
recognition tasks, where DRCN provides a considerable improvement (up to ~8% in
accuracy) over the prior state-of-the-art algorithms. Interestingly, we also
observe that the reconstruction pipeline of DRCN transforms images from the
source domain into images whose appearance resembles the target dataset. This
suggests that DRCN's performance is due to constructing a single composite
representation that encodes information about both the structure of target
images and the classification of source images. Finally, we provide a formal
analysis to justify the algorithm's objective in domain adaptation context.



Protein quality assessment (QA) by ranking and selecting protein models has
long been viewed as one of the major challenges for protein tertiary structure
prediction. Especially, estimating the quality of a single protein model, which
is important for selecting a few good models out of a large model pool
consisting of mostly low-quality models, is still a largely unsolved problem.
We introduce a novel single-model quality assessment method DeepQA based on
deep belief network that utilizes a number of selected features describing the
quality of a model from different perspectives, such as energy, physio-chemical
characteristics, and structural information. The deep belief network is trained
on several large datasets consisting of models from the Critical Assessment of
Protein Structure Prediction (CASP) experiments, several publicly available
datasets, and models generated by our in-house ab initio method. Our experiment
demonstrate that deep belief network has better performance compared to Support
Vector Machines and Neural Networks on the protein model quality assessment
problem, and our method DeepQA achieves the state-of-the-art performance on
CASP11 dataset. It also outperformed two well-established methods in selecting
good outlier models from a large set of models of mostly low quality generated
by ab initio modeling methods. DeepQA is a useful tool for protein single model
quality assessment and protein structure prediction. The source code,
executable, document and training/test datasets of DeepQA for Linux is freely
available to non-commercial users at http://cactus.rnet.missouri.edu/DeepQA/.



Although artificial neural networks have shown great promise in applications
including computer vision and speech recognition, there remains considerable
practical and theoretical difficulty in optimizing their parameters. The
seemingly unreasonable success of gradient descent methods in minimizing these
non-convex functions remains poorly understood. In this work we offer some
theoretical guarantees for networks with piecewise affine activation functions,
which have in recent years become the norm. We prove three main results.
Firstly, that the network is piecewise convex as a function of the input data.
Secondly, that the network, considered as a function of the parameters in a
single layer, all others held constant, is again piecewise convex. Finally,
that the network as a function of all its parameters is piecewise multi-convex,
a generalization of biconvexity. From here we characterize the local minima and
stationary points of the training objective, showing that they minimize certain
subsets of the parameter space. We then analyze the performance of two
optimization algorithms on multi-convex problems: gradient descent, and a
method which repeatedly solves a number of convex sub-problems. We prove
necessary convergence conditions for the first algorithm and both necessary and
sufficient conditions for the second, after introducing regularization to the
objective. Finally, we remark on the remaining difficulty of the global
optimization problem. Under the squared error objective, we show that by
varying the training data, a single rectifier neuron admits local minima
arbitrarily far apart, both in objective value and parameter space.



Recent years have seen significant market penetration for voice-based
personal assistants such as Apple's Siri. However, despite this success, user
take-up is frustratingly low. This position paper argues that there is a
habitability gap caused by the inevitable mismatch between the capabilities and
expectations of human users and the features and benefits provided by
contemporary technology. Suggestions are made as to how such problems might be
mitigated, but a more worrisome question emerges: "is spoken language
all-or-nothing"? The answer, based on contemporary views on the special nature
of (spoken) language, is that there may indeed be a fundamental limit to the
interaction that can take place between mismatched interlocutors (such as
humans and machines). However, it is concluded that interactions between native
and non-native speakers, or between adults and children, or even between humans
and dogs, might provide critical inspiration for the design of future
speech-based human-machine interaction.



Image generation remains a fundamental problem in artificial intelligence in
general and deep learning in specific. The generative adversarial network (GAN)
was successful in generating high quality samples of natural images. We propose
a model called composite generative adversarial network, that reveals the
complex structure of images with multiple generators in which each generator
generates some part of the image. Those parts are combined by alpha blending
process to create a new single image. It can generate, for example, background
and face sequentially with two generators, after training on face dataset.
Training was done in an unsupervised way without any labels about what each
generator should generate. We found possibilities of learning the structure by
using this generative model empirically.



Aims. We present an innovative artificial neural network (ANN) architecture,
called Generative ANN (GANN), that computes the forward model, that is it
learns the function that relates the unknown outputs (stellar atmospheric
parameters, in this case) to the given inputs (spectra). Such a model can be
integrated in a Bayesian framework to estimate the posterior distribution of
the outputs. Methods. The architecture of the GANN follows the same scheme as a
normal ANN, but with the inputs and outputs inverted. We train the network with
the set of atmospheric parameters (Teff, logg, [Fe/H] and [alpha/Fe]),
obtaining the stellar spectra for such inputs. The residuals between the
spectra in the grid and the estimated spectra are minimized using a validation
dataset to keep solutions as general as possible. Results. The performance of
both conventional ANNs and GANNs to estimate the stellar parameters as a
function of the star brightness is presented and compared for different
Galactic populations. GANNs provide significantly improved parameterizations
for early and intermediate spectral types with rich and intermediate
metallicities. The behaviour of both algorithms is very similar for our sample
of late-type stars, obtaining residuals in the derivation of [Fe/H] and
[alpha/Fe] below 0.1dex for stars with Gaia magnitude Grvs<12, which accounts
for a number in the order of four million stars to be observed by the Radial
Velocity Spectrograph of the Gaia satellite. Conclusions. Uncertainty
estimation of computed astrophysical parameters is crucial for the validation
of the parameterization itself and for the subsequent exploitation by the
astronomical community. GANNs produce not only the parameters for a given
spectrum, but a goodness-of-fit between the observed spectrum and the predicted
one for a given set of parameters. Moreover, they allow us to obtain the full
posterior distribution...



Natural Language Inference is an important task for Natural Language
Understanding. It is concerned with classifying the logical relation between
two sentences. In this paper, we propose several text generative neural
networks for generating text hypothesis, which allows construction of new
Natural Language Inference datasets. To evaluate the models, we propose a new
metric -- the accuracy of the classifier trained on the generated dataset. The
accuracy obtained by our best generative model is only 2.7% lower than the
accuracy of the classifier trained on the original, human crafted dataset.
Furthermore, the best generated dataset combined with the original dataset
achieves the highest accuracy. The best model learns a mapping embedding for
each training example. By comparing various metrics we show that datasets that
obtain higher ROUGE or METEOR scores do not necessarily yield higher
classification accuracies. We also provide analysis of what are the
characteristics of a good dataset including the distinguishability of the
generated datasets from the original one.



Wearable cameras allow people to record their daily activities from a
user-centered (First Person Vision) perspective. Due to their favorable
location, wearable cameras frequently capture the hands of the user, and may
thus represent a promising user-machine interaction tool for different
applications. Existent First Person Vision methods handle hand segmentation as
a background-foreground problem, ignoring two important facts: i) hands are not
a single "skin-like" moving element, but a pair of interacting cooperative
entities, ii) close hand interactions may lead to hand-to-hand occlusions and,
as a consequence, create a single hand-like segment. These facts complicate a
proper understanding of hand movements and interactions. Our approach extends
traditional background-foreground strategies, by including a
hand-identification step (left-right) based on a Maxwell distribution of angle
and position. Hand-to-hand occlusions are addressed by exploiting temporal
superpixels. The experimental results show that, in addition to a reliable
left/right hand-segmentation, our approach considerably improves the
traditional background-foreground hand-segmentation.



While question answering (QA) with neural network, i.e. neural QA, has
achieved promising results in recent years, lacking of large scale real-word QA
dataset is still a challenge for developing and evaluating neural QA system. To
alleviate this problem, we propose a large scale human annotated real-world QA
dataset WebQA with more than 42k questions and 556k evidences. As existing
neural QA methods resolve QA either as sequence generation or
classification/ranking problem, they face challenges of expensive softmax
computation, unseen answers handling or separate candidate answer generation
component. In this work, we cast neural QA as a sequence labeling problem and
propose an end-to-end sequence labeling model, which overcomes all the above
challenges. Experimental results on WebQA show that our model outperforms the
baselines significantly with an F1 score of 74.69% with word-based input, and
the performance drops only 3.72 F1 points with more challenging character-based
input.



The blind application of machine learning runs the risk of amplifying biases
present in data. Such a danger is facing us with word embedding, a popular
framework to represent text data as vectors which has been used in many machine
learning and natural language processing tasks. We show that even word
embeddings trained on Google News articles exhibit female/male gender
stereotypes to a disturbing extent. This raises concerns because their
widespread use, as we describe, often tends to amplify these biases.
Geometrically, gender bias is first shown to be captured by a direction in the
word embedding. Second, gender neutral words are shown to be linearly separable
from gender definition words in the word embedding. Using these properties, we
provide a methodology for modifying an embedding to remove gender stereotypes,
such as the association between between the words receptionist and female,
while maintaining desired associations such as between the words queen and
female. We define metrics to quantify both direct and indirect gender biases in
embeddings, and develop algorithms to "debias" the embedding. Using
crowd-worker evaluation as well as standard benchmarks, we empirically
demonstrate that our algorithms significantly reduce gender bias in embeddings
while preserving the its useful properties such as the ability to cluster
related concepts and to solve analogy tasks. The resulting embeddings can be
used in applications without amplifying gender bias.



Word embedding methods revolve around learning continuous distributed vector
representations of words with neural networks, which can capture semantic
and/or syntactic cues, and in turn be used to induce similarity measures among
words, sentences and documents in context. Celebrated methods can be
categorized as prediction-based and count-based methods according to the
training objectives and model architectures. Their pros and cons have been
extensively analyzed and evaluated in recent studies, but there is relatively
less work continuing the line of research to develop an enhanced learning
method that brings together the advantages of the two model families. In
addition, the interpretation of the learned word representations still remains
somewhat opaque. Motivated by the observations and considering the pressing
need, this paper presents a novel method for learning the word representations,
which not only inherits the advantages of classic word embedding methods but
also offers a clearer and more rigorous interpretation of the learned word
representations. Built upon the proposed word embedding method, we further
formulate a translation-based language modeling framework for the extractive
speech summarization task. A series of empirical evaluations demonstrate the
effectiveness of the proposed word representation learning and language
modeling techniques in extractive speech summarization.



We present a novel method for the compensation of long duration data gaps in
audio signals, in particular music. The concealment of such signal defects is
based on a graph that encodes signal structure in terms of time-persistent
spectral similarity. A suitable candidate segment for the substitution of the
lost content is proposed by an intuitive optimization scheme and smoothly
inserted into the gap. Extensive listening tests show that the proposed
algorithm provides highly promising results when applied to a variety of
real-world music signals.



Actions may not proceed as planned; they may be interrupted, resumed or
overridden. This is a challenge to handle in a natural language understanding
system. We describe extensions to an existing implementation for the control of
autonomous systems by natural language, to enable such systems to handle
incoming language requests regarding actions. Language Communication with
Autonomous Systems (LCAS) has been extended with support for X-nets,
parameterized executable schemas representing actions. X-nets enable the system
to control actions at a desired level of granularity, while providing a
mechanism for language requests to be processed asynchronously. Standard
semantics supported include requests to stop, continue, or override the
existing action. The specific domain demonstrated is the control of motion of a
simulated robot, but the approach is general, and could be applied to other
domains.



3D action recognition - analysis of human actions based on 3D skeleton data -
becomes popular recently due to its succinctness, robustness, and
view-invariant representation. Recent attempts on this problem suggested to
develop RNN-based learning methods to model the contextual dependency in the
temporal domain. In this paper, we extend this idea to spatio-temporal domains
to analyze the hidden sources of action-related information within the input
data over both domains concurrently. Inspired by the graphical structure of the
human skeleton, we further propose a more powerful tree-structure based
traversal method. To handle the noise and occlusion in 3D skeleton data, we
introduce new gating mechanism within LSTM to learn the reliability of the
sequential input data and accordingly adjust its effect on updating the
long-term context information stored in the memory cell. Our method achieves
state-of-the-art performance on 4 challenging benchmark datasets for 3D human
action analysis.



Efficient usage of the knowledge provided by the Linked Data community is
often hindered by the need for domain experts to formulate the right SPARQL
queries to answer questions. For new questions they have to decide which
datasets are suitable and in which terminology and modelling style to phrase
the SPARQL query.
  In this work we present an evolutionary algorithm to help with this
challenging task. Given a training list of source-target node-pair examples our
algorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The
learned patterns can be visualised to form the basis for further investigation,
or they can be used to predict target nodes for new source nodes.
  Amongst others, we apply our algorithm to a dataset of several hundred human
associations (such as "circle - square") to find patterns for them in DBpedia.
We show the scalability of the algorithm by running it against a SPARQL
endpoint loaded with > 7.9 billion triples. Further, we use the resulting
SPARQL queries to mimic human associations with a Mean Average Precision (MAP)
of 39.9 % and a Recall@10 of 63.9 %.



We present Tweet2Vec, a novel method for generating general-purpose vector
representation of tweets. The model learns tweet embeddings using
character-level CNN-LSTM encoder-decoder. We trained our model on 3 million,
randomly selected English-language tweets. The model was evaluated using two
methods: tweet semantic similarity and tweet sentiment categorization,
outperforming the previous state-of-the-art in both tasks. The evaluations
demonstrate the power of the tweet embeddings generated by our model for
various tweet categorization tasks. The vector representations generated by our
model are generic, and hence can be applied to a variety of tasks. Though the
model presented in this paper is trained on English-language tweets, the method
presented can be used to learn tweet embeddings for different languages.



Most application development happens in the context of complex APIs;
reference documentation for APIs has grown tremendously in variety, complexity,
and volume, and can be difficult to navigate. There is a growing need to
develop well-organized ways to access the knowledge latent in the
documentation; several research efforts deal with the organization (ontology)
of API-related knowledge. Extensive knowledge-engineering work, supported by a
rigorous qualitative analysis, by Maalej & Robillard [3] has identified a
useful taxonomy of API knowledge. Based on this taxonomy, we introduce a domain
independent technique to extract the knowledge types from the given API
reference documentation. Our system, OntoCat, introduces total nine different
features and their semantic and statistical combinations to classify the
different knowledge types. We tested OntoCat on python API reference
documentation. Our experimental results show the effectiveness of the system
and opens the scope of probably related research areas (i.e., user behavior,
documentation quality, etc.).



This survey outlines a general and modular theory for proving approximation
guarantees for equilibria of auctions in complex settings. This theory
complements traditional economic techniques, which generally focus on exact and
optimal solutions and are accordingly limited to relatively stylized settings.
  We highlight three user-friendly analytical tools: smoothness-type
inequalities, which immediately yield approximation guarantees for many auction
formats of interest in the special case of complete information and
deterministic strategies; extension theorems, which extend such guarantees to
randomized strategies, no-regret learning outcomes, and incomplete-information
settings; and composition theorems, which extend such guarantees from simpler
to more complex auctions. Combining these tools yields tight worst-case
approximation guarantees for the equilibria of many widely-used auction
formats.



Unstructured data refers to information that does not have a predefined data
model or is not organized in a pre-defined manner. Loosely speaking,
unstructured data refers to text data that is generated by humans. In
after-sales service businesses, there are two main sources of unstructured
data: customer complaints, which generally describe symptoms, and technician
comments, which outline diagnostics and treatment information. A legitimate
customer complaint can eventually be tracked to a failure or a claim. However,
there is a delay between the time of a customer complaint and the time of a
failure or a claim. A proactive strategy aimed at analyzing customer complaints
for symptoms can help service providers detect reliability problems in advance
and initiate corrective actions such as recalls. This paper introduces
essential text mining concepts in the context of reliability analysis and a
method to detect emerging reliability issues. The application of the method is
illustrated using a case study.



The rapid development of autonomous vehicles spurred a careful investigation
of the potential benefits of all-autonomous transportation networks. Most
studies conclude that autonomous systems can enable drastic improvements in
performance. A widely studied concept is all-autonomous, collision-free
intersections, where vehicles arriving in a traffic intersection with no
traffic light adjust their speeds to cross safely through the intersection as
quickly as possible. In this paper, we propose a coordination control algorithm
for this problem, assuming stochastic models for the arrival times of the
vehicles. The proposed algorithm provides provable guarantees on safety and
performance. More precisely, it is shown that no collisions occur surely, and
moreover a rigorous upper bound is provided for the expected wait time. The
algorithm is also demonstrated in simulations. The proposed algorithms are
inspired by polling systems. In fact, the problem studied in this paper leads
to a new polling system where customers are subject to differential
constraints, which may be interesting in its own right.



We present three results on the complexity of Minimax Approval Voting. First,
we study Minimax Approval Voting parameterized by the Hamming distance $d$ from
the solution to the votes. We show Minimax Approval Voting admits no algorithm
running in time $\mathcal{O}^\star(2^{o(d\log d)})$, unless the Exponential
Time Hypothesis (ETH) fails. This means that the $\mathcal{O}^\star(d^{2d})$
algorithm of Misra et al. [AAMAS 2015] is essentially optimal. Motivated by
this, we then show a parameterized approximation scheme, running in time
$\mathcal{O}^\star(\left({3}/{\epsilon}\right)^{2d})$, which is essentially
tight assuming ETH. Finally, we get a new polynomial-time randomized
approximation scheme for Minimax Approval Voting, which runs in time
$n^{\mathcal{O}(1/\epsilon^2 \cdot \log(1/\epsilon))} \cdot \mathrm{poly}(m)$,
almost matching the running time of the fastest known PTAS for Closest String
due to Ma and Sun [SIAM J. Comp. 2009].



Data association, the reasoning over correspondence between targets and
measurements, is a problem of fundamental importance in target tracking.
Recently, belief propagation (BP) has emerged as a promising method for
estimating the marginal probabilities of measurement to target association,
providing fast, accurate estimates. The excellent performance of BP in the
particular formulation used may be attributed to the convexity of the
underlying free energy which it implicitly optimises. This paper studies
multiple scan data association problems, i.e., problems that reason over
correspondence between targets and several sets of measurements, which may
correspond to different sensors or different time steps. We find that the
multiple scan extension of the single scan BP formulation is non-convex and
demonstrate the undesirable behaviour that can result. A convex free energy is
constructed using the recently proposed fractional free energy (FFE). A
convergent, BP-like algorithm is provided for the single scan FFE, and employed
in optimising the multiple scan free energy using primal-dual coordinate
ascent. Finally, based on a variational interpretation of joint probabilistic
data association (JPDA), we develop a sequential variant of the algorithm that
is similar to JPDA, but retains consistency constraints from prior scans. The
performance of the proposed methods is demonstrated on a bearings only target
localisation problem.



One of the most difficult problems in the development of intelligent systems
is the construction of the underlying knowledge base. As a consequence, the
rate of progress in the development of this type of system is directly related
to the speed with which knowledge bases can be assembled, and on its quality.
We attempt to solve the knowledge acquisition problem, for a Business
Information System, developing a supervised multistrategy learning paradigm.
This paradigm is centred on a collaborative data mining strategy, where groups
of experts collaborate using data-mining process on the supervised acquisition
of new knowledge extracted from heterogeneous machine learning data models.
  The Actias system is our approach to this paradigm. It is the result of
applying the graphic logic based language of sketches to knowledge integration.
The system is a data mining collaborative workplace, where the Information
System knowledge base is an algebraic structure. It results from the
integration of background knowledge with new insights extracted from data
models, generated for specific data modelling tasks, and represented as rules
using the sketches language.



Automatically searching for optimal hyperparameter configurations is of
crucial importance for applying deep learning algorithms in practice. Recently,
Bayesian optimization has been proposed for optimizing hyperparameters of
various machine learning algorithms. Those methods adopt probabilistic
surrogate models like Gaussian processes to approximate and minimize the
validation error function of hyperparameter values. However, probabilistic
surrogates require accurate estimates of sufficient statistics (e.g.,
covariance) of the error distribution and thus need many function evaluations
with a sizeable number of hyperparameters. This makes them inefficient for
optimizing hyperparameters of deep learning algorithms, which are highly
expensive to evaluate. In this work, we propose a new deterministic and
efficient hyperparameter optimization method that employs radial basis
functions as error surrogates. The proposed mixed integer algorithm, called
HORD, searches the surrogate for the most promising hyperparameter values
through dynamic coordinate search and requires many fewer function evaluations.
HORD does well in low dimensions but it is exceptionally better in higher
dimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural
networks demonstrate HORD significantly outperforms the well-established
Bayesian optimization methods such as GP, SMAC, and TPE. For instance, on
average, HORD is more than 6 times faster than GP-EI in obtaining the best
configuration of 19 hyperparameters.



IoT Big Data requires new machine learning methods able to scale to large
size of data arriving at high speed. Decision trees are popular machine
learning models since they are very effective, yet easy to interpret and
visualize. In the literature, we can find distributed algorithms for learning
decision trees, and also streaming algorithms, but not algorithms that combine
both features. In this paper we present the Vertical Hoeffding Tree (VHT), the
first distributed streaming algorithm for learning decision trees. It features
a novel way of distributing decision trees via vertical parallelism. The
algorithm is implemented on top of Apache SAMOA, a platform for mining
distributed data streams, and thus able to run on real-world clusters. We run
several experiments to study the accuracy and throughput performance of our new
VHT algorithm, as well as its ability to scale while keeping its superior
performance with respect to non-distributed decision trees.



As we shift more of our lives into the virtual domain, the volume of data
shared on the web keeps increasing and presents a threat to our privacy. This
works contributes to the understanding of privacy implications of such data
sharing by analysing how well people are recognisable in social media data. To
facilitate a systematic study we define a number of scenarios considering
factors such as how many heads of a person are tagged and if those heads are
obfuscated or not. We propose a robust person recognition system that can
handle large variations in pose and clothing, and can be trained with few
training samples. Our results indicate that a handful of images is enough to
threaten users' privacy, even in the presence of obfuscation. We show detailed
experimental results, and discuss their implications.



In this paper, we present an operational system for cyber threat intelligence
gathering from various social platforms on the Internet particularly sites on
the darknet and deepnet. We focus our attention to collecting information from
hacker forum discussions and marketplaces offering products and services
focusing on malicious hacking. We have developed an operational system for
obtaining information from these sites for the purposes of identifying emerging
cyber threats. Currently, this system collects on average 305 high-quality
cyber threat warnings each week. These threat warnings include information on
newly developed malware and exploits that have not yet been deployed in a
cyber-attack. This provides a significant service to cyber-defenders. The
system is significantly augmented through the use of various data mining and
machine learning techniques. With the use of machine learning models, we are
able to recall 92% of products in marketplaces and 80% of discussions on forums
relating to malicious hacking with high precision. We perform preliminary
analysis on the data collected, demonstrating its application to aid a security
expert for better threat analysis.



As robots aspire for long-term autonomous operations in complex dynamic
environments, the ability to reliably take mission-critical decisions in
ambiguous situations becomes critical. This motivates the need to build systems
that have situational awareness to assess how qualified they are at that moment
to make a decision. We call this self-evaluating capability as introspection.
In this paper, we take a small step in this direction and propose a generic
framework for introspective behavior in perception systems. Our goal is to
learn a model to reliably predict failures in a given system, with respect to a
task, directly from input sensor data. We present this in the context of
vision-based autonomous MAV flight in outdoor natural environments, and show
that it effectively handles uncertain situations.



During their first years of life, infants learn the language(s) of their
environment at an amazing speed despite large cross cultural variations in
amount and complexity of the available language input. Understanding this
simple fact still escapes current cognitive and linguistic theories. Recently,
spectacular progress in the engineering science, notably, machine learning and
wearable technology, offer the promise of revolutionizing the study of
cognitive development. Machine learning offers powerful learning algorithms
that can achieve human-like performance on many linguistic tasks. Wearable
sensors can capture vast amounts of data, which enable the reconstruction of
the sensory experience of infants in their natural environment. The project of
'reverse engineering' language development, i.e., of building an effective
system that mimics infant's achievements appears therefore to be within reach.
Here, we analyze the conditions under which such a project can contribute to
our scientific understanding of early language development. We argue that
instead of defining a sub-problem or simplifying the data, computational models
should address the full complexity of the learning situation, and take as input
the raw sensory signals available to infants. This implies that (1) accessible
but privacy-preserving repositories of home data be setup and widely shared,
and (2) models be evaluated at different linguistic levels through a benchmark
of psycholinguist tests that can be passed by machines and humans alike, (3)
linguistically and psychologically plausible learning architectures be scaled
up to real data using probabilistic/optimization principles from machine
learning. We discuss the feasibility of this approach and present preliminary
results.



The DLVHEX system implements the HEX-semantics, which integrates answer set
programming (ASP) with arbitrary external sources. Since its first release ten
years ago, significant advancements were achieved. Most importantly, the
exploitation of properties of external sources led to efficiency improvements
and flexibility enhancements of the language, and technical improvements on the
system side increased user's convenience. In this paper, we present the current
status of the system and point out the most important recent enhancements over
early versions. While existing literature focuses on theoretical aspects and
specific components, a bird's eye view of the overall system is missing. In
order to promote the system for real-world applications, we further present
applications which were already successfully realized on top of DLVHEX. This
paper is under consideration for acceptance in Theory and Practice of Logic
Programming.



As data science continues to grow in popularity, there will be an increasing
need to make data science tools more scalable, flexible, and accessible. In
particular, automated machine learning (AutoML) systems seek to automate the
process of designing and optimizing machine learning pipelines. In this
chapter, we present a genetic programming-based AutoML system called TPOT that
optimizes a series of feature preprocessors and machine learning models with
the goal of maximizing classification accuracy on a supervised classification
problem. Further, we analyze a large database of pipelines that were previously
used to solve various supervised classification problems and identify 100 short
series of machine learning operations that appear the most frequently, which we
call the building blocks of machine learning pipelines. We harness these
building blocks to initialize TPOT with promising solutions, and find that this
sensible initialization method significantly improves TPOT's performance on one
benchmark at no cost of significantly degrading performance on the others.
Thus, sensible initialization with machine learning pipeline building blocks
shows promise for GP-based AutoML systems, and should be further refined in
future work.



Keyphrases efficiently summarize a document's content and are used in various
document processing and retrieval tasks. Several unsupervised techniques and
classifiers exist for extracting keyphrases from text documents. Most of these
methods operate at a phrase-level and rely on part-of-speech (POS) filters for
candidate phrase generation. In addition, they do not directly handle
keyphrases of varying lengths. We overcome these modeling shortcomings by
addressing keyphrase extraction as a sequential labeling task in this paper. We
explore a basic set of features commonly used in NLP tasks as well as
predictions from various unsupervised methods to train our taggers. In addition
to a more natural modeling for the keyphrase extraction problem, we show that
tagging models yield significant performance benefits over existing
state-of-the-art extraction methods.



Autonomous robots need to be able to adapt to unforeseen situations and to
acquire new skills through trial and error. Reinforcement learning in principle
offers a suitable methodological framework for this kind of autonomous
learning. However current computational reinforcement learning agents mostly
learn each individual skill entirely from scratch. How can we enable artificial
agents, such as robots, to acquire some form of generic knowledge, which they
could leverage for the learning of new skills? This paper argues that, like the
brain, the cognitive system of artificial agents has to develop a world model
to support adaptive behavior and learning. Inspiration is taken from two recent
developments in the cognitive science literature: predictive processing
theories of cognition, and the sensorimotor contingencies theory of perception.
Based on these, a hypothesis is formulated about what the content of
information might be that is encoded in an internal world model, and how an
agent could autonomously acquire it. A computational model is described to
formalize this hypothesis, and is evaluated in a series of simulation
experiments.



The ability to transfer knowledge gained in previous tasks into new contexts
is one of the most important mechanisms of human learning. Despite this,
adapting autonomous behavior to be reused in partially similar settings is
still an open problem in current robotics research. In this paper, we take a
small step in this direction and propose a generic framework for learning
transferable motion policies. Our goal is to solve a learning problem in a
target domain by utilizing the training data in a different but related source
domain. We present this in the context of an autonomous MAV flight using
monocular reactive control, and demonstrate the efficacy of our proposed
approach through extensive real-world flight experiments in outdoor cluttered
environments.



Answer Set Programming (ASP) is a popular logic programming paradigm that has
been applied for solving a variety of complex problems. Among the most
challenging real-world applications of ASP are two industrial problems defined
by Siemens: the Partner Units Problem (PUP) and the Combined Configuration
Problem (CCP). The hardest instances of PUP and CCP are out of reach for
state-of-the-art ASP solvers. Experiments show that the performance of ASP
solvers could be significantly improved by embedding domain-specific
heuristics, but a proper effective integration of such criteria in
off-the-shelf ASP implementations is not obvious. In this paper the combination
of ASP and domain-specific heuristics is studied with the goal of effectively
solving real-world problem instances of PUP and CCP. As a byproduct of this
activity, the ASP solver WASP was extended with an interface that eases
embedding new external heuristics in the solver. The evaluation shows that our
domain-heuristic-driven ASP solver finds solutions for all the real-world
instances of PUP and CCP ever provided by Siemens. This paper is under
consideration for acceptance in TPLP.



The ability to learn a model is essential for the success of autonomous
agents. Unfortunately, learning a model is difficult in partially observable
environments, where latent environmental factors influence what the agent
observes. In the absence of a supervisory training signal, autonomous agents
therefore require a mechanism to autonomously discover these environmental
factors, or sensorimotor contexts.
  This paper presents a method to discover sensorimotor contexts in partially
observable environments, by constructing a hierarchical transition model. The
method is evaluated in a simulation experiment, in which a robot learns that
different rooms are characterized by different objects that are found in them.



This paper proposes Relational Similarity Machines (RSM): a fast, accurate,
and flexible relational learning framework for supervised and semi-supervised
learning tasks. Despite the importance of relational learning, most existing
methods are hard to adapt to different settings, due to issues with efficiency,
scalability, accuracy, and flexibility for handling a wide variety of
classification problems, data, constraints, and tasks. For instance, many
existing methods perform poorly for multi-class classification problems, graphs
that are sparsely labeled or network data with low relational autocorrelation.
In contrast, the proposed relational learning framework is designed to be (i)
fast for learning and inference at real-time interactive rates, and (ii)
flexible for a variety of learning settings (multi-class problems), constraints
(few labeled instances), and application domains. The experiments demonstrate
the effectiveness of RSM for a variety of tasks and data.



This volume contains the Proceedings of the 2016 Workshop on Semantic Spaces
at the Intersection of NLP, Physics and Cognitive Science (SLPCS 2016), which
was held on the 11th of June at the University of Strathclyde, Glasgow, and was
co-located with Quantum Physics and Logic (QPL 2016). Exploiting the common
ground provided by the concept of a vector space, the workshop brought together
researchers working at the intersection of Natural Language Processing (NLP),
cognitive science, and physics, offering them an appropriate forum for
presenting their uniquely motivated work and ideas. The interplay between these
three disciplines inspired theoretically motivated approaches to the
understanding of how word meanings interact with each other in sentences and
discourse, how diagrammatic reasoning depicts and simplifies this interaction,
how language models are determined by input from the world, and how word and
sentence meanings interact logically. This first edition of the workshop
consisted of three invited talks from distinguished speakers (Hans Briegel,
Peter G\"ardenfors, Dominic Widdows) and eight presentations of selected
contributed papers. Each submission was refereed by at least three members of
the Programme Committee, who delivered detailed and insightful comments and
suggestions.



Computational Social Choice (ComSoc) is a rapidly developing field at the
intersection of computer science, economics, social choice, and political
science. The study of tournaments is fundamental to ComSoc and many results
have been published about tournament solution sets and reasoning in
tournaments. Theoretical results in ComSoc tend to be worst case and tell us
little about performance in practice. To this end we detail some experiments on
tournaments using real wold data from soccer and tennis. We make three main
contributions to the understanding of tournaments using real world data from
English Premier League, the German Bundesliga, and the ATP World Tour: (1) we
find that the NP-hard question of finding a seeding for which a given team can
win a tournament is easily solvable in real world instances, (2) using detailed
and principled methodology from statistical physics we show that our real world
data obeys a log-normal distribution; and (3) leveraging our log-normal
distribution result and using robust statistical methods, we show that the
popular Condorcet Random (CR) tournament model does not generate realistic
tournament data.



In a developmental framework, autonomous robots need to explore the world and
learn how to interact with it. Without an a priori model of the system, this
opens the challenging problem of having robots master their interface with the
world: how to perceive their environment using their sensors, and how to act in
it using their motors. The sensorimotor approach of perception claims that a
naive agent can learn to master this interface by capturing regularities in the
way its actions transform its sensory inputs. In this paper, we apply such an
approach to the discovery and mastery of the visual field associated with a
visual sensor. A computational model is formalized and applied to a simulated
system to illustrate the approach.



We propose applying the categorical compositional scheme of [6] to conceptual
space models of cognition. In order to do this we introduce the category of
convex relations as a new setting for categorical compositional semantics,
emphasizing the convex structure important to conceptual space applications. We
show how conceptual spaces for composite types such as adjectives and verbs can
be constructed. We illustrate this new model on detailed examples.



In previous work with J. Hedges, we formalised a generalised quantifiers
theory of natural language in categorical compositional distributional
semantics with the help of bialgebras. In this paper, we show how quantifier
scope ambiguity can be represented in that setting and how this representation
can be generalised to branching quantifiers.



The ultimate goals of serious education games (SEG) are to facilitate
learning and maximizing enjoyment during playing SEGs. In SEG development,
there are normally two spaces to be taken into account: knowledge space
regarding learning materials and content space regarding games to be used to
convey learning materials. How to deploy the learning materials seamlessly and
effectively into game content becomes one of the most challenging problems in
SEG development. Unlike previous work where experts in education have to be
used heavily, we proposed a novel approach that works toward minimizing the
efforts of education experts in mapping learning materials to content space.
For a proof-of-concept, we apply the proposed approach in developing an SEG
game, named \emph{Chem Dungeon}, as a case study in order to demonstrate the
effectiveness of our proposed approach. This SEG game has been tested with a
number of users, and the user survey suggests our method works reasonably well.



We develop a novel bi-directional attention model for dependency parsing,
which learns to agree on headword predictions from the forward and backward
parsing directions. The parsing procedure for each direction is formulated as
sequentially querying the memory component that stores continuous headword
embeddings. The proposed parser makes use of {\it soft} headword embeddings,
allowing the model to implicitly capture high-order parsing history without
dramatically increasing the computational complexity. We conduct experiments on
English, Chinese, and 12 other languages from the CoNLL 2006 shared task,
showing that the proposed model achieves state-of-the-art unlabeled attachment
scores on 6 languages.



The electronic health record (EHR) provides an unprecedented opportunity to
build actionable tools to support physicians at the point of care. In this
paper, we investigate survival analysis in the context of EHR data. We
introduce deep survival analysis, a hierarchical generative approach to
survival analysis. It departs from previous approaches in two primary ways: (1)
all observations, including covariates, are modeled jointly conditioned on a
rich latent structure; and (2) the observations are aligned by their failure
time, rather than by an arbitrary time zero as in traditional survival
analysis. Further, it (3) scalably handles heterogeneous (continuous and
discrete) data types that occur in the EHR. We validate deep survival analysis
model by stratifying patients according to risk of developing coronary heart
disease (CHD). Specifically, we study a dataset of 313,000 patients
corresponding to 5.5 million months of observations. When compared to the
clinically validated Framingham CHD risk score, deep survival analysis is
significantly superior in stratifying patients according to their risk.



Deep neural networks have become increasingly successful at solving classic
perception problems such as object recognition, semantic segmentation, and
scene understanding, often reaching or surpassing human-level accuracy. This
success is due in part to the ability of DNNs to learn useful representations
of high-dimensional inputs, a problem that humans must also solve. We examine
the relationship between the representations learned by these networks and
human psychological representations recovered from similarity judgments. We
find that deep features learned in service of object classification account for
a significant amount of the variance in human similarity judgments for a set of
animal images. However, these features do not capture some qualitative
distinctions that are a key part of human representations. To remedy this, we
develop a method for adapting deep features to align with human similarity
judgments, resulting in image representations that can potentially be used to
extend the scope of psychological experiments.



We introduce a new method for location recovery from pair-wise directions
that leverages an efficient convex program that comes with exact recovery
guarantees, even in the presence of adversarial outliers. When pairwise
directions represent scaled relative positions between pairs of views
(estimated for instance with epipolar geometry) our method can be used for
location recovery, that is the determination of relative pose up to a single
unknown scale. For this task, our method yields performance comparable to the
state-of-the-art with an order of magnitude speed-up. Our proposed numerical
framework is flexible in that it accommodates other approaches to location
recovery and can be used to speed up other methods. These properties are
demonstrated by extensively testing against state-of-the-art methods for
location recovery on 13 large, irregular collections of images of real scenes
in addition to simulated data with ground truth.



Adaptive behavior is mainly the result of adaptive brains. We go a step
beyond and claim that the brain does not only adapt to its surrounding reality
but rather, it builds itself up to constructs its own reality. That is, rather
than just trying to passively understand its environment, the brain is the
architect of its own reality in an active process where its internal models of
the external world frame how its new interactions with the environment are
assimilated. These internal models represent relevant predictive patterns of
interaction all over the different brain structures: perceptual, sensorimotor,
motor, etc. The emergence of adaptive behavior arises from this
self-constructive nature of the brain, based on the following principles of
organization: self-experimental, self- growing, and self-repairing.
Self-experimental, since to ensure survival, the self-constructive brain (SCB)
is an active machine capable of performing experiments of its own interactions
with the environment by mental simulation. Self-growing, since it dynamically
and incrementally constructs internal structures in order to build a model of
the world as it gathers statistics from its interactions with the environment.
Self-repairing, since to survive the SCB must also be robust and capable of
finding ways to repair parts of previously working structures and hence
re-construct a previous relevant pattern of activity.



Probabilistic models learned as density estimators can be exploited in
representation learning beside being toolboxes used to answer inference queries
only. However, how to extract useful representations highly depends on the
particular model involved. We argue that tractable inference, i.e. inference
that can be computed in polynomial time, can enable general schemes to extract
features from black box models. We plan to investigate how Tractable
Probabilistic Models (TPMs) can be exploited to generate embeddings by random
query evaluations. We devise two experimental designs to assess and compare
different TPMs as feature extractors in an unsupervised representation learning
framework. We show some experimental results on standard image datasets by
applying such a method to Sum-Product Networks and Mixture of Trees as
tractable models generating embeddings.



Several methods exist to infer causal networks from massive volumes of
observational data. However, almost all existing methods require a considerable
length of time series data to capture cause and effect relationships. In
contrast, memory-less transition networks or Markov Chain data, which refers to
one-step transitions to and from an event, have not been explored for causality
inference even though such data is widely available. We find that causal
network can be inferred from characteristics of four unique distribution zones
around each event. We call this Composition of Transitions and show that cause,
effect, and random events exhibit different behavior in their compositions. We
applied machine learning models to learn these different behaviors and to infer
causality. We name this new method Causality Inference using Composition of
Transitions (CICT). To evaluate CICT, we used an administrative inpatient
healthcare dataset to set up a network of patients transitions between
different diagnoses. We show that CICT is highly accurate in inferring whether
the transition between a pair of events is causal or random and performs well
in identifying the direction of causality in a bi-directional association.



We present an inductive spatio-temporal learning framework rooted in
inductive logic programming. With an emphasis on visuo-spatial language, logic,
and cognition, the framework supports learning with relational spatio-temporal
features identifiable in a range of domains involving the processing and
interpretation of dynamic visuo-spatial imagery. We present a prototypical
system, and an example application in the domain of computing for visual arts
and computational cognitive science.



We present Mean Box Pooling, a novel visual representation that pools over
CNN representations of a large number, highly overlapping object proposals. We
show that such representation together with nCCA, a successful multimodal
embedding technique, achieves state-of-the-art performance on the Visual
Madlibs task. Moreover, inspired by the nCCA's objective function, we extend
classical CNN+LSTM approach to train the network by directly maximizing the
similarity between the internal representation of the deep learning
architecture and candidate answers. Again, such approach achieves a significant
improvement over the prior work that also uses CNN+LSTM approach on Visual
Madlibs.



Deriving an effective facial expression recognition component is important
for a successful human-computer interaction system. Nonetheless, recognizing
facial expression remains a challenging task. This paper describes a novel
approach towards facial expression recognition task. The proposed method is
motivated by the success of Convolutional Neural Networks (CNN) on the face
recognition problem. Unlike other works, we focus on achieving good accuracy
while requiring only a small sample data for training. Scale Invariant Feature
Transform (SIFT) features are used to increase the performance on small data as
SIFT does not require extensive training data to generate useful features. In
this paper, both Dense SIFT and regular SIFT are studied and compared when
merged with CNN features. Moreover, an aggregator of the models is developed.
The proposed approach is tested on the FER-2013 and CK+ datasets. Results
demonstrate the superiority of CNN with Dense SIFT over conventional CNN and
CNN with SIFT. The accuracy even increased when all the models are aggregated
which generates state-of-art results on FER-2013 and CK+ datasets, where it
achieved 73.4% on FER-2013 and 99.1% on CK+.



The problem of Learning from Demonstration is targeted at learning to perform
tasks based on observed examples. One approach to Learning from Demonstration
is Inverse Reinforcement Learning, in which actions are observed to infer
rewards. This work combines a feature based state evaluation approach to
Inverse Reinforcement Learning with neuroevolution, a paradigm for modifying
neural networks based on their performance on a given task. Neural networks are
used to learn from a demonstrated expert policy and are evolved to generate a
policy similar to the demonstration. The algorithm is discussed and evaluated
against competitive feature-based Inverse Reinforcement Learning approaches. At
the cost of execution time, neural networks allow for non-linear combinations
of features in state evaluations. These valuations may correspond to state
value or state reward. This results in better correspondence to observed
examples as opposed to using linear combinations. This work also extends
existing work on Bayesian Non-Parametric Feature Construction for Inverse
Reinforcement Learning by using non-linear combinations of intermediate data to
improve performance. The algorithm is observed to be specifically suitable for
a linearly solvable non-deterministic Markov Decision Processes in which
multiple rewards are sparsely scattered in state space. A conclusive
performance hierarchy between evaluated algorithms is presented.



A visual type theory is a cognitive tool that has much in common with
language, and may be regarded as an exceptional form of spatial text adjunct. A
mathematical visual type theory, called NPM, has been under development that
can be viewed as an early-stage project in mathematical knowledge management
and mathematical user interface development. We discuss in greater detail the
notion of a visual type theory, report on progress towards a usable
mathematical visual type theory, and discuss the outlook for future work on
this project.



Quantitative analysis of soccer players' passing ability focuses on
descriptive statistics without considering the players' real contribution to
the passing and ball possession strategy of their team. Which player is able to
help the build-up of an attack, or to maintain the possession of the ball? We
introduce a novel methodology called QPass to answer questions like these
quantitatively. Based on the analysis of an entire season, we rank the players
based on the intrinsic value of their passes using QPass. We derive an album of
pass trajectories for different gaming styles. Our methodology reveals a quite
counterintuitive paradigm: losing the ball possession could lead to better
chances to win a game.



We investigate a novel cluster-of-bandit algorithm CAB for collaborative
recommendation tasks that implements the underlying feedback sharing mechanism
by estimating the neighborhood of users in a context-dependent manner. CAB
makes sharp departures from the state of the art by incorporating collaborative
effects into inference as well as learning processes in a manner that
seamlessly interleaving explore-exploit tradeoffs and collaborative steps. We
prove regret bounds under various assumptions on the data, which exhibit a
crisp dependence on the expected number of clusters over the users, a natural
measure of the statistical difficulty of the learning task. Experiments on
production and real-world datasets show that CAB offers significantly increased
prediction performance against a representative pool of state-of-the-art
methods.



We accommodate the Integrated Connectionist/Symbolic Architecture (ICS) of
[32] within the categorical compositional semantics (CatCo) of [13], forming a
model of categorical compositional cognition (CatCog). This resolves intrinsic
problems with ICS such as the fact that representations inhabit an unbounded
space and that sentences with differing tree structures cannot be directly
compared. We do so in a way that makes the most of the grammatical structure
available, in contrast to strategies like circular convolution. Using the CatCo
model also allows us to make use of tools developed for CatCo such as the
representation of ambiguity and logical reasoning via density matrices,
structural meanings for words such as relative pronouns, and addressing over-
and under-extension, all of which are present in cognitive processes. Moreover
the CatCog framework is sufficiently flexible to allow for entirely different
representations of meaning, such as conceptual spaces. Interestingly, since the
CatCo model was largely inspired by categorical quantum mechanics, so is
CatCog.



'Health utilities' measure patient preferences for perfect health compared to
specific unhealthy states, such as asthma, a fractured hip, or colon cancer.
When integrated over time, these estimations are called quality adjusted life
years (QALYs). Until now, characterizing health utilities (HUs) required
detailed patient interviews or written surveys. While reliable and specific,
this data remained costly due to efforts to locate, enlist and coordinate
participants. Thus the scope, context and temporality of diseases examined has
remained limited.
  Now that more than a billion people use social media, we propose a novel
strategy: use natural language processing to analyze public online
conversations for signals of the severity of medical conditions and correlate
these to known HUs using machine learning. In this work, we filter a dataset
that originally contained 2 billion tweets for relevant content on 60 diseases.
Using this data, our algorithm successfully distinguished mild from severe
diseases, which had previously been categorized only by traditional techniques.
This represents progress towards two related applications: first, predicting
HUs where such information is nonexistent; and second, (where rich HU data
already exists) estimating temporal or geographic patterns of disease severity
through data mining.



Previous studies have proposed image-based clutter measures that correlate
with human search times and/or eye movements. However, most models do not take
into account the fact that the effects of clutter interact with the foveated
nature of the human visual system: visual clutter further from the fovea has an
increasing detrimental influence on perception. Here, we introduce a new
foveated clutter model to predict the detrimental effects in target search
utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz
et al.) as our non foveated clutter model, and we stack a peripheral
architecture on top of Feature Congestion for our foveated model. We introduce
the Peripheral Integration Feature Congestion (PIFC) coefficient, as a
fundamental ingredient of our model that modulates clutter as a non-linear gain
contingent on eccentricity. We finally show that Foveated Feature Congestion
(FFC) clutter scores r(44) = -0.82 correlate better with target detection (hit
rate) than regular Feature Congestion r(44) = -0.19 in forced fixation search.
Thus, our model allows us to enrich clutter perception research by computing
fixation specific clutter maps. A toolbox for creating peripheral
architectures: Piranhas: Peripheral Architectures for Natural, Hybrid and
Artificial Systems will be made available.



In this paper, a geometric framework for neural networks is proposed. This
framework uses the inner product space structure underlying the parameter set
to perform gradient descent not in a component-based form, but in a
coordinate-free manner. Convolutional neural networks are described in this
framework in a compact form, with the gradients of standard --- and
higher-order --- loss functions calculated for each layer of the network. This
approach can be applied to other network structures and provides a basis on
which to create new networks.



We study machine learning formulations of inductive program synthesis; given
input-output examples, we try to synthesize source code that maps inputs to
corresponding outputs. Our aims are to develop new machine learning approaches
based on neural networks and graphical models, and to understand the
capabilities of machine learning techniques relative to traditional
alternatives, such as those based on constraint solving from the programming
languages community.
  Our key contribution is the proposal of TerpreT, a domain-specific language
for expressing program synthesis problems. TerpreT is similar to a
probabilistic programming language: a model is composed of a specification of a
program representation (declarations of random variables) and an interpreter
describing how programs map inputs to outputs (a model connecting unknowns to
observations). The inference task is to observe a set of input-output examples
and infer the underlying program. TerpreT has two main benefits. First, it
enables rapid exploration of a range of domains, program representations, and
interpreter models. Second, it separates the model specification from the
inference algorithm, allowing like-to-like comparisons between different
approaches to inference. From a single TerpreT specification we automatically
perform inference using four different back-ends. These are based on gradient
descent, linear program (LP) relaxations for graphical models, discrete
satisfiability solving, and the Sketch program synthesis system.
  We illustrate the value of TerpreT by developing several interpreter models
and performing an empirical comparison between alternative inference
algorithms. Our key empirical finding is that constraint solvers dominate the
gradient descent and LP-based formulations. We conclude with suggestions for
the machine learning community to make progress on program synthesis.



Explicit high-order feature interactions efficiently capture essential
structural knowledge about the data of interest and have been used for
constructing generative models. We present a supervised discriminative
High-Order Parametric Embedding (HOPE) approach to data visualization and
compression. Compared to deep embedding models with complicated deep
architectures, HOPE generates more effective high-order feature mapping through
an embarrassingly simple shallow model. Furthermore, two approaches to
generating a small number of exemplars conveying high-order interactions to
represent large-scale data sets are proposed. These exemplars in combination
with the feature mapping learned by HOPE effectively capture essential data
variations. Moreover, through HOPE, these exemplars are employed to increase
the computational efficiency of kNN classification for fast information
retrieval by thousands of times. For classification in two-dimensional
embedding space on MNIST and USPS datasets, our shallow method HOPE with simple
Sigmoid transformations significantly outperforms state-of-the-art supervised
deep embedding models based on deep neural networks, and even achieved
historically low test error rate of 0.65% in two-dimensional space on MNIST,
which demonstrates the representational efficiency and power of supervised
shallow models with high-order feature interactions.



Model-based collaborative filtering analyzes user-item interactions to infer
latent factors that represent user preferences and item characteristics in
order to predict future interactions. Most collaborative filtering algorithms
assume that these latent factors are static, although it has been shown that
user preferences and item perceptions drift over time. In this paper, we
propose a conjugate and numerically stable dynamic matrix factorization (DCPF)
based on compound Poisson matrix factorization that models the smoothly
drifting latent factors using Gamma-Markov chains. We propose a numerically
stable Gamma chain construction, and then present a stochastic variational
inference approach to estimate the parameters of our model. We apply our model
to time-stamped ratings data sets: Netflix, Yelp, and Last.fm, where DCPF
achieves a higher predictive accuracy than state-of-the-art static and dynamic
factorization models.



Finding the most effective way to aggregate multi-subject fMRI data is a
long-standing and challenging problem. It is of increasing interest in
contemporary fMRI studies of human cognition due to the scarcity of data per
subject and the variability of brain anatomy and functional response across
subjects. Recent work on latent factor models shows promising results in this
task but this approach does not preserve spatial locality in the brain. We
examine two ways to combine the ideas of a factor model and a searchlight based
analysis to aggregate multi-subject fMRI data while preserving spatial
locality. We first do this directly by combining a recent factor method known
as a shared response model with searchlight analysis. Then we design a
multi-view convolutional autoencoder for the same task. Both approaches
preserve spatial locality and have competitive or better performance compared
with standard searchlight analysis and the shared response model applied across
the whole brain. We also report a system design to handle the computational
challenge of training the convolutional autoencoder.



Descriptions are often provided along with recommendations to help users'
discovery. Recommending automatically generated music playlists (e.g.
personalised playlists) introduces the problem of generating descriptions. In
this paper, we propose a method for generating music playlist descriptions,
which is called as music captioning. In the proposed method, audio content
analysis and natural language processing are adopted to utilise the information
of each track.



Probabilistic techniques are central to data analysis, but different
approaches can be difficult to apply, combine, and compare. This paper
introduces composable generative population models (CGPMs), a computational
abstraction that extends directed graphical models and can be used to describe
and compose a broad class of probabilistic data analysis techniques. Examples
include hierarchical Bayesian models, multivariate kernel methods,
discriminative machine learning, clustering algorithms, dimensionality
reduction, and arbitrary probabilistic programs. We also demonstrate the
integration of CGPMs into BayesDB, a probabilistic programming platform that
can express data analysis tasks using a modeling language and a structured
query language. The practical value is illustrated in two ways. First, CGPMs
are used in an analysis that identifies satellite data records which probably
violate Kepler's Third Law, by composing causal probabilistic programs with
non-parametric Bayes in under 50 lines of probabilistic code. Second, for
several representative data analysis tasks, we report on lines of code and
accuracy measurements of various CGPMs, plus comparisons with standard baseline
solutions from Python and MATLAB libraries.



State-of-the-art answer set programming (ASP) solvers rely on a program
called a grounder to convert non-ground programs containing variables into
variable-free, propositional programs. The size of this grounding depends
heavily on the size of the non-ground rules, and thus, reducing the size of
such rules is a promising approach to improve solving performance. To this end,
in this paper we announce lpopt, a tool that decomposes large logic programming
rules into smaller rules that are easier to handle for current solvers. The
tool is specifically tailored to handle the standard syntax of the ASP language
(ASP-Core) and makes it easier for users to write efficient and intuitive ASP
programs, which would otherwise often require significant hand-tuning by expert
ASP engineers. It is based on an idea proposed by Morak and Woltran (2012) that
we extend significantly in order to handle the full ASP syntax, including
complex constructs like aggregates, weak constraints, and arithmetic
expressions. We present the algorithm, the theoretical foundations on how to
treat these constructs, as well as an experimental evaluation showing the
viability of our approach.



Accuracy and interpretability are two dominant features of successful
predictive models. Typically, a choice must be made in favor of complex black
box models such as recurrent neural networks (RNN) for accuracy versus less
accurate but more interpretable traditional models such as logistic regression.
This tradeoff poses challenges in medicine where both accuracy and
interpretability are important. We addressed this challenge by developing the
REverse Time AttentIoN model (RETAIN) for application to Electronic Health
Records (EHR) data. RETAIN achieves high accuracy while remaining clinically
interpretable and is based on a two-level neural attention model that detects
influential past visits and significant clinical variables within those visits
(e.g. key diagnoses). RETAIN mimics physician practice by attending the EHR
data in a reverse time order so that recent clinical visits are likely to
receive higher attention. RETAIN was tested on a large health system EHR
dataset with 14 million visits completed by 263K patients over an 8 year period
and demonstrated predictive accuracy and computational scalability comparable
to state-of-the-art methods such as RNN, and ease of interpretability
comparable to traditional models.



Knowledge graph construction consists of two tasks: extracting information
from external resources (knowledge population) and inferring missing
information through a statistical analysis on the extracted information
(knowledge completion). In many cases, insufficient external resources in the
knowledge population hinder the subsequent statistical inference. The gap
between these two processes can be reduced by an incremental population
approach. We propose a new probabilistic knowledge graph factorisation method
that benefits from the path structure of existing knowledge (e.g. syllogism)
and enables a common modelling approach to be used for both incremental
population and knowledge completion tasks. More specifically, the probabilistic
formulation allows us to develop an incremental population algorithm that
trades off exploitation-exploration. Experiments on three benchmark datasets
show that the balanced exploitation-exploration helps the incremental
population, and the additional path structure helps to predict missing
information in knowledge completion.



One way to solve lasso problems when the dictionary does not fit into
available memory is to first screen the dictionary to remove unneeded features.
Prior research has shown that sequential screening methods offer the greatest
promise in this endeavor. Most existing work on sequential screening targets
the context of tuning parameter selection, where one screens and solves a
sequence of $N$ lasso problems with a fixed grid of geometrically spaced
regularization parameters. In contrast, we focus on the scenario where a target
regularization parameter has already been chosen via cross-validated model
selection, and we then need to solve many lasso instances using this fixed
value. In this context, we propose and explore a feedback controlled sequential
screening scheme. Feedback is used at each iteration to select the next problem
to be solved. This allows the sequence of problems to be adapted to the
instance presented and the number of intermediate problems to be automatically
selected. We demonstrate our feedback scheme using several datasets including a
dictionary of approximate size 100,000 by 300,000.



Recently dictionary screening has been proposed as an effective way to
improve the computational efficiency of solving the lasso problem, which is one
of the most commonly used method for learning sparse representations. To
address today's ever increasing large dataset, effective screening relies on a
tight region bound on the solution to the dual lasso. Typical region bounds are
in the form of an intersection of a sphere and multiple half spaces. One way to
tighten the region bound is using more half spaces, which however, adds to the
overhead of solving the high dimensional optimization problem in lasso
screening. This paper reveals the interesting property that the optimization
problem only depends on the projection of features onto the subspace spanned by
the normals of the half spaces. This property converts an optimization problem
in high dimension to much lower dimension, and thus sheds light on reducing the
computation overhead of lasso screening based on tighter region bounds.



We introduce an unsupervised discriminative model for the task of retrieving
experts in online document collections. We exclusively employ textual evidence
and avoid explicit feature engineering by learning distributed word
representations in an unsupervised way. We compare our model to
state-of-the-art unsupervised statistical vector space and probabilistic
generative approaches. Our proposed log-linear model achieves the retrieval
performance levels of state-of-the-art document-centric methods with the low
inference cost of so-called profile-centric approaches. It yields a
statistically significant improved ranking over vector space and generative
models in most cases, matching the performance of supervised methods on various
benchmarks. That is, by using solely text we can do as well as methods that
work with external evidence and/or relevance feedback. A contrastive analysis
of rankings produced by discriminative and generative approaches shows that
they have complementary strengths due to the ability of the unsupervised
discriminative model to perform semantic matching.



Incremental clustering approaches have been proposed for handling large data
when given data set is too large to be stored. The key idea of these approaches
is to find representatives to represent each cluster in each data chunk and
final data analysis is carried out based on those identified representatives
from all the chunks. However, most of the incremental approaches are used for
single view data. As large multi-view data generated from multiple sources
becomes prevalent nowadays, there is a need for incremental clustering
approaches to handle both large and multi-view data. In this paper we propose a
new incremental clustering approach called incremental minimax optimization
based fuzzy clustering (IminimaxFCM) to handle large multi-view data. In
IminimaxFCM, representatives with multiple views are identified to represent
each cluster by integrating multiple complementary views using minimax
optimization. The detailed problem formulation, updating rules derivation, and
the in-depth analysis of the proposed IminimaxFCM are provided. Experimental
studies on several real world multi-view data sets have been conducted. We
observed that IminimaxFCM outperforms related incremental fuzzy clustering in
terms of clustering accuracy, demonstrating the great potential of IminimaxFCM
for large multi-view data analysis.



Multi-view data clustering refers to categorizing a data set by making good
use of related information from multiple representations of the data. It
becomes important nowadays because more and more data can be collected in a
variety of ways, in different settings and from different sources, so each data
set can be represented by different sets of features to form different views of
it. Many approaches have been proposed to improve clustering performance by
exploring and integrating heterogeneous information underlying different views.
In this paper, we propose a new multi-view fuzzy clustering approach called
MinimaxFCM by using minimax optimization based on well-known Fuzzy c means. In
MinimaxFCM the consensus clustering results are generated based on minimax
optimization in which the maximum disagreements of different weighted views are
minimized. Moreover, the weight of each view can be learned automatically in
the clustering process. In addition, there is only one parameter to be set
besides the fuzzifier. The detailed problem formulation, updating rules
derivation, and the in-depth analysis of the proposed MinimaxFCM are provided
here. Experimental studies on nine multi-view data sets including real world
image and document data sets have been conducted. We observed that MinimaxFCM
outperforms related multi-view clustering approaches in terms of clustering
accuracy, demonstrating the great potential of MinimaxFCM for multi-view data
analysis.



A great video title describes the most salient event compactly and captures
the viewer's attention. In contrast, video captioning tends to generate
sentences that describe the video as a whole. Although generating a video title
automatically is a very useful task, it is much less addressed than video
captioning. We address video title generation for the first time by proposing
two methods that extend state-of-the-art video captioners to this new task.
First, we make video captioners highlight sensitive by priming them with a
highlight detector. Our framework allows for jointly training a model for title
generation and video highlight localization. Second, we induce high sentence
diversity in video captioners, so that the generated titles are also diverse
and catchy. This means that a large number of sentences might be required to
learn the sentence structure of titles. Hence, we propose a novel sentence
augmentation method to train a captioner with additional sentence-only examples
that come without corresponding videos. We collected a large-scale Video Titles
in the Wild (VTW) dataset of 18100 automatically crawled user-generated videos
and titles. On VTW, our methods consistently improve title prediction accuracy,
and achieve the best performance in both automatic and human evaluation.
Finally, our sentence augmentation method also outperforms the baselines on the
M-VAD dataset.



The ability to reason beyond established knowledge allows Organic Chemists to
solve synthetic problems and to invent novel transformations. Here, we propose
a model which mimics chemical reasoning and formalises reaction prediction as
finding missing links in a knowledge graph. We have constructed a knowledge
graph containing 14.4 million molecules and 8.2 million binary reactions, which
represents the bulk of all chemical reactions ever published in the scientific
literature. Our model outperforms a rule-based expert system in the reaction
prediction task for 180,000 randomly selected binary reactions. We show that
our data-driven model generalises even beyond known reaction types, and is thus
capable of effectively (re-) discovering novel transformations (even including
transition-metal catalysed reactions). Our model enables computers to infer
hypotheses about reactivity and reactions by only considering the intrinsic
local structure of the graph, and because each single reaction prediction is
typically achieved in a sub-second time frame, our model can be used as a
high-throughput generator of reaction hypotheses for reaction discovery.



Artificial intelligence and machine learning are in a period of astounding
growth. However, there are concerns that these technologies may be used, either
with or without intention, to perpetuate the prejudice and unfairness that
unfortunately characterizes many human institutions. Here we show for the first
time that human-like semantic biases result from the application of standard
machine learning to ordinary language---the same sort of language humans are
exposed to every day. We replicate a spectrum of standard human biases as
exposed by the Implicit Association Test and other well-known psychological
studies. We replicate these using a widely used, purely statistical
machine-learning model---namely, the GloVe word embedding---trained on a corpus
of text from the Web. Our results indicate that language itself contains
recoverable and accurate imprints of our historic biases, whether these are
morally neutral as towards insects or flowers, problematic as towards race or
gender, or even simply veridical, reflecting the {\em status quo} for the
distribution of gender with respect to careers or first names. These
regularities are captured by machine learning along with the rest of semantics.
In addition to our empirical findings concerning language, we also contribute
new methods for evaluating bias in text, the Word Embedding Association Test
(WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results
have implications not only for AI and machine learning, but also for the fields
of psychology, sociology, and human ethics, since they raise the possibility
that mere exposure to everyday language can account for the biases we replicate
here.



We introduce a novel latent vector space model that jointly learns the latent
representations of words, e-commerce products and a mapping between the two
without the need for explicit annotations. The power of the model lies in its
ability to directly model the discriminative relation between products and a
particular word. We compare our method to existing latent vector space models
(LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank
setting. Our latent vector space model achieves its enhanced performance as it
learns better product representations. Furthermore, the mapping from words to
products and the representations of words benefit directly from the errors
propagated back from the product representations during parameter estimation.
We provide an in-depth analysis of the performance of our model and analyze the
structure of the learned representations.



Formal approaches for automated causality analysis, fault localization,
explanation of events, accountability and blaming have been proposed
independently by several communities --- in particular, AI, concurrency,
model-based diagnosis, formal methods. Work on these topics has significantly
gained speed during the last years. The goals of CREST are to bring together
and foster exchange between researchers from the different communities, and to
present and discuss recent advances and new ideas in the field.
  The workshop program consisted of a set of invited and contributed
presentations that illustrate different techniques for, and applications of,
causality analysis and fault localization.
  The program was anchored by two keynote talks. The keynote by Hana Chockler
(King's College) provided a broad perspective on the application of causal
reasoning based on Halpern and Pearl's definitions of actual causality to a
variety of application domains ranging from formal verification to legal
reasoning. The keynote by Chao Wang (Virginia Tech) concentrated on
constraint-based analysis techniques for debugging and verifying concurrent
programs.
  Workshop papers deal with compositional causality analysis and a wide
spectrum of application for causal reasoning, such as debugging of
probabilistic models, accountability and responsibility, hazard analysis in
practice based on Lewis' counterfactuals, and fault localization and repair.



Zero-Shot learning has been shown to be an efficient strategy for domain
adaptation. In this context, this paper builds on the recent work of Bucher et
al. [1], which proposed an approach to solve Zero-Shot classification problems
(ZSC) by introducing a novel metric learning based objective function. This
objective function allows to learn an optimal embedding of the attributes
jointly with a measure of similarity between images and attributes. This paper
extends their approach by proposing several schemes to control the generation
of the negative pairs, resulting in a significant improvement of the
performance and giving above state-of-the-art results on three challenging ZSC
datasets.



Recurrent neural networks have recently been used for learning to describe
images using natural language. However, it has been observed that these models
generalize poorly to scenes that were not observed during training, possibly
depending too strongly on the statistics of the text in the training data. Here
we propose to describe images using short structured representations, aiming to
capture the crux of a description. These structured representations allow us to
tease-out and evaluate separately two types of generalization: standard
generalization to new images with similar scenes, and generalization to new
combinations of known entities. We compare two learning approaches on the
MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show,
Attend and Tell), and a simple structured prediction model on top of a deep
network. We find that the structured model generalizes to new compositions
substantially better than the LSTM, ~7 times the accuracy of predicting
structured representations. By providing a concrete method to quantify
generalization for unseen combinations, we argue that structured
representations and compositional splits are a useful benchmark for image
captioning, and advocate compositional models that capture linguistic and
visual structure.



Context: Topic modeling finds human-readable structures in unstructured
textual data. A widely used topic modeler is Latent Dirichlet allocation. When
run on different datasets, LDA suffers from "order effects" i.e. different
topics are generated if the order of training data is shuffled. Such order
effects introduce a systematic error for any study. This error can relate to
misleading results;specifically, inaccurate topic descriptions and a reduction
in the efficacy of text mining classification results. Objective: To provide a
method in which distributions generated by LDA are more stable and can be used
for further analysis. Method: We use LDADE, a search-based software engineering
tool that tunes LDA's parameters using DE (Differential Evolution). LDADE is
evaluated on data from a programmer information exchange site (Stackoverflow),
title and abstract text of thousands ofSoftware Engineering (SE) papers, and
software defect reports from NASA. Results were collected across different
implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different
platforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using
Gibbs sampling). Results were scored via topic stability and text mining
classification accuracy. Results: In all treatments: (i) standard LDA exhibits
very large topic instability; (ii) LDADE's tunings dramatically reduce cluster
instability; (iii) LDADE also leads to improved performances for supervised as
well as unsupervised learning. Conclusion: Due to topic instability, using
standard LDA with its "off-the-shelf" settings should now be depreciated. Also,
in future, we should require SE papers that use LDA to test and (if needed)
mitigate LDA topic instability. Finally, LDADE is a candidate technology for
effectively and efficiently reducing that instability.



Visual question answering (VQA) systems are emerging from a desire to empower
users to ask any natural language question about visual content and receive a
valid answer in response. However, close examination of the VQA problem reveals
an unavoidable, entangled problem that multiple humans may or may not always
agree on a single answer to a visual question. We train a model to
automatically predict from a visual question whether a crowd would agree on a
single answer. We then propose how to exploit this system in a novel
application to efficiently allocate human effort to collect answers to visual
questions. Specifically, we propose a crowdsourcing system that automatically
solicits fewer human responses when answer agreement is expected and more human
responses when answer disagreement is expected. Our system improves upon
existing crowdsourcing systems, typically eliminating at least 20% of human
effort with no loss to the information collected from the crowd.



In this paper, an Extreme Learning Machine (ELM) based technique for
Multi-label classification problems is proposed and discussed. In multi-label
classification, each of the input data samples belongs to one or more than one
class labels. The traditional binary and multi-class classification problems
are the subset of the multi-label problem with the number of labels
corresponding to each sample limited to one. The proposed ELM based multi-label
classification technique is evaluated with six different benchmark multi-label
datasets from different domains such as multimedia, text and biology. A
detailed comparison of the results is made by comparing the proposed method
with the results from nine state of the arts techniques for five different
evaluation metrics. The nine methods are chosen from different categories of
multi-label methods. The comparative results shows that the proposed Extreme
Learning Machine based multi-label classification technique is a better
alternative than the existing state of the art methods for multi-label
problems.



Innovation diffusion has been studied extensively in a variety of
disciplines, including sociology, economics, marketing, ecology, and computer
science. Traditional literature on innovation diffusion has been dominated by
models of aggregate behavior and trends. However, the agent-based modeling
(ABM) paradigm is gaining popularity as it captures agent heterogeneity and
enables fine-grained modeling of interactions mediated by social and geographic
networks. While most ABM work on innovation diffusion is theoretical,
empirically grounded models are increasingly important, particularly in guiding
policy decisions. We present a critical review of empirically grounded
agent-based models of innovation diffusion, developing a categorization of this
research based on types of agent models as well as applications. By connecting
the modeling methodologies in the fields of information and innovation
diffusion, we suggest that the maximum likelihood estimation framework widely
used in the former is a promising paradigm for calibration of agent-based
models for innovation diffusion. Although many advances have been made to
standardize ABM methodology, we identify four major issues in model calibration
and validation, and suggest potential solutions.



The tremendous success of ImageNet-trained deep features on a wide range of
transfer tasks begs the question: what are the properties of the ImageNet
dataset that are critical for learning good, general-purpose features? This
work provides an empirical investigation of various facets of this question: Is
more pre-training data always better? How does feature quality depend on the
number of training examples per class? Does adding more object classes improve
performance? For the same data budget, how should the data be split into
classes? Is fine-grained recognition necessary for learning good features?
Given the same number of training classes, is it better to have coarse classes
or fine-grained classes? Which is better: more classes or more examples per
class? To answer these and related questions, we pre-trained CNN features on
various subsets of the ImageNet dataset and evaluated transfer performance on
PASCAL detection, PASCAL action classification, and SUN scene classification
tasks. Our overall findings suggest that most changes in the choice of
pre-training data long thought to be critical do not significantly affect
transfer performance.? Given the same number of training classes, is it better
to have coarse classes or fine-grained classes? Which is better: more classes
or more examples per class?



As machines have become more intelligent, there has been a renewed interest
in methods for measuring their intelligence. A common approach is to propose
tasks for which a human excels, but one which machines find difficult. However,
an ideal task should also be easy to evaluate and not be easily gameable. We
begin with a case study exploring the recently popular task of image captioning
and its limitations as a task for measuring machine intelligence. An
alternative and more promising task is Visual Question Answering that tests a
machine's ability to reason about language and vision. We describe a dataset
unprecedented in size created for the task that contains over 760,000 human
generated questions about images. Using around 10 million human generated
answers, machines may be easily evaluated.



In this paper a high speed neural network classifier based on extreme
learning machines for multi-label classification problem is proposed and
dis-cussed. Multi-label classification is a superset of traditional binary and
multi-class classification problems. The proposed work extends the extreme
learning machine technique to adapt to the multi-label problems. As opposed to
the single-label problem, both the number of labels the sample belongs to, and
each of those target labels are to be identified for multi-label classification
resulting in in-creased complexity. The proposed high speed multi-label
classifier is applied to six benchmark datasets comprising of different
application areas such as multi-media, text and biology. The training time and
testing time of the classifier are compared with those of the state-of-the-arts
methods. Experimental studies show that for all the six datasets, our proposed
technique have faster execution speed and better performance, thereby
outperforming all the existing multi-label clas-sification methods.



In this paper, a novel extreme learning machine based online multi-label
classifier for real-time data streams is proposed. Multi-label classification
is one of the actively researched machine learning paradigm that has gained
much attention in the recent years due to its rapidly increasing real world
applications. In contrast to traditional binary and multi-class classification,
multi-label classification involves association of each of the input samples
with a set of target labels simultaneously. There are no real-time online
neural network based multi-label classifier available in the literature. In
this paper, we exploit the inherent nature of high speed exhibited by the
extreme learning machines to develop a novel online real-time classifier for
multi-label data streams. The developed classifier is experimented with
datasets from different application domains for consistency, performance and
speed. The experimental studies show that the proposed method outperforms the
existing state-of-the-art techniques in terms of speed and accuracy and can
classify multi-label data streams in real-time.



Deep neural networks have shown striking progress and obtained
state-of-the-art results in many AI research fields in the recent years.
However, it is often unsatisfying to not know why they predict what they do. In
this paper, we address the problem of interpreting Visual Question Answering
(VQA) models. Specifically, we are interested in finding what part of the input
(pixels in images or words in questions) the VQA model focuses on while
answering the question. To tackle this problem, we use two visualization
techniques -- guided backpropagation and occlusion -- to find important words
in the question and important regions in the image. We then present qualitative
and quantitative analyses of these importance maps. We found that even without
explicit attention mechanisms, VQA models may sometimes be implicitly attending
to relevant regions in the image, and often to appropriate words in the
question.



This paper explores the capabilities of convolutional neural networks to deal
with a task that is easily manageable for humans: perceiving 3D pose of a human
body from varying angles. However, in our approach, we are restricted to using
a monocular vision system. For this purpose, we apply a convolutional neural
network approach on RGB videos and extend it to three dimensional convolutions.
This is done via encoding the time dimension in videos as the 3\ts{rd}
dimension in convolutional space, and directly regressing to human body joint
positions in 3D coordinate space. This research shows the ability of such a
network to achieve state-of-the-art performance on the selected Human3.6M
dataset, thus demonstrating the possibility of successfully representing
temporal data with an additional dimension in the convolutional operation.



In this paper, a progressive learning technique for multi-class
classification is proposed. This newly developed learning technique is
independent of the number of class constraints and it can learn new classes
while still retaining the knowledge of previous classes. Whenever a new class
(non-native to the knowledge learnt thus far) is encountered, the neural
network structure gets remodeled automatically by facilitating new neurons and
interconnections, and the parameters are calculated in such a way that it
retains the knowledge learnt thus far. This technique is suitable for
real-world applications where the number of classes is often unknown and online
learning from real-time data is required. The consistency and the complexity of
the progressive learning technique are analyzed. Several standard datasets are
used to evaluate the performance of the developed technique. A comparative
study shows that the developed technique is superior.



In this paper, a high-speed online neural network classifier based on extreme
learning machines for multi-label classification is proposed. In multi-label
classification, each of the input data sample belongs to one or more than one
of the target labels. The traditional binary and multi-class classification
where each sample belongs to only one target class forms the subset of
multi-label classification. Multi-label classification problems are far more
complex than binary and multi-class classification problems, as both the number
of target labels and each of the target labels corresponding to each of the
input samples are to be identified. The proposed work exploits the high-speed
nature of the extreme learning machines to achieve real-time multi-label
classification of streaming data. A new threshold-based online sequential
learning algorithm is proposed for high speed and streaming data classification
of multi-label problems. The proposed method is experimented with six different
datasets from different application domains such as multimedia, text, and
biology. The hamming loss, accuracy, training time and testing time of the
proposed technique is compared with nine different state-of-the-art methods.
Experimental studies shows that the proposed technique outperforms the existing
multi-label classifiers in terms of performance and speed.



We present a loss function for neural networks that encompasses an idea of
trivial versus non-trivial predictions, such that the network jointly
determines its own prediction goals and learns to satisfy them. This permits
the network to choose sub-sets of a problem which are most amenable to its
abilities to focus on solving, while discarding 'distracting' elements that
interfere with its learning. To do this, the network first transforms the raw
data into a higher-level categorical representation, and then trains a
predictor from that new time series to its future. To prevent a trivial
solution of mapping the signal to zero, we introduce a measure of
non-triviality via a contrast between the prediction error of the learned model
with a naive model of the overall signal statistics. The transform can learn to
discard uninformative and unpredictable components of the signal in favor of
the features which are both highly predictive and highly predictable. This
creates a coarse-grained model of the time-series dynamics, focusing on
predicting the slowly varying latent parameters which control the statistics of
the time-series, rather than predicting the fast details directly. The result
is a semi-supervised algorithm which is capable of extracting latent
parameters, segmenting sections of time-series with differing statistics, and
building a higher-level representation of the underlying dynamics from
unlabeled data.



The community deception problem is about how to hide a target community C
from community detection algorithms. The need for deception emerges whenever a
group of entities (e.g., activists, police enforcements) want to cooperate
while concealing their existence as a community. In this paper we introduce and
formalize the community deception problem. To solve this problem, we describe
algorithms that carefully rewire the connections of C's members. We
experimentally show how several existing community detection algorithms can be
deceived, and quantify the level of deception by introducing a deception score.
We believe that our study is intriguing since, while showing how deception can
be realized it raises awareness for the design of novel detection algorithms
robust to deception techniques.



The computation and storage requirements for Deep Neural Networks (DNNs) are
usually high. This issue limits their deployability on ubiquitous computing
devices such as smart phones, wearables and autonomous drones. In this paper,
we propose ternary neural networks (TNNs) in order to make deep learning more
resource-efficient. We train these TNNs using a teacher-student approach based
on a novel, layer-wise greedy methodology. Thanks to our two-stage training
procedure, the teacher network is still able to use state-of-the-art methods
such as dropout and batch normalization to increase accuracy and reduce
training time. Using only ternary weights and activations, the student ternary
network learns to mimic the behavior of its teacher network without using any
multiplication. Unlike its -1,1 binary counterparts, a ternary neural network
inherently prunes the smaller weights by setting them to zero during training.
This makes them sparser and thus more energy-efficient. We design a
purpose-built hardware architecture for TNNs and implement it on FPGA and ASIC.
We evaluate TNNs on several benchmark datasets and demonstrate up to 3.1x
better energy efficiency with respect to the state of the art while also
improving accuracy.



Despite significant developments in Proof Theory, surprisingly little
attention has been devoted to the concept of proof verifier. In particular, the
mathematical community may be interested in studying different types of proof
verifiers (people, programs, oracles, communities, superintelligences) as
mathematical objects. Such an effort could reveal their properties, their
powers and limitations (particularly in human mathematicians), minimum and
maximum complexity, as well as self-verification and self-reference issues. We
propose an initial classification system for verifiers and provide some
rudimentary analysis of solved and open problems in this important domain. Our
main contribution is a formal introduction of the notion of unverifiability,
for which the paper could serve as a general citation in domains of theorem
proving, as well as software and AI verification.



This paper describes a new kind of knowledge representation and mining system
which we are calling the Semantic Knowledge Graph. At its heart, the Semantic
Knowledge Graph leverages an inverted index, along with a complementary
uninverted index, to represent nodes (terms) and edges (the documents within
intersecting postings lists for multiple terms/nodes). This provides a layer of
indirection between each pair of nodes and their corresponding edge, enabling
edges to materialize dynamically from underlying corpus statistics. As a
result, any combination of nodes can have edges to any other nodes materialize
and be scored to reveal latent relationships between the nodes. This provides
numerous benefits: the knowledge graph can be built automatically from a
real-world corpus of data, new nodes - along with their combined edges - can be
instantly materialized from any arbitrary combination of preexisting nodes
(using set operations), and a full model of the semantic relationships between
all entities within a domain can be represented and dynamically traversed using
a highly compact representation of the graph. Such a system has widespread
applications in areas as diverse as knowledge modeling and reasoning, natural
language processing, anomaly detection, data cleansing, semantic search,
analytics, data classification, root cause analysis, and recommendations
systems. The main contribution of this paper is the introduction of a novel
system - the Semantic Knowledge Graph - which is able to dynamically discover
and score interesting relationships between any arbitrary combination of
entities (words, phrases, or extracted concepts) through dynamically
materializing nodes and edges from a compact graphical representation built
automatically from a corpus of data representative of a knowledge domain.



Classification involves the learning of the mapping function that associates
input samples to corresponding target label. There are two major categories of
classification problems: Single-label classification and Multi-label
classification. Traditional binary and multi-class classifications are
sub-categories of single-label classification. Several classifiers are
developed for binary, multi-class and multi-label classification problems, but
there are no classifiers available in the literature capable of performing all
three types of classification. In this paper, a novel online universal
classifier capable of performing all the three types of classification is
proposed. Being a high speed online classifier, the proposed technique can be
applied to streaming data applications. The performance of the developed
classifier is evaluated using datasets from binary, multi-class and multi-label
problems. The results obtained are compared with state-of-the-art techniques
from each of the classification types.



Have you ever looked at a machine learning classification model and thought,
I could have made that? Well, that is what we test in this project, comparing
XGBoost trained on human engineered features to training directly on data. The
human engineered features do not outperform XGBoost trained di- rectly on the
data, but they are comparable. This project con- tributes a novel method for
utilizing human created classifi- cation models on high dimensional datasets.



Q-learning is a simple and powerful tool in solving dynamic problems where
environments are unknown. It uses a balance of exploration and exploitation to
find an optimal solution to the problem. In this paper, we propose using four
basic emotions: joy, sadness, fear, and anger to influence a Qlearning agent.
Simulations show that the proposed affective agent requires lesser number of
steps to find the optimal path. We found when affective agent finds the optimal
path, the ratio between exploration to exploitation gradually decreases,
indicating lower total step count in the long run



A mesoscopic approach to modeling pedestrian simulation with multiple exits
is proposed in this paper. A floor field based on Qlearning Algorithm is used.
Attractiveness of exits to pedestrian typically is based on shortest path.
However, several factors may influence pedestrian choice of exits. Scenarios
with multiple exits are presented and effect of Q-learning rewards system on
navigation is investigated



Starting from a generalization of the standard axioms for a monoid we present
a stepwise development of various, mutually equivalent foundational axiom
systems for category theory. Our axiom sets have been formalized in the
Isabelle/HOL interactive proof assistant, and this formalization utilizes a
semantically correct embedding of free logic in classical higher-order logic.
The modeling and formal analysis of our axiom sets has been significantly
supported by series of experiments with automated reasoning tools integrated
with Isabelle/HOL. We also address the relation of our axiom systems to
alternative proposals from the literature, including an axiom set proposed by
Freyd and Scedrov for which we reveal a technical issue (when encoded in free
logic where free variables range over defined and undefined objects): either
all operations, e.g. morphism composition, are total or their axiom system is
inconsistent. The repair for this problem is quite straightforward, however.



Studies on microscopic pedestrian requires large amounts of trajectory data
from real-world pedestrian crowds. Such data collection, if done manually,
needs tremendous effort and is very time consuming. Though many studies have
asserted the possibility of automating this task using video cameras, we found
that only a few have demonstrated good performance in very crowded situations
or from a top-angled view scene. This paper deals with tracking pedestrian
crowd under heavy occlusions from an angular scene. Our automated tracking
system consists of two modules that perform sequentially. The first module
detects moving objects as blobs. The second module is a tracking system. We
employ probability distribution from the detection of each pedestrian and use
Bayesian update to track the next position. The result of such tracking is a
database of pedestrian trajectories over time and space. With certain prior
information, we showed that the system can track a large number of people under
occlusion and clutter scene.



Markov Random Fields (MRFs), a formulation widely used in generative image
modeling, have long been plagued by the lack of expressive power. This issue is
primarily due to the fact that conventional MRFs formulations tend to use
simplistic factors to capture local patterns. In this paper, we move beyond
such limitations, and propose a novel MRF model that uses fully-connected
neurons to express the complex interactions among pixels. Through theoretical
analysis, we reveal an inherent connection between this model and recurrent
neural networks, and thereon derive an approximated feed-forward network that
couples multiple RNNs along opposite directions. This formulation combines the
expressive power of deep neural networks and the cyclic dependency structure of
MRF in a unified model, bringing the modeling capability to a new level. The
feed-forward approximation also allows it to be efficiently learned from data.
Experimental results on a variety of low-level vision tasks show notable
improvement over state-of-the-arts.



In this work we introduce a convolutional neural network (CNN) that jointly
handles low-, mid-, and high-level vision tasks in a unified architecture that
is trained end-to-end. Such a universal network can act like a `swiss knife'
for vision tasks; we call this architecture an UberNet to indicate its
overarching nature.
  We address two main technical challenges that emerge when broadening up the
range of tasks handled by a single CNN: (i) training a deep architecture while
relying on diverse training sets and (ii) training many (potentially unlimited)
tasks with a limited memory budget. Properly addressing these two problems
allows us to train accurate predictors for a host of tasks, without
compromising accuracy.
  Through these advances we train in an end-to-end manner a CNN that
simultaneously addresses (a) boundary detection (b) normal estimation (c)
saliency estimation (d) semantic segmentation (e) human part segmentation (f)
semantic boundary detection, (g) region proposal generation and object
detection. We obtain competitive performance while jointly addressing all of
these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this
system can be found at http://cvn.ecp.fr/ubernet/.



Though deep learning has pushed the boundaries of classification forward, in
recent years hints of the limits of standard classification have begun to
emerge. Problems such as fooling, adding new classes over time, and the need to
retrain learning models only for small changes to the original problem all
point to a potential shortcoming in the classic classification regime, where a
comprehensive a priori knowledge of the possible classes or concepts is
critical. Without such knowledge, classifiers misjudge the limits of their
knowledge and overgeneralization therefore becomes a serious obstacle to
consistent performance. In response to these challenges, this paper extends the
classic regime by reframing classification instead with the assumption that
concepts present in the training set are only a sample of the hypothetical
final set of concepts. To bring learning models into this new paradigm, a novel
elaboration of standard architectures called the competitive overcomplete
output layer (COOL) neural network is introduced. Experiments demonstrate the
effectiveness of COOL by applying it to fooling, separable concept learning,
one-class neural networks, and standard classification benchmarks. The results
suggest that, unlike conventional classifiers, the amount of generalization in
COOL networks can be tuned to match the problem.



Hebbian plasticity is a powerful principle that allows biological brains to
learn from their lifetime experience. By contrast, artificial neural networks
trained with backpropagation generally have fixed connection weights that do
not change once training is complete. While recent methods can endow neural
networks with long-term memories, Hebbian plasticity is currently not amenable
to gradient descent. Here we derive analytical expressions for activity
gradients in neural networks with Hebbian plastic connections. Using these
expressions, we can use backpropagation to train not just the baseline weights
of the connections, but also their plasticity. As a result, the networks "learn
how to learn" in order to solve the problem at hand: the trained networks
automatically perform fast learning of unpredictable environmental features
during their lifetime, expanding the range of solvable problems. We test the
algorithm on various on-line learning tasks, including pattern completion,
one-shot learning, and reversal learning. The algorithm successfully learns how
to learn the relevant associations from one-shot instruction, and fine-tunes
the temporal dynamics of plasticity to allow for continual learning in response
to changing environmental parameters. We conclude that backpropagation of
Hebbian plasticity offers a powerful model for lifelong learning.



Trajectory analysis is essential in many applications. In this paper, we
address the problem of representing motion trajectories in a highly informative
way, and consequently utilize it for analyzing trajectories. Our approach first
leverages the complete information from given trajectories to construct a
thermal transfer field which provides a context-rich way to describe the global
motion pattern in a scene. Then, a 3D tube is derived which depicts an input
trajectory by integrating its surrounding motion patterns contained in the
thermal transfer field. The 3D tube effectively: 1) maintains the movement
information of a trajectory, 2) embeds the complete contextual motion pattern
around a trajectory, 3) visualizes information about a trajectory in a clear
and unified way. We further introduce a droplet-based process. It derives a
droplet vector from a 3D tube, so as to characterize the high-dimensional 3D
tube information in a simple but effective way. Finally, we apply our
tube-and-droplet representation to trajectory analysis applications including
trajectory clustering, trajectory classification & abnormality detection, and
3D action recognition. Experimental comparisons with state-of-the-art
algorithms demonstrate the effectiveness of our approach.



This paper presents a simple end-to-end model for speech recognition,
combining a convolutional network based acoustic model and a graph decoding. It
is trained to output letters, with transcribed speech, without the need for
force alignment of phonemes. We introduce an automatic segmentation criterion
for training from sequence annotation without alignment that is on par with CTC
while being simpler. We show competitive results in word error rate on the
Librispeech corpus with MFCC features, and promising results from raw waveform.



Process mining is a research field focused on the analysis of event data with
the aim of extracting insights in processes. Applying process mining techniques
on data from smart home environments has the potential to provide valuable
insights in (un)healthy habits and to contribute to ambient assisted living
solutions. Finding the right event labels to enable application of process
mining techniques is however far from trivial, as simply using the triggering
sensor as the label for sensor events results in uninformative models that
allow for too much behavior (overgeneralizing). Refinements of sensor level
event labels suggested by domain experts have shown to enable discovery of more
precise and insightful process models. However, there exist no automated
approach to generate refinements of event labels in the context of process
mining. In this paper we propose a framework for automated generation of label
refinements based on the time attribute of events. We show on a case study with
real life smart home event data that behaviorally more specific, and therefore
more insightful, process models can be found by using automatically generated
refined labels in process discovery.



The point of this note is to prove that a language is in the complexity class
PP if and only if the strings of the language encode valid inferences in a
Bayesian network defined using function-free first-order logic with equality.



Causal inference from observational data is a subject of active research and
development in statistics and computer science. Many toolkits have been
developed for this purpose that depends on statistical software. However, these
toolkits do not scale to large datasets. In this paper we describe a suite of
techniques for expressing causal inference tasks from observational data in
SQL. This suite supports the state-of-the-art methods for causal inference and
run at scale within a database engine. In addition, we introduce several
optimization techniques that significantly speedup causal inference, both in
the online and offline setting. We evaluate the quality and performance of our
techniques by experiments of real datasets.



We present a computable algorithm that assigns probabilities to every logical
statement in a given formal language, and refines those probabilities over
time. For instance, if the language is Peano arithmetic, it assigns
probabilities to all arithmetical statements, including claims about the twin
prime conjecture, the outputs of long-running computations, and its own
probabilities. We show that our algorithm, an instance of what we call a
logical inductor, satisfies a number of intuitive desiderata, including: (1) it
learns to predict patterns of truth and falsehood in logical statements, often
long before having the resources to evaluate the statements, so long as the
patterns can be written down in polynomial time; (2) it learns to use
appropriate statistical summaries to predict sequences of statements whose
truth values appear pseudorandom; and (3) it learns to have accurate beliefs
about its own current beliefs, in a manner that avoids the standard paradoxes
of self-reference. For example, if a given computer program only ever produces
outputs in a certain range, a logical inductor learns this fact in a timely
manner; and if late digits in the decimal expansion of $\pi$ are difficult to
predict, then a logical inductor learns to assign $\approx 10\%$ probability to
"the $n$th digit of $\pi$ is a 7" for large $n$. Logical inductors also learn
to trust their future beliefs more than their current beliefs, and their
beliefs are coherent in the limit (whenever $\phi \implies \psi$,
$\mathbb{P}_\infty(\phi) \le \mathbb{P}_\infty(\psi)$, and so on); and logical
inductors strictly dominate the universal semimeasure in the limit.
  These properties and many others all follow from a single logical induction
criterion, which is motivated by a series of stock trading analogies. Roughly
speaking, each logical sentence $\phi$ is associated with a stock that is worth
\$1 per share if [...]



Efforts at understanding the computational processes in the brain have met
with limited success, despite their importance and potential uses in building
intelligent machines. We propose a simple new model which draws on recent
findings in Neuroscience and the Applied Mathematics of interacting Dynamical
Systems. The Feynman Machine is a Universal Computer for Dynamical Systems,
analogous to the Turing Machine for symbolic computing, but with several
important differences. We demonstrate that networks and hierarchies of simple
interacting Dynamical Systems, each adaptively learning to forecast its
evolution, are capable of automatically building sensorimotor models of the
external and internal world. We identify such networks in mammalian neocortex,
and show how existing theories of cortical computation combine with our model
to explain the power and flexibility of mammalian intelligence. These findings
lead directly to new architectures for machine intelligence. A suite of
software implementations has been built based on these principles, and applied
to a number of spatiotemporal learning tasks.



Statistical characteristics of network traffic have attracted a significant
amount of research for automated network intrusion detection, some of which
looked at applications of natural statistical laws such as Zipf's law,
Benford's law and the Pareto distribution. In this paper, we present the
application of Benford's law to a new network flow metric "flow size
difference", which have not been studied before by other researchers, to build
an unsupervised flow-based intrusion detection system (IDS). The method was
inspired by our observation on a large number of TCP flow datasets where normal
flows tend to follow Benford's law closely but malicious flows tend to deviate
significantly from it. The proposed IDS is unsupervised, so it can be easily
deployed without any training. It has two simple operational parameters with a
clear semantic meaning, allowing the IDS operator to set and adapt their values
intuitively to adjust the overall performance of the IDS. We tested the
proposed IDS on two (one closed and one public) datasets, and proved its
efficiency in terms of AUC (area under the ROC curve). Our work showed the
"flow size difference" has a great potential to improve the performance of any
flow-based network IDSs.



Bayesian methods for machine learning have been widely investigated, yielding
principled methods for incorporating prior information into inference
algorithms. In this survey, we provide an in-depth review of the role of
Bayesian methods for the reinforcement learning (RL) paradigm. The major
incentives for incorporating Bayesian reasoning in RL are: 1) it provides an
elegant approach to action-selection (exploration/exploitation) as a function
of the uncertainty in learning; and 2) it provides a machinery to incorporate
prior knowledge into the algorithms. We first discuss models and methods for
Bayesian inference in the simple single-step Bandit model. We then review the
extensive recent literature on Bayesian methods for model-based RL, where prior
information can be expressed on the parameters of the Markov model. We also
present Bayesian methods for model-free RL, where priors are expressed over the
value function or policy class. The objective of the paper is to provide a
comprehensive survey on Bayesian RL algorithms and their theoretical and
empirical properties.



Relational learning deals with data that are characterized by relational
structures. An important task is collective classification, which is to jointly
classify networked objects. While it holds a great promise to produce a better
accuracy than non-collective classifiers, collective classification is
computational challenging and has not leveraged on the recent breakthroughs of
deep learning. We present Column Network (CLN), a novel deep learning model for
collective classification in multi-relational domains. CLN has many desirable
theoretical properties: (i) it encodes multi-relations between any two
instances; (ii) it is deep and compact, allowing complex functions to be
approximated at the network level with a small set of free parameters; (iii)
local and relational features are learned simultaneously; (iv) long-range,
higher-order dependencies between instances are supported naturally; and (v)
crucially, learning and inference are efficient, linear in the size of the
network and the number of relations. We evaluate CLN on multiple real-world
applications: (a) delay prediction in software projects, (b) PubMed Diabetes
publication classification and (c) film genre classification. In all
applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.



In this article we propose a method to refine the clustering results obtained
with the nonnegative matrix factorization (NMF) technique, imposing consistency
constraints on the final labeling of the data. The research community focused
its effort on the initialization and on the optimization part of this method,
without paying attention to the final cluster assignments. We propose a game
theoretic framework in which each object to be clustered is represented as a
player, which has to choose its cluster membership. The information obtained
with NMF is used to initialize the strategy space of the players and a weighted
graph is used to model the interactions among the players. These interactions
allow the players to choose a cluster which is coherent with the clusters
chosen by similar players, a property which is not guaranteed by NMF, since it
produces a soft clustering of the data. The results on common benchmarks show
that our model is able to improve the performances of many NMF formulations.



Analyses of text corpora over time can reveal trends in beliefs, interest,
and sentiment about a topic. We focus on views expressed about artificial
intelligence (AI) in the New York Times over a 30-year period. General
interest, awareness, and discussion about AI has waxed and waned since the
field was founded in 1956. We present a set of measures that captures levels of
engagement, measures of pessimism and optimism, the prevalence of specific
hopes and concerns, and topics that are linked to discussions about AI over
decades. We find that discussion of AI has increased sharply since 2009, and
that these discussions have been consistently more optimistic than pessimistic.
However, when we examine specific concerns, we find that worries of loss of
control of AI, ethical concerns for AI, and the negative impact of AI on work
have grown in recent years. We also find that hopes for AI in healthcare and
education have increased over time.



A Bayesian agent acting in a multi-agent environment learns to predict the
other agents' policies if its prior assigns positive probability to them (in
other words, its prior contains a \emph{grain of truth}). Finding a reasonably
large class of policies that contains the Bayes-optimal policies with respect
to this class is known as the \emph{grain of truth problem}. Only small classes
are known to have a grain of truth and the literature contains several related
impossibility results. In this paper we present a formal and general solution
to the full grain of truth problem: we construct a class of policies that
contains all computable policies as well as Bayes-optimal policies for every
lower semicomputable prior over the class. When the environment is unknown,
Bayes-optimal agents may fail to act optimally even asymptotically. However,
agents based on Thompson sampling converge to play {\epsilon}-Nash equilibria
in arbitrary unknown computable multi-agent environments. While these results
are purely theoretical, we show that they can be computationally approximated
arbitrarily closely.



Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA
Challenges are an established and important way to drive scientific progress.
They make research comparable on a well-defined benchmark with equal test
conditions for all participants. However, such challenge events occur only
occasionally, are limited to a small number of contestants, and the test
conditions are very difficult to replicate after the main event. We present a
new physical benchmark challenge for robotic picking: the ACRV Picking
Benchmark (APB). Designed to be reproducible, it consists of a set of 42 common
objects, a widely available shelf, and exact guidelines for object arrangement
using stencils. A well-defined evaluation protocol enables the comparison of
\emph{complete} robotic systems -- including perception and manipulation --
instead of sub-systems only. Our paper also describes and reports results
achieved by an open baseline system based on a Baxter robot.



This paper proposes to improve visual question answering (VQA) with
structured representations of both scene contents and questions. A key
challenge in VQA is to require joint reasoning over the visual and text
domains. The predominant CNN/LSTM-based approach to VQA is limited by
monolithic vector representations that largely ignore structure in the scene
and in the form of the question. CNN feature vectors cannot effectively capture
situations as simple as multiple object instances, and LSTMs process questions
as series of words, which does not reflect the true complexity of language
structure. We instead propose to build graphs over the scene objects and over
the question words, and we describe a deep neural network that exploits the
structure in these representations. This shows significant benefit over the
sequential processing of LSTMs. The overall efficacy of our approach is
demonstrated by significant improvements over the state-of-the-art, from 71.2%
to 74.4% in accuracy on the "abstract scenes" multiple-choice benchmark, and
from 34.7% to 39.1% in accuracy over pairs of "balanced" scenes, i.e. images
with fine-grained differences and opposite yes/no answers to a same question.



Biosurveillance, a relatively young field, has recently increased in
importance because of its relevance to national security and global health.
Databases and tools describing particular subsets of disease are becoming
increasingly common in the field. However, a common method to describe those
diseases is lacking. Here, we present the Anthology of Biosurveillance Diseases
(ABD), an ontology of infectious diseases of biosurveillance relevance.



Understanding the nature of dark energy, the mysterious force driving the
accelerated expansion of the Universe, is a major challenge of modern
cosmology. The next generation of cosmological surveys, specifically designed
to address this issue, rely on accurate measurements of the apparent shapes of
distant galaxies. However, shape measurement methods suffer from various
unavoidable biases and therefore will rely on a precise calibration to meet the
accuracy requirements of the science analysis. This calibration process remains
an open challenge as it requires large sets of high quality galaxy images. To
this end, we study the application of deep conditional generative models in
generating realistic galaxy images. In particular we consider variations on
conditional variational autoencoder and introduce a new adversarial objective
for training of conditional generative networks. Our results suggest a reliable
alternative to the acquisition of expensive high quality observations for
generating the calibration data needed by the next generation of cosmological
surveys.



The Gaussian mixture model is a classic technique for clustering and data
modeling that is used in numerous applications. With the rise of big data,
there is a need for parameter estimation techniques that can handle streaming
data and distribute the computation over several processors. While online
variants of the Expectation Maximization (EM) algorithm exist, their data
efficiency is reduced by a stochastic approximation of the E-step and it is not
clear how to distribute the computation over multiple processors. We propose a
Bayesian learning technique that lends itself naturally to online and
distributed computation. Since the Bayesian posterior is not tractable, we
project it onto a family of tractable distributions after each observation by
matching a set of sufficient moments. This Bayesian moment matching technique
compares favorably to online EM in terms of time and accuracy on a set of data
modeling benchmarks.



Recent progress in randomized motion planners has led to the development of a
new class of sampling-based algorithms that provide asymptotic optimality
guarantees, notably the RRT* and the PRM* algorithms. Careful analysis reveals
that the so-called "rewiring" step in these algorithms can be interpreted as a
local policy iteration (PI) step (i.e., a local policy evaluation step followed
by a local policy improvement step) so that asymptotically, as the number of
samples tend to infinity, both algorithms converge to the optimal path almost
surely (with probability 1). Policy iteration, along with value iteration (VI)
are common methods for solving dynamic programming (DP) problems. Based on this
observation, recently, the RRT$^{\#}$ algorithm has been proposed, which
performs, during each iteration, Bellman updates (aka "backups") on those
vertices of the graph that have the potential of being part of the optimal path
(i.e., the "promising" vertices). The RRT$^{\#}$ algorithm thus utilizes
dynamic programming ideas and implements them incrementally on randomly
generated graphs to obtain high quality solutions. In this work, and based on
this key insight, we explore a different class of dynamic programming
algorithms for solving shortest-path problems on random graphs generated by
iterative sampling methods. These class of algorithms utilize policy iteration
instead of value iteration, and thus are better suited for massive
parallelization. Contrary to the RRT* algorithm, the policy improvement during
the rewiring step is not performed only locally but rather on a set of vertices
that are classified as "promising" during the current iteration. This tends to
speed-up the whole process. The resulting algorithm, aptly named Policy
Iteration-RRT$^{\#}$ (PI-RRT$^{\#}$) is the first of a new class of DP-inspired
algorithms for randomized motion planning that utilize PI methods.



The ability to automatically recognize a person's behavioral context can
contribute to health monitoring, aging care and many other domains. Validating
context recognition in-the-wild is crucial to promote practical applications
that work in real-life settings. We collected over 300k minutes of sensor data
with context labels from 60 subjects. Unlike previous studies, our subjects
used their own personal phone, in any way that was convenient to them, and
engaged in their routine in their natural environments. Unscripted behavior and
unconstrained phone usage resulted in situations that are harder to recognize.
We demonstrate how fusion of multi-modal sensors is important for resolving
such cases. We present a baseline system, and encourage researchers to use our
public dataset to compare methods and improve context recognition in-the-wild.



When we say "I know why he was late", we know not only the fact that he was
late, but also an explanation of this fact. We propose a logical framework of
"knowing why" inspired by the existing formal studies on why-questions,
scientific explanation, and justification logic. We introduce the Ky_i operator
into the language of epistemic logic to express "agent i knows why phi" and
propose a Kripke-style semantics of such expressions in terms of knowing an
explanation of phi. We obtain two sound and complete axiomatizations w.r.t. two
different model classes depending on different assumptions about introspection.



High-speed, low-latency obstacle avoidance that is insensitive to sensor
noise is essential for enabling multiple decentralized robots to function
reliably in cluttered and dynamic environments. While other distributed
multi-agent collision avoidance systems exist, these systems require online
geometric optimization where tedious parameter tuning and perfect sensing are
necessary.
  We present a novel end-to-end framework to generate reactive collision
avoidance policy for efficient distributed multi-agent navigation. Our method
formulates an agent's navigation strategy as a deep neural network mapping from
the observed noisy sensor measurements to the agent's steering commands in
terms of movement velocity. We train the network on a large number of frames of
collision avoidance data collected by repeatedly running a multi-agent
simulator with different parameter settings. We validate the learned deep
neural network policy in a set of simulated and real scenarios with noisy
measurements and demonstrate that our method is able to generate a robust
navigation strategy that is insensitive to imperfect sensing and works reliably
in all situations. We also show that our method can be well generalized to
scenarios that do not appear in our training data, including scenes with static
obstacles and agents with different sizes. Videos are available at
https://sites.google.com/view/deepmaca.



To solve hard problems, AI relies on a variety of disciplines such as logic,
probabilistic reasoning, machine learning and mathematical programming.
Although it is widely accepted that solving real-world problems requires an
integration amongst these, contemporary representation methodologies offer
little support for this.
  In an attempt to alleviate this situation, we introduce a new declarative
programming framework that provides abstractions of well-known problems such as
SAT, Bayesian inference, generative models, and convex optimization. The
semantics of programs is defined in terms of first-order structures with
semiring labels, which allows us to freely combine and integrate problems from
different AI disciplines.



In this paper, we deal with two challenges for measuring the similarity of
the subject identities in practical video-based face recognition - the
variation of the head pose in uncontrolled environments and the computational
expense of processing videos. Since the frame-wise feature mean is unable to
characterize the pose diversity among frames, we define and preserve the
overall pose diversity and closeness in a video. Then, identity will be the
only source of variation across videos since the pose varies even within a
single video. Instead of simply using all the frames, we select those faces
whose pose point is closest to the centroid of the K-means cluster containing
that pose point. Then, we represent a video as a bag of frame-wise deep face
features while the number of features has been reduced from hundreds to K.
Since the video representation can well represent the identity, now we measure
the subject similarity between two videos as the max correlation among all
possible pairs in the two bags of features. On the official 5,000 video-pairs
of the YouTube Face dataset for face verification, our algorithm achieves a
comparable performance with VGG-face that averages over deep features of all
frames. Other vision tasks can also benefit from the generic idea of employing
geometric cues to improve the descriptiveness of deep features.



We identify the main actors in the Isabelle and Coq communities and describe
how they affect and influence their peers. This work explores selected
foundations of social networking analysis that we expect to be useful in the
context of the ProofPeer project, which is developing a new model for
interactive theorem proving based on collaboration and social interactions.



Recent advances in biosensors technology and mobile electroencephalographic
(EEG) interfaces have opened new application fields for cognitive monitoring. A
computable biomarker for the assessment of spontaneous aesthetic brain
responses during music listening is introduced here. It derives from
well-established measures of cross-frequency coupling (CFC) and quantifies the
music-induced alterations in the dynamic relationships between brain rhythms.
During a stage of exploratory analysis, and using the signals from a suitably
designed experiment, we established the biomarker, which acts on brain
activations recorded over the left prefrontal cortex and focuses on the
functional coupling between high-beta and low-gamma oscillations. Based on data
from an additional experimental paradigm, we validated the introduced biomarker
and showed its relevance for expressing the subjective aesthetic appreciation
of a piece of music. Our approach resulted in an affordable tool that can
promote human-machine interaction and, by serving as a personalized music
annotation strategy, can be potentially integrated into modern flexible music
recommendation systems.
  Keywords: Cross-frequency coupling; Human-computer interaction;
Brain-computer interface



In this paper we describe approaches for discovering acoustic concepts and
relations in text. The first major goal is to be able to identify text phrases
which contain a notion of audibility and can be termed as a sound or an
acoustic concept. We also propose a method to define an acoustic scene through
a set of sound concepts. We use pattern matching and parts of speech tags to
generate sound concepts from large scale text corpora. We use dependency
parsing and LSTM recurrent neural network to predict a set of sound concepts
for a given acoustic scene. These methods are not only helpful in creating an
acoustic knowledge base but in the future can also directly help acoustic event
and scene detection research.



Nowadays, several crowdsourcing projects exploit social choice methods for
computing an aggregate ranking of alternatives given individual rankings
provided by workers. Motivated by such systems, we consider a setting where
each worker is asked to rank a fixed (small) number of alternatives and, then,
a positional scoring rule is used to compute the aggregate ranking. Among the
apparently infinite such rules, what is the best one to use? To answer this
question, we assume that we have partial access to an underlying true ranking.
Then, the important optimization problem to be solved is to compute the
positional scoring rule whose outcome, when applied to the profile of
individual rankings, is as close as possible to the part of the underlying true
ranking we know. We study this fundamental problem from a theoretical viewpoint
and present positive and negative complexity results and, furthermore,
complement our theoretical findings with experiments on real-world and
synthetic data.



The goal of this thesis is to investigate the potential of predictive
modelling for football injuries. This work was conducted in close collaboration
with Tottenham Hotspurs FC (THFC), the PGA European tour and the participation
of Wolverhampton Wanderers (WW).
  Three investigations were conducted:
  1. Predicting the recovery time of football injuries using the UEFA injury
recordings: The UEFA recordings is a common standard for recording injuries in
professional football. For this investigation, three datasets of UEFA injury
recordings were available. Different machine learning algorithms were used in
order to build a predictive model. The performance of the machine learning
models is then improved by using feature selection conducted through
correlation-based subset feature selection and random forests.
  2. Predicting injuries in professional football using exposure records: The
relationship between exposure (in training hours and match hours) in
professional football athletes and injury incidence was studied. A common
problem in football is understanding how the training schedule of an athlete
can affect the chance of him getting injured. The task was to predict the
number of days a player can train before he gets injured.
  3. Predicting intrinsic injury incidence using in-training GPS measurements:
A significant percentage of football injuries can be attributed to overtraining
and fatigue. GPS data collected during training sessions might provide
indicators of fatigue, or might be used to detect very intense training
sessions which can lead to overtraining. This research used GPS data gathered
during training sessions of the first team of THFC, in order to predict whether
an injury would take place during a week.



Mixture models and topic models generate each observation from a single
cluster, but standard variational posteriors for each observation assign
positive probability to all possible clusters. This requires dense storage and
runtime costs that scale with the total number of clusters, even though
typically only a few clusters have significant posterior mass for any data
point. We propose a constrained family of sparse variational distributions that
allow at most $L$ non-zero entries, where the tunable threshold $L$ trades off
speed for accuracy. Previous sparse approximations have used hard assignments
($L=1$), but we find that moderate values of $L>1$ provide superior
performance. Our approach easily integrates with stochastic or incremental
optimization algorithms to scale to millions of examples. Experiments training
mixture models of image patches and topic models for news articles show that
our approach produces better-quality models in far less time than baseline
methods.



Learning based on networks of real neurons, and by extension biologically
inspired models of neural networks, has yet to find general learning rules
leading to widespread applications. In this paper, we argue for the existence
of a principle allowing to steer the dynamics of a biologically inspired neural
network. Using carefully timed external stimulation, the network can be driven
towards a desired dynamical state. We term this principle "Learning by
Stimulation Avoidance" (LSA). We demonstrate through simulation that the
minimal sufficient conditions leading to LSA in artificial networks are also
sufficient to reproduce learning results similar to those obtained in
biological neurons by Shahaf and Marom [1]. We examine the mechanism's basic
dynamics in a reduced network, and demonstrate how it scales up to a network of
100 neurons. We show that LSA has a higher explanatory power than existing
hypotheses about the response of biological neural networks to external
simulation, and can be used as a learning rule for an embodied application:
learning of wall avoidance by a simulated robot. The surge in popularity of
artificial neural networks is mostly directed to disembodied models of neurons
with biologically irrelevant dynamics: to the authors' knowledge, this is the
first work demonstrating sensory-motor learning with random spiking networks
through pure Hebbian learning.



[Background]: Systematic Literature Review (SLR) has become an important
software engineering research method but costs tremendous efforts. [Aim]: This
paper proposes an approach to leverage on empirically evolved ontology to
support automating key SLR activities. [Method]: First, we propose an ontology,
SLRONT, built on SLR experiences and best practices as a groundwork to capture
common terminologies and their relationships during SLR processes; second, we
present an extended version of SLRONT, the COSONT and instantiate it with the
knowledge and concepts extracted from structured abstracts. Case studies
illustrate the details of applying it for supporting SLR steps. [Results]:
Results show that through using COSONT, we acquire the same conclusion compared
with sheer manual works, but the efforts involved is significantly reduced.
[Conclusions]: The approach of using ontology could effectively and efficiently
support the conducting of systematic literature review.



Neural Machine Translation (NMT) is an end-to-end learning approach for
automated translation, with the potential to overcome many of the weaknesses of
conventional phrase-based translation systems. Unfortunately, NMT systems are
known to be computationally expensive both in training and in translation
inference. Also, most NMT systems have difficulty with rare words. These issues
have hindered NMT's use in practical deployments and services, where both
accuracy and speed are essential. In this work, we present GNMT, Google's
Neural Machine Translation system, which attempts to address many of these
issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder
layers using attention and residual connections. To improve parallelism and
therefore decrease training time, our attention mechanism connects the bottom
layer of the decoder to the top layer of the encoder. To accelerate the final
translation speed, we employ low-precision arithmetic during inference
computations. To improve handling of rare words, we divide words into a limited
set of common sub-word units ("wordpieces") for both input and output. This
method provides a good balance between the flexibility of "character"-delimited
models and the efficiency of "word"-delimited models, naturally handles
translation of rare words, and ultimately improves the overall accuracy of the
system. Our beam search technique employs a length-normalization procedure and
uses a coverage penalty, which encourages generation of an output sentence that
is most likely to cover all the words in the source sentence. On the WMT'14
English-to-French and English-to-German benchmarks, GNMT achieves competitive
results to state-of-the-art. Using a human side-by-side evaluation on a set of
isolated simple sentences, it reduces translation errors by an average of 60%
compared to Google's phrase-based production system.



We introduce an online neural sequence to sequence model that learns to
alternate between encoding and decoding segments of the input as it is read. By
independently tracking the encoding and decoding representations our algorithm
permits exact polynomial marginalization of the latent segmentation during
training, and during decoding beam search is employed to find the best
alignment path together with the predicted output sequence. Our model tackles
the bottleneck of vanilla encoder-decoders that have to read and memorize the
entire input sequence in their fixed-length hidden states before producing any
output. It is different from previous attentive models in that, instead of
treating the attention weights as output of a deterministic function, our model
assigns attention weights to a sequential latent variable which can be
marginalized out and permits online generation. Experiments on abstractive
sentence summarization and morphological inflection show significant
performance gains over the baseline encoder-decoders.



Decision making is an important component in a speaker verification system.
For the conventional GMM-UBM architecture, the decision is usually conducted
based on the log likelihood ratio of the test utterance against the GMM of the
claimed speaker and the UBM. This single-score decision is simple but tends to
be sensitive to the complex variations in speech signals (e.g. text content,
channel, speaking style, etc.). In this paper, we propose a decision making
approach based on multiple scores derived from a set of cohort GMMs (cohort
scores). Importantly, these cohort scores are not simply averaged as in
conventional cohort methods; instead, we employ a powerful discriminative model
as the decision maker. Experimental results show that the proposed method
delivers substantial performance improvement over the baseline system,
especially when a deep neural network (DNN) is used as the decision maker, and
the DNN input involves some statistical features derived from the cohort
scores.



PLDA is a popular normalization approach for the i-vector model, and it has
delivered state-of-the-art performance in speaker verification. However, PLDA
training requires a large amount of labelled development data, which is highly
expensive in most cases. We present a cheap PLDA training approach, which
assumes that speakers in the same session can be easily separated, and speakers
in different sessions are simply different. This results in `weak labels' which
are not fully accurate but cheap, leading to a weak PLDA training.
  Our experimental results on real-life large-scale telephony customer service
achieves demonstrated that the weak training can offer good performance when
human-labelled data are limited. More interestingly, the weak training can be
employed as a discriminative adaptation approach, which is more efficient than
the prevailing unsupervised method when human-labelled data are insufficient.



Robust principal component analysis (RPCA) has been widely used for
recovering low-rank matrices in many data mining and machine learning problems.
It separates a data matrix into a low-rank part and a sparse part. The convex
approach has been well studied in the literature. However, state-of-the-art
algorithms for the convex approach usually have relatively high complexity due
to the need of solving (partial) singular value decompositions of large
matrices. A non-convex approach, AltProj, has also been proposed with lighter
complexity and better scalability. Given the true rank $r$ of the underlying
low rank matrix, AltProj has a complexity of $O(r^2dn)$, where $d\times n$ is
the size of data matrix. In this paper, we propose a novel factorization-based
model of RPCA, which has a complexity of $O(kdn)$, where $k$ is an upper bound
of the true rank. Our method does not need the precise value of the true rank.
From extensive experiments, we observe that AltProj can work only when $r$ is
precisely known in advance; however, when the needed rank parameter $r$ is
specified to a value different from the true rank, AltProj cannot fully
separate the two parts while our method succeeds. Even when both work, our
method is about 4 times faster than AltProj. Our method can be used as a
light-weight, scalable tool for RPCA in the absence of the precise value of the
true rank.



Recently, end-to-end memory networks have shown promising results on Question
Answering task, which encode the past facts into an explicit memory and perform
reasoning ability by making multiple computational steps on the memory.
However, memory networks conduct the reasoning on sentence-level memory to
output coarse semantic vectors and do not further take any attention mechanism
to focus on words, which may lead to the model lose some detail information,
especially when the answers are rare or unknown words. In this paper, we
propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the
past facts into sentence-level memory and word-level memory respectively. Then,
(k)-max pooling is exploited following reasoning module on the sentence-level
memory to sample the (k) most relevant sentences to a question and feed these
sentences into attention mechanism on the word-level memory to focus the words
in the selected sentences. Finally, the prediction is jointly learned over the
outputs of the sentence-level reasoning module and the word-level attention
mechanism. The experimental results demonstrate that our approach successfully
conducts answer selection on unknown words and achieves a better performance
than memory networks.



Model predictive control (MPC) is a popular control method that has proved
effective for robotics, among other fields. MPC performs re-planning at every
time step. Re-planning is done with a limited horizon per computational and
real-time constraints and often also for robustness to potential model errors.
However, the limited horizon leads to suboptimal performance. In this work, we
consider the iterative learning setting, where the same task can be repeated
several times, and propose a policy improvement scheme for MPC. The main idea
is that between executions we can, offline, run MPC with a longer horizon,
resulting in a hindsight plan. To bring the next real-world execution closer to
the hindsight plan, our approach learns to re-shape the original cost function
with the goal of satisfying the following property: short horizon planning (as
realistic during real executions) with respect to the shaped cost should result
in mimicking the hindsight plan. This effectively consolidates long-term
reasoning into the short-horizon planning. We empirically evaluate our approach
in contact-rich manipulation tasks both in simulated and real environments,
such as peg insertion by a real PR2 robot.



We present a novel semi-supervised approach for sequence transduction and
apply it to semantic parsing. The unsupervised component is based on a
generative model in which latent sentences generate the unpaired logical forms.
We apply this method to a number of semantic parsing tasks focusing on domains
with limited access to labelled training data and extend those datasets with
synthetically generated logical forms.



This paper presents an end-to-end approach for tracking static and dynamic
objects for an autonomous vehicle driving through crowded urban environments.
Unlike traditional approaches to tracking, this method is learned end-to-end,
and is able to directly predict a full unoccluded occupancy grid map from raw
laser input data. Inspired by the recently presented DeepTracking approach
[Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the
temporal evolution of the state of the environment, and propose to use Spatial
Transformer modules to exploit estimates of the egomotion of the vehicle. Our
results demonstrate the ability to track a range of objects, including cars,
buses, pedestrians, and cyclists through occlusion, from both moving and
stationary platforms, using a single learned model. Experimental results
demonstrate that the model can also predict the future states of objects from
current inputs, with greater accuracy than previous work.



Understanding, predicting, and generating object motions and transformations
is a core problem in artificial intelligence. Modeling sequences of evolving
images may provide better representations and models of motion and may
ultimately be used for forecasting, simulation, or video generation.
Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in
complex patterns and one needs to infer the underlying pattern sequence and
generate the next image in the sequence. For this, we develop a novel
Contextual Generative Adversarial Network based on Recurrent Neural Networks
(Context-RNN-GANs), where both the generator and the discriminator modules are
based on contextual history (modeled as RNNs) and the adversarial discriminator
guides the generator to produce realistic images for the particular time step
in the image sequence. We evaluate the Context-RNN-GAN model (and its variants)
on a novel dataset of Diagrammatic Abstract Reasoning, where it performs
competitively with 10th-grade human performance but there is still scope for
interesting improvements as compared to college-grade human performance. We
also evaluate our model on a standard video next-frame prediction task,
achieving improved performance over comparable state-of-the-art.



Gaussian state space models have been used for decades as generative models
of sequential data. They admit an intuitive probabilistic interpretation, have
a simple functional form, and enjoy widespread adoption. We introduce a unified
algorithm to efficiently learn a broad class of linear and non-linear state
space models, including variants where the emission and transition
distributions are modeled by deep neural networks. Our learning algorithm
simultaneously learns a compiled inference network and the generative model,
leveraging a structured variational approximation parameterized by recurrent
neural networks to mimic the posterior distribution. We apply the learning
algorithm to both synthetic and real-world datasets, demonstrating its
scalability and versatility. We find that using the structured approximation to
the posterior results in models with significantly higher held-out likelihood.



In this paper we propose cross-modal convolutional neural networks (X-CNNs),
a novel biologically inspired type of CNN architectures, treating gradient
descent-specialised CNNs as individual units of processing in a larger-scale
network topology, while allowing for unconstrained information flow and/or
weight sharing between analogous hidden layers of the network---thus
generalising the already well-established concept of neural network ensembles
(where information typically may flow only between the output layers of the
individual networks). The constituent networks are individually designed to
learn the output function on their own subset of the input data, after which
cross-connections between them are introduced after each pooling operation to
periodically allow for information exchange between them. This injection of
knowledge into a model (by prior partition of the input data through domain
knowledge or unsupervised methods) is expected to yield greatest returns in
sparse data environments, which are typically less suitable for training CNNs.
For evaluation purposes, we have compared a standard four-layer CNN as well as
a sophisticated FitNet4 architecture against their cross-modal variants on the
CIFAR-10 and CIFAR-100 datasets with differing percentages of the training data
being removed, and find that at lower levels of data availability, the X-CNNs
significantly outperform their baselines (typically providing a 2--6% benefit,
depending on the dataset size and whether data augmentation is used), while
still maintaining an edge on all of the full dataset tests.



Convolutional networks have marked their place over the last few years as the
best performing model for various visual tasks. They are, however, most suited
for supervised learning from large amounts of labeled data. Previous attempts
have been made to use unlabeled data to improve model performance by applying
unsupervised techniques. These attempts require different architectures and
training methods. In this work we present a novel approach for unsupervised
training of Convolutional networks that is based on contrasting between spatial
regions within images. This criterion can be employed within conventional
neural networks and trained using standard techniques such as SGD and
back-propagation, thus complementing supervised methods.



Bayesian optimization has become a fundamental global optimization algorithm
in many problems where sample efficiency is of paramount importance. Recently,
there has been proposed a large number of new applications in fields such as
robotics, machine learning, experimental design, simulation, etc. In this
paper, we focus on several problems that appear in robotics and autonomous
systems: algorithm tuning, automatic control and intelligent design. All those
problems can be mapped to global optimization problems. However, they become
hard optimization problems. Bayesian optimization internally uses a
probabilistic surrogate model (e.g.: Gaussian process) to learn from the
process and reduce the number of samples required. In order to generalize to
unknown functions in a black-box fashion, the common assumption is that the
underlying function can be modeled with a stationary process. Nonstationary
Gaussian process regression cannot generalize easily and it typically requires
prior knowledge of the function. Some works have designed techniques to
generalize Bayesian optimization to nonstationary functions in an indirect way,
but using techniques originally designed for regression, where the objective is
to improve the quality of the surrogate model everywhere. Instead optimization
should focus on improving the surrogate model near the optimum. In this paper,
we present a novel kernel function specially designed for Bayesian
optimization, that allows nonstationary behavior of the surrogate model in an
adaptive local region. In our experiments, we found that this new kernel
results in an improved local search (exploitation), without penalizing the
global search (exploration). We provide results in well-known benchmarks and
real applications. The new method outperforms the state of the art in Bayesian
optimization both in stationary and nonstationary problems.



Reinforcement learning holds the promise of enabling autonomous robots to
learn large repertoires of behavioral skills with minimal human intervention.
However, robotic applications of reinforcement learning often compromise the
autonomy of the learning process in favor of achieving training times that are
practical for real physical systems. This typically involves introducing
hand-engineered policy representations and human-supplied demonstrations. Deep
reinforcement learning alleviates this limitation by training general-purpose
neural network policies, but applications of direct deep reinforcement learning
algorithms have so far been restricted to simulated settings and relatively
simple tasks, due to their apparent high sample complexity. In this paper, we
demonstrate that a recent deep reinforcement learning algorithm based on
off-policy training of deep Q-functions can scale to complex 3D manipulation
tasks and can learn deep neural network policies efficiently enough to train on
real physical robots. We demonstrate that the training times can be further
reduced by parallelizing the algorithm across multiple robots which pool their
policy updates asynchronously. Our experimental evaluation shows that our
method can learn a variety of 3D manipulation skills in simulation and a
complex door opening skill on real robots without any prior demonstrations or
manually designed representations.



In principle, reinforcement learning and policy search methods can enable
robots to learn highly complex and general skills that may allow them to
function amid the complexity and diversity of the real world. However, training
a policy that generalizes well across a wide range of real-world conditions
requires far greater quantity and diversity of experience than is practical to
collect with a single robot. Fortunately, it is possible for multiple robots to
share their experience with one another, and thereby, learn a policy
collectively. In this work, we explore distributed and asynchronous policy
learning as a means to achieve generalization and improved training times on
challenging, real-world manipulation tasks. We propose a distributed and
asynchronous version of Guided Policy Search and use it to demonstrate
collective policy learning on a vision-based door opening task using four
robots. We show that it achieves better generalization, utilization, and
training times than the single robot alternative.



A key challenge in scaling up robot learning to many skills and environments
is removing the need for human supervision, so that robots can collect their
own data and improve their own performance without being limited by the cost of
requesting human feedback. Model-based reinforcement learning holds the promise
of enabling an agent to learn to predict the effects of its actions, which
could provide flexible predictive models for a wide range of tasks and
environments, without detailed human supervision. We develop a method for
combining deep action-conditioned video prediction models with model-predictive
control that uses entirely unlabeled training data. Our approach does not
require a calibrated camera, an instrumented training set-up, nor precise
sensing and actuation. Our results show that our method enables a real robot to
perform nonprehensile manipulation -- pushing objects -- and can handle novel
objects not seen during training.



Networks represent relationships between entities in many complex systems,
spanning from online social interactions to biological cell development and
brain connectivity. In many cases, relationships between entities are
unambiguously known: are two users 'friends' in a social network? Do two
researchers collaborate on a published paper? Do two road segments in a
transportation system intersect? These are directly observable in the system in
question. In most cases, relationship between nodes are not directly observable
and must be inferred: does one gene regulate the expression of another? Do two
animals who physically co-locate have a social bond? Who infected whom in a
disease outbreak in a population?
  Existing approaches for inferring networks from data are found across many
application domains and use specialized knowledge to infer and measure the
quality of inferred network for a specific task or hypothesis. However, current
research lacks a rigorous methodology which employs standard statistical
validation on inferred models. In this survey, we examine (1) how network
representations are constructed from underlying data, (2) the variety of
questions and tasks on these representations over several domains, and (3)
validation strategies for measuring the inferred network's capability of
answering questions on the system of interest.



Many fields are now snowed under with an avalanche of data, which raises
considerable challenges for computer scientists. Meanwhile, robotics (among
other fields) can often only use a few dozen data points because acquiring them
involves a process that is expensive or time-consuming. How can an algorithm
learn with only a few data points?



There is a practically unlimited amount of natural language data available.
Still, recent work in text comprehension has focused on datasets which are
small relative to current computing possibilities. This article is making a
case for the community to move to larger data and as a step in that direction
it is proposing the BookTest, a new dataset similar to the popular Children's
Book Test (CBT), however more than 60 times larger. We show that training on
the new data improves the accuracy of our Attention-Sum Reader model on the
original CBT test data by a much larger margin than many recent attempts to
improve the model architecture. On one version of the dataset our ensemble even
exceeds the human baseline provided by Facebook. We then show in our own human
study that there is still space for further improvement.



We present a weakly-supervised approach to segmenting proposed drivable paths
in images with the goal of autonomous driving in complex urban environments.
Using recorded routes from a data collection vehicle, our proposed method
generates vast quantities of labelled images containing proposed paths and
obstacles without requiring manual annotation, which we then use to train a
deep semantic segmentation network. With the trained network we can segment
proposed paths and obstacles at run-time using a vehicle equipped with only a
monocular camera without relying on explicit modelling of road or lane
markings. We evaluate our method on the large-scale KITTI and Oxford RobotCar
datasets and demonstrate reliable path proposal and obstacle segmentation in a
wide variety of environments under a range of lighting, weather and traffic
conditions. We illustrate how the method can generalise to multiple path
proposals at intersections and outline plans to incorporate the system into a
framework for autonomous urban driving.



Sample complexity and safety are major challenges when learning policies with
reinforcement learning for real-world tasks, especially when the policies are
represented using rich function approximators like deep neural networks.
Model-based methods where the real-world target domain is approximated using a
simulated source domain provide an avenue to tackle the above challenges by
augmenting real data with simulated data. However, discrepancies between the
simulated source domain and the target domain pose a challenge for simulated
training. We introduce the EPOpt algorithm, which uses an ensemble of simulated
source domains and a form of adversarial training to learn policies that are
robust and generalize to a broad range of possible target domains, including
unmodeled effects. Further, the probability distribution over source domains in
the ensemble can be adapted using data from target domain and approximate
Bayesian methods, to progressively make it a better approximation. Thus,
learning on a model ensemble, along with source domain adaptation, provides the
benefit of both robustness and learning/adaptation.



Face recognition (FR) is the most preferred mode for biometric-based
surveillance, due to its passive nature of detecting subjects, amongst all
different types of biometric traits. FR under surveillance scenario does not
give satisfactory performance due to low contrast, noise and poor illumination
conditions on probes, as compared to the training samples. A state-of-the-art
technology, Deep Learning, even fails to perform well in these scenarios. We
propose a novel soft-margin based learning method for multiple feature-kernel
combinations, followed by feature transformed using Domain Adaptation, which
outperforms many recent state-of-the-art techniques, when tested using three
real-world surveillance face datasets.



The recently introduced Intelligent Trial and Error algorithm (IT\&E) enables
robots to creatively adapt to damage in a matter of minutes by combining an
off-line evolutionary algorithm and an on-line learning algorithm based on
Bayesian Optimization. We extend the IT\&E algorithm to allow for robots to
learn to compensate for damages while executing their task(s). This leads to a
semi-episodic learning scheme that increases the robot's lifetime autonomy and
adaptivity. Preliminary experiments on a toy simulation and a 6-legged robot
locomotion task show promising results.



Visual Question Answering (VQA) is a recent problem in computer vision and
natural language processing that has garnered a large amount of interest from
the deep learning, computer vision, and natural language processing
communities. In VQA, an algorithm needs to answer text-based questions about
images. Since the release of the first VQA dataset in 2014, additional datasets
have been released and many algorithms have been proposed. In this review, we
critically examine the current state of VQA in terms of problem formulation,
existing datasets, evaluation metrics, and algorithms. In particular, we
discuss the limitations of current datasets with regard to their ability to
properly train and assess VQA algorithms. We then exhaustively review existing
algorithms for VQA. Finally, we discuss possible future directions for VQA and
image understanding research.



Deep Neural Networks (DNNs) have become very popular for prediction in many
areas. Their strength is in representation with a high number of parameters
that are commonly learned via gradient descent or similar optimization methods.
However, the representation is non-standardized, and the gradient calculation
methods are often performed using component-based approaches that break
parameters down into scalar units, instead of considering the parameters as
whole entities. In this work, these problems are addressed. Standard notation
is used to represent DNNs in a compact framework. Gradients of DNN loss
functions are calculated directly over the inner product space on which the
parameters are defined. This framework is general and is applied to two common
network types: the Multilayer Perceptron and the Deep Autoencoder.



Identity verification based on authenticity assessment of a handwritten
signature is an important issue in biometrics. There are many effective methods
for signature verification taking into account dynamics of a signing process.
Methods based on partitioning take a very important place among them. In this
paper we propose a new approach to signature partitioning. Its most important
feature is the possibility of selecting and processing of hybrid partitions in
order to increase a precision of the test signature analysis. Partitions are
formed by a combination of vertical and horizontal sections of the signature.
Vertical sections correspond to the initial, middle, and final time moments of
the signing process. In turn, horizontal sections correspond to the signature
areas associated with high and low pen velocity and high and low pen pressure
on the surface of a graphics tablet. Our previous research on vertical and
horizontal sections of the dynamic signature (created independently) led us to
develop the algorithm presented in this paper. Selection of sections, among
others, allows us to define the stability of the signing process in the
partitions, promoting signature areas of greater stability (and vice versa). In
the test of the proposed method two databases were used: public MCYT-100 and
paid BioSecure.



One essential task in information extraction from the medical corpus is drug
name recognition. Compared with text sources come from other domains, the
medical text is special and has unique characteristics. In addition, the
medical text mining poses more challenges, e.g., more unstructured text, the
fast growing of new terms addition, a wide range of name variation for the same
drug. The mining is even more challenging due to the lack of labeled dataset
sources and external knowledge, as well as multiple token representations for a
single drug name that is more common in the real application setting. Although
many approaches have been proposed to overwhelm the task, some problems
remained with poor F-score performance (less than 0.75). This paper presents a
new treatment in data representation techniques to overcome some of those
challenges. We propose three data representation techniques based on the
characteristics of word distribution and word similarities as a result of word
embedding training. The first technique is evaluated with the standard NN
model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two
deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked
Denoising Encoders). The third technique represents the sentence as a sequence
that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term
Memory). In extracting the drug name entities, the third technique gives the
best F-score performance compared to the state of the art, with its average
F-score being 0.8645.



A machine learning method needs to adapt to over time changes in the
environment. Such changes are known as concept drift. In this paper, we propose
concept drift tackling method as an enhancement of Online Sequential Extreme
Learning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by
adding adaptive capability for classification and regression problem. The
scheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme
that works well to handle real drift, virtual drift, and hybrid drift. The
AOS-ELM also works well for sudden drift and recurrent context change type. The
scheme is a simple unified method implemented in simple lines of code. We
evaluated AOS-ELM on regression and classification problem by using concept
drift public data set (SEA and STAGGER) and other public data sets such as
MNIST, USPS, and IDS. Experiments show that our method gives higher kappa value
compared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice
does not need hidden nodes increase, we address some issues related to the
increasing of the hidden nodes such as error condition and rank values. We
propose taking the rank of the pseudoinverse matrix as an indicator parameter
to detect underfitting condition.



A typical modern optimization technique is usually either heuristic or
metaheuristic. This technique has managed to solve some optimization problems
in the research area of science, engineering, and industry. However,
implementation strategy of metaheuristic for accuracy improvement on
convolution neural networks (CNN), a famous deep learning method, is still
rarely investigated. Deep learning relates to a type of machine learning
technique, where its aim is to move closer to the goal of artificial
intelligence of creating a machine that could successfully perform any
intellectual tasks that can be carried out by a human. In this paper, we
propose the implementation strategy of three popular metaheuristic approaches,
that is, simulated annealing, differential evolution, and harmony search, to
optimize CNN. The performances of these metaheuristic methods in optimizing CNN
on classifying MNIST and CIFAR dataset were evaluated and compared.
Furthermore, the proposed methods are also compared with the original CNN.
Although the proposed methods show an increase in the computation time, their
accuracy has also been improved (up to 7.14 percent).



In big data era, the data continuously generated and its distribution may
keep changes overtime. These challenges in online stream of data are known as
concept drift. In this paper, we proposed the Adaptive Convolutional ELM method
(ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid
Extreme Learning Machine (ELM) model plus adaptive capability. This method is
aimed for concept drift handling. We enhanced the CNN as convolutional
hiererchical features representation learner combined with Elastic ELM
(E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM
(AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1)
and matrices concatenation ensembles for concept drift adaptability in ensemble
level (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works
well in classifier level and ensemble level while most current methods only
proposed to work on either one of the levels.
  We verified our method in extended MNIST data set and not MNIST data set. We
set the experiment to simulate virtual drift, real drift, and hybrid drift
event and we demonstrated how our CNNELM adaptability works. Our proposed
method works well and gives better accuracy, computation scalability, and
concept drifts adaptability compared to the regular ELM and CNN. Further
researches are still required to study the optimum parameters and to use more
varied image data set.



We propose a technique for producing "visual explanations" for decisions from
a large class of CNN-based models, making them more transparent. Our approach -
Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of
any target concept, flowing into the final convolutional layer to produce a
coarse localization map highlighting the important regions in the image for
predicting the concept. Unlike previous approaches, GradCAM is applicable to a
wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g.
VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in
tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any
architectural changes or re-training. We combine GradCAM with fine-grained
visualizations to create a high-resolution class-discriminative visualization
and apply it to off-the-shelf image classification, captioning, and visual
question answering (VQA) models, including ResNet-based architectures. In the
context of image classification models, our visualizations (a) lend insights
into their failure modes (showing that seemingly unreasonable predictions have
reasonable explanations), (b) are robust to adversarial images, (c) outperform
previous methods on weakly-supervised localization, (d) are more faithful to
the underlying model and (e) help achieve generalization by identifying dataset
bias. For captioning and VQA, our visualizations show that even non-attention
based models can localize inputs. Finally, we conduct human studies to measure
if GradCAM explanations help users establish trust in predictions from deep
networks and show that GradCAM helps untrained users successfully discern a
"stronger" deep network from a "weaker" one. Our code is available at
https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found
at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.



Neural sequence models are widely used to model time-series data in many
fields. Equally ubiquitous is the usage of beam search (BS) as an approximate
inference algorithm to decode output sequences from these models. BS explores
the search space in a greedy left-right fashion retaining only the top-$B$
candidates -- resulting in sequences that differ only slightly from each other.
Producing lists of nearly identical sequences is not only computationally
wasteful but also typically fails to capture the inherent ambiguity of complex
AI tasks. To overcome this problem, we propose \emph{Diverse Beam Search}
(DBS), an alternative to BS that decodes a list of diverse outputs by
optimizing for a diversity-augmented objective. We observe that our method
finds better top-1 solutions by controlling for the exploration and
exploitation of the search space -- implying that DBS is a \emph{better search
algorithm}. Moreover, these gains are achieved with minimal computational or
memory overhead as compared to beam search. To demonstrate the broad
applicability of our method, we present results on image captioning, machine
translation and visual question generation using both standard quantitative
metrics and qualitative human studies. Our method consistently outperforms BS
and previously proposed techniques for diverse decoding from neural sequence
models.



Antichain based semantics for general rough sets were introduced recently by
the present author. In her paper two different semantics, one for general rough
sets and another for general approximation spaces over quasi-equivalence
relations, were developed. These semantics are improved and studied further
from a lateral algebraic logic perspective in this research. The main results
concern the structure of the algebras and deductive systems in the context.



This research introduces a new strategy in cluster ensemble selection by
using Independency and Diversity metrics. In recent years, Diversity and
Quality, which are two metrics in evaluation procedure, have been used for
selecting basic clustering results in the cluster ensemble selection. Although
quality can improve the final results in cluster ensemble, it cannot control
the procedures of generating basic results, which causes a gap in prediction of
the generated basic results' accuracy. Instead of quality, this paper
introduces Independency as a supplementary method to be used in conjunction
with Diversity. Therefore, this paper uses a heuristic metric, which is based
on the procedure of converting code to graph in Software Testing, in order to
calculate the Independency of two basic clustering algorithms. Moreover, a new
modeling language, which we called as "Clustering Algorithms Independency
Language" (CAIL), is introduced in order to generate graphs which depict
Independency of algorithms. Also, Uniformity, which is a new similarity metric,
has been introduced for evaluating the diversity of basic results. As a
credential, our experimental results on varied different standard data sets
show that the proposed framework improves the accuracy of final results
dramatically in comparison with other cluster ensemble methods.



The crux of the problem in KDD Cup 2016 involves developing data mining
techniques to rank research institutions based on publications. Rank importance
of research institutions are derived from predictions on the number of full
research papers that would potentially get accepted in upcoming top-tier
conferences, utilizing public information on the web. This paper describes our
solution to KDD Cup 2016. We used a two step approach in which we first
identify full research papers corresponding to each conference of interest and
then train two variants of exponential smoothing models to make predictions.
Our solution achieves an overall score of 0.7508, while the winning submission
scored 0.7656 in the overall results.



Local Process Model (LPM) discovery is focused on the mining of a set of
process models where each model describes the behavior represented in the event
log only partially, i.e. subsets of possible events are taken into account to
create so-called local process models. Often such smaller models provide
valuable insights into the behavior of the process, especially when no adequate
and comprehensible single overall process model exists that is able to describe
the traces of the process from start to end. The practical application of LPM
discovery is however hindered by computational issues in the case of logs with
many activities (problems may already occur when there are more than 17 unique
activities). In this paper, we explore three heuristics to discover subsets of
activities that lead to useful log projections with the goal of speeding up LPM
discovery considerably while still finding high-quality LPMs. We found that a
Markov clustering approach to create projection sets results in the largest
improvement of execution time, with discovered LPMs still being better than
with the use of randomly generated activity sets of the same size. Another
heuristic, based on log entropy, yields a more moderate speedup, but enables
the discovery of higher quality LPMs. The third heuristic, based on the
relative information gain, shows unstable performance: for some data sets the
speedup and LPM quality are higher than with the log entropy based method,
while for other data sets there is no speedup at all.



It is difficult to train a personalized task-oriented dialogue system because
the data collected from each individual is often insufficient. Personalized
dialogue systems trained on a small dataset can overfit and make it difficult
to adapt to different user needs. One way to solve this problem is to consider
a collection of multiple users' data as a source domain and an individual
user's data as a target domain, and to perform a transfer learning from the
source to the target domain. By following this idea, we propose
"PETAL"(PErsonalized Task-oriented diALogue), a transfer-learning framework
based on POMDP to learn a personalized dialogue system. The system first learns
common dialogue knowledge from the source domain and then adapts this knowledge
to the target user. This framework can avoid the negative transfer problem by
considering differences between source and target users. The policy in the
personalized POMDP can learn to choose different actions appropriately for
different users. Experimental results on a real-world coffee-shopping data and
simulation data show that our personalized dialogue system can choose different
optimal actions for different users, and thus effectively improve the dialogue
quality under the personalized setting.



Modern robotics applications that involve human-robot interaction require
robots to be able to communicate with humans seamlessly and effectively.
Natural language provides a flexible and efficient medium through which robots
can exchange information with their human partners. Significant advancements
have been made in developing robots capable of interpreting free-form
instructions, but less attention has been devoted to endowing robots with the
ability to generate natural language. We propose a navigational guide model
that enables robots to generate natural language instructions that allow humans
to navigate a priori unknown environments. We first decide which information to
share with the user according to their preferences, using a policy trained from
human demonstrations via inverse reinforcement learning. We then "translate"
this information into a natural language instruction using a neural
sequence-to-sequence model that learns to generate free-form instructions from
natural language corpora. We evaluate our method on a benchmark route
instruction dataset and achieve a BLEU score of 72.18% when compared to
human-generated reference instructions. We additionally conduct navigation
experiments with human participants that demonstrate that our method generates
instructions that people follow as accurately and easily as those produced by
humans.



It is generally difficult to make any statements about the expected
prediction error in an univariate setting without further knowledge about how
the data were generated. Recent work showed that knowledge about the real
underlying causal structure of a data generation process has implications for
various machine learning settings. Assuming an additive noise and an
independence between data generating mechanism and its input, we draw a novel
connection between the intrinsic causal relationship of two variables and the
expected prediction error. We formulate the theorem that the expected error of
the true data generating function as prediction model is generally smaller when
the effect is predicted from its cause and, on the contrary, greater when the
cause is predicted from its effect. The theorem implies an asymmetry in the
error depending on the prediction direction. This is further corroborated with
empirical evaluations in artificial and real-world data sets.



Autonomous driving is a multi-agent setting where the host vehicle must apply
sophisticated negotiation skills with other road users when overtaking, giving
way, merging, taking left and right turns and while pushing ahead in
unstructured urban roadways. Since there are many possible scenarios, manually
tackling all possible cases will likely yield a too simplistic policy.
Moreover, one must balance between unexpected behavior of other
drivers/pedestrians and at the same time not to be too defensive so that normal
traffic flow is maintained.
  In this paper we apply deep reinforcement learning to the problem of forming
long term driving strategies. We note that there are two major challenges that
make autonomous driving different from other robotic tasks. First, is the
necessity for ensuring functional safety - something that machine learning has
difficulty with given that performance is optimized at the level of an
expectation over many instances. Second, the Markov Decision Process model
often used in robotics is problematic in our case because of unpredictable
behavior of other agents in this multi-agent scenario. We make three
contributions in our work. First, we show how policy gradient iterations can be
used without Markovian assumptions. Second, we decompose the problem into a
composition of a Policy for Desires (which is to be learned) and trajectory
planning with hard constraints (which is not learned). The goal of Desires is
to enable comfort of driving, while hard constraints guarantees the safety of
driving. Third, we introduce a hierarchical temporal abstraction we call an
"Option Graph" with a gating mechanism that significantly reduces the effective
horizon and thereby reducing the variance of the gradient estimation even
further.



Developing control policies in simulation is often more practical and safer
than directly running experiments in the real world. This applies to policies
obtained from planning and optimization, and even more so to policies obtained
from reinforcement learning, which is often very data demanding. However, a
policy that succeeds in simulation often doesn't work when deployed on a real
robot. Nevertheless, often the overall gist of what the policy does in
simulation remains valid in the real world. In this paper we investigate such
settings, where the sequence of states traversed in simulation remains
reasonable for the real world, even if the details of the controls are not, as
could be the case when the key differences lie in detailed friction, contact,
mass and geometry properties. During execution, at each time step our approach
computes what the simulation-based control policy would do, but then, rather
than executing these controls on the real robot, our approach computes what the
simulation expects the resulting next state(s) will be, and then relies on a
learned deep inverse dynamics model to decide which real-world action is most
suitable to achieve those next states. Deep models are only as good as their
training data, and we also propose an approach for data collection to
(incrementally) learn the deep inverse dynamics model. Our experiments shows
our approach compares favorably with various baselines that have been developed
for dealing with simulation to real world model discrepancy, including output
error control and Gaussian dynamics adaptation.



An accurate and reliable image based fruit detection system is critical for
supporting higher level agriculture tasks such as yield mapping and robotic
harvesting. This paper presents the use of a state-of-the-art object detection
framework, Faster R-CNN, in the context of fruit detection in orchards,
including mangoes, almonds and apples. Ablation studies are presented to better
understand the practical deployment of the detection network, including how
much training data is required to capture variability in the dataset. Data
augmentation techniques are shown to yield significant performance gains,
resulting in a greater than two-fold reduction in the number of training images
required. In contrast, transferring knowledge between orchards contributed to
negligible performance gain over initialising the Deep Convolutional Neural
Network directly from ImageNet features. Finally, to operate over orchard data
containing between 100-1000 fruit per image, a tiling approach is introduced
for the Faster R-CNN framework. The study has resulted in the best yet
detection performance for these orchards relative to previous works, with an
F1-score of >0.9 achieved for apples and mangoes.



A fall is an abnormal activity that occurs rarely, so it is hard to collect
real data for falls. It is, therefore, difficult to use supervised learning
methods to automatically detect falls. Another challenge in using machine
learning methods to automatically detect falls is the choice of engineered
features. In this paper, we propose to use an ensemble of autoencoders to
extract features from different channels of wearable sensor data trained only
on normal activities. We show that the traditional approach of choosing a
threshold as the maximum of the reconstruction error on the training normal
data is not the right way to identify unseen falls. We propose two methods for
automatic tightening of reconstruction error from only the normal activities
for better identification of unseen falls. We present our results on two
activity recognition datasets and show the efficacy of our proposed method
against traditional autoencoder models and two standard one-class
classification methods.



We study truthful mechanisms for matching and related problems in a partial
information setting, where the agents' true utilities are hidden, and the
algorithm only has access to ordinal preference information. Our model is
motivated by the fact that in many settings, agents cannot express the
numerical values of their utility for different outcomes, but are still able to
rank the outcomes in their order of preference. Specifically, we study problems
where the ground truth exists in the form of a weighted graph of agent
utilities, but the algorithm can only elicit the agents' private information in
the form of a preference ordering for each agent induced by the underlying
weights. Against this backdrop, we design truthful algorithms to approximate
the true optimum solution with respect to the hidden weights. Our techniques
yield universally truthful algorithms for a number of graph problems: a
1.76-approximation algorithm for Max-Weight Matching, 2-approximation algorithm
for Max k-matching, a 6-approximation algorithm for Densest k-subgraph, and a
2-approximation algorithm for Max Traveling Salesman as long as the hidden
weights constitute a metric. We also provide improved approximation algorithms
for such problems when the agents are not able to lie about their preferences.
Our results are the first non-trivial truthful approximation algorithms for
these problems, and indicate that in many situations, we can design robust
algorithms even when the agents may lie and only provide ordinal information
instead of precise utilities.



This paper presents a deep learning architecture for the semantic decoder
component of a Statistical Spoken Dialogue System. In a slot-filling dialogue,
the semantic decoder predicts the dialogue act and a set of slot-value pairs
from a set of n-best hypotheses returned by the Automatic Speech Recognition.
Most current models for spoken language understanding assume (i) word-aligned
semantic annotations as in sequence taggers and (ii) delexicalisation, or a
mapping of input words to domain-specific concepts using heuristics that try to
capture morphological variation but that do not scale to other domains nor to
language variation (e.g., morphology, synonyms, paraphrasing ). In this work
the semantic decoder is trained using unaligned semantic annotations and it
uses distributed semantic representation learning to overcome the limitations
of explicit delexicalisation. The proposed architecture uses a convolutional
neural network for the sentence representation and a long-short term memory
network for the context representation. Results are presented for the publicly
available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a
significantly higher word error rate (WER).



With the advent of extremely high dimensional datasets, dimensionality
reduction techniques are becoming mandatory. Among many techniques, feature
selection has been growing in interest as an important tool to identify
relevant features on huge datasets --both in number of instances and
features--. The purpose of this work is to demonstrate that standard feature
selection methods can be parallelized in Big Data platforms like Apache Spark,
boosting both performance and accuracy. We thus propose a distributed
implementation of a generic feature selection framework which includes a wide
group of well-known Information Theoretic methods. Experimental results on a
wide set of real-world datasets show that our distributed framework is capable
of dealing with ultra-high dimensional datasets as well as those with a huge
number of samples in a short period of time, outperforming the sequential
version in all the cases studied.



Bilinear models provide rich representations compared with linear models.
They have been applied in various visual tasks, such as object recognition,
segmentation, and visual question-answering, to get state-of-the-art
performances taking advantage of the expanded representations. However,
bilinear representations tend to be high-dimensional, limiting the
applicability to computationally complex tasks. We propose low-rank bilinear
pooling using Hadamard product for an efficient attention mechanism of
multimodal learning. We show that our model outperforms compact bilinear
pooling in visual question-answering tasks with the state-of-the-art results on
the VQA dataset, having a better parsimonious property.



This paper studies the generalization error of invariant classifiers. In
particular, we consider the common scenario where the classification task is
invariant to certain transformations of the input, and that the classifier is
constructed (or learned) to be invariant to these transformations. Our approach
relies on factoring the input space into a product of a base space and a set of
transformations. We show that whereas the generalization error of a
non-invariant classifier is proportional to the complexity of the input space,
the generalization error of an invariant classifier is proportional to the
complexity of the base space. We also derive a set of sufficient conditions on
the geometry of the base space and the set of transformations that ensure that
the complexity of the base space is much smaller than the complexity of the
input space. Our analysis applies to general classifiers such as convolutional
neural networks. We demonstrate the implications of the developed theory for
such classifiers with experiments on the MNIST and CIFAR-10 datasets.



In this paper, we define a novel census signal temporal logic (CensusSTL)
that focuses on the number of agents in different subsets of a group that
complete a certain task specified by the signal temporal logic (STL). CensusSTL
consists of an "inner logic" STL formula and an "outer logic" STL formula. We
present a new inference algorithm to infer CensusSTL formulae from the
trajectory data of a group of agents. We first identify the "inner logic" STL
formula and then infer the subgroups based on whether the agents' behaviors
satisfy the "inner logic" formula at each time point. We use two different
approaches to infer the subgroups based on similarity and complementarity,
respectively. The "outer logic" CensusSTL formula is inferred from the census
trajectories of different subgroups. We apply the algorithm in analyzing data
from a soccer match by inferring the CensusSTL formula for different subgroups
of a soccer team.



Conventional deep neural networks (DNN) for speech acoustic modeling rely on
Gaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary
class labels as the targets for DNN training. Subword classes in speech
recognition systems correspond to context-dependent tied states or senones. The
present work addresses some limitations of GMM-HMM senone alignments for DNN
training. We hypothesize that the senone probabilities obtained from a DNN
trained with binary labels can provide more accurate targets to learn better
acoustic models. However, DNN outputs bear inaccuracies which are exhibited as
high dimensional unstructured noise, whereas the informative components are
structured and low-dimensional. We exploit principle component analysis (PCA)
and sparse coding to characterize the senone subspaces. Enhanced probabilities
obtained from low-rank and sparse reconstructions are used as soft-targets for
DNN acoustic modeling, that also enables training with untranscribed data.
Experiments conducted on AMI corpus shows 4.6% relative reduction in word error
rate.



Probabilistic programming languages (PPLs) are a powerful modeling tool, able
to represent any computable probability distribution. Unfortunately,
probabilistic program inference is often intractable, and existing PPLs mostly
rely on expensive, approximate sampling-based methods. To alleviate this
problem, one could try to learn from past inferences, so that future inferences
run faster. This strategy is known as amortized inference; it has recently been
applied to Bayesian networks and deep generative models. This paper proposes a
system for amortized inference in PPLs. In our system, amortization comes in
the form of a parameterized guide program. Guide programs have similar
structure to the original program, but can have richer data flow, including
neural network components. These networks can be optimized so that the guide
approximately samples from the posterior distribution defined by the original
program. We present a flexible interface for defining guide programs and a
stochastic gradient-based scheme for optimizing guide parameters, as well as
some preliminary results on automatically deriving guide programs. We explore
in detail the common machine learning pattern in which a 'local' model is
specified by 'global' random values and used to generate independent observed
data points; this gives rise to amortized local inference supporting global
model learning.



Fuzzy controllers are efficient and interpretable system controllers for
continuous state and action spaces. To date, such controllers have been
constructed manually or trained automatically either using expert-generated
problem-specific cost functions or incorporating detailed knowledge about the
optimal control strategy. Both requirements for automatic training processes
are not found in most real-world reinforcement learning (RL) problems. In such
applications, online learning is often prohibited for safety reasons because
online learning requires exploration of the problem's dynamics during policy
training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL)
approach that can construct fuzzy RL policies solely by training parameters on
world models that simulate real system dynamics. These world models are created
by employing an autonomous machine learning technique that uses previously
generated transition samples of a real system. To the best of our knowledge,
this approach is the first to relate self-organizing fuzzy controllers to
model-based batch RL. Therefore, FPSRL is intended to solve problems in domains
where online learning is prohibited, system dynamics are relatively easy to
model from previously generated default policy transition samples, and it is
expected that a relatively easily interpretable control policy exists. The
efficiency of the proposed approach with problems from such domains is
demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole
balancing, and cart-pole swing-up. Our experimental results demonstrate
high-performing, interpretable fuzzy policies.



This paper examines use of dynamic probabilistic networks (DPN) for human
action recognition. The actions of lifting objects and walking in the room,
sitting in the room and neutral standing pose were used for testing the
classification. The research used the dynamic interrelation between various
different regions of interest (ROI) on the human body (face, body, arms, legs)
and the time series based events related to the these ROIs. This dynamic links
are then used to recognize the human behavioral aspects in the scene. First a
model is developed to identify the human activities in an indoor scene and this
model is dependent on the key features and interlinks between the various
dynamic events using DPNs. The sub ROI are classified with DPN to associate the
combined interlink with a specific human activity. The recognition accuracy
performance between indoor (controlled lighting conditions) is compared with
the outdoor lighting conditions. The accuracy in outdoor scenes was lower than
the controlled environment.



The long-term memory of most connectionist systems lies entirely in the
weights of the system. Since the number of weights is typically fixed, this
bounds the total amount of knowledge that can be learned and stored. Though
this is not normally a problem for a neural network designed for a specific
task, such a bound is undesirable for a system that continually learns over an
open range of domains. To address this, we describe a lifelong learning system
that leverages a fast, though non-differentiable, content-addressable memory
which can be exploited to encode both a long history of sequential episodic
knowledge and semantic knowledge over many episodes for an unbounded number of
domains. This opens the door for investigation into transfer learning, and
leveraging prior knowledge that has been learned over a lifetime of experiences
to new domains.



Hypothesis testing is an important cognitive process that supports human
reasoning. In this paper, we introduce a computational hypothesis testing
approach based on memory augmented neural networks. Our approach involves a
hypothesis testing loop that reconsiders and progressively refines a previously
formed hypothesis in order to generate new hypotheses to test. We apply the
proposed approach to language comprehension task by using Neural Semantic
Encoders (NSE). Our NSE models achieve the state-of-the-art results showing an
absolute improvement of 1.2% to 2.6% accuracy over previous results obtained by
single and ensemble systems on standard machine comprehension benchmarks such
as the Children's Book Test (CBT) and Who-Did-What (WDW) news article datasets.



Answering open-ended questions is an essential capability for any intelligent
agent. One of the most interesting recent open-ended question answering
challenges is Visual Question Answering (VQA) which attempts to evaluate a
system's visual understanding through its answers to natural language questions
about images. There exist many approaches to VQA, the majority of which do not
exhibit deeper semantic understanding of the candidate answers they produce. We
study the importance of generating plausible answers to a given question by
introducing the novel task of `Answer Proposal': for a given open-ended
question, a system should generate a ranked list of candidate answers informed
by the semantics of the question. We experiment with various models including a
neural generative model as well as a semantic graph matching one. We provide
both intrinsic and extrinsic evaluations for the task of Answer Proposal,
showing that our best model learns to propose plausible answers with a high
recall and performs competitively with some other solutions to VQA.



In recent years, traditional cybersecurity safeguards have proven ineffective
against insider threats. Famous cases of sensitive information leaks caused by
insiders, including the WikiLeaks release of diplomatic cables and the Edward
Snowden incident, have greatly harmed the U.S. government's relationship with
other governments and with its own citizens. Data Leak Prevention (DLP) is a
solution for detecting and preventing information leaks from within an
organization's network. However, state-of-art DLP detection models are only
able to detect very limited types of sensitive information, and research in the
field has been hindered due to the lack of available sensitive texts. Many
researchers have focused on document-based detection with artificially labeled
"confidential documents" for which security labels are assigned to the entire
document, when in reality only a portion of the document is sensitive. This
type of whole-document based security labeling increases the chances of
preventing authorized users from accessing non-sensitive information within
sensitive documents. In this paper, we introduce Automated Classification
Enabled by Security Similarity (ACESS), a new and innovative detection model
that penetrates the complexity of big text security classification/detection.
To analyze the ACESS system, we constructed a novel dataset, containing
formerly classified paragraphs from diplomatic cables made public by the
WikiLeaks organization. To our knowledge this paper is the first to analyze a
dataset that contains actual formerly sensitive information annotated at
paragraph granularity.



We quantify a source of ineffectual computations when processing the
multiplications of the convolutional layers in Deep Neural Networks (DNNs) and
propose Pragmatic (PRA), an architecture that exploits it improving performance
and energy efficiency. The source of these ineffectual computations is best
understood in the context of conventional multipliers which generate internally
multiple terms, that is, products of the multiplicand and powers of two, which
added together produce the final product [1]. At runtime, many of these terms
are zero as they are generated when the multiplicand is combined with the
zero-bits of the multiplicator. While conventional bit-parallel multipliers
calculate all terms in parallel to reduce individual product latency, PRA
calculates only the non-zero terms using a) on-the-fly conversion of the
multiplicator representation into an explicit list of powers of two, and b)
hybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA
exploits two sources of ineffectual computations: 1) the aforementioned zero
product terms which are the result of the lack of explicitness in the
multiplicator representation, and 2) the excess in the representation precision
used for both multiplicants and multiplicators, e.g., [2]. Measurements
demonstrate that for the convolutional layers, a straightforward variant of PRA
improves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by
1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on
average compared to DaDN and STR. An improved cross lane synchronication scheme
boosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits
persist even with an 8-bit quantized representation [5].



Deep neural networks have achieved impressive experimental results in image
classification, but can surprisingly be unstable with respect to adversarial
perturbations, that is, minimal changes to the input image that cause the
network to misclassify it. With potential applications including perception
modules and end-to-end controllers for self-driving cars, this raises concerns
about their safety. We develop a novel automated verification framework for
feed-forward multi-layer neural networks based on Satisfiability Modulo Theory
(SMT). We focus on safety of image classification decisions with respect to
image manipulations, such as scratches or changes to camera angle or lighting
conditions that would result in the same class being assigned by a human, and
define safety for an individual decision in terms of invariance of the
classification within a small neighbourhood of the original image. We enable
exhaustive search of the region by employing discretisation, and propagate the
analysis layer by layer. Our method works directly with the network code and,
in contrast to existing methods, can guarantee that adversarial examples, if
they exist, are found for the given region and family of manipulations. If
found, adversarial examples can be shown to human testers and/or used to
fine-tune the network. We implement the techniques using Z3 and evaluate them
on state-of-the-art networks, including regularised and deep learning networks.
We also compare against existing techniques to search for adversarial examples
and estimate network robustness.



Decision makers, such as doctors and judges, make crucial decisions such as
recommending treatments to patients, and granting bails to defendants on a
daily basis. Such decisions typically involve weighting the potential benefits
of taking an action against the costs involved. In this work, we aim to
automate this task of learning \emph{cost-effective, interpretable and
actionable treatment regimes}. We formulate this as a problem of learning a
decision list -- a sequence of if-then-else rules -- which maps characteristics
of subjects (eg., diagnostic test results of patients) to treatments. We
propose a novel objective to construct a decision list which maximizes outcomes
for the population, and minimizes overall costs. We model the problem of
learning such a list as a Markov Decision Process (MDP) and employ a variant of
the Upper Confidence Bound for Trees (UCT) strategy which leverages customized
checks for pruning the search space effectively. Experimental results on real
world observational data capturing judicial bail decisions and treatment
recommendations for asthma patients demonstrate the effectiveness of our
approach.



In this work, we investigate the application of Reinforcement Learning to two
well known decision dilemmas, namely Newcomb's Problem and Prisoner's Dilemma.
These problems are exemplary for dilemmas that autonomous agents are faced with
when interacting with humans. Furthermore, we argue that a Newcomb-like
formulation is more adequate in the human-machine interaction case and
demonstrate empirically that the unmodified Reinforcement Learning algorithms
end up with the well known maximum expected utility solution.



This special issue is dedicated to get a better picture of the relationships
between computational linguistics and cognitive science. It specifically raises
two questions: "what is the potential contribution of computational language
modeling to cognitive science?" and conversely: "what is the influence of
cognitive science in contemporary computational linguistics?"



Meaning has been called the "holy grail" of a variety of scientific
disciplines, ranging from linguistics to philosophy, psychology and the
neurosciences. The field of Artifical Intelligence (AI) is very much a part of
that list: the development of sophisticated natural language semantics is a
sine qua non for achieving a level of intelligence comparable to humans.
Embodiment theories in cognitive science hold that human semantic
representation depends on sensori-motor experience; the abundant evidence that
human meaning representation is grounded in the perception of physical reality
leads to the conclusion that meaning must depend on a fusion of multiple
(perceptual) modalities. Despite this, AI research in general, and its
subdisciplines such as computational linguistics and computer vision in
particular, have focused primarily on tasks that involve a single modality.
Here, we propose virtual embodiment as an alternative, long-term strategy for
AI research that is multi-modal in nature and that allows for the kind of
scalability required to develop the field coherently and incrementally, in an
ethically responsible fashion.



We propose a novel method of regularization for recurrent neural networks
called suprisal-driven zoneout. In this method, states zoneout (maintain their
previous value rather than updating), when the suprisal (discrepancy between
the last state's prediction and target) is small. Thus regularization is
adaptive and input-driven on a per-neuron basis. We demonstrate the
effectiveness of this idea by achieving state-of-the-art bits per character of
1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to
the best known highly-engineered compression methods.



We present a fast variational Bayesian algorithm for performing non-negative
matrix factorisation and tri-factorisation. We show that our approach achieves
faster convergence per iteration and timestep (wall-clock) than Gibbs sampling
and non-probabilistic approaches, and do not require additional samples to
estimate the posterior. We show that in particular for matrix tri-factorisation
convergence is difficult, but our variational Bayesian approach offers a fast
solution, allowing the tri-factorisation approach to be used more effectively.



The emerging field of quantum machine learning has the potential to
substantially aid in the problems and scope of artificial intelligence. This is
only enhanced by recent successes in the field of classical machine learning.
In this work we propose an approach for the systematic treatment of machine
learning, from the perspective of quantum information. Our approach is general
and covers all three main branches of machine learning: supervised,
unsupervised and reinforcement learning. While quantum improvements in
supervised and unsupervised learning have been reported, reinforcement learning
has received much less attention. Within our approach, we tackle the problem of
quantum enhancements in reinforcement learning as well, and propose a
systematic scheme for providing improvements. As an example, we show that
quadratic improvements in learning efficiency, and exponential improvements in
performance over limited time periods, can be obtained for a broad class of
learning problems.



Given a state-of-the-art deep neural network classifier, we show the
existence of a universal (image-agnostic) and very small perturbation vector
that causes natural images to be misclassified with high probability. We
propose a systematic algorithm for computing universal perturbations, and show
that state-of-the-art deep neural networks are highly vulnerable to such
perturbations, albeit being quasi-imperceptible to the human eye. We further
empirically analyze these universal perturbations and show, in particular, that
they generalize very well across neural networks. The surprising existence of
universal perturbations reveals important geometric correlations among the
high-dimensional decision boundary of classifiers. It further outlines
potential security breaches with the existence of single directions in the
input space that adversaries can possibly exploit to break a classifier on most
natural images.



Food and nutrition occupy an increasingly prevalent space on the web, and
dishes and recipes shared online provide an invaluable mirror into culinary
cultures and attitudes around the world. More specifically, ingredients,
flavors, and nutrition information become strong signals of the taste
preferences of individuals and civilizations. However, there is little
understanding of these palate varieties. In this paper, we present a
large-scale study of recipes published on the web and their content, aiming to
understand cuisines and culinary habits around the world. Using a database of
more than 157K recipes from over 200 different cuisines, we analyze
ingredients, flavors, and nutritional values which distinguish dishes from
different regions, and use this knowledge to assess the predictability of
recipes from different cuisines. We then use country health statistics to
understand the relation between these factors and health indicators of
different nations, such as obesity, diabetes, migration, and health
expenditure. Our results confirm the strong effects of geographical and
cultural similarities on recipes, health indicators, and culinary preferences
across the globe.



We formalize synthesis of shared control protocols with correctness
guarantees for temporal logic specifications. More specifically, we introduce a
modeling formalism in which both a human and an autonomy protocol can issue
commands to a robot towards performing a certain task. These commands are
blended into a joint input to the robot. The autonomy protocol is synthesized
using an abstraction of possible human commands accounting for randomness in
decisions caused by factors such as fatigue or incomprehensibility of the
problem at hand. The synthesis is designed to ensure that the resulting robot
behavior satisfies given safety and performance specifications, e.g., in
temporal logic. Our solution is based on nonlinear programming and we address
the inherent scalability issue by presenting alternative methods. We assess the
feasibility and the scalability of the approach by an experimental evaluation.



We propose the Hit-and-Run algorithm for planning and sampling problems in
non-convex spaces. For sampling, we show the first analysis of the Hit-and-Run
algorithm in non-convex spaces and show that it mixes fast as long as certain
smoothness conditions are satisfied. In particular, our analysis reveals an
intriguing connection between fast mixing and the existence of smooth
measure-preserving mappings from a convex space to the non-convex space. For
planning, we show advantages of Hit-and-Run compared to state-of-the-art
planning methods such as Rapidly-Exploring Random Trees.



Many applications in speech, robotics, finance, and biology deal with
sequential data, where ordering matters and recurrent structures are common.
However, this structure cannot be easily captured by standard kernel functions.
To model such structure, we propose expressive closed-form kernel functions for
Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the
inductive biases of long short-term memory (LSTM) recurrent networks, while
retaining the non-parametric probabilistic advantages of Gaussian processes. We
learn the properties of the proposed kernels by optimizing the Gaussian process
marginal likelihood using a new provably convergent semi-stochastic gradient
procedure and exploit the structure of these kernels for scalable training and
prediction. This approach provides a practical representation for Bayesian
LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and
thoroughly investigate a consequential autonomous driving application, where
the predictive uncertainties provided by GP-LSTM are uniquely valuable.



In Bayesian statistics probability distributions express beliefs. However,
for many problems the beliefs cannot be computed analytically and
approximations of beliefs are needed. We seek a loss function that quantifies
how "embarrassing" it is to communicate a given approximation. We reproduce and
discuss an old proof showing that there is only one ranking under the
requirements that (1) the best ranked approximation is the non-approximated
belief and (2) that the ranking judges approximations only by their predictions
for actual outcomes. The loss function that is obtained in the derivation is
equal to the Kullback-Leibler divergence when normalized. This loss function is
frequently used in the literature. However, there seems to be confusion about
the correct order in which its functional arguments, the approximated and
non-approximated beliefs, should be used. The correct order ensures that the
recipient of a communication is only deprived of the minimal amount of
information. We hope that the elementary derivation settles the apparent
confusion. For example when approximating beliefs with Gaussian distributions
the optimal approximation is given by moment matching. This is in contrast to
many suggested computational schemes.



Ontologies in the biomedical domain are numerous, highly specialized and very
expensive to develop. Thus, a crucial prerequisite for ontology adoption and
reuse is effective support for exploring and finding existing ontologies.
Towards that goal, the National Center for Biomedical Ontology (NCBO) has
developed BioPortal---an online repository designed to support users in
exploring and finding more than 500 existing biomedical ontologies. In 2016,
BioPortal represents one of the largest portals for exploration of semantic
biomedical vocabularies and terminologies, which is used by many researchers
and practitioners. While usage of this portal is high, we know very little
about how exactly users search and explore ontologies and what kind of usage
patterns or user groups exist in the first place. Deeper insights into user
behavior on such portals can provide valuable information to devise strategies
for a better support of users in exploring and finding existing ontologies, and
thereby enable better ontology reuse. To that end, we study and group users
according to their browsing behavior on BioPortal using data mining techniques.
Additionally, we use the obtained groups to characterize and compare
exploration strategies across ontologies. In particular, we were able to
identify seven distinct browsing-behavior types, which all make use of
different functionality provided by BioPortal. For example, Search Explorers
make extensive use of the search functionality while Ontology Tree Explorers
mainly rely on the class hierarchy to explore ontologies. Further, we show that
specific characteristics of ontologies influence the way users explore and
interact with the website. Our results may guide the development of more
user-oriented systems for ontology exploration on the Web.



Pattern sampling has been proposed as a potential solution to the infamous
pattern explosion. Instead of enumerating all patterns that satisfy the
constraints, individual patterns are sampled proportional to a given quality
measure. Several sampling algorithms have been proposed, but each of them has
its limitations when it comes to 1) flexibility in terms of quality measures
and constraints that can be used, and/or 2) guarantees with respect to sampling
accuracy. We therefore present Flexics, the first flexible pattern sampler that
supports a broad class of quality measures and constraints, while providing
strong guarantees regarding sampling accuracy. To achieve this, we leverage the
perspective on pattern mining as a constraint satisfaction problem and build
upon the latest advances in sampling solutions in SAT as well as existing
pattern mining algorithms. Furthermore, the proposed algorithm is applicable to
a variety of pattern languages, which allows us to introduce and tackle the
novel task of sampling sets of patterns. We introduce and empirically evaluate
two variants of Flexics: 1) a generic variant that addresses the well-known
itemset sampling task and the novel pattern set sampling task as well as a wide
range of expressive constraints within these tasks, and 2) a specialized
variant that exploits existing frequent itemset techniques to achieve
substantial speed-ups. Experiments show that Flexics is both accurate and
efficient, making it a useful tool for pattern-based data exploration.



We focus on generative autoencoders, such as variational or adversarial
autoencoders, which jointly learn a generative model alongside an inference
model. Generative autoencoders are those which are trained to softly enforce a
prior on the latent distribution learned by the inference model. We call the
distribution to which the inference model maps observed samples, the learned
latent distribution, which may not be consistent with the prior. We formulate a
Markov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively
decoding and encoding, which allows us to sample from the learned latent
distribution. Since, the generative model learns to map from the learned latent
distribution, rather than the prior, we may use MCMC to improve the quality of
samples drawn from the generative model, especially when the learned latent
distribution is far from the prior. Using MCMC sampling, we are able to reveal
previously unseen differences between generative autoencoders trained either
with or without a denoising criterion.



Vision-based object detection is one of the fundamental functions in numerous
traffic scene applications such as self-driving vehicle systems and advance
driver assistance systems (ADAS). However, it is also a challenging task due to
the diversity of traffic scene and the storage, power and computing source
limitations of the platforms for traffic scene applications. This paper
presents a generalized Haar filter based deep network which is suitable for the
object detection tasks in traffic scene. In this approach, we first decompose a
object detection task into several easier local regression tasks. Then, we
handle the local regression tasks by using several tiny deep networks which
simultaneously output the bounding boxes, categories and confidence scores of
detected objects. To reduce the consumption of storage and computing resources,
the weights of the deep networks are constrained to the form of generalized
Haar filter in training phase. Additionally, we introduce the strategy of
sparse windows generation to improve the efficiency of the algorithm. Finally,
we perform several experiments to validate the performance of our proposed
approach. Experimental results demonstrate that the proposed approach is both
efficient and effective in traffic scene compared with the state-of-the-art.



This work proposes a novel framework for the development of new products and
services in transportation through an open innovation approach based on
automatic content analysis of social media data. The framework is able to
extract users comments from Online Social Networks (OSN), to process and
analyze text through information extraction and sentiment analysis techniques
to obtain relevant information about product reception on the market. A use
case was developed using the mobile application Uber, which is today one of the
fastest growing technology companies in the world. We measured how a
controversial, highly diffused event influences the volume of tweets about Uber
and the perception of its users. While there is no change in the image of Uber,
a large increase in the number of tweets mentioning the company is observed,
which meant a free and important diffusion of its product.



We introduce a method for using deep neural networks to amortize the cost of
inference in models from the family induced by universal probabilistic
programming languages, establishing a framework that combines the strengths of
probabilistic programming and deep learning methods. We call what we do
"compilation of inference" because our method transforms a denotational
specification of an inference problem in the form of a probabilistic program
written in a universal programming language into a trained neural network
denoted in a neural network specification language. When at test time this
neural network is fed observational data and executed, it performs approximate
inference in the original model specified by the probabilistic program. Our
training objective and learning procedure are designed to allow the trained
neural network to be used as a proposal distribution in a sequential importance
sampling inference engine. We illustrate our method on mixture models and
Captcha solving and show significant speedups in the efficiency of inference.



Harnessing the statistical power of neural networks to perform language
understanding and symbolic reasoning is difficult, when it requires executing
efficient discrete operations against a large knowledge-base. In this work, we
introduce a Neural Symbolic Machine, which contains (a) a neural "programmer",
i.e., a sequence-to-sequence model that maps language utterances to programs
and utilizes a key-variable memory to handle compositionality (b) a symbolic
"computer", i.e., a Lisp interpreter that performs program execution, and helps
find good programs by pruning the search space. We apply REINFORCE to directly
optimize the task reward of this structured prediction problem. To train with
weak supervision and improve the stability of REINFORCE, we augment it with an
iterative maximum-likelihood training process. NSM outperforms the
state-of-the-art on the WebQuestionsSP dataset when trained from
question-answer pairs only, without requiring any feature engineering or
domain-specific knowledge.



Pairwise comparison is an important tool in multi-attribute decision making.
Pairwise comparison matrices (PCM) have been applied for ranking criteria and
for scoring alternatives according to a given criterion. Our paper presents a
special application of incomplete PCMs: ranking of professional tennis players
based on their results against each other. The selected 25 players have been on
the top of the ATP rankings for a shorter or longer period in the last 40
years. Some of them have never met on the court. One of the aims of the paper
is to provide ranking of the selected players, however, the analysis of
incomplete pairwise comparison matrices is also in the focus. The eigenvector
method and the logarithmic least squares method were used to calculate weights
from incomplete PCMs. In our results the top three players of four decades were
Nadal, Federer and Sampras. Some questions have been raised on the properties
of incomplete PCMs and remains open for further investigation.



Link prediction, the problem of identifying missing links among a set of
inter-related data entities, is a popular field of research due to its
application to graph-like domains. Producing consistent evaluations of the
performance of the many link prediction algorithms being proposed can be
challenging due to variable graph properties, such as size and density. In this
paper we first discuss traditional data mining solutions which are applicable
to link prediction evaluation, arguing about their capacity for producing
faithful and useful evaluations. We also introduce an innovative modification
to a traditional evaluation methodology with the goal of adapting it to the
problem of evaluating link prediction algorithms when applied to large graphs,
by tackling the problem of class imbalance. We empirically evaluate the
proposed methodology and, building on these findings, make a case for its
importance on the evaluation of large-scale graph processing.



Determining the optimal size and orientation of small-scale residential based
PV arrays will become increasingly complex in the future smart grid environment
with the introduction of smart meters and dynamic tariffs. However consumers
can leverage the availability of smart meter data to conduct a more detailed
exploration of PV investment options for their particular circumstances. In
this paper, an optimization method for PV orientation and sizing is proposed
whereby maximizing the PV investment value is set as the defining objective.
Solar insolation and PV array models are described to form the basis of the PV
array optimization strategy. A constrained particle swarm optimization
algorithm is selected due to its strong performance in non-linear applications.
The optimization algorithm is applied to real-world metered data to quantify
the possible investment value of a PV installation under different energy
retailers and tariff structures. The arrangement with the highest value is
determined to enable prospective small-scale PV investors to select the most
cost-effective system.



The GANs are generative models whose random samples realistically reflect
natural images. It also can generate samples with specific attributes by
concatenating a condition vector into the input, yet research on this field is
not well studied. We propose novel methods of conditioning generative
adversarial networks (GANs) that achieve state-of-the-art results on MNIST and
CIFAR-10. We mainly introduce two models: an information retrieving model that
extracts conditional information from the samples, and a spatial bilinear
pooling model that forms bilinear features derived from the spatial cross
product of an image and a condition vector. These methods significantly enhance
log-likelihood of test data under the conditional distributions compared to the
methods of concatenation.



We propose a method to classify the causal relationship between two discrete
variables given only the joint distribution of the variables, acknowledging
that the method is subject to an inherent baseline error. We assume that the
causal system is acyclicity, but we do allow for hidden common causes. Our
algorithm presupposes that the probability distributions $P(C)$ of a cause $C$
is independent from the probability distribution $P(E\mid C)$ of the
cause-effect mechanism. While our classifier is trained with a Bayesian
assumption of flat hyperpriors, we do not make this assumption about our test
data. This work connects to recent developments on the identifiability of
causal models over continuous variables under the assumption of "independent
mechanisms". Carefully-commented Python notebooks that reproduce all our
experiments are available online at
http://vision.caltech.edu/~kchalupk/code.html.



Recurrent neural networks are a powerful tool for modeling sequential data,
but the dependence of each timestep's computation on the previous timestep's
output limits parallelism and makes RNNs unwieldy for very long sequences. We
introduce quasi-recurrent neural networks (QRNNs), an approach to neural
sequence modeling that alternates convolutional layers, which apply in parallel
across timesteps, and a minimalist recurrent pooling function that applies in
parallel across channels. Despite lacking trainable recurrent layers, stacked
QRNNs have better predictive accuracy than stacked LSTMs of the same hidden
size. Due to their increased parallelism, they are up to 16 times faster at
train and test time. Experiments on language modeling, sentiment
classification, and character-level neural machine translation demonstrate
these advantages and underline the viability of QRNNs as a basic building block
for a variety of sequence tasks.



Neural networks are powerful and flexible models that work well for many
difficult learning tasks in image, speech and natural language understanding.
Despite their success, neural networks are still hard to design. In this paper,
we use a recurrent network to generate the model descriptions of neural
networks and train this RNN with reinforcement learning to maximize the
expected accuracy of the generated architectures on a validation set. On the
CIFAR-10 dataset, our method, starting from scratch, can design a novel network
architecture that rivals the best human-invented architecture in terms of test
set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is
0.09 percent better and 1.05x faster than the previous state-of-the-art model
that used a similar architectural scheme. On the Penn Treebank dataset, our
model can compose a novel recurrent cell that outperforms the widely-used LSTM
cell, and other state-of-the-art baselines. Our cell achieves a test set
perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than
the previous state-of-the-art model. The cell can also be transferred to the
character language modeling task on PTB and achieves a state-of-the-art
perplexity of 1.214.



Policy gradient is an efficient technique for improving a policy in a
reinforcement learning setting. However, vanilla online variants are on-policy
only and not able to take advantage of off-policy data. In this paper we
describe a new technique that combines policy gradient with off-policy
Q-learning, drawing experience from a replay buffer. This is motivated by
making a connection between the fixed points of the regularized policy gradient
algorithm and the Q-values. This connection allows us to estimate the Q-values
from the action preferences of the policy, to which we apply Q-learning
updates. We refer to the new technique as 'PGQL', for policy gradient and
Q-learning. We also establish an equivalency between action-value fitting
techniques and actor-critic algorithms, showing that regularized policy
gradient techniques can be interpreted as advantage function learning
algorithms. We conclude with some numerical examples that demonstrate improved
data efficiency and stability of PGQL. In particular, we tested PGQL on the
full suite of Atari games and achieved performance exceeding that of both
asynchronous advantage actor-critic (A3C) and Q-learning.



One of the most important fields in robotics is the optimization of
controllers. Currently, robots are treated as a black box in this optimization
process, which is the reason why derivative-free optimization methods such as
evolutionary algorithms or reinforcement learning are omnipresent. We propose
an implementation of a modern physics engine, which has the ability to
differentiate control parameters. This has been implemented on both CPU and
GPU. We show how this speeds up the optimization process, even for small
problems, and why it will scale to bigger problems. We explain why this is an
alternative approach to deep Q-learning, for using deep learning in robotics.
Lastly, we argue that this is a big step for deep learning in robotics, as it
opens up new possibilities to optimize robots, both in hardware and software.



In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based
language model designed to directly capture the global semantic meaning
relating words in a document via latent topics. Because of their sequential
nature, RNNs are good at capturing the local structure of a word sequence -
both semantic and syntactic - but might face difficulty remembering long-range
dependencies. Intuitively, these long-range dependencies are of semantic
nature. In contrast, latent topic models are able to capture the global
underlying semantic structure of a document but do not account for word
ordering. The proposed TopicRNN model integrates the merits of RNNs and latent
topic models: it captures local (syntactic) dependencies using an RNN and
global (semantic) dependencies using latent topics. Unlike previous work on
contextual RNN language modeling, our model is learned end-to-end. Empirical
results on word prediction show that TopicRNN outperforms existing contextual
RNN baselines. In addition, TopicRNN can be used as an unsupervised feature
extractor for documents. We do this for sentiment analysis on the IMDB movie
review dataset and report an error rate of $6.28\%$. This is comparable to the
state-of-the-art $5.91\%$ resulting from a semi-supervised approach. Finally,
TopicRNN also yields sensible topics, making it a useful alternative to
document models such as latent Dirichlet allocation.



Datasets with hundreds of variables and many missing values are commonplace.
In this setting, it is both statistically and computationally challenging to
detect true predictive relationships between variables and also to suppress
false positives. This paper proposes an approach that combines probabilistic
programming, information theory, and non-parametric Bayes. It shows how to use
Bayesian non-parametric modeling to (i) build an ensemble of joint probability
models for all the variables; (ii) efficiently detect marginal independencies;
and (iii) estimate the conditional mutual information between arbitrary subsets
of variables, subject to a broad class of constraints. Users can access these
capabilities using BayesDB, a probabilistic programming platform for
probabilistic data analysis, by writing queries in a simple, SQL-like language.
This paper demonstrates empirically that the method can (i) detect
context-specific (in)dependencies on challenging synthetic problems and (ii)
yield improved sensitivity and specificity over baselines from statistics and
machine learning, on a real-world database of over 300 sparsely observed
indicators of macroeconomic development and public health.



We present an approach to sensorimotor control in immersive environments. Our
approach utilizes a high-dimensional sensory stream and a lower-dimensional
measurement stream. The cotemporal structure of these streams provides a rich
supervisory signal, which enables training a sensorimotor control model by
interacting with the environment. The model is trained using supervised
learning techniques, but without extraneous supervision. It learns to act based
on raw sensory input from a complex three-dimensional environment. The
presented formulation enables learning without a fixed goal at training time,
and pursuing dynamically changing goals at test time. We conduct extensive
experiments in three-dimensional simulations based on the classical
first-person game Doom. The results demonstrate that the presented approach
outperforms sophisticated prior formulations, particularly on challenging
tasks. The results also show that trained models successfully generalize across
environments and goals. A model trained using the presented approach won the
Full Deathmatch track of the Visual Doom AI Competition, which was held in
previously unseen environments.



Question answering (QA) has been the subject of a resurgence over the past
years. The said resurgence has led to a multitude of question answering (QA)
systems being developed both by companies and research facilities. While a few
components of QA systems get reused across implementations, most systems do not
leverage the full potential of component reuse. Hence, the development of QA
systems is currently still a tedious and time-consuming process. We address the
challenge of accelerating the creation of novel or tailored QA systems by
presenting a concept for a self-wiring approach to composing QA systems. Our
approach will allow the reuse of existing, web-based QA systems or modules
while developing new QA platforms. To this end, it will rely on QA modules
being described using the Web Ontology Language. Based on these descriptions,
our approach will be able to automatically compose QA systems using a
data-driven approach automatically.



Instability and variability of Deep Reinforcement Learning (DRL) algorithms
tend to adversely affect their performance. Averaged-DQN is a simple extension
to the DQN algorithm, based on averaging previously learned Q-values estimates,
which leads to a more stable training procedure and improved performance by
reducing approximation error variance in the target values. To understand the
effect of the algorithm, we examine the source of value function estimation
errors and provide an analytical comparison within a simplified model. We
further present experiments on the Arcade Learning Environment benchmark that
demonstrate significantly improved stability and performance due to the
proposed extension.



One of the classical problems in machine learning and data mining is feature
selection. A feature selection algorithm is expected to be quick, and at the
same time it should show high performance. MeLiF algorithm effectively solves
this problem using ensembles of ranking filters. This article describes two
different ways to improve MeLiF algorithm performance with parallelization.
Experiments show that proposed schemes significantly improves algorithm
performance and increase feature selection quality.



Many algorithms for data analysis exist, especially for classification
problems. To solve a data analysis problem, a proper algorithm should be
chosen, and also its hyperparameters should be selected. In this paper, we
present a new method for the simultaneous selection of an algorithm and its
hyperparameters. In order to do so, we reduced this problem to the multi-armed
bandit problem. We consider an algorithm as an arm and algorithm
hyperparameters search during a fixed time as the corresponding arm play. We
also suggest a problem-specific reward function. We performed the experiments
on 10 real datasets and compare the suggested method with the existing one
implemented in Auto-WEKA. The results show that our method is significantly
better in most of the cases and never worse than the Auto-WEKA.



We examine three probabilistic concepts related to the sentence "two
variables have no bearing on each other". We explore the relationships between
these three concepts and establish their relevance to the process of
constructing similarity networks---a tool for acquiring probabilistic knowledge
from human experts. We also establish a precise relationship between
connectedness in Bayesian networks and relevance in probability.



We introduce the hierarchical compositional network (HCN), a directed
generative model able to discover and disentangle, without supervision, the
building blocks of a set of binary images. The building blocks are binary
features defined hierarchically as a composition of some of the features in the
layer immediately below, arranged in a particular manner. At a high level, HCN
is similar to a sigmoid belief network with pooling. Inference and learning in
HCN are very challenging and existing variational approximations do not work
satisfactorily. A main contribution of this work is to show that both can be
addressed using max-product message passing (MPMP) with a particular schedule
(no EM required). Also, using MPMP as an inference engine for HCN makes new
tasks simple: adding supervision information, classifying images, or performing
inpainting all correspond to clamping some variables of the model to their
known values and running MPMP on the rest. When used for classification, fast
inference with HCN has exactly the same functional form as a convolutional
neural network (CNN) with linear activations and binary weights. However, HCN's
features are qualitatively very different.



We propose the Gaussian attention model for content-based neural memory
access. With the proposed attention model, a neural network has the additional
degree of freedom to control the focus of its attention from a laser sharp
attention to a broad attention. It is applicable whenever we can assume that
the distance in the latent space reflects some notion of semantics. We use the
proposed attention model as a scoring function for the embedding of a knowledge
base into a continuous vector space and then train a model that performs
question answering about the entities in the knowledge base. The proposed
attention model can handle both the propagation of uncertainty when following a
series of relations and also the conjunction of conditions in a natural way. On
a dataset of soccer players who participated in the FIFA World Cup 2014, we
demonstrate that our model can handle both path queries and conjunctive queries
well.



We formulate learning of a binary autoencoder as a biconvex optimization
problem which learns from the pairwise correlations between encoded and decoded
bits. Among all possible algorithms that use this information, ours finds the
autoencoder that reconstructs its inputs with worst-case optimal loss. The
optimal decoder is a single layer of artificial neurons, emerging entirely from
the minimax loss minimization, and with weights learned by convex optimization.
All this is reflected in competitive experimental results, demonstrating that
binary autoencoding can be done efficiently by conveying information in
pairwise correlations in an optimal fashion.



We consider the problem of density estimation on Riemannian manifolds.
Density estimation on manifolds has many applications in fluid-mechanics,
optics and plasma physics and it appears often when dealing with angular
variables (such as used in protein folding, robot limbs, gene-expression) and
in general directional statistics. In spite of the multitude of algorithms
available for density estimation in the Euclidean spaces $\mathbf{R}^n$ that
scale to large n (e.g. normalizing flows, kernel methods and variational
approximations), most of these methods are not immediately suitable for density
estimation in more general Riemannian manifolds. We revisit techniques related
to homeomorphisms from differential geometry for projecting densities to
sub-manifolds and use it to generalize the idea of normalizing flows to more
general Riemannian manifolds. The resulting algorithm is scalable, simple to
implement and suitable for use with automatic differentiation. We demonstrate
concrete examples of this method on the n-sphere $\mathbf{S}^n$.



Humans can learn concepts or recognize items from just a handful of examples,
while machines require many more samples to perform the same task. In this
paper, we build a computational model to investigate the possibility of this
kind of rapid learning. The proposed method aims to improve the learning task
of input from sensory memory by leveraging the information retrieved from
long-term memory. We present a simple and intuitive technique called cognitive
discriminative mappings (CDM) to explore the cognitive problem. First, CDM
separates and clusters the data instances retrieved from long-term memory into
distinct classes with a discrimination method in working memory when a sensory
input triggers the algorithm. CDM then maps each sensory data instance to be as
close as possible to the median point of the data group with the same class.
The experimental results demonstrate that the CDM approach is effective for
learning the discriminative features of supervised classifications with few
training sensory input instances.



We formulate sequence to sequence transduction as a noisy channel decoding
problem and use recurrent neural networks to parameterise the source and
channel models. Unlike direct models which can suffer from explaining-away
effects during training, noisy channel models must produce outputs that explain
their inputs, and their component models can be trained with not only paired
training samples but also unpaired samples from the marginal output
distribution. Using a latent variable to control how much of the conditioning
sequence the channel model needs to read in order to generate a subsequent
symbol, we obtain a tractable and effective beam search decoder. Experimental
results on abstractive sentence summarisation, morphological inflection, and
machine translation show that noisy channel models outperform direct models,
and that they significantly benefit from increased amounts of unpaired output
data that direct models cannot easily use.



Modeling the structure of coherent texts is a key NLP problem. The task of
coherently organizing a given set of sentences has been commonly used to build
and evaluate models that understand such structure. We propose an end-to-end
unsupervised deep learning approach based on the set-to-sequence framework to
address this problem. Our model strongly outperforms prior methods in the order
discrimination task and a novel task of ordering abstracts from scientific
articles. Furthermore, our work shows that useful text representations can be
obtained by learning to order sentences. Visualizing the learned sentence
representations shows that the model captures high-level logical structure in
paragraphs. Our representations perform comparably to state-of-the-art
pre-training methods on sentence similarity and paraphrase detection tasks.



Continuous optimization is an important problem in many areas of AI,
including vision, robotics, probabilistic inference, and machine learning.
Unfortunately, most real-world optimization problems are nonconvex, causing
standard convex techniques to find only local optima, even with extensions like
random restarts and simulated annealing. We observe that, in many cases, the
local modes of the objective function have combinatorial structure, and thus
ideas from combinatorial optimization can be brought to bear. Based on this, we
propose a problem-decomposition approach to nonconvex optimization. Similarly
to DPLL-style SAT solvers and recursive conditioning in probabilistic
inference, our algorithm, RDIS, recursively sets variables so as to simplify
and decompose the objective function into approximately independent
sub-functions, until the remaining functions are simple enough to be optimized
by standard techniques like gradient descent. The variables to set are chosen
by graph partitioning, ensuring decomposition whenever possible. We show
analytically that RDIS can solve a broad class of nonconvex optimization
problems exponentially faster than gradient descent with random restarts.
Experimentally, RDIS outperforms standard techniques on problems like structure
from motion and protein folding.



Deep reinforcement learning (deep RL) has been successful in learning
sophisticated behaviors automatically; however, the learning process requires a
huge number of trials. In contrast, animals can learn new tasks in just a few
trials, benefiting from their prior knowledge about the world. This paper seeks
to bridge this gap. Rather than designing a "fast" reinforcement learning
algorithm, we propose to represent it as a recurrent neural network (RNN) and
learn it from data. In our proposed method, RL$^2$, the algorithm is encoded in
the weights of the RNN, which are learned slowly through a general-purpose
("slow") RL algorithm. The RNN receives all information a typical RL algorithm
would receive, including observations, actions, rewards, and termination flags;
and it retains its state across episodes in a given Markov Decision Process
(MDP). The activations of the RNN store the state of the "fast" RL algorithm on
the current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both
small-scale and large-scale problems. On the small-scale side, we train it to
solve randomly generated multi-arm bandit problems and finite MDPs. After
RL$^2$ is trained, its performance on new MDPs is close to human-designed
algorithms with optimality guarantees. On the large-scale side, we test RL$^2$
on a vision-based navigation task and show that it scales up to
high-dimensional problems.



Acquiring your first language is an incredible feat and not easily
duplicated. Learning to communicate using nothing but a few pictureless books,
a corpus, would likely be impossible even for humans. Nevertheless, this is the
dominating approach in most natural language processing today. As an
alternative, we propose the use of situated interactions between agents as a
driving force for communication, and the framework of Deep Recurrent Q-Networks
for evolving a shared language grounded in the provided environment. We task
the agents with interactive image search in the form of the game Guess Who?.
The images from the game provide a non trivial environment for the agents to
discuss and a natural grounding for the concepts they decide to encode in their
communication. Our experiments show that the agents learn not only to encode
physical concepts in their words, i.e. grounding, but also that the agents
learn to hold a multi-step dialogue remembering the state of the dialogue from
step to step.



A new agent architecture called Limited Instruction Set Agent (LISA) is
introduced for autonomous control. The new architecture is based on previous
implementations of AgentSpeak and it is structurally simpler than its
predecessors with the aim of facilitating design-time and run-time verification
methods. The process of abstracting the LISA system to two different types of
discrete probabilistic models (DTMC and MDP) is investigated and illustrated.
The LISA system provides a tool for complete modelling of the agent and the
environment for probabilistic verification. The agent program can be
automatically compiled into a DTMC or a MDP model for verification with Prism.
The automatically generated Prism model can be used for both design-time and
run-time verification. The run-time verification is investigated and
illustrated in the LISA system as an internal modelling mechanism for
prediction of future outcomes.



Importance sampling is often used in machine learning when training and
testing data come from different distributions. In this paper we propose a new
variant of importance sampling that can reduce the variance of importance
sampling-based estimates by orders of magnitude when the supports of the
training and testing distributions differ. After motivating and presenting our
new importance sampling estimator, we provide a detailed theoretical analysis
that characterizes both its bias and variance relative to the ordinary
importance sampling estimator (in various settings, which include cases where
ordinary importance sampling is biased, while our new estimator is not, and
vice versa). We conclude with an example of how our new importance sampling
estimator can be used to improve estimates of how well a new treatment policy
for diabetes will work for an individual, using only data from when the
individual used a previous treatment policy.



This paper describes the USTC_NELSLIP systems submitted to the Trilingual
Entity Detection and Linking (EDL) track in 2016 TAC Knowledge Base Population
(KBP) contests. We have built two systems for entity discovery and mention
detection (MD): one uses the conditional RNNLM and the other one uses the
attention-based encoder-decoder framework. The entity linking (EL) system
consists of two modules: a rule based candidate generation and a neural
networks probability ranking model. Moreover, some simple string matching rules
are used for NIL clustering. At the end, our best system has achieved an F1
score of 0.624 in the end-to-end typed mention ceaf plus metric.



Most neural network models for document classification on social media focus
on text infor-mation to the neglect of other information on these platforms. In
this paper, we classify post stance on social media channels and develop UTCNN,
a neural network model that incorporates user tastes, topic tastes, and user
comments on posts. UTCNN not only works on social media texts, but also
analyzes texts in forums and message boards. Experiments performed on Chinese
Facebook data and English online debate forum data show that UTCNN achieves a
0.755 macro-average f-score for supportive, neutral, and unsupportive stance
classes on Facebook data, which is significantly better than models in which
either user, topic, or comment information is withheld. This model design
greatly mitigates the lack of data for the minor class without the use of
oversampling. In addition, UTCNN yields a 0.842 accuracy on English online
debate forum data, which also significantly outperforms results from previous
work as well as other deep learning models, showing that UTCNN performs well
regardless of language or platform.



Subjective questions such as `does neymar dive', or `is clinton lying', or
`is trump a fascist', are popular queries to web search engines, as can be seen
by autocompletion suggestions on Google, Yahoo and Bing. In the era of
cognitive computing, beyond search, they could be handled as hypotheses issued
for evaluation. Our vision is to leverage on unstructured data and metadata of
the rich user-generated multimedia that is often shared as material evidence in
favor or against hypotheses in social media platforms. In this paper we present
two preliminary experiments along those lines and discuss challenges for a
cognitive computing system that collects material evidence from user-generated
multimedia towards aggregating it into some form of collective decision on the
hypothesis.



Learning to navigate in complex environments with dynamic elements is an
important milestone in developing AI agents. In this work we formulate the
navigation question as a reinforcement learning problem and show that data
efficiency and task performance can be dramatically improved by relying on
additional auxiliary tasks leveraging multimodal sensory inputs. In particular
we consider jointly learning the goal-driven reinforcement learning problem
with auxiliary depth prediction and loop closure classification tasks. This
approach can learn to navigate from raw sensory input in complicated 3D mazes,
approaching human-level performance even under conditions where the goal
location changes frequently. We provide detailed analysis of the agent
behaviour, its ability to localise, and its network activity dynamics, showing
that the agent implicitly learns key navigation abilities.



Designing effective exploration-exploitation algorithms in Markov decision
processes (MDPs) with large state-action spaces is the main challenge in
reinforcement learning (RL). In fact, the learning performance degrades with
the number of states and actions in the MDP. However, MDPs often exhibit a
low-dimensional latent structure in practice, where a small hidden state is
observable through a possibly large number of observations. In this paper, we
study the setting of rich-observation Markov decision processes (\richmdp),
where hidden states are mapped to observations through an injective mapping, so
that an observation can be generated by only one hidden state. While this
mapping is unknown a priori, we introduce a spectral decomposition method that
consistently estimates how observations are clustered in the hidden states. The
estimated clustering is then integrated into an optimistic algorithm for RL
(UCRL), which operates on the smaller clustered space. The resulting algorithm
proceeds through phases and we show that its per-step regret (i.e., the
difference in cumulative reward between the algorithm and the optimal policy)
decreases as more observations are clustered together and finally, matches the
(ideal) performance of an RL algorithm running directly on the hidden MDP.



We propose a scalable approach to learn video-based question answering (QA):
answer a "free-form natural language question" about a video content. Our
approach automatically harvests a large number of videos and descriptions
freely available online. Then, a large number of candidate QA pairs are
automatically generated from descriptions rather than manually annotated. Next,
we use these candidate QA pairs to train a number of video-based QA methods
extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et
al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect
candidate QA pairs, we propose a self-paced learning procedure to iteratively
identify them and mitigate their effects in training. Finally, we evaluate
performance on manually generated video-based QA pairs. The results show that
our self-paced learning procedure is effective, and the extended SS model
outperforms various baselines.



We consider the problem of identifying the causal direction between two
discrete random variables using observational data. Unlike previous work, we
keep the most general functional model but make an assumption on the unobserved
exogenous variable: Inspired by Occam's razor, we assume that the exogenous
variable is simple in the true causal direction. We quantify simplicity using
R\'enyi entropy. Our main result is that, under natural assumptions, if the
exogenous variable has low $H_0$ entropy (cardinality) in the true direction,
it must have high $H_0$ entropy in the wrong direction. We establish several
algorithmic hardness results about estimating the minimum entropy exogenous
variable. We show that the problem of finding the exogenous variable with
minimum entropy is equivalent to the problem of finding minimum joint entropy
given $n$ marginal distributions, also known as minimum entropy coupling
problem. We propose an efficient greedy algorithm for the minimum entropy
coupling problem, that for $n=2$ provably finds a local optimum. This gives a
greedy algorithm for finding the exogenous variable with minimum $H_1$ (Shannon
Entropy). Our greedy entropy-based causal inference algorithm has similar
performance to the state of the art additive noise models in real datasets. One
advantage of our approach is that we make no use of the values of random
variables but only their distributions. Our method can therefore be used for
causal inference for both ordinal and also categorical data, unlike additive
noise models.



The domain of single crossing preference profiles is a widely studied domain
in social choice theory. It has been generalized to the domain of single
crossing preference profiles with respect to trees which inherits many
desirable properties from the single crossing domain, for example, transitivity
of majority relation, existence of polynomial time algorithms for finding
winners of Kemeny voting rule, etc. In this paper, we consider a further
generalization of the domain of single crossing profiles on trees to the domain
consisting of all preference profiles which can be extended to single crossing
preference profiles with respect to some tree by adding more preferences to it.
We call this domain the weakly single crossing domain on trees. We present a
polynomial time algorithm for recognizing weakly single crossing profiles on
trees. We then move on to develop a polynomial time algorithm with low query
complexity for eliciting weakly single crossing profiles on trees even when we
do not know any tree with respect to which the closure of the input profile is
single crossing and the preferences can be queried only sequentially; moreover,
the sequential order is also unknown. We complement the performance of our
preference elicitation algorithm by proving that our algorithm makes an optimal
number of queries up to constant factors when the number of preferences is
large compared to the number of candidates, even if the input profile is known
to be single crossing with respect to some given tree and the preferences can
be accessed randomly.



We propose an approach to build a neural machine translation system with no
supervised resources (i.e., no parallel corpora) using multimodal embedded
representation over texts and images. Based on the assumption that text
documents are often likely to be described with other multimedia information
(e.g., images) somewhat related to the content, we try to indirectly estimate
the relevance between two languages. Using multimedia as the "pivot", we
project all modalities into one common hidden space where samples belonging to
similar semantic concepts should come close to each other, whatever the
observed space of each sample is. This modality-agnostic representation is the
key to bridging the gap between different modalities. Putting a decoder on top
of it, our network can flexibly draw the outputs from any input modality.
Notably, in the testing phase, we need only source language texts as the input
for translation. In experiments, we tested our method on two benchmarks to show
that it can achieve reasonable translation performance. We compared and
investigated several possible implementations and found that an end-to-end
model that simultaneously optimized both rank loss in multimodal encoders and
cross-entropy loss in decoders performed the best.



Max-cut, clustering, and many other partitioning problems that are of
significant importance to machine learning and other scientific fields are
NP-hard, a reality that has motivated researchers to develop a wealth of
approximation algorithms and heuristics. Although the best algorithm to use
typically depends on the specific application domain, a worst-case analysis is
often used to compare algorithms. This may be misleading if worst-case
instances occur infrequently, and thus there is a demand for optimization
methods which return the algorithm configuration best suited for the given
application's typical inputs. We address this problem for clustering, max-cut,
and other partitioning problems, such as integer quadratic programming, by
designing computationally efficient and sample efficient learning algorithms
which receive samples from an application-specific distribution over problem
instances and learn a partitioning algorithm with high expected performance.
Our algorithms learn over common integer quadratic programming and clustering
algorithm families: SDP rounding algorithms and agglomerative clustering
algorithms with dynamic programming. For our sample complexity analysis, we
provide tight bounds on the pseudodimension of these algorithm classes, and
show that surprisingly, even for classes of algorithms parameterized by a
single parameter, the pseudo-dimension is superconstant. In this way, our work
both contributes to the foundations of algorithm configuration and pushes the
boundaries of learning theory, since the algorithm classes we analyze consist
of multi-stage optimization procedures and are significantly more complex than
classes typically studied in learning theory.



Recent studies on knowledge base completion, the task of recovering missing
facts based on observed facts, demonstrate the importance of learning
embeddings from multi-step relations. Due to the size of knowledge bases,
previous works manually design relation paths of observed triplets in symbolic
space (e.g. random walk) to learn multi-step relations during training.
However, these approaches suffer some limitations as most paths are not
informative, and it is prohibitively expensive to consider all possible paths.
To address the limitations, we propose learning to traverse in vector space
directly without the need of symbolic space guidance. To remember the
connections between related observed triplets and be able to adaptively change
relation paths in vector space, we propose Implicit ReasoNets (IRNs), that is
composed of a global memory and a controller module to learn multi-step
relation paths in vector space and infer missing facts jointly without any
human-designed procedure. Without using any axillary information, our proposed
model achieves state-of-the-art results on popular knowledge base completion
benchmarks.



Real-time parking occupancy information is critical for a parking management
system to facilitate drivers to park more efficiently. Recent advances in
connected and automated vehicle technologies enable sensor-equipped cars (probe
cars) to detect and broadcast available parking spaces when driving through
parking lots. In this paper, we evaluate the impact of market penetration of
probe cars on the system performance, and investigate different parking
guidance policies to improve the data acquisition process. We adopt a
simulation-based approach to impose four policies on an off- street parking lot
influencing the behavior of probe cars to park in assigned parking spaces. This
in turn effects the scanning route and the parking space occupancy estimations.
The last policy we propose is a near-optimal guidance strategy that maximizes
the information gain of posteriors. The results suggest that an efficient
information gathering policy can compensate for low penetration of connected
and automated vehicles. We also highlight the policy trade-off that occur while
attempting to maximize information gain through explorations and improve
assignment accuracy through exploitations. Our results can assist urban policy
makers in designing and managing smart parking systems.



Recent years have witnessed increasing interest in the potential benefits of
`intelligent' autonomous machines such as robots. Honda's Asimo humanoid robot,
iRobot's Roomba robot vacuum cleaner and Google's driverless cars have fired
the imagination of the general public, and social media buzz with speculation
about a utopian world of helpful robot assistants or the coming robot
apocalypse! However, there is a long way to go before autonomous systems reach
the level of capabilities required for even the simplest of tasks involving
human-robot interaction - especially if it involves communicative behaviour
such as speech and language. Of course the field of Artificial Intelligence
(AI) has made great strides in these areas, and has moved on from abstract
high-level rule-based paradigms to embodied architectures whose operations are
grounded in real physical environments. What is still missing, however, is an
overarching theory of intelligent communicative behaviour that informs
system-level design decisions in order to provide a more coherent approach to
system integration. This chapter introduces the beginnings of such a framework
inspired by the principles of Perceptual Control Theory (PCT). In particular,
it is observed that PCT has hitherto tended to view perceptual processes as a
relatively straightforward series of transformations from sensation to
perception, and has overlooked the potential of powerful generative model-based
solutions that have emerged in practical fields such as visual or auditory
scene analysis. Starting from first principles, a sequence of arguments is
presented which not only shows how these ideas might be integrated into PCT,
but which also extend PCT towards a remarkably symmetric architecture for a
needs-driven communicative agent. It is concluded that, if behaviour is the
control of perception, then perception is the simulation of behaviour.



Creating aesthetically pleasing pieces of art, including music, has been a
long-term goal for artificial intelligence research. Despite recent successes
of long-short term memory (LSTM) recurrent neural networks (RNNs) in sequential
learning, LSTM neural networks have not, by themselves, been able to generate
natural-sounding music conforming to music theory. To transcend this
inadequacy, we put forward a novel method for music composition that combines
the LSTM with Grammars motivated by music theory. The main tenets of music
theory are encoded as grammar argumented (GA) filters on the training data,
such that the machine can be trained to generate music inheriting the
naturalness of human-composed pieces from the original dataset while adhering
to the rules of music theory. Unlike previous approaches, pitches and durations
are encoded as one semantic entity, which we refer to as note-level encoding.
This allows easy implementation of music theory grammars, as well as closer
emulation of the thinking pattern of a musician. Although the GA rules are
applied to the training data and never directly to the LSTM music generation,
our machine still composes music that possess high incidences of diatonic scale
notes, small pitch intervals and chords, in deference to music theory.



Part of the appeal of Visual Question Answering (VQA) is its promise to
answer new questions about previously unseen images. Most current methods
demand training questions that illustrate every possible concept, and will
therefore never achieve this capability, since the volume of required training
data would be prohibitive. Answering general questions about images requires
methods capable of Zero-Shot VQA, that is, methods able to answer questions
beyond the scope of the training questions. We propose a new evaluation
protocol for VQA methods which measures their ability to perform Zero-Shot VQA,
and in doing so highlights significant practical deficiencies of current
approaches, some of which are masked by the biases in current datasets. We
propose and evaluate several strategies for achieving Zero-Shot VQA, including
methods based on pretrained word embeddings, object classifiers with semantic
embeddings, and test-time retrieval of example images. Our extensive
experiments are intended to serve as baselines for Zero-Shot VQA, and they also
achieve state-of-the-art performance in the standard VQA evaluation setting.



In recent years deep reinforcement learning (RL) systems have attained
superhuman performance in a number of challenging task domains. However, a
major limitation of such applications is their demand for massive amounts of
training data. A critical present objective is thus to develop deep RL methods
that can adapt rapidly to new tasks. In the present work we introduce a novel
approach to this challenge, which we refer to as deep meta-reinforcement
learning. Previous work has shown that recurrent networks can support
meta-learning in a fully supervised context. We extend this approach to the RL
setting. What emerges is a system that is trained using one RL algorithm, but
whose recurrent dynamics implement a second, quite separate RL procedure. This
second, learned RL algorithm can differ from the original one in arbitrary
ways. Importantly, because it is learned, it is configured to exploit structure
in the training domain. We unpack these points in a series of seven
proof-of-concept experiments, each of which examines a key aspect of deep
meta-RL. We consider prospects for extending and scaling up the approach, and
also point out some potentially important implications for neuroscience.



At the core of interpretable machine learning is the question of whether
humans are able to make accurate predictions about a model's behavior. Assumed
in this question are three properties of the interpretable output: coverage,
precision, and effort. Coverage refers to how often humans think they can
predict the model's behavior, precision to how accurate humans are in those
predictions, and effort is either the up-front effort required in interpreting
the model, or the effort required to make predictions about a model's behavior.
  In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that
produces high-precision rule-based explanations for which the coverage
boundaries are very clear. We compare aLIME to linear LIME with simulated
experiments, and demonstrate the flexibility of aLIME with qualitative examples
from a variety of domains and tasks.



Training deep neural networks for solving machine learning problems is one
great challenge in the field, mainly due to its associated optimisation problem
being highly non-convex. Recent developments have suggested that many training
algorithms do not suffer from undesired local minima under certain scenario,
and consequently led to great efforts in pursuing mathematical explanations for
such observations. This work provides an alternative mathematical understanding
of the challenge from a smooth optimisation perspective. By assuming exact
learning of finite samples, sufficient conditions are identified via a critical
point analysis to ensure any local minimum to be globally minimal as well.
Furthermore, a state of the art algorithm, known as the Generalised
Gauss-Newton (GGN) algorithm, is rigorously revisited as an approximate
Newton's algorithm, which shares the property of being locally quadratically
convergent to a global minimum under the condition of exact learning.



Monte Carlo Tree Search (MCTS) is a technique to guide search in a large
decision space by taking random samples and evaluating their outcome. In this
work, we study MCTS methods in the context of the connection calculus and
implement them on top of the leanCoP prover. This includes proposing useful
proof-state evaluation heuristics that are learned from previous proofs, and
proposing and automatically improving suitable MCTS strategies in this context.
The system is trained and evaluated on a large suite of related problems coming
from the Mizar proof assistant, showing that it is capable to find new and
different proofs. To our knowledge, this is the first time MCTS has been
applied to theorem proving.



Gaussian processes (GP) provide a prior over functions and allow finding
complex regularities in data. Gaussian processes are successfully used for
classification/regression problems and dimensionality reduction. In this work
we consider the classification problem only. The complexity of standard methods
for GP-classification scales cubically with the size of the training dataset.
This complexity makes them inapplicable to big data problems. Therefore, a
variety of methods were introduced to overcome this limitation. In the paper we
focus on methods based on so called inducing inputs. This approach is based on
variational inference and proposes a particular lower bound for marginal
likelihood (evidence). This bound is then maximized w.r.t. parameters of kernel
function of the Gaussian process, thus fitting the model to data. The
computational complexity of this method is $O(nm^2)$, where $m$ is the number
of inducing inputs used by the model and is assumed to be substantially smaller
than the size of the dataset $n$. Recently, a new evidence lower bound for
GP-classification problem was introduced. It allows using stochastic
optimization, which makes it suitable for big data problems. However, the new
lower bound depends on $O(m^2)$ variational parameter, which makes optimization
challenging in case of big m. In this work we develop a new approach for
training inducing input GP models for classification problems. Here we use
quadratic approximation of several terms in the aforementioned evidence lower
bound, obtaining analytical expressions for optimal values of most of the
parameters in the optimization, thus sufficiently reducing the dimension of
optimization space. In our experiments we achieve as well or better results,
compared to the existing method. Moreover, our method doesn't require the user
to manually set the learning rate, making it more practical, than the existing
method.



In order to be useful, visualizations need to be interpretable. This paper
uses a user-based approach to combine and assess quality measures in order to
better model user preferences. Results show that cluster separability measures
are outperformed by a neighborhood conservation measure, even though the former
are usually considered as intuitively representative of user motives. Moreover,
combining measures, as opposed to using a single measure, further improves
prediction performances.



Recurrent neural networks (RNNs) have been used extensively and with
increasing success to model various types of sequential data. Much of this
progress has been achieved through devising recurrent units and architectures
with the flexibility to capture complex statistics in the data, such as long
range dependency or localized attention phenomena. However, while many
sequential data (such as video, speech or language) can have highly variable
information flow, most recurrent models still consume input features at a
constant rate and perform a constant number of computations per time step,
which can be detrimental to both speed and model capacity. In this paper, we
explore a modification to existing recurrent units which allows them to learn
to vary the amount of computation they perform at each step, without prior
knowledge of the sequence's time structure. We show experimentally that not
only do our models require fewer operations, they also lead to better
performance overall on evaluation tasks.



A directed graph where there is exactly one edge between every pair of
vertices is called a {\em tournament}. Finding the "best" set of vertices of a
tournament is a well studied problem in social choice theory. A {\em tournament
solution} takes a tournament as input and outputs a subset of vertices of the
input tournament. However, in many applications, for example, choosing the best
set of drugs from a given set of drugs, the edges of the tournament are given
only implicitly and knowing the orientation of an edge is costly. In such
scenarios, we would like to know the best set of vertices (according to some
tournament solution) by "querying" as few edges as possible. We, in this paper,
precisely study this problem for commonly used tournament solutions: given an
oracle access to the edges of a tournament T, find $f(T)$ by querying as few
edges as possible, for a tournament solution f. We first show that the set of
Condorcet non-losers in a tournament can be found by querying $2n-\lfloor \log
n \rfloor -2$ edges only and this is tight in the sense that every algorithm
for finding the set of Condorcet non-losers needs to query at least $2n-\lfloor
\log n \rfloor -2$ edges in the worst case, where $n$ is the number of vertices
in the input tournament. We then move on to study other popular tournament
solutions and show that any algorithm for finding the Copeland set, the Slater
set, the Markov set, the bipartisan set, the uncovered set, the Banks set, and
the top cycle must query $\Omega(n^2)$ edges in the worst case. On the positive
side, we are able to circumvent our strong query complexity lower bound results
by proving that, if the size of the top cycle of the input tournament is at
most $k$, then we can find all the tournament solutions mentioned above by
querying $O(nk + \frac{n\log n}{\log(1-\frac{1}{k})})$ edges only.



In this paper we introduce a model of lifelong learning, based on a Network
of Experts. New tasks / experts are learned and added to the model
sequentially, building on what was learned before. To ensure scalability of
this process,data from previous tasks cannot be stored and hence is not
available when learning a new task. A critical issue in such context, not
addressed in the literature so far, relates to the decision which expert to
deploy at test time. We introduce a set of gating autoencoders that learn a
representation for the task at hand, and, at test time, automatically forward
the test sample to the relevant expert. This also brings memory efficiency as
only one expert network has to be loaded into memory at any given time.
Further, the autoencoders inherently capture the relatedness of one task to
another, based on which the most relevant prior model to be used for training a
new expert, with finetuning or learning without-forgetting, can be selected. We
evaluate our method on image classification and video prediction problems.



Researchers have recently started investigating deep neural networks for
dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq)
models have shown promising results for unstructured tasks, such as word-level
dialogue response generation. The hope is that such models will be able to
leverage massive amounts of data to learn meaningful natural language
representations and response generation strategies, while requiring a minimum
amount of domain knowledge and hand-crafting. An important challenge is to
develop models that can effectively incorporate dialogue context and generate
meaningful and diverse responses. In support of this goal, we review recently
proposed models based on generative encoder-decoder neural network
architectures, and show that these models have better ability to incorporate
long-term dialogue history, to model uncertainty and ambiguity in dialogue, and
to generate responses with high-level compositional structure.



Structural causal models (SCMs), also known as non-parametric structural
equation models (NP-SEMs), are widely used for causal modeling purposes. In
this paper, we give a rigorous treatment of structural causal models, dealing
with measure-theoretic complications that arise in the presence of cyclic
relations. The central question studied in this paper is: given a (possibly
cyclic) SCM defined on a large system (consisting of observable endogenous and
latent exogenous variables), can we "project it down" to an SCM that describes
a subsystem (consisting of a subset of the observed endogenous variables and
possibly different latent exogenous variables) in order to obtain a more
parsimonious but equivalent representation of the subsystem? We define a
marginalization operation that effectively removes a subset of the endogenous
variables from the model, and a class of mappings, exogenous
reparameterizations, that can be used to reduce the space of exogenous
variables. We show that both operations preserve the causal semantics of the
model and that under mild conditions they can lead to a significant reduction
of the model complexity, at least in terms of the number of variables in the
model. We argue that for the task of estimating an SCM from data, the existence
of "smooth" reductions would be desirable. We provide several conditions under
which the existence of such reductions can be shown, but also provide a
counterexample that shows that such reductions do not exist in general. The
latter result implies that existing approaches to estimate linear or Markovian
SCMs from data cannot be extended to general SCMs.



Credit card plays a very important rule in today's economy. It becomes an
unavoidable part of household, business and global activities. Although using
credit cards provides enormous benefits when used carefully and
responsibly,significant credit and financial damages may be caused by
fraudulent activities. Many techniques have been proposed to confront the
growth in credit card fraud. However, all of these techniques have the same
goal of avoiding the credit card fraud; each one has its own drawbacks,
advantages and characteristics. In this paper, after investigating difficulties
of credit card fraud detection, we seek to review the state of the art in
credit card fraud detection techniques, data sets and evaluation criteria.The
advantages and disadvantages of fraud detection methods are enumerated and
compared.Furthermore, a classification of mentioned techniques into two main
fraud detection approaches, namely, misuses (supervised) and anomaly detection
(unsupervised) is presented. Again, a classification of techniques is proposed
based on capability to process the numerical and categorical data sets.
Different data sets used in literature are then described and grouped into real
and synthesized data and the effective and common attributes are extracted for
further usage.Moreover, evaluation employed criterions in literature are
collected and discussed.Consequently, open issues for credit card fraud
detection are explained as guidelines for new researchers.



It is critical for advanced manufacturing machines to autonomously execute a
task by following an end-user's natural language (NL) instructions. However, NL
instructions are usually ambiguous and abstract so that the machines may
misunderstand and incorrectly execute the task. To address this NL-based
human-machine communication problem and enable the machines to appropriately
execute tasks by following the end-user's NL instructions, we developed a
Machine-Executable-Plan-Generation (exePlan) method. The exePlan method
conducts task-centered semantic analysis to extract task-related information
from ambiguous NL instructions. In addition, the method specifies machine
execution parameters to generate a machine-executable plan by interpreting
abstract NL instructions. To evaluate the exePlan method, an industrial robot
Baxter was instructed by NL to perform three types of industrial tasks {'drill
a hole', 'clean a spot', 'install a screw'}. The experiment results proved that
the exePlan method was effective in generating machine-executable plans from
the end-user's NL instructions. Such a method has the promise to endow a
machine with the ability of NL-instructed task execution.



In the classical cake cutting problem, a resource must be divided among
agents with different utilities so that each agent believes they have received
a fair share of the resource relative to the other agents. We introduce a
variant of the problem in which we model an underlying social network on the
agents with a graph, and agents only evaluate their shares relative to their
neighbors' in the network. This formulation captures many situations in which
it is unrealistic to assume a global view, and also exposes interesting
phenomena in the original problem.
  Specifically, we say an allocation is locally envy-free if no agent envies a
neighbor's allocation and locally proportional if each agent values her own
allocation as much as the average value of her neighbor's allocations, with the
former implying the latter. While global envy-freeness implies local
envy-freeness, global proportionality does not imply local proportionality, or
vice versa. A general result is that for any two distinct graphs on the same
set of nodes and an allocation, there exists a set of valuation functions such
that the allocation is locally proportional on one but not the other.
  We fully characterize the set of graphs for which an oblivious single-cutter
protocol-- a protocol that uses a single agent to cut the cake into pieces
--admits a bounded protocol with $O(n^2)$ query complexity for locally
envy-free allocations in the Robertson-Webb model. We also consider the price
of envy-freeness, which compares the total utility of an optimal allocation to
the best utility of an allocation that is envy-free. We show that a lower bound
of $\Omega(\sqrt{n})$ on the price of envy-freeness for global allocations in
fact holds for local envy-freeness in any connected undirected graph. Thus,
sparse graphs surprisingly do not provide more flexibility with respect to the
quality of envy-free allocations.



Deep Neural Networks often require good regularizers to generalize well.
Dropout is one such regularizer that is widely used among Deep Learning
practitioners. Recent work has shown that Dropout can also be viewed as
performing Approximate Bayesian Inference over the network parameters. In this
work, we generalize this notion and introduce a rich family of regularizers
which we call Generalized Dropout. One set of methods in this family, called
Dropout++, is a version of Dropout with trainable parameters. Classical Dropout
emerges as a special case of this method. Another member of this family selects
the width of neural network layers. Experiments show that these methods help in
improving generalization performance over Dropout.



Many prediction problems can be phrased as inferences over local
neighborhoods of graphs. The graph represents the interaction between entities,
and the neighborhood of each entity contains information that allows the
inferences or predictions. We present an approach for applying machine learning
directly to such graph neighborhoods, yielding predicitons for graph nodes on
the basis of the structure of their local neighborhood and the features of the
nodes in it. Our approach allows predictions to be learned directly from
examples, bypassing the step of creating and tuning an inference model or
summarizing the neighborhoods via a fixed set of hand-crafted features. The
approach is based on a multi-level architecture built from Long Short-Term
Memory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood
from data. We demonstrate the effectiveness of the proposed technique on a
synthetic example and on real-world data related to crowdsourced grading,
Bitcoin transactions, and Wikipedia edit reversions.



Survival analysis is a fundamental tool in medical research to identify
predictors of adverse events and develop systems for clinical decision support.
In order to leverage large amounts of patient data, efficient optimisation
routines are paramount. We propose an efficient training algorithm for the
kernel survival support vector machine (SSVM). We directly optimise the primal
objective function and employ truncated Newton optimisation and order statistic
trees to significantly lower computational costs compared to previous training
algorithms, which require $O(n^4)$ space and $O(p n^6)$ time for datasets with
$n$ samples and $p$ features. Our results demonstrate that our proposed
optimisation scheme allows analysing data of a much larger scale with no loss
in prediction performance. Experiments on synthetic and 5 real-world datasets
show that our technique outperforms existing kernel SSVM formulations if the
amount of right censoring is high ($\geq85\%$), and performs comparably
otherwise.



Reinforcement learning is concerned with identifying reward-maximizing
behaviour policies in environments that are initially unknown. State-of-the-art
reinforcement learning approaches, such as deep Q-networks, are model-free and
learn to act effectively across a wide range of environments such as Atari
games, but require huge amounts of data. Model-based techniques are more
data-efficient, but need to acquire explicit knowledge about the environment.
  In this paper, we take a step towards using model-based techniques in
environments with a high-dimensional visual state space by demonstrating that
it is possible to learn system dynamics and the reward structure jointly. Our
contribution is to extend a recently developed deep neural network for video
frame prediction in Atari games to enable reward prediction as well. To this
end, we phrase a joint optimization problem for minimizing both video frame and
reward reconstruction loss, and adapt network parameters accordingly. Empirical
evaluations on five Atari games demonstrate accurate cumulative reward
prediction of up to 200 frames. We consider these results as opening up
important directions for model-based reinforcement learning in complex,
initially unknown environments.



Limbo is an open-source C++11 library for Bayesian optimization which is
designed to be both highly flexible and very fast. It can be used to optimize
functions for which the gradient is unknown, evaluations are expensive, and
runtime cost matters (e.g., on embedded systems or robots). Benchmarks on
standard functions show that Limbo is about 2 times faster than BayesOpt
(another C++ library) for a similar accuracy.



Complex problems may require sophisticated, non-linear learning methods such
as kernel machines or deep neural networks to achieve state of the art
prediction accuracies. However, high prediction accuracies are not the only
objective to consider when solving problems using machine learning. Instead,
particular scientific applications require some explanation of the learned
prediction function. Unfortunately, most methods do not come with out of the
box straight forward interpretation. Even linear prediction functions are not
straight forward to explain if features exhibit complex correlation structure.
  In this paper, we propose the Measure of Feature Importance (MFI). MFI is
general and can be applied to any arbitrary learning machine (including kernel
machines and deep learning). MFI is intrinsically non-linear and can detect
features that by itself are inconspicuous and only impact the prediction
function through their interaction with other features. Lastly, MFI can be used
for both --- model-based feature importance and instance-based feature
importance (i.e, measuring the importance of a feature for a particular data
point).



Recent work in model-agnostic explanations of black-box machine learning has
demonstrated that interpretability of complex models does not have to come at
the cost of accuracy or model flexibility. However, it is not clear what kind
of explanations, such as linear models, decision trees, and rule lists, are the
appropriate family to consider, and different tasks and models may benefit from
different kinds of explanations. Instead of picking a single family of
representations, in this work we propose to use "programs" as model-agnostic
explanations. We show that small programs can be expressive yet intuitive as
explanations, and generalize over a number of existing interpretable families.
We propose a prototype program induction method based on simulated annealing
that approximates the local behavior of black-box classifiers around a specific
prediction using random perturbations. Finally, we present preliminary
application on small datasets and show that the generated explanations are
intuitive and accurate for a number of classifiers.



Markov logic networks (MLNs) reconcile two opposing schools in machine
learning and artificial intelligence: causal networks, which account for
uncertainty extremely well, and first-order logic, which allows for formal
deduction. An MLN is essentially a first-order logic template to generate
Markov networks. Inference in MLNs is probabilistic and it is often performed
by approximate methods such as Markov chain Monte Carlo (MCMC) Gibbs sampling.
An MLN has many regular, symmetric structures that can be exploited at both
first-order level and in the generated Markov network. We analyze the graph
structures that are produced by various lifting methods and investigate the
extent to which quantum protocols can be used to speed up Gibbs sampling with
state preparation and measurement schemes. We review different such approaches,
discuss their advantages, theoretical limitations, and their appeal to
implementations. We find that a straightforward application of a recent result
yields exponential speedup compared to classical heuristics in approximate
probabilistic inference, thereby demonstrating another example where advanced
quantum resources can potentially prove useful in machine learning.



To enhance developer productivity, all modern integrated development
environments (IDEs) include code suggestion functionality that proposes likely
next tokens at the cursor. While current IDEs work well for statically-typed
languages, their reliance on type annotations means that they do not provide
the same level of support for dynamic programming languages as for
statically-typed languages. Moreover, suggestion engines in modern IDEs do not
propose expressions or multi-statement idiomatic code. Recent work has shown
that language models can improve code suggestion systems by learning from
software repositories. This paper introduces a neural language model with a
sparse pointer network aimed at capturing very long-range dependencies. We
release a large-scale code suggestion corpus of 41M lines of Python code
crawled from GitHub. On this corpus, we found standard neural language models
to perform well at suggesting local phenomena, but struggle to refer to
identifiers that are introduced many tokens in the past. By augmenting a neural
language model with a pointer network specialized in referring to predefined
classes of identifiers, we obtain a much lower perplexity and a 5 percentage
points increase in accuracy for code suggestion compared to an LSTM baseline.
In fact, this increase in code suggestion accuracy is due to a 13 times more
accurate prediction of identifiers. Furthermore, a qualitative analysis shows
this model indeed captures interesting long-range dependencies, like referring
to a class member defined over 60 tokens in the past.



Multivariate Pattern (MVP) classification can map different cognitive states
to the brain tasks. One of the main challenges in MVP analysis is validating
the generated results across subjects. However, analyzing multi-subject fMRI
data requires accurate functional alignments between neuronal activities of
different subjects, which can rapidly increase the performance and robustness
of the final results. Hyperalignment (HA) is one of the most effective
functional alignment methods, which can be mathematically formulated by the
Canonical Correlation Analysis (CCA) methods. Since HA mostly uses the
unsupervised CCA techniques, its solution may not be optimized for MVP
analysis. By incorporating the idea of Local Discriminant Analysis (LDA) into
CCA, this paper proposes Local Discriminant Hyperalignment (LDHA) as a novel
supervised HA method, which can provide better functional alignment for MVP
analysis. Indeed, the locality is defined based on the stimuli categories in
the train-set, where the correlation between all stimuli in the same category
will be maximized and the correlation between distinct categories of stimuli
approaches to near zero. Experimental studies on multi-subject MVP analysis
confirm that the LDHA method achieves superior performance to other
state-of-the-art HA algorithms.



Training robots to perceive, act and communicate using multiple modalities
still represents a challenging problem, particularly if robots are expected to
learn efficiently from small sets of example interactions. We describe a
learning approach as a step in this direction, where we teach a humanoid robot
how to play the game of noughts and crosses. Given that multiple multimodal
skills can be trained to play this game, we focus our attention to training the
robot to perceive the game, and to interact in this game. Our multimodal deep
reinforcement learning agent perceives multimodal features and exhibits verbal
and non-verbal actions while playing. Experimental results using simulations
show that the robot can learn to win or draw up to 98% of the games. A pilot
test of the proposed multimodal system for the targeted game---integrating
speech, vision and gestures---reports that reasonable and fluent interactions
can be achieved using the proposed approach.



We introduce the task of Visual Dialog, which requires an AI agent to hold a
meaningful dialog with humans in natural, conversational language about visual
content. Specifically, given an image, a dialog history, and a question about
the image, the agent has to ground the question in image, infer context from
history, and answer the question accurately. Visual Dialog is disentangled
enough from a specific downstream task so as to serve as a general test of
machine intelligence, while being grounded in vision enough to allow objective
evaluation of individual responses and benchmark progress. We develop a novel
two-person chat data-collection protocol to curate a large-scale Visual Dialog
dataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10
question-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog
question-answer pairs.
  We introduce a family of neural encoder-decoder models for Visual Dialog with
3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --
and 2 decoders (generative and discriminative), which outperform a number of
sophisticated baselines. We propose a retrieval-based evaluation protocol for
Visual Dialog where the AI agent is asked to sort a set of candidate answers
and evaluated on metrics such as mean-reciprocal-rank of human response. We
quantify gap between machine and human performance on the Visual Dialog task
via human studies. Putting it all together, we demonstrate the first 'visual
chatbot'! Our dataset, code, trained models and visual chatbot are available on
https://visualdialog.org



We establish a link between multiwinner elections and apportionment problems
by showing how approval-based multiwinner election rules can be interpreted as
methods of apportionment. We consider several multiwinner rules and observe
that they induce apportionment methods that are well-established in the
literature on proportional representation. For instance, we show that
Proportional Approval Voting induces the D'Hondt method and that Monroe's rule
induces the largest reminder method. We also consider properties of
apportionment methods and exhibit multiwinner rules that induce apportionment
methods satisfying these properties.



Inventing targeted proof search strategies for specific problem sets is a
difficult task. State-of-the-art automated theorem provers (ATPs) such as E
allow a large number of user-specified proof search strategies described in a
rich domain specific language. Several machine learning methods that invent
strategies automatically for ATPs were proposed previously. One of them is the
Blind Strategymaker (BliStr), a system for automated invention of ATP
strategies.
  In this paper we introduce BliStrTune -- a hierarchical extension of BliStr.
BliStrTune allows exploring much larger space of E strategies by interleaving
search for high-level parameters with their fine-tuning. We use BliStrTune to
invent new strategies based also on new clause weight functions targeted at
problems from large ITP libraries. We show that the new strategies
significantly improve E's performance in solving problems from the Mizar
Mathematical Library.



We introduce the BIN_COUNTS constraint, which deals with the problem of
counting the number of decision variables in a set which are assigned values
that lie in given bins. We illustrate a decomposition and a filtering algorithm
that achieves generalised arc consistency. We contrast the filtering power of
these two approaches and we discuss a number of applications. We show that
BIN_COUNTS can be employed to develop a decomposition for the $\chi^2$ test
constraint, a new statistical constraint that we introduce in this work. We
also show how this new constraint can be employed in the context of the
Balanced Academic Curriculum Problem and of the Balanced Nursing Workload
Problem. For both these problems we carry out numerical studies involving our
reformulations. Finally, we present a further application of the $\chi^2$ test
constraint in the context of confidence interval analysis.



This paper addresses the task of set prediction using deep learning. This is
important because the output of many computer vision tasks, including image
tagging and object detection, are naturally expressed as sets of entities
rather than vectors. As opposed to a vector, the size of a set is not fixed in
advance, and it is invariant to the ordering of entities within it. We define a
likelihood for a set distribution and learn its parameters using a deep neural
network. We also derive a loss for predicting a discrete distribution
corresponding to set cardinality. Set prediction is demonstrated on the problem
of multi-class image classification. Moreover, we show that the proposed
cardinality loss can also trivially be applied to the tasks of object counting
and pedestrian detection. Our approach outperforms existing methods in all
three cases on standard datasets.



With regard to a computational representation of literary plot, this paper
looks at the use of sentiment analysis for happy ending detection in German
novels. Its focus lies on the investigation of previously proposed sentiment
features in order to gain insight about the relevance of specific features on
the one hand and the implications of their performance on the other hand.
Therefore, we study various partitionings of novels, considering the highly
variable concept of "ending". We also show that our approach, even though still
rather simple, can potentially lead to substantial findings relevant to
literary studies.



This article presents a new quantum-like model for cognition explicitly based
on knowledge. It is shown that this model, called QKT (quantum knowledge-based
theory), is able to coherently describe some experimental results that are
problematic for the prior quantum-like decision models. In particular, I
consider the experimental results relevant to the post-decision cognitive
dissonance, the problems relevant to the question order effect and response
replicability, and those relevant to the grand-reciprocity equations. A new set
of postulates is proposed, which evidence the different meaning given to the
projectors and to the quantum states. In the final part, I show that the use of
quantum gates can help to better describe and understand the evolution of
quantum-like models.



The family of temporal difference (TD) methods span a spectrum from
computationally frugal linear methods like TD({\lambda}) to data efficient
least squares methods. Least square methods make the best use of available data
directly computing the TD solution and thus do not require tuning a typically
highly sensitive learning rate parameter, but require quadratic computation and
storage. Recent algorithmic developments have yielded several sub-quadratic
methods that use an approximation to the least squares TD solution, but incur
bias. In this paper, we propose a new family of accelerated gradient TD (ATD)
methods that (1) provide similar data efficiency benefits to least-squares
methods, at a fraction of the computation and storage (2) significantly reduce
parameter sensitivity compared to linear TD methods, and (3) are asymptotically
unbiased. We illustrate these claims with a proof of convergence in expectation
and experiments on several benchmark domains and a large-scale industrial
energy allocation domain.



Unobserved or unknown confounders complicate even the simplest attempts to
estimate the effect of one variable on another using observational data. When
cause and effect are both affected by unobserved confounders, methods based on
identifying natural experiments have been proposed to eliminate confounds.
However, their validity is hard to verify because they depend on assumptions
about the independence of variables, that by definition, cannot be measured. In
this paper we investigate a particular scenario in time series data that
permits causal identification in the presence of unobserved confounders and
present an algorithm to automatically find such scenarios. Specifically, we
examine what we call the split-door setting, when the effect variable can be
split up into two parts: one that is potentially affected by the cause, and
another that is independent of it. We show that when both of these variables
are caused by the same (unobserved) confounders, the problem of identification
reduces to that of testing for independence among observed variables. We
discuss various situations in which split-door variables are commonly recorded
in both online and offline settings, and demonstrate the method by estimating
the causal impact of Amazon's recommender system, obtaining more than 23,000
natural experiments that provide similar---but more precise---estimates than
past studies.



We describe a neural attention model with a learnable retinal sampling
lattice. The model is trained on a visual search task requiring the
classification of an object embedded in a visual scene amidst background
distractors using the smallest number of fixations. We explore the tiling
properties that emerge in the model's retinal sampling lattice after training.
Specifically, we show that this lattice resembles the eccentricity dependent
sampling lattice of the primate retina, with a high resolution region in the
fovea surrounded by a low resolution periphery. Furthermore, we find conditions
where these emergent properties are amplified or eliminated providing clues to
their function.



There exist many problem domains where the interpretability of neural network
models is essential for deployment. Here we introduce a recurrent architecture
composed of input-switched affine transformations - in other words an RNN
without any explicit nonlinearities, but with input-dependent recurrent
weights. This simple form allows the RNN to be analyzed via straightforward
linear methods: we can exactly characterize the linear contribution of each
input to the model predictions; we can use a change-of-basis to disentangle
input, output, and computational hidden unit subspaces; we can fully
reverse-engineer the architecture's solution to a simple task. Despite this
ease of interpretation, the input switched affine network achieves reasonable
performance on a text modeling tasks, and allows greater computational
efficiency than networks with standard nonlinearities.



With the advent of semantic web, various tools and techniques have been
introduced for presenting and organizing knowledge. Concept hierarchies are one
such technique which gained significant attention due to its usefulness in
creating domain ontologies that are considered as an integral part of semantic
web. Automated concept hierarchy learning algorithms focus on extracting
relevant concepts from unstructured text corpus and connect them together by
identifying some potential relations exist between them. In this paper, we
propose a novel approach for identifying relevant concepts from plain text and
then learns hierarchy of concepts by exploiting subsumption relation between
them. To start with, we model topics using a probabilistic topic model and then
make use of some lightweight linguistic process to extract semantically rich
concepts. Then we connect concepts by identifying an "is-a" relationship
between pair of concepts. The proposed method is completely unsupervised and
there is no need for a domain specific training corpus for concept extraction
and learning. Experiments on large and real-world text corpora such as BBC News
dataset and Reuters News corpus shows that the proposed method outperforms some
of the existing methods for concept extraction and efficient concept hierarchy
learning is possible if the overall task is guided by a probabilistic topic
modeling algorithm.



We study a group of new methods to solve an open problem that is the shortest
paths problem on a given fix-weighted instance. It is the real significance at
a considerable altitude to reach our aim to meet these qualities of generic,
efficiency, precision which we generally require to a methodology. Besides our
proof to guarantee our measures might work normally, we pay more interest to
root out the vital theory about calculation and logic in favor of our extension
to range over a wide field about decision, operator, economy, management,
robot, AI and etc.



The applicability of fractional order (FO) automatic generation control (AGC)
for power system frequency oscillation damping is investigated in this paper,
employing distributed energy generation. The hybrid power system employs
various autonomous generation systems like wind turbine, solar photovoltaic,
diesel engine, fuel-cell and aqua electrolyzer along with other energy storage
devices like the battery and flywheel. The controller is placed in a remote
location while receiving and sending signals over an unreliable communication
network with stochastic delay. The controller parameters are tuned using robust
optimization techniques employing different variants of Particle Swarm
Optimization (PSO) and are compared with the corresponding optimal solutions.
An archival based strategy is used for reducing the number of function
evaluations for the robust optimization methods. The solutions obtained through
the robust optimization are able to handle higher variation in the controller
gains and orders without significant decrease in the system performance. This
is desirable from the FO controller implementation point of view, as the design
is able to accommodate variations in the system parameter which may result due
to the approximation of FO operators, using different realization methods and
order of accuracy. Also a comparison is made between the FO and the integer
order (IO) controllers to highlight the merits and demerits of each scheme.



This paper investigates the operation of a hybrid power system through a
novel fuzzy control scheme. The hybrid power system employs various autonomous
generation systems like wind turbine, solar photovoltaic, diesel engine,
fuel-cell, aqua electrolyzer etc. Other energy storage devices like the
battery, flywheel and ultra-capacitor are also present in the network. A novel
fractional order (FO) fuzzy control scheme is employed and its parameters are
tuned with a particle swarm optimization (PSO) algorithm augmented with two
chaotic maps for achieving an improved performance. This FO fuzzy controller
shows better performance over the classical PID, and the integer order fuzzy
PID controller in both linear and nonlinear operating regimes. The FO fuzzy
controller also shows stronger robustness properties against system parameter
variation and rate constraint nonlinearity, than that with the other controller
structures. The robustness is a highly desirable property in such a scenario
since many components of the hybrid power system may be switched on/off or may
run at lower/higher power output, at different time instants.



Humans are remarkably adept at interpreting the gaze direction of other
individuals in their surroundings. This skill is at the core of the ability to
engage in joint visual attention, which is essential for establishing social
interactions. How accurate are humans in determining the gaze direction of
others in lifelike scenes, when they can move their heads and eyes freely, and
what are the sources of information for the underlying perceptual processes?
These questions pose a challenge from both empirical and computational
perspectives, due to the complexity of the visual input in real-life
situations. Here we measure empirically human accuracy in perceiving the gaze
direction of others in lifelike scenes, and study computationally the sources
of information and representations underlying this cognitive capacity. We show
that humans perform better in face-to-face conditions compared with recorded
conditions, and that this advantage is not due to the availability of input
dynamics. We further show that humans are still performing well when only the
eyes-region is visible, rather than the whole face. We develop a computational
model, which replicates the pattern of human performance, including the finding
that the eyes-region contains on its own, the required information for
estimating both head orientation and direction of gaze. Consistent with
neurophysiological findings on task-specific face regions in the brain, the
learned computational representations reproduce perceptual effects such as the
Wollaston illusion, when trained to estimate direction of gaze, but not when
trained to recognize objects or faces.



Exploration in multi-task reinforcement learning is critical in training
agents to deduce the underlying MDP. Many of the existing exploration
frameworks such as $E^3$, $R_{max}$, Thompson sampling assume a single
stationary MDP and are not suitable for system identification in the multi-task
setting. We present a novel method to facilitate exploration in multi-task
reinforcement learning using deep generative models. We supplement our method
with a low dimensional energy model to learn the underlying MDP distribution
and provide a resilient and adaptive exploration signal to the agent. We
evaluate our method on a new set of environments and provide intuitive
interpretation of our results.



Two potential bottlenecks on the expressiveness of recurrent neural networks
(RNNs) are their ability to store information about the task in their
parameters, and to store information about the input history in their units. We
show experimentally that all common RNN architectures achieve nearly the same
per-task and per-unit capacity bounds with careful training, for a variety of
tasks and stacking depths. They can store an amount of task information which
is linear in the number of parameters, and is approximately 5 bits per
parameter. They can additionally store approximately one real number from their
input history per hidden unit. We further find that for several tasks it is the
per-task parameter capacity bound that determines performance. These results
suggest that many previous results comparing RNN architectures are driven
primarily by differences in training effectiveness, rather than differences in
capacity. Supporting this observation, we compare training difficulty for
several architectures, and show that vanilla RNNs are far more difficult to
train, yet have slightly higher capacity. Finally, we propose two novel RNN
architectures, one of which is easier to train than the LSTM or GRU for deeply
stacked architectures.



This paper presents a framework to tackle combinatorial optimization problems
using neural networks and reinforcement learning. We focus on the traveling
salesman problem (TSP) and train a recurrent network that, given a set of city
coordinates, predicts a distribution over different city permutations. Using
negative tour length as the reward signal, we optimize the parameters of the
recurrent network using a policy gradient method. We compare learning the
network parameters on a set of training graphs against learning them on
individual test graphs. Despite the computational expense, without much
engineering and heuristic designing, Neural Combinatorial Optimization achieves
close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied
to the KnapSack, another NP-hard problem, the same method obtains optimal
solutions for instances with up to 200 items.



We describe a new method called t-ETE for finding a low-dimensional embedding
of a set of objects in Euclidean space. We formulate the embedding problem as a
joint ranking problem over a set of triplets, where each triplet captures the
relative similarities between three objects in the set. By exploiting recent
advances in robust ranking, t-ETE produces high-quality embeddings even in the
presence of a significant amount of noise and better preserves local scale than
known methods, such as t-STE and t-SNE. In particular, our method produces
significantly better results than t-SNE on signature datasets while also being
faster to compute.



We present an online deliberation system using mutual evaluation in order to
collaboratively develop solutions. Participants submit their proposals and
evaluate each other's proposals; some of them may then be invited by the system
to rewrite 'problematic' proposals. Two cases are discussed: a proposal
supported by many, but not by a given person, who is then invited to rewrite it
for making yet more acceptable; and a poorly presented but presumably
interesting proposal. The first of these cases has been successfully
implemented. Proposals are evaluated along two axes-understandability (or
clarity, or, more generally, quality), and agreement. The latter is used by the
system to cluster proposals according to their ideas, while the former is used
both to present the best proposals on top of their clusters, and to find poorly
written proposals candidates for rewriting. These functionalities may be
considered as important components of a large scale online deliberation system.



Understanding how brain functions has been an intriguing topic for years.
With the recent progress on collecting massive data and developing advanced
technology, people have become interested in addressing the challenge of
decoding brain wave data into meaningful mind states, with many machine
learning models and algorithms being revisited and developed, especially the
ones that handle time series data because of the nature of brain waves.
However, many of these time series models, like HMM with hidden state in
discrete space or State Space Model with hidden state in continuous space, only
work with one source of data and cannot handle different sources of information
simultaneously. In this paper, we propose an extension of State Space Model to
work with different sources of information together with its learning and
inference algorithms. We apply this model to decode the mind state of students
during lectures based on their brain waves and reach a significant better
results compared to traditional methods.



We introduce Joint Causal Inference (JCI), a powerful formulation of causal
discovery from multiple datasets that allows to jointly learn both the causal
structure and targets of interventions from statistical independences in pooled
data. Compared with existing constraint-based approaches for causal discovery
from multiple data sets, JCI offers several advantages: it allows for several
different types of interventions in a unified fashion, it can learn
intervention targets, it systematically pools data across different datasets
which improves the statistical power of independence tests, and most
importantly, it improves on the accuracy and identifiability of the predicted
causal relations. A technical complication that arises in JCI is the occurrence
of faithfulness violations due to deterministic relations. We propose a simple
but effective strategy for dealing with this type of faithfulness violations.
We implement it in ACID, a determinism-tolerant extension of Ancestral Causal
Inference (ACI) (Magliacane et al., 2016), a recently proposed logic-based
causal discovery method that improves reliability of the output by exploiting
redundant information in the data. We illustrate the benefits of JCI with ACID
with an evaluation on a simulated dataset.



Cybersecurity is increasingly threatened by advanced and persistent attacks.
As these attacks are often designed to disable a system (or a critical
resource, e.g., a user account) repeatedly, it is crucial for the defender to
keep updating its security measures to strike a balance between the risk of
being compromised and the cost of security updates. Moreover, these decisions
often need to be made with limited and delayed feedback due to the stealthy
nature of advanced attacks. In addition to targeted attacks, such an optimal
timing policy under incomplete information has broad applications in
cybersecurity. Examples include key rotation, password change, application of
patches, and virtual machine refreshing. However, rigorous studies of optimal
timing are rare. Further, existing solutions typically rely on a pre-defined
attack model that is known to the defender, which is often not the case in
practice. In this work, we make an initial effort towards achieving optimal
timing of security updates in the face of unknown stealthy attacks. We consider
a variant of the influential FlipIt game model with asymmetric feedback and
unknown attack time distribution, which provides a general model to consecutive
security updates. The defender's problem is then modeled as a time associative
bandit problem with dependent arms. We derive upper confidence bound based
learning policies that achieve low regret compared with optimal periodic
defense strategies that can only be derived when attack time distributions are
known.



Problems such as predicting a new shading field (Y) for an image (X) are
ambiguous: many very distinct solutions are good. Representing this ambiguity
requires building a conditional model P(Y|X) of the prediction, conditioned on
the image. Such a model is difficult to train, because we do not usually have
training data containing many different shadings for the same image. As a
result, we need different training examples to share data to produce good
models. This presents a danger we call "code space collapse" - the training
procedure produces a model that has a very good loss score, but which
represents the conditional distribution poorly. We demonstrate an improved
method for building conditional models by exploiting a metric constraint on
training data that prevents code space collapse. We demonstrate our model on
two example tasks using real data: image saturation adjustment, image
relighting. We describe quantitative metrics to evaluate ambiguous generation
results. Our results quantitatively and qualitatively outperform different
strong baselines.



The paper analyzes the interaction between humans and computers in terms of
response time in solving the image-based CAPTCHA. In particular, the analysis
focuses on the attitude of the different Internet users in easily solving four
different types of image-based CAPTCHAs which include facial expressions like:
animated character, old woman, surprised face, worried face. To pursue this
goal, an experiment is realized involving 100 Internet users in solving the
four types of CAPTCHAs, differentiated by age, Internet experience, and
education level. The response times are collected for each user. Then,
association rules are extracted from user data, for evaluating the dependence
of the response time in solving the CAPTCHA from age, education level and
experience in internet usage by statistical analysis. The results implicitly
capture the users' psychological states showing in what states the users are
more sensible. It reveals to be a novelty and a meaningful analysis in the
state-of-the-art.



We present a method for inducing new dialogue systems from very small amounts
of unannotated dialogue data, showing how word-level exploration using
Reinforcement Learning (RL), combined with an incremental and semantic grammar
- Dynamic Syntax (DS) - allows systems to discover, generate, and understand
many new dialogue variants. The method avoids the use of expensive and
time-consuming dialogue act annotations, and supports more natural
(incremental) dialogues than turn-based systems. Here, language generation and
dialogue management are treated as a joint decision/optimisation problem, and
the MDP model for RL is constructed automatically. With an implemented system,
we show that this method enables a wide range of dialogue variations to be
automatically captured, even when the system is trained from only a single
dialogue. The variants include question-answer pairs, over- and
under-answering, self- and other-corrections, clarification interaction,
split-utterances, and ellipsis. This generalisation property results from the
structural knowledge and constraints present within the DS grammar, and
highlights some limitations of recent systems built using machine learning
techniques only.



The ability to perform effective off-policy learning would revolutionize the
process of building better interactive systems, such as search engines and
recommendation systems for e-commerce, computational advertising and news.
Recent approaches for off-policy evaluation and learning in these settings
appear promising. With this paper, we provide real-world data and a
standardized test-bed to systematically investigate these algorithms using data
from display advertising. In particular, we consider the problem of filling a
banner ad with an aggregate of multiple products the user may want to purchase.
This paper presents our test-bed, the sanity checks we ran to ensure its
validity, and shows results comparing state-of-the-art off-policy learning
methods like doubly robust optimization, POEM, and reductions to supervised
learning using regression baselines. Our results show experimental evidence
that recent off-policy learning methods can improve upon state-of-the-art
supervised learning techniques on a large-scale real-world data set.



Advances in neural variational inference have facilitated the learning of
powerful directed graphical models with continuous latent variables, such as
variational autoencoders. The hope is that such models will learn to represent
rich, multi-modal latent factors in real-world data, such as natural language
text. However, current models often assume simplistic priors on the latent
variables - such as the uni-modal Gaussian distribution - which are incapable
of representing complex latent factors efficiently. To overcome this
restriction, we propose the simple, but highly flexible, piecewise constant
distribution. This distribution has the capacity to represent an exponential
number of modes of a latent target distribution, while remaining mathematically
tractable. Our results demonstrate that incorporating this new latent
distribution into different models yields substantial improvements in natural
language processing tasks such as document modeling and natural language
generation for dialogue.



A number of recent approaches to policy learning in 2D game domains have been
successful going directly from raw input images to actions. However when
employed in complex 3D environments, they typically suffer from challenges
related to partial observability, combinatorial exploration spaces, path
planning, and a scarcity of rewarding scenarios. Inspired from prior work in
human cognition that indicates how humans employ a variety of semantic concepts
and abstractions (object categories, localisation, etc.) to reason about the
world, we build an agent-model that incorporates such abstractions into its
policy-learning framework. We augment the raw image input to a Deep Q-Learning
Network (DQN), by adding details of objects and structural elements
encountered, along with the agent's localisation. The different components are
automatically extracted and composed into a topological representation using
on-the-fly object detection and 3D-scene reconstruction.We evaluate the
efficacy of our approach in Doom, a 3D first-person combat game that exhibits a
number of challenges discussed, and show that our augmented framework
consistently learns better, more effective policies.



Deep reinforcement learning (RL) can acquire complex behaviors from low-level
inputs, such as images. However, real-world applications of such methods
require generalizing to the vast variability of the real world. Deep networks
are known to achieve remarkable generalization when provided with massive
amounts of labeled data, but can we provide this breadth of experience to an RL
agent, such as a robot? The robot might continuously learn as it explores the
world around it, even while deployed. However, this learning requires access to
a reward function, which is often hard to measure in real-world domains, where
the reward could depend on, for example, unknown positions of objects or the
emotional state of the user. Conversely, it is often quite practical to provide
the agent with reward functions in a limited set of situations, such as when a
human supervisor is present or in a controlled setting. Can we make use of this
limited supervision, and still benefit from the breadth of experience an agent
might collect on its own? In this paper, we formalize this problem as
semisupervised reinforcement learning, where the reward function can only be
evaluated in a set of "labeled" MDPs, and the agent must generalize its
behavior to the wide range of states it might encounter in a set of "unlabeled"
MDPs, by using experience from both settings. Our proposed method infers the
task objective in the unlabeled MDPs through an algorithm that resembles
inverse RL, using the agent's own prior experience in the labeled MDPs as a
kind of demonstration of optimal behavior. We evaluate our method on
challenging tasks that require control directly from images, and show that our
approach can improve the generalization of a learned deep neural network policy
by using experience for which no reward function is available. We also show
that our method outperforms direct supervised learning of the reward.



Due to physiological variation, patients diagnosed with the same condition
may exhibit divergent, but related, responses to the same treatments. Hidden
Parameter Markov Decision Processes (HiP-MDPs) tackle this transfer-learning
problem by embedding these tasks into a low-dimensional space. However, the
original formulation of HiP-MDP had a critical flaw: the embedding uncertainty
was modeled independently of the agent's state uncertainty, requiring an
unnatural training procedure in which all tasks visited every part of the state
space---possible for robots that can be moved to a particular location,
impossible for human patients. We update the HiP-MDP framework and extend it to
more robustly develop personalized medicine strategies for HIV treatment.



Recently it has been shown that policy-gradient methods for reinforcement
learning can be utilized to train deep end-to-end systems directly on
non-differentiable metrics for the task at hand. In this paper we consider the
problem of optimizing image captioning systems using reinforcement learning,
and show that by carefully optimizing our systems using the test metrics of the
MSCOCO task, significant gains in performance can be realized. Our systems are
built using a new optimization approach that we call self-critical sequence
training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather
than estimating a "baseline" to normalize the rewards and reduce variance,
utilizes the output of its own test-time inference algorithm to normalize the
rewards it experiences. Using this approach, estimating the reward signal (as
actor-critic methods must do) and estimating normalization (as REINFORCE
algorithms typically do) is avoided, while at the same time harmonizing the
model with respect to its test-time inference procedure. Empirically we find
that directly optimizing the CIDEr metric with SCST and greedy decoding at
test-time is highly effective. Our results on the MSCOCO evaluation sever
establish a new state-of-the-art on the task, improving the best result in
terms of CIDEr from 104.9 to 114.7.



Autonomous systems can be used to search for sparse signals in a large space;
e.g., aerial robots can be deployed to localize threats, detect gas leaks, or
respond to distress calls. Intuitively, search algorithms may increase
efficiency by collecting aggregate measurements summarizing large contiguous
regions. However, most existing search methods either ignore the possibility of
such region observations (e.g., Bayesian optimization and multi-armed bandits)
or make strong assumptions about the sensing mechanism that allow each
measurement to arbitrarily encode all signals in the entire environment (e.g.,
compressive sensing). We propose an algorithm that actively collects data to
search for sparse signals using only noisy measurements of the average values
on rectangular regions (including single points), based on the greedy
maximization of information gain. We analyze our algorithm in 1d and show that
it requires $\tilde{O}(\frac{n}{\mu^2}+k^2)$ measurements to recover all of $k$
signal locations with small Bayes error, where $\mu$ and $n$ are the signal
strength and the size of the search space, respectively. We also show that
active designs can be fundamentally more efficient than passive designs with
region sensing, contrasting with the results of Arias-Castro, Candes, and
Davenport (2013). We demonstrate the empirical performance of our algorithm on
a search problem using satellite image data and in high dimensions.



An important problem for HCI researchers is to estimate the parameter values
of a cognitive model from behavioral data. This is a difficult problem, because
of the substantial complexity and variety in human behavioral strategies. We
report an investigation into a new approach using approximate Bayesian
computation (ABC) to condition model parameters to data and prior knowledge. As
the case study we examine menu interaction, where we have click time data only
to infer a cognitive model that implements a search behaviour with parameters
such as fixation duration and recall probability. Our results demonstrate that
ABC (i) improves estimates of model parameter values, (ii) enables meaningful
comparisons between model variants, and (iii) supports fitting models to
individual users. ABC provides ample opportunities for theoretical HCI research
by allowing principled inference of model parameter values and their
uncertainty.



We present probabilistic neural programs, a framework for program induction
that permits flexible specification of both a computational model and inference
algorithm while simultaneously enabling the use of deep neural networks.
Probabilistic neural programs combine a computation graph for specifying a
neural network with an operator for weighted nondeterministic choice. Thus, a
program describes both a collection of decisions as well as the neural network
architecture used to make each one. We evaluate our approach on a challenging
diagram question answering task where probabilistic neural programs correctly
execute nearly twice as many programs as a baseline model.



Machine learning is making substantial progress in diverse applications. The
success is mostly due to advances in deep learning. However, deep learning can
make mistakes and its generalization abilities to new tasks are questionable.
We ask when and how one can combine network outputs, when (i) details of the
observations are evaluated by learned deep components and (ii) facts and
confirmation rules are available in knowledge based systems. We show that in
limited contexts the required number of training samples can be low and
self-improvement of pre-trained networks in more general context is possible.
We argue that the combination of sparse outlier detection with deep components
that can support each other diminish the fragility of deep methods, an
important requirement for engineering applications. We argue that supervised
learning of labels may be fully eliminated under certain conditions: a
component based architecture together with a knowledge based system can train
itself and provide high quality answers. We demonstrate these concepts on the
State Farm Distracted Driver Detection benchmark. We argue that the view of the
Study Panel (2016) may overestimate the requirements on `years of focused
research' and `careful, unique construction' for `AI systems'.



We consider parallel asynchronous Markov Chain Monte Carlo (MCMC) sampling
for problems where we can leverage (stochastic) gradients to define continuous
dynamics which explore the target distribution. We outline a solution strategy
for this setting based on stochastic gradient Hamiltonian Monte Carlo sampling
(SGHMC) which we alter to include an elastic coupling term that ties together
multiple MCMC instances. The proposed strategy turns inherently sequential HMC
algorithms into asynchronous parallel versions. First experiments empirically
show that the resulting parallel sampler significantly speeds up exploration of
the target distribution, when compared to standard SGHMC, and is less prone to
the harmful effects of stale gradients than a naive parallelization approach.



The ability to learn tasks in a sequential fashion is crucial to the
development of artificial intelligence. Neural networks are not, in general,
capable of this and it has been widely thought that catastrophic forgetting is
an inevitable feature of connectionist models. We show that it is possible to
overcome this limitation and train networks that can maintain expertise on
tasks which they have not experienced for a long time. Our approach remembers
old tasks by selectively slowing down learning on the weights important for
those tasks. We demonstrate our approach is scalable and effective by solving a
set of classification tasks based on the MNIST hand written digit dataset and
by learning several Atari 2600 games sequentially.



We study machine learning formulations of inductive program synthesis; that
is, given input-output examples, synthesize source code that maps inputs to
corresponding outputs. Our key contribution is TerpreT, a domain-specific
language for expressing program synthesis problems. A TerpreT model is composed
of a specification of a program representation and an interpreter that
describes how programs map inputs to outputs. The inference task is to observe
a set of input-output examples and infer the underlying program. From a TerpreT
model we automatically perform inference using four different back-ends:
gradient descent (thus each TerpreT model can be seen as defining a
differentiable interpreter), linear program (LP) relaxations for graphical
models, discrete satisfiability solving, and the Sketch program synthesis
system. TerpreT has two main benefits. First, it enables rapid exploration of a
range of domains, program representations, and interpreter models. Second, it
separates the model specification from the inference algorithm, allowing proper
comparisons between different approaches to inference.
  We illustrate the value of TerpreT by developing several interpreter models
and performing an extensive empirical comparison between alternative inference
algorithms on a variety of program models. To our knowledge, this is the first
work to compare gradient-based search over program space to traditional
search-based alternatives. Our key empirical finding is that constraint solvers
dominate the gradient descent and LP-based formulations.
  This is a workshop summary of a longer report at arXiv:1608.04428



Problems at the intersection of vision and language are of significant
importance both as challenging research questions and for the rich set of
applications they enable. However, inherent structure in our world and bias in
our language tend to be a simpler signal for learning than visual modalities,
resulting in models that ignore visual information, leading to an inflated
sense of their capability.
  We propose to counter these language priors for the task of Visual Question
Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance
the popular VQA dataset by collecting complementary images such that every
question in our balanced dataset is associated with not just a single image,
but rather a pair of similar images that result in two different answers to the
question. Our dataset is by construction more balanced than the original VQA
dataset and has approximately twice the number of image-question pairs. Our
complete balanced dataset is publicly available at www.visualqa.org as part of
the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA
v2.0).
  We further benchmark a number of state-of-art VQA models on our balanced
dataset. All models perform significantly worse on our balanced dataset,
suggesting that these models have indeed learned to exploit language priors.
This finding provides the first concrete empirical evidence for what seems to
be a qualitative sense among practitioners.
  Finally, our data collection protocol for identifying complementary images
enables us to develop a novel interpretable model, which in addition to
providing an answer to the given (image, question) pair, also provides a
counter-example based explanation. Specifically, it identifies an image that is
similar to the original image, but it believes has a different answer to the
same question. This can help in building trust for machines among their users.



We tackle the prediction of instructor intervention in student posts from
discussion forums in Massive Open Online Courses (MOOCs). Our key finding is
that using automatically obtained discourse relations improves the prediction
of when instructors intervene in student discussions, when compared with a
state-of-the-art, feature-rich baseline. Our supervised classifier makes use of
an automatic discourse parser which outputs Penn Discourse Treebank (PDTB) tags
that represent in-post discourse features. We show PDTB relation-based features
increase the robustness of the classifier and complement baseline features in
recalling more diverse instructor intervention patterns. In comprehensive
experiments over 14 MOOC offerings from several disciplines, the PDTB discourse
features improve performance on average. The resultant models are less
dependent on domain-specific vocabulary, allowing them to better generalize to
new courses.



This paper introduces ALYSIA: Automated LYrical SongwrIting Application.
ALYSIA is based on a machine learning model using Random Forests, and we
discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA
was used to create original pop songs that were subsequently recorded and
produced. Finally, we discuss our vision for the future of Automated
Songwriting for both co-creative and autonomous systems.



Software estimation is a crucial task in software engineering. Software
estimation encompasses cost, effort, schedule, and size. The importance of
software estimation becomes critical in the early stages of the software life
cycle when the details of software have not been revealed yet. Several
commercial and non-commercial tools exist to estimate software in the early
stages. Most software effort estimation methods require software size as one of
the important metric inputs and consequently, software size estimation in the
early stages becomes essential. One of the approaches that has been used for
about two decades in the early size and effort estimation is called use case
points. Use case points method relies on the use case diagram to estimate the
size and effort of software projects. Although the use case points method has
been widely used, it has some limitations that might adversely affect the
accuracy of estimation. This paper presents some techniques using fuzzy logic
and neural networks to improve the accuracy of the use case points method.
Results showed that an improvement up to 22% can be obtained using the proposed
approach.



We propose a scheme for training a computerized agent to perform complex
human tasks such as highway steering. The scheme is designed to follow a
natural learning process whereby a human instructor teaches a computerized
trainee. The learning process consists of five elements: (i) unsupervised
feature learning; (ii) supervised imitation learning; (iii) supervised reward
induction; (iv) supervised safety module construction; and (v) reinforcement
learning. We implemented the last four elements of the scheme using deep
convolutional networks and applied it to successfully create a computerized
agent capable of autonomous highway steering over the well-known racing game
Assetto Corsa. We demonstrate that the use of the last four elements is
essential to effectively carry out the steering task using vision alone,
without access to a driving simulator internals, and operating in wall-clock
time. This is made possible also through the introduction of a safety network,
a novel way for preventing the agent from performing catastrophic mistakes
during the reinforcement learning stage.



Extending the success of deep neural networks to natural language
understanding and symbolic reasoning requires complex operations and external
memory. Recent neural program induction approaches have attempted to address
this problem, but are typically limited to differentiable memory, and
consequently cannot scale beyond small synthetic tasks. In this work, we
propose the Manager-Programmer-Computer framework, which integrates neural
networks with non-differentiable memory to support abstract, scalable and
precise operations through a friendly neural computer interface. Specifically,
we introduce a Neural Symbolic Machine, which contains a sequence-to-sequence
neural "programmer", and a non-differentiable "computer" that is a Lisp
interpreter with code assist. To successfully apply REINFORCE for training, we
augment it with approximate gold programs found by an iterative maximum
likelihood training process. NSM is able to learn a semantic parser from weak
supervision over a large knowledge base. It achieves new state-of-the-art
performance on WebQuestionsSP, a challenging semantic parsing dataset, with
weak supervision. Compared to previous approaches, NSM is end-to-end, therefore
does not rely on feature engineering or domain specific knowledge.



Communicating and sharing intelligence among agents is an important facet of
achieving Artificial General Intelligence. As a first step towards this
challenge, we introduce a novel framework for image generation: Message Passing
Multi-Agent Generative Adversarial Networks (MPM GANs). While GANs have
recently been shown to be very effective for image generation and other tasks,
these networks have been limited to mostly single generator-discriminator
networks. We show that we can obtain multi-agent GANs that communicate through
message passing to achieve better image generation. The objectives of the
individual agents in this framework are two fold: a co-operation objective and
a competing objective. The co-operation objective ensures that the message
sharing mechanism guides the other generator to generate better than itself
while the competing objective encourages each generator to generate better than
its counterpart. We analyze and visualize the messages that these GANs share
among themselves in various scenarios. We quantitatively show that the message
sharing formulation serves as a regularizer for the adversarial training.
Qualitatively, we show that the different generators capture different traits
of the underlying data distribution.



In this paper we extend the principle of proportional representation to
rankings. We consider the setting where alternatives need to be ranked based on
approval preferences. In this setting, proportional representation requires
that cohesive groups of voters are represented proportionally in each initial
segment of the ranking. Proportional rankings are desirable in situations where
initial segments of different lengths may be relevant, e.g., hiring decisions
(if it is unclear how many positions are to be filled), the presentation of
competing proposals on a liquid democracy platform (if it is unclear how many
proposals participants are taking into consideration), or recommender systems
(if a ranking has to accommodate different user types). We study the
proportional representation provided by several ranking methods and prove
theoretical guarantees. Furthermore, we experimentally evaluate these methods
and present preliminary evidence as to which methods are most suitable for
producing proportional rankings.



The method presented extends a given regression neural network to make its
performance improve. The modification affects the learning procedure only,
hence the extension may be easily omitted during evaluation without any change
in prediction. It means that the modified model may be evaluated as quickly as
the original one but tends to perform better.
  This improvement is possible because the modification gives better expressive
power, provides better behaved gradients and works as a regularization. The
knowledge gained by the temporarily extended neural network is contained in the
parameters shared with the original neural network.
  The only cost is an increase in learning time.



Scarce data is a major challenge to scaling robot learning to truly complex
tasks, as we need to generalize locally learned policies over different
"contexts". Bayesian optimization approaches to contextual policy search (CPS)
offer data-efficient policy learning that generalize over a context space. We
propose to improve data- efficiency by factoring typically considered contexts
into two components: target- type contexts that correspond to a desired outcome
of the learned behavior, e.g. target position for throwing a ball; and
environment type contexts that correspond to some state of the environment,
e.g. initial ball position or wind speed. Our key observation is that
experience can be directly generalized over target-type contexts. Based on that
we introduce Factored Contextual Policy Search with Bayesian Optimization for
both passive and active learning settings. Preliminary results show faster
policy generalization on a simulated toy problem.



In this chapter, we present CORrelation ALignment (CORAL), a simple yet
effective method for unsupervised domain adaptation. CORAL minimizes domain
shift by aligning the second-order statistics of source and target
distributions, without requiring any target labels. In contrast to subspace
manifold methods, it aligns the original feature distributions of the source
and target domains, rather than the bases of lower-dimensional subspaces. It is
also much simpler than other distribution matching methods. CORAL performs
remarkably well in extensive evaluations on standard benchmark datasets. We
first describe a solution that applies a linear transformation to source
features to align them with target features before classifier training. For
linear classifiers, we propose to equivalently apply CORAL to the classifier
weights, leading to added efficiency when the number of classifiers is small
but the number and dimensionality of target examples are very high. The
resulting CORAL Linear Discriminant Analysis (CORAL-LDA) outperforms LDA by a
large margin on standard domain adaptation benchmarks. Finally, we extend CORAL
to learn a nonlinear transformation that aligns correlations of layer
activations in deep neural networks (DNNs). The resulting Deep CORAL approach
works seamlessly with DNNs and achieves state-of-the-art performance on
standard benchmark datasets. Our code is available
at:~\url{https://github.com/VisionLearningGroup/CORAL}



The field of connectomics faces unprecedented "big data" challenges. To
reconstruct neuronal connectivity, automated pixel-level segmentation is
required for petabytes of streaming electron microscopy data. Existing
algorithms provide relatively good accuracy but are unacceptably slow, and
would require years to extract connectivity graphs from even a single cubic
millimeter of neural tissue. Here we present a viable real-time solution, a
multi-pass pipeline optimized for shared-memory multicore systems, capable of
processing data at near the terabyte-per-hour pace of multi-beam electron
microscopes. The pipeline makes an initial fast-pass over the data, and then
makes a second slow-pass to iteratively correct errors in the output of the
fast-pass. We demonstrate the accuracy of a sparse slow-pass reconstruction
algorithm and suggest new methods for detecting morphological errors. Our
fast-pass approach provided many algorithmic challenges, including the design
and implementation of novel shallow convolutional neural nets and the
parallelization of watershed and object-merging techniques. We use it to
reconstruct, from image stack to skeletons, the full dataset of Kasthuri et al.
(463 GB capturing 120,000 cubic microns) in a matter of hours on a single
multicore machine rather than the weeks it has taken in the past on much larger
distributed systems.



Although Generative Adversarial Networks achieve state-of-the-art results on
a variety of generative tasks, they are regarded as highly unstable and prone
to miss modes. We argue that these bad behaviors of GANs are due to the very
particular functional shape of the trained discriminators in high dimensional
spaces, which can easily make training stuck or push probability mass in the
wrong direction, towards that of higher concentration than that of the data
generating distribution. We introduce several ways of regularizing the
objective, which can dramatically stabilize the training of GAN models. We also
show that our regularizers can help the fair distribution of probability mass
across the modes of the data generating distribution, during the early phases
of training and thus providing a unified solution to the missing modes problem.



A key limitation of sampling algorithms for approximate inference is that it
is difficult to quantify their approximation error. Widely used sampling
schemes, such as sequential importance sampling with resampling and
Metropolis-Hastings, produce output samples drawn from a distribution that may
be far from the target posterior distribution. This paper shows how to
upper-bound the symmetric KL divergence between the output distribution of a
broad class of sequential Monte Carlo (SMC) samplers and their target posterior
distributions, subject to assumptions about the accuracy of a separate
gold-standard sampler. The proposed method applies to samplers that combine
multiple particles, multinomial resampling, and rejuvenation kernels. The
experiments show the technique being used to estimate bounds on the divergence
of SMC samplers for posterior inference in a Bayesian linear regression model
and a Dirichlet process mixture model.



Providing accurate predictions is challenging for machine learning algorithms
when the number of features is larger than the number of samples in the data.
Prior knowledge can improve machine learning models by indicating relevant
variables and parameter values. Yet, this prior knowledge is often tacit and
only available from domain experts. We present a novel approach that uses
interactive visualization to elicit the tacit prior knowledge and uses it to
improve the accuracy of prediction models. The main component of our approach
is a user model that models the domain expert's knowledge of the relevance of
different features for a prediction task. In particular, based on the expert's
earlier input, the user model guides the selection of the features on which to
elicit user's knowledge next. The results of a controlled user study show that
the user model significantly improves prior knowledge elicitation and
prediction accuracy, when predicting the relative citation counts of scientific
documents in a specific domain.



We study the online estimation of the optimal policy of a Markov decision
process (MDP). We propose a class of Stochastic Primal-Dual (SPD) methods which
exploit the inherent minimax duality of Bellman equations. The SPD methods
update a few coordinates of the value and policy estimates as a new state
transition is observed. These methods use small storage and has low
computational complexity per iteration. The SPD methods find an
absolute-$\epsilon$-optimal policy, with high probability, using
$\mathcal{O}\left(\frac{|\mathcal{S}|^4 |\mathcal{A}|^2\sigma^2
}{(1-\gamma)^6\epsilon^2} \right)$ iterations/samples for the infinite-horizon
discounted-reward MDP and $\mathcal{O}\left(\frac{|\mathcal{S}|^4
|\mathcal{A}|^2H^6\sigma^2 }{\epsilon^2} \right)$ for the finite-horizon MDP.



We consider the problem of predicting the next observation given a sequence
of past observations, and consider the extent to which accurate prediction
requires complex algorithms that explicitly leverage long-range dependencies.
Perhaps surprisingly, our positive results show that for a broad class of
sequences, there is an algorithm that predicts well on average, and bases its
predictions only on the most recent few observation together with a set of
simple summary statistics of the past observations. Specifically, we show that
for any distribution over observations, if the mutual information between past
observations and future observations is upper bounded by $I$, then a simple
Markov model over the most recent $I/\epsilon$ observations obtains expected KL
error $\epsilon$---and hence $\ell_1$ error $\sqrt{\epsilon}$---with respect to
the optimal predictor that has access to the entire past and knows the data
generating distribution. For a Hidden Markov Model with $n$ hidden states, $I$
is bounded by $\log n$, a quantity that does not depend on the mixing time, and
we show that the trivial prediction algorithm based on the empirical
frequencies of length $O(\log n/\epsilon)$ windows of observations achieves
this error, provided the length of the sequence is $d^{\Omega(\log
n/\epsilon)}$, where $d$ is the size of the observation alphabet.
  We also establish that this result cannot be improved upon, even for the
class of HMMs, in the following two senses: First, for HMMs with $n$ hidden
states, a window length of $\log n/\epsilon$ is information-theoretically
necessary to achieve expected $\ell_1$ error $\sqrt{\epsilon}$. Second, the
$d^{\Theta(\log n/\epsilon)}$ samples required to estimate the Markov model for
an observation alphabet of size $d$ is necessary for any computationally
tractable learning algorithm, assuming the hardness of strongly refuting a
certain class of CSPs.



Model checking of strategic ability under imperfect information is known to
be hard. The complexity results range from NP-completeness to undecidability,
depending on the precise setup of the problem. No less importantly, fixpoint
equivalences do not generally hold for imperfect information strategies, which
seriously hampers incremental synthesis of winning strategies. In this paper,
we propose translations of ATLir formulae that provide lower and upper bounds
for their truth values, and are cheaper to verify than the original
specifications. That is, if the expression is verified as true then the
corresponding formula of ATLir should also hold in the given model. We begin by
showing where the straightforward approach does not work. Then, we propose how
it can be modified to obtain guaranteed lower bounds. To this end, we alter the
next-step operator in such a way that traversing one's indistinguishability
relation is seen as atomic activity. Most interestingly, the lower
approximation is provided by a fixpoint expression that uses a nonstandard
variant of the next-step ability operator. We show the correctness of the
translations, establish their computational complexity, and validate the
approach by experiments with a scalable scenario of Bridge play.



Random backpropagation (RBP) is a variant of the backpropagation algorithm
for training neural networks, where the transpose of the forward matrices are
replaced by fixed random matrices in the calculation of the weight updates. It
is remarkable both because of its effectiveness, in spite of using random
matrices to communicate error information, and because it completely removes
the taxing requirement of maintaining symmetric weights in a physical neural
system. To better understand random backpropagation, we first connect it to the
notions of local learning and learning channels. Through this connection, we
derive several alternatives to RBP, including skipped RBP (SRPB), adaptive RBP
(ARBP), sparse RBP, and their combinations (e.g. ASRBP) and analyze their
computational complexity. We then study their behavior through simulations
using the MNIST and CIFAR-10 bechnmark datasets. These simulations show that
most of these variants work robustly, almost as well as backpropagation, and
that multiplication by the derivatives of the activation functions is
important. As a follow-up, we study also the low-end of the number of bits
required to communicate error information over the learning channel. We then
provide partial intuitive explanations for some of the remarkable properties of
RBP and its variations. Finally, we prove several mathematical results,
including the convergence to fixed points of linear chains of arbitrary length,
the convergence to fixed points of linear autoencoders with decorrelated data,
the long-term existence of solutions for linear systems with a single hidden
layer and convergence in special cases, and the convergence to fixed points of
non-linear chains, when the derivative of the activation functions is included.



Mobile robots with complex morphology are essential for traversing rough
terrains in Urban Search & Rescue missions (USAR). Since teleoperation of the
complex morphology causes high cognitive load of the operator, the morphology
is controlled autonomously. The autonomous control measures the robot state and
surrounding terrain which is usually only partially observable, and thus the
data are often incomplete. We marginalize the control over the missing
measurements and evaluate an explicit safety condition. If the safety condition
is violated, tactile terrain exploration by the body-mounted robotic arm
gathers the missing data.



This paper presents the design of a supervisory algorithm that monitors
safety at road intersections and overrides drivers with a safe input when
necessary. The design of the supervisor consists of two parts: safety
verification and control design. Safety verification is the problem to
determine if vehicles will be able to cross the intersection without colliding
with current drivers' inputs. We translate this safety verification problem
into a jobshop scheduling problem, which minimizes the maximum lateness and
evaluates if the optimal cost is zero. The zero optimal cost corresponds to the
case in which all vehicles can cross each conflict area without collisions.
Computing the optimal cost requires solving a Mixed Integer Nonlinear
Programming (MINLP) problem due to the nonlinear second-order dynamics of the
vehicles. We therefore estimate this optimal cost by formulating two related
Mixed Integer Linear Programming (MILP) problems that assume simpler vehicle
dynamics. We prove that these two MILP problems yield lower and upper bounds of
the optimal cost. We also quantify the worst case approximation errors of these
MILP problems. We design the supervisor to override the vehicles with a safe
control input if the MILP problem that computes the upper bound yields a
positive optimal cost. We theoretically demonstrate that the supervisor keeps
the intersection safe and is non-blocking. Computer simulations further
validate that the algorithms can run in real time for problems of realistic
size.



In this paper, we study the problem of author identification under
double-blind review setting, which is to identify potential authors given
information of an anonymized paper. Different from existing approaches that
rely heavily on feature engineering, we propose to use network embedding
approach to address the problem, which can automatically represent nodes into
lower dimensional feature vectors. However, there are two major limitations in
recent studies on network embedding: (1) they are usually general-purpose
embedding methods, which are independent of the specific tasks; and (2) most of
these approaches can only deal with homogeneous networks, where the
heterogeneity of the network is ignored. Hence, challenges faced here are two
folds: (1) how to embed the network under the guidance of the author
identification task, and (2) how to select the best type of information due to
the heterogeneity of the network.
  To address the challenges, we propose a task-guided and path-augmented
heterogeneous network embedding model. In our model, nodes are first embedded
as vectors in latent feature space. Embeddings are then shared and jointly
trained according to task-specific and network-general objectives. We extend
the existing unsupervised network embedding to incorporate meta paths in
heterogeneous networks, and select paths according to the specific task. The
guidance from author identification task for network embedding is provided both
explicitly in joint training and implicitly during meta path selection. Our
experiments demonstrate that by using path-augmented network embedding with
task guidance, our model can obtain significantly better accuracy at
identifying the true authors comparing to existing methods.



Representations are fundamental to artificial intelligence. The performance
of a learning system depends on the type of representation used for
representing the data. Typically, these representations are hand-engineered
using domain knowledge. More recently, the trend is to learn these
representations through stochastic gradient descent in multi-layer neural
networks, which is called backprop. Learning the representations directly from
the incoming data stream reduces the human labour involved in designing a
learning system. More importantly, this allows in scaling of a learning system
for difficult tasks. In this paper, we introduce a new incremental learning
algorithm called crossprop, which learns incoming weights of hidden units based
on the meta-gradient descent approach, that was previously introduced by Sutton
(1992) and Schraudolph (1999) for learning step-sizes. The final update
equation introduces an additional memory parameter for each of these weights
and generalizes the backprop update equation. From our experiments, we show
that crossprop learns and reuses its feature representation while tackling new
and unseen tasks whereas backprop relearns a new feature representation.



Bayesian Optimization (BO) has become a core method for solving expensive
black-box optimization problems. While much research focussed on the choice of
the acquisition function, we focus on online length-scale adaption and the
choice of kernel function. Instead of choosing hyperparameters in view of
maximum likelihood on past data, we propose to use the acquisition function to
decide on hyperparameter adaptation more robustly and in view of the future
optimization progress. Further, we propose a particular kernel function that
includes non-stationarity and local anisotropy and thereby implicitly
integrates the efficiency of local convex optimization with global Bayesian
optimization. Comparisons to state-of-the art BO methods underline the
efficiency of these mechanisms on global optimization benchmarks.



Transcriptional profiling on microarrays to obtain gene expressions has been
used to facilitate cancer diagnosis. We propose a deep generative machine
learning architecture (called DeepCancer) that learn features from unlabeled
microarray data. These models have been used in conjunction with conventional
classifiers that perform classification of the tissue samples as either being
cancerous or non-cancerous. The proposed model has been tested on two different
clinical datasets. The evaluation demonstrates that DeepCancer model achieves a
very high precision score, while significantly controlling the false positive
and false negative scores.



Synthesizing high-quality images from text descriptions is a challenging
problem in computer vision and has many practical applications. Samples
generated by existing text-to-image approaches can roughly reflect the meaning
of the given descriptions, but they fail to contain necessary details and vivid
object parts. In this paper, we propose Stacked Generative Adversarial Networks
(StackGAN) to generate 256x256 photo-realistic images conditioned on text
descriptions. We decompose the hard problem into more manageable sub-problems
through a sketch-refinement process. The Stage-I GAN sketches the primitive
shape and colors of the object based on the given text description, yielding
Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text
descriptions as inputs, and generates high-resolution images with
photo-realistic details. It is able to rectify defects in Stage-I results and
add compelling details with the refinement process. To improve the diversity of
the synthesized images and stabilize the training of the conditional-GAN, we
introduce a novel Conditioning Augmentation technique that encourages
smoothness in the latent conditioning manifold. Extensive experiments and
comparisons with state-of-the-arts on benchmark datasets demonstrate that the
proposed method achieves significant improvements on generating photo-realistic
images conditioned on text descriptions.



Prediction in a small-sized sample with a large number of covariates, the
"small n, large p" problem, is challenging. This setting is encountered in
multiple applications, such as precision medicine, where obtaining additional
samples can be extremely costly or even impossible, and extensive research
effort has recently been dedicated to finding principled solutions for accurate
prediction. However, a valuable source of additional information, domain
experts, has not yet been efficiently exploited. We formulate knowledge
elicitation generally as a probabilistic inference process, where expert
knowledge is sequentially queried to improve predictions. In the specific case
of sparse linear regression, where we assume the expert has knowledge about the
values of the regression coefficients or about the relevance of the features,
we propose an algorithm and computational approximation for fast and efficient
interaction, which sequentially identifies the most informative features on
which to query expert knowledge. Evaluations of our method in experiments with
simulated and real users show improved prediction accuracy already with a small
effort from the expert.



Sparsity-constrained optimization is an important and challenging problem
that has wide applicability in data mining, machine learning, and statistics.
In this paper, we focus on sparsity-constrained optimization in cases where the
cost function is a general nonlinear function and, in particular, the sparsity
constraint is defined by a graph-structured sparsity model. Existing methods
explore this problem in the context of sparse estimation in linear models. To
the best of our knowledge, this is the first work to present an efficient
approximation algorithm, namely, Graph-structured Matching Pursuit (Graph-Mp),
to optimize a general nonlinear function subject to graph-structured
constraints. We prove that our algorithm enjoys the strong guarantees analogous
to those designed for linear models in terms of convergence rate and
approximation accuracy. As a case study, we specialize Graph-Mp to optimize a
number of well-known graph scan statistic models for the connected subgraph
detection task, and empirical evidence demonstrates that our general algorithm
performs superior over state-of-the-art methods that are designed specifically
for the task of connected subgraph detection.



Multiple instance learning (MIL) is a form of weakly supervised learning
where training instances are arranged in sets, called bags, and a label is
provided for the entire bag. This formulation is gaining interest because it
naturally fits various problems and allows to leverage weakly labeled data.
Consequently, it has been used in diverse application fields such as computer
vision and document classification. However, learning from bags raises
important challenges that are unique to MIL. This paper provides a
comprehensive survey of the characteristics which define and differentiate the
types of MIL problems. Until now, these problem characteristics have not been
formally identified and described. As a result, the variations in performance
of MIL algorithms from one data set to another are difficult to explain. In
this paper, MIL problem characteristics are grouped into four broad categories:
the composition of the bags, the types of data distribution, the ambiguity of
instance labels, and the task to be performed. Methods specialized to address
each category are reviewed. Then, the extent to which these characteristics
manifest themselves in key MIL application areas are described. Finally,
experiments are conducted to compare the performance of 16 state-of-the-art MIL
methods on selected problem characteristics. This paper provides insight on how
the problem characteristics affect MIL algorithms, recommendations for future
benchmarking and promising avenues for research.



We provide a brief technical description of an online platform for disease
monitoring, titled as the Flu Detector (fludetector.cs.ucl.ac.uk). Flu
Detector, in its current version (v.0.5), uses either Twitter or Google search
data in conjunction with statistical Natural Language Processing models to
estimate the rate of influenza-like illness in the population of England. Its
back-end is a live service that collects online data, utilises modern
technologies for large-scale text processing, and finally applies statistical
inference models that are trained offline. The front-end visualises the various
disease rate estimates. Notably, the models based on Google data achieve a high
level of accuracy with respect to the most recent four flu seasons in England
(2012/13 to 2015/16). This highlighted Flu Detector as having a great potential
of becoming a complementary source to the domestic traditional flu surveillance
schemes.



Several methods exist for a computer to generate music based on data
including Markov chains, recurrent neural networks, recombinancy, and grammars.
We explore the use of unit selection and concatenation as a means of generating
music using a procedure based on ranking, where, we consider a unit to be a
variable length number of measures of music. We first examine whether a unit
selection method, that is restricted to a finite size unit library, can be
sufficient for encompassing a wide spectrum of music. We do this by developing
a deep autoencoder that encodes a musical input and reconstructs the input by
selecting from the library. We then describe a generative model that combines a
deep structured semantic model (DSSM) with an LSTM to predict the next unit,
where units consist of four, two, and one measures of music. We evaluate the
generative model using objective metrics including mean rank and accuracy and
with a subjective listening test in which expert musicians are asked to
complete a forced-choiced ranking task. We compare our model to a note-level
generative baseline that consists of a stacked LSTM trained to predict forward
by one note.



Given a knowledge base (KB) rich in facts about common nouns or generics,
such as "all trees produce oxygen" or "some animals live in forests", we
consider the problem of deriving additional such facts at a high precision.
While this problem has received much attention for named entity KBs such as
Freebase, little emphasis has been placed on generics despite their importance
for capturing general knowledge. Different from named entity KBs, generics KBs
involve implicit or explicit quantification, have more complex underlying
regularities, are substantially more incomplete, and violate the commonly used
locally closed world assumption (LCWA). Consequently, existing completion
methods struggle with this new task. We observe that external information, such
as relation schemas and entity taxonomies, if used correctly, can be
surprisingly powerful in addressing the challenges associated with generics.
Using this insight, we propose a simple yet effective knowledge guided tensor
factorization approach that achieves state-of-the-art results on two generics
KBs for science, doubling their size at 74\%-86\% precision. Further, to
address the paucity of facts about rare entities such as oriole (a bird), we
present a novel taxonomy guided submodular active learning method to collect
additional annotations that are over five times more effective in inferring
further new facts than multiple active learning baselines.



We propose an online, end-to-end, neural generative conversational model for
open-domain dialogue. It is trained using a unique combination of offline
two-phase supervised learning and online human-in-the-loop active learning.
While most existing research proposes offline supervision or hand-crafted
reward functions for online reinforcement, we devise a novel interactive
learning mechanism based on hamming-diverse beam search for response generation
and one-character user-feedback at each step. Experiments show that our model
inherently promotes the generation of semantically relevant and interesting
responses, and can be used to train agents with customized personas, moods and
conversational styles.



A key drawback of the current generation of artificial decision-makers is
that they do not adapt well to changes in unexpected situations. This paper
addresses the situation in which an AI for aerial dog fighting, with tunable
parameters that govern its behavior, will optimize behavior with respect to an
objective function that must be evaluated and learned through simulations. Once
this objective function has been modeled, the agent can then choose its desired
behavior in different situations. Bayesian optimization with a Gaussian Process
surrogate is used as the method for investigating the objective function. One
key benefit is that during optimization the Gaussian Process learns a global
estimate of the true objective function, with predicted outcomes and a
statistical measure of confidence in areas that haven't been investigated yet.
However, standard Bayesian optimization does not perform consistently or
provide an accurate Gaussian Process surrogate function for highly volatile
objective functions. We treat these problems by introducing a novel sampling
technique called Hybrid Repeat/Multi-point Sampling. This technique gives the
AI ability to learn optimum behaviors in a highly uncertain environment. More
importantly, it not only improves the reliability of the optimization, but also
creates a better model of the entire objective surface. With this improved
model the agent is equipped to better adapt behaviors.



The first International Workshop on Verification and Validation of
Cyber-Physical Systems (V2CPS-16) was held in conjunction with the 12th
International Conference on integration of Formal Methods (iFM 2016) in
Reykjavik, Iceland. The purpose of V2CPS-16 was to bring together researchers
and experts of the fields of formal verification and cyber-physical systems
(CPS) to cover the theme of this workshop, namely a wide spectrum of
verification and validation methods including (but not limited to) control,
simulation, formal methods, etc.
  A CPS is an integration of networked computational and physical processes
with meaningful inter-effects; the former monitors, controls, and affects the
latter, while the latter also impacts the former. CPSs have applications in a
wide-range of systems spanning robotics, transportation, communication,
infrastructure, energy, and manufacturing. Many safety-critical systems such as
chemical processes, medical devices, aircraft flight control, and automotive
systems, are indeed CPS. The advanced capabilities of CPS require complex
software and synthesis algorithms, which are hard to verify. In fact, many
problems in this area are undecidable. Thus, a major step is to find particular
abstractions of such systems which might be algorithmically verifiable
regarding specific properties of such systems, describing the partial/overall
behaviors of CPSs.



A key requirement for the current generation of artificial decision-makers is
that they should adapt well to changes in unexpected situations. This paper
addresses the situation in which an AI for aerial dog fighting, with tunable
parameters that govern its behavior, must optimize behavior with respect to an
objective function that is evaluated and learned through simulations. Bayesian
optimization with a Gaussian Process surrogate is used as the method for
investigating the objective function. One key benefit is that during
optimization, the Gaussian Process learns a global estimate of the true
objective function, with predicted outcomes and a statistical measure of
confidence in areas that haven't been investigated yet. Having a model of the
objective function is important for being able to understand possible outcomes
in the decision space; for example this is crucial for training and providing
feedback to human pilots. However, standard Bayesian optimization does not
perform consistently or provide an accurate Gaussian Process surrogate function
for highly volatile objective functions. We treat these problems by introducing
a novel sampling technique called Hybrid Repeat/Multi-point Sampling. This
technique gives the AI ability to learn optimum behaviors in a highly uncertain
environment. More importantly, it not only improves the reliability of the
optimization, but also creates a better model of the entire objective surface.
With this improved model the agent is equipped to more accurately/efficiently
predict performance in unexplored scenarios.



Recent advances have shown the capability of Fully Convolutional Neural
Networks (FCN) to model cost functions for motion planning in the context of
learning driving preferences purely based on demonstration data from human
drivers. While pure learning from demonstrations in the framework of Inverse
Reinforcement Learning (IRL) is a promising approach, we can benefit from well
informed human priors and incorporate them into the learning process. Our work
achieves this by pretraining a model to regress to a manual cost function and
refining it based on Maximum Entropy Deep Inverse Reinforcement Learning. When
injecting prior knowledge as pretraining for the network, we achieve higher
robustness, more visually distinct obstacle boundaries, and the ability to
capture instances of obstacles that elude models that purely learn from
demonstration data. Furthermore, by exploiting these human priors, the
resulting model can more accurately handle corner cases that are scarcely seen
in the demonstration data, such as stairs, slopes, and underpasses.



In this paper we present an agent-based model (ABM) of scientific inquiry
aimed at investigating how different social networks impact the efficiency of
scientists in acquiring knowledge. As such, the ABM is a computational tool for
tackling issues in the domain of scientific methodology and science policy. In
contrast to existing ABMs of science, our model aims to represent the
argumentative dynamics that underlies scientific practice. To this end we
employ abstract argumentation theory as the core design feature of the model.
This helps to avoid a number of problematic idealizations which are present in
other ABMs of science and which impede their relevance for actual scientific
practice.



Whereas CNNs have demonstrated immense progress in many vision problems, they
suffer from a dependence on monumental amounts of labeled training data. On the
other hand, dictionary learning does not scale to the size of problems that
CNNs can handle, despite being very effective at low-level vision tasks such as
denoising and inpainting. Recently, interest has grown in adapting dictionary
learning methods for supervised tasks such as classification and inverse
problems. We propose two new network layers that are based on dictionary
learning: a sparse factorization layer and a convolutional sparse factorization
layer, analogous to fully-connected and convolutional layers, respectively.
Using our derivations, these layers can be dropped in to existing CNNs, trained
together in an end-to-end fashion with back-propagation, and leverage
semisupervision in ways classical CNNs cannot. We experimentally compare
networks with these two new layers against a baseline CNN. Our results
demonstrate that networks with either of the sparse factorization layers are
able to outperform classical CNNs when supervised data are few. They also show
performance improvements in certain tasks when compared to the CNN with no
sparse factorization layers with the same exact number of parameters.



We introduce a method for imposing higher-level structure on generated,
polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a
generative model is combined with gradient descent constraint optimization to
provide further control over the generation process. Among other things, this
allows for the use of a "template" piece, from which some structural properties
can be extracted, and transferred as constraints to newly generated material.
The sampling process is guided with Simulated Annealing in order to avoid local
optima, and find solutions that both satisfy the constraints, and are
relatively stable with respect to the C-RBM. Results show that with this
approach it is possible to control the higher level self-similarity structure,
the meter, as well as tonal properties of the resulting musical piece while
preserving its local musical coherence.



Deep models are the defacto standard in visual decision models due to their
impressive performance on a wide array of visual tasks. However, they are
frequently seen as opaque and are unable to explain their decisions. In
contrast, humans can justify their decisions with natural language and point to
the evidence in the visual world which led to their decisions. We postulate
that deep models can do this as well and propose our Pointing and Justification
(PJ-X) model which can justify its decision with a sentence and point to the
evidence by introspecting its decision and explanation process using an
attention mechanism. Unfortunately there is no dataset available with reference
explanations for visual decision making. We thus collect two datasets in two
domains where it is interesting and challenging to explain decisions. First, we
extend the visual question answering task to not only provide an answer but
also a natural language explanation for the answer. Second, we focus on
explaining human activities which is traditionally more challenging than object
classification. We extensively evaluate our PJ-X model, both on the
justification and pointing tasks, by comparing it to prior models and ablations
using both automatic and human evaluations.



This paper introduces the probabilistic module interface, which allows
encapsulation of complex probabilistic models with latent variables alongside
custom stochastic approximate inference machinery, and provides a
platform-agnostic abstraction barrier separating the model internals from the
host probabilistic inference system. The interface can be seen as a stochastic
generalization of a standard simulation and density interface for probabilistic
primitives. We show that sound approximate inference algorithms can be
constructed for networks of probabilistic modules, and we demonstrate that the
interface can be implemented using learned stochastic inference networks and
MCMC and SMC approximate inference programs.



User acceptance of artificial intelligence agents might depend on their
ability to explain their reasoning, which requires adding an interpretability
layer that fa- cilitates users to understand their behavior. This paper focuses
on adding an in- terpretable layer on top of Semantic Textual Similarity (STS),
which measures the degree of semantic equivalence between two sentences. The
interpretability layer is formalized as the alignment between pairs of segments
across the two sentences, where the relation between the segments is labeled
with a relation type and a similarity score. We present a publicly available
dataset of sentence pairs annotated following the formalization. We then
develop a system trained on this dataset which, given a sentence pair, explains
what is similar and different, in the form of graded and typed segment
alignments. When evaluated on the dataset, the system performs better than an
informed baseline, showing that the dataset and task are well-defined and
feasible. Most importantly, two user studies show how the system output can be
used to automatically produce explanations in natural language. Users performed
better when having access to the explanations, pro- viding preliminary evidence
that our dataset and method to automatically produce explanations is useful in
real applications.



We demonstrate the possibility of classifying causal systems into kinds that
share a common structure without first constructing an explicit dynamical model
or using prior knowledge of the system dynamics. The algorithmic ability to
determine whether arbitrary systems are governed by causal relations of the
same form offers significant practical applications in the development and
validation of dynamical models. It is also of theoretical interest as an
essential stage in the scientific inference of laws from empirical data. The
algorithm presented is based on the dynamical symmetry approach to dynamical
kinds. A dynamical symmetry with respect to time is an intervention on one or
more variables of a system that commutes with the time evolution of the system.
A dynamical kind is a class of systems sharing a set of dynamical symmetries.
The algorithm presented classifies deterministic, time-dependent causal systems
by directly comparing their exhibited symmetries. Using simulated, noisy data
from a variety of nonlinear systems, we show that this algorithm correctly
sorts systems into dynamical kinds. It is robust under significant sampling
error, is immune to violations of normality in sampling error, and fails
gracefully with increasing dynamical similarity. The algorithm we demonstrate
is the first to address this aspect of automated scientific discovery.



Existing models based on artificial neural networks (ANNs) for sentence
classification often do not incorporate the context in which sentences appear,
and classify sentences individually. However, traditional sentence
classification approaches have been shown to greatly benefit from jointly
classifying subsequent sentences, such as with conditional random fields. In
this work, we present an ANN architecture that combines the effectiveness of
typical ANN models to classify sentences in isolation, with the strength of
structured prediction. Our model achieves state-of-the-art results on two
different datasets for sequential sentence classification in medical abstracts.



Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to
large MAPF instances by searching for MAPF plans on 2 levels: The high-level
search resolves collisions between agents, and the low-level search plans paths
for single agents under the constraints imposed by the high-level search. We
make the following contributions to solve the MAPF problem with imperfect plan
execution with small average makespans: First, we formalize the MAPF Problem
with Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the
use of robust plan-execution policies for valid MAPF-DP plans to control how
each agent proceeds along its path. Second, we discuss 2 classes of
decentralized robust plan-execution policies (called Fully Synchronized
Policies and Minimal Communication Policies) that prevent collisions during
plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP
solver (called Approximate Minimization in Expectation) that generates valid
MAPF-DP plans.



In this paper we consider the problem of robot navigation in simple maze-like
environments where the robot has to rely on its onboard sensors to perform the
navigation task. In particular, we are interested in solutions to this problem
that do not require localization, mapping or planning. Additionally, we require
that our solution can quickly adapt to new situations (e.g., changing
navigation goals and environments). To meet these criteria we frame this
problem as a sequence of related reinforcement learning tasks. We propose a
successor feature based deep reinforcement learning algorithm that can learn to
transfer knowledge from previously mastered navigation tasks to new problem
instances. Our algorithm substantially decreases the required learning time
after the first task instance has been solved, which makes it easily adaptable
to changing environments. We validate our method in both simulated and real
robot experiments with a Robotino and compare it to a set of baseline methods
including classical planning-based navigation.



A softmax operator applied to a set of values acts somewhat like the
maximization function and somewhat like an average. In sequential decision
making, softmax is often used in settings where it is necessary to maximize
utility but also to hedge against problems that arise from putting all of one's
weight behind a single maximum utility decision. The Boltzmann softmax operator
is the most commonly used softmax operator in this setting, but we show that
this operator is prone to misbehavior. In this work, we study a differentiable
softmax operator that, among other properties, is a non-expansion ensuring a
convergent behavior in learning and planning. We introduce a variant of SARSA
algorithm that, by utilizing the new operator, computes a Boltzmann policy with
a state-dependent temperature parameter. We show that the algorithm is
convergent and that it performs favorably in practice.



Despite widespread interests in reinforcement-learning for task-oriented
dialogue systems, several obstacles can frustrate research and development
progress. First, reinforcement learners typically require interaction with the
environment, so conventional dialogue corpora cannot be used directly. Second,
each task presents specific challenges, requiring separate corpus of
task-specific annotated data. Third, collecting and annotating human-machine or
human-human conversations for task-oriented dialogues requires extensive domain
knowledge. Because building an appropriate dataset can be both financially
costly and time-consuming, one popular approach is to build a user simulator
based upon a corpus of example dialogues. Then, one can train reinforcement
learning agents in an online fashion as they interact with the simulator.
Dialogue agents trained on these simulators can serve as an effective starting
point. Once agents master the simulator, they may be deployed in a real
environment to interact with humans, and continue to be trained online. To ease
empirical algorithmic comparisons in dialogues, this paper introduces a new,
publicly available simulation framework, where our simulator, designed for the
movie-booking domain, leverages both rules and collected data. The simulator
supports two tasks: movie ticket booking and movie seeking. Finally, we
demonstrate several agents and detail the procedure to add and test your own
agent in the proposed framework.



We study the TAPF (combined target-assignment and path-finding) problem for
teams of agents in known terrain, which generalizes both the anonymous and
non-anonymous multi-agent path-finding problems. Each of the teams is given the
same number of targets as there are agents in the team. Each agent has to move
to exactly one target given to its team such that all targets are visited. The
TAPF problem is to first assign agents to targets and then plan collision-free
paths for the agents to their targets in a way such that the makespan is
minimized. We present the CBM (Conflict-Based Min-Cost-Flow) algorithm, a
hierarchical algorithm that solves TAPF instances optimally by combining ideas
from anonymous and non-anonymous multi-agent path-finding algorithms. On the
low level, CBM uses a min-cost max-flow algorithm on a time-expanded network to
assign all agents in a single team to targets and plan their paths. On the high
level, CBM uses conflict-based search to resolve collisions among agents in
different teams. Theoretically, we prove that CBM is correct, complete and
optimal. Experimentally, we show the scalability of CBM to TAPF instances with
dozens of teams and hundreds of agents and adapt it to a simulated warehouse
system.



The increasing availability of implicit feedback datasets has raised the
interest in developing effective collaborative filtering techniques able to
deal asymmetrically with unambiguous positive feedback and ambiguous negative
feedback. In this paper, we propose a principled kernel-based collaborative
filtering method for top-N item recommendation with implicit feedback. We
present an efficient implementation using the linear kernel, and we show how to
generalize it to kernels of the dot product family preserving the efficiency.
We also investigate on the elements which influence the sparsity of a standard
cosine kernel. This analysis shows that the sparsity of the kernel strongly
depends on the properties of the dataset, in particular on the long tail
distribution. We compare our method with state-of-the-art algorithms achieving
good results both in terms of efficiency and effectiveness.



In this project we propose a new approach for emotion recognition using
web-based similarity (e.g. confidence, PMI and PMING). We aim to extract basic
emotions from short sentences with emotional content (e.g. news titles, tweets,
captions), performing a web-based quantitative evaluation of semantic proximity
between each word of the analyzed sentence and each emotion of a psychological
model (e.g. Plutchik, Ekman, Lovheim). The phases of the extraction include:
text preprocessing (tokenization, stop words, filtering), search engine
automated query, HTML parsing of results (i.e. scraping), estimation of
semantic proximity, ranking of emotions according to proximity measures. The
main idea is that, since it is possible to generalize semantic similarity under
the assumption that similar concepts co-occur in documents indexed in search
engines, therefore also emotions can be generalized in the same way, through
tags or terms that express them in a particular language, ranking emotions.
Training results are compared to human evaluation, then additional comparative
tests on results are performed, both for the global ranking correlation (e.g.
Kendall, Spearman, Pearson) both for the evaluation of the emotion linked to
each single word. Different from sentiment analysis, our approach works at a
deeper level of abstraction, aiming at recognizing specific emotions and not
only the positive/negative sentiment, in order to predict emotions as semantic
data.



In this paper, we consider a realistic and meaningful scenario in the context
of smart grids where an electricity retailer serves three different types of
customers, i.e., customers with an optimal home energy management system
embedded in their smart meters (C-HEMS), customers with only smart meters
(C-SM), and customers without smart meters (C-NONE). The main objective of this
paper is to support the retailer to make optimal day-ahead dynamic pricing
decisions in such a mixed customer pool. To this end, we propose a two-level
decision-making framework where the retailer acting as upper-level agent
firstly announces its electricity prices of next 24 hours and customers acting
as lower-level agents subsequently schedule their energy usages accordingly.
For the lower level problem, we model the price responsiveness of different
customers according to their unique characteristics. For the upper level
problem, we optimize the dynamic prices for the retailer to maximize its profit
subject to realistic market constraints. The above two-level model is tackled
by genetic algorithms (GAs) based distributed optimization methods while its
feasibility and effectiveness are confirmed via simulation results.



Representing a dialog policy as a recurrent neural network (RNN) is
attractive because it handles partial observability, infers a latent
representation of state, and can be optimized with supervised learning (SL) or
reinforcement learning (RL). For RL, a policy gradient approach is natural, but
is sample inefficient. In this paper, we present 3 methods for reducing the
number of dialogs required to optimize an RNN-based dialog policy with RL. The
key idea is to maintain a second RNN which predicts the value of the current
policy, and to apply experience replay to both networks. On two tasks, these
methods reduce the number of dialogs/episodes required by about a third, vs.
standard policy gradient methods.



Algorithms for equilibrium computation generally make no attempt to ensure
that the computed strategies are understandable by humans. For instance the
strategies for the strongest poker agents are represented as massive binary
files. In many situations, we would like to compute strategies that can
actually be implemented by humans, who may have computational limitations and
may only be able to remember a small number of features or components of the
strategies that have been computed. We study poker games where private
information distributions can be arbitrary. We create a large training set of
game instances and solutions, by randomly selecting the information
probabilities, and present algorithms that learn from the training instances in
order to perform well in games with unseen information distributions. We are
able to conclude several new fundamental rules about poker strategy that can be
easily implemented by humans.



This paper analyzes customer product-choice behavior based on the recency and
frequency of each customer's page views on e-commerce sites. Recently, we
devised an optimization model for estimating product-choice probabilities that
satisfy monotonicity, convexity, and concavity constraints with respect to
recency and frequency. This shape-restricted model delivered high predictive
performance even when there were few training samples. However, typical
e-commerce sites deal in many different varieties of products, so the
predictive performance of the model can be further improved by integration of
such product heterogeneity. For this purpose, we develop a novel latent-class
shape-restricted model for estimating product-choice probabilities for each
latent class of products. We also give a tailored expectation-maximization
algorithm for parameter estimation. Computational results demonstrate that
higher predictive performance is achieved with our latent-class model than with
the previous shape-restricted model and common latent-class logistic
regression.



A dominant paradigm for deep learning based object detection relies on a
"bottom-up" approach using "passive" scoring of class agnostic proposals. These
approaches are efficient but lack of holistic analysis of scene-level context.
In this paper, we present an "action-driven" detection mechanism using our
"top-down" visual attention model. We localize an object by taking sequential
actions that the attention model provides. The attention model conditioned with
an image region provides required actions to get closer toward a target object.
An action at each time step is weak itself but an ensemble of the sequential
actions makes a bounding-box accurately converge to a target object boundary.
This attention model we call AttentionNet is composed of a convolutional neural
network. During our whole detection procedure, we only utilize the actions from
a single AttentionNet without any modules for object proposals nor post
bounding-box regression. We evaluate our top-down detection mechanism over the
PASCAL VOC series and ILSVRC CLS-LOC dataset, and achieve state-of-the-art
performances compared to the major bottom-up detection methods. In particular,
our detection mechanism shows a strong advantage in elaborate localization by
outperforming Faster R-CNN with a margin of +7.1% over PASCAL VOC 2007 when we
increase the IoU threshold for positive detection to 0.7.



This article analyzes the stochastic runtime of a Cross-Entropy Algorithm on
two classes of traveling salesman problems. The algorithm shares main features
of the famous Max-Min Ant System with iteration-best reinforcement.
  For simple instances that have a $\{1,n\}$-valued distance function and a
unique optimal solution, we prove a stochastic runtime of $O(n^{6+\epsilon})$
with the vertex-based random solution generation, and a stochastic runtime of
$O(n^{3+\epsilon}\ln n)$ with the edge-based random solution generation for an
arbitrary $\epsilon\in (0,1)$. These runtimes are very close to the known
expected runtime for variants of Max-Min Ant System with best-so-far
reinforcement. They are obtained for the stronger notion of stochastic runtime,
which means that an optimal solution is obtained in that time with an
overwhelming probability, i.e., a probability tending exponentially fast to one
with growing problem size.
  We also inspect more complex instances with $n$ vertices positioned on an
$m\times m$ grid. When the $n$ vertices span a convex polygon, we obtain a
stochastic runtime of $O(n^{3}m^{5+\epsilon})$ with the vertex-based random
solution generation, and a stochastic runtime of $O(n^{2}m^{5+\epsilon})$ for
the edge-based random solution generation. When there are $k = O(1)$ many
vertices inside a convex polygon spanned by the other $n-k$ vertices, we obtain
a stochastic runtime of $O(n^{4}m^{5+\epsilon}+n^{6k-1}m^{\epsilon})$ with the
vertex-based random solution generation, and a stochastic runtime of
$O(n^{3}m^{5+\epsilon}+n^{3k}m^{\epsilon})$ with the edge-based random solution
generation. These runtimes are better than the expected runtime for the
so-called $(\mu\!+\!\lambda)$ EA reported in a recent article, and again
obtained for the stronger notion of stochastic runtime.



We introduce ARES, an efficient approximation algorithm for generating
optimal plans (action sequences) that take an initial state of a Markov
Decision Process (MDP) to a state whose cost is below a specified (convergence)
threshold. ARES uses Particle Swarm Optimization, with adaptive sizing for both
the receding horizon and the particle swarm. Inspired by Importance Splitting,
the length of the horizon and the number of particles are chosen such that at
least one particle reaches a next-level state, that is, a state where the cost
decreases by a required delta from the previous-level state. The level relation
on states and the plans constructed by ARES implicitly define a Lyapunov
function and an optimal policy, respectively, both of which could be explicitly
generated by applying ARES to all states of the MDP, up to some topological
equivalence relation. We also assess the effectiveness of ARES by statistically
evaluating its rate of success in generating optimal plans. The ARES algorithm
resulted from our desire to clarify if flying in V-formation is a flocking
policy that optimizes energy conservation, clear view, and velocity alignment.
That is, we were interested to see if one could find optimal plans that bring a
flock from an arbitrary initial state to a state exhibiting a single connected
V-formation. For flocks with 7 birds, ARES is able to generate a plan that
leads to a V-formation in 95% of the 8,000 random initial configurations within
63 seconds, on average. ARES can also be easily customized into a
model-predictive controller (MPC) with an adaptive receding horizon and
statistical guarantees of convergence. To the best of our knowledge, our
adaptive-sizing approach is the first to provide convergence guarantees in
receding-horizon techniques.



Deep-learning has dramatically changed the world overnight. It greatly
boosted the development of visual perception, object detection, and speech
recognition, etc. That was attributed to the multiple convolutional processing
layers for abstraction of learning representations from massive data. The
advantages of deep convolutional structures in data processing motivated the
applications of artificial intelligence methods in robotic problems, especially
perception and control system, the two typical and challenging problems in
robotics. This paper presents a survey of the deep-learning research landscape
in mobile robotics. We start with introducing the definition and development of
deep-learning in related fields, especially the essential distinctions between
image processing and robotic tasks. We described and discussed several typical
applications and related works in this domain, followed by the benefits from
deep-learning, and related existing frameworks. Besides, operation in the
complex dynamic environment is regarded as a critical bottleneck for mobile
robots, such as that for autonomous driving. We thus further emphasize the
recent achievement on how deep-learning contributes to navigation and control
systems for mobile robots. At the end, we discuss the open challenges and
research frontiers.



The raise of complexity of technical systems also raises knowledge required
to set them up and to maintain them. The cost to evolve such systems can be
prohibitive. In the field of Autonomic Computing, technical systems should
therefore have various self-healing capabilities allowing system owners to
provide only partial, potentially inconsistent updates of the system. The
self-healing or self-integrating system shall find out the remaining changes to
communications and functionalities in order to accommodate change and yet still
restore function. This issue becomes even more interesting in context of
Internet of Things and Industrial Internet where previously unexpected device
combinations can be assembled in order to provide a surprising new function. In
order to pursue higher levels of self-integration capabilities I propose to
think of self-integration as sophisticated error correcting communications.
Therefore, this paper discusses an extended scope of error correction with the
purpose to emphasize error correction's role as an integrated element of
bi-directional communication channels in self-integrating, autonomic
communication scenarios.



This paper investigates a type of instability that is linked to the greedy
policy improvement in approximated reinforcement learning. We show empirically
that non-deterministic policy improvement can stabilize methods like LSPI by
controlling the improvements' stochasticity. Additionally we show that a
suitable representation of the value function also stabilizes the solution to
some degree. The presented approach is simple and should also be easily
transferable to more sophisticated algorithms like deep reinforcement learning.



While the solution counting problem for propositional satisfiability (#SAT)
has received renewed attention in recent years, this research trend has not
affected other AI solving paradigms like answer set programming (ASP). Although
ASP solvers are designed to enumerate all solutions, and counting can therefore
be easily done, the involved materialization of all solutions is a clear
bottleneck for the counting problem of ASP (#ASP). In this paper we propose
dynamic programming-based #ASP algorithms that exploit the structure of the
underlying (ground) ASP program. Experimental results for a prototype
implementation show promise when compared to existing solvers.



The past year saw the introduction of new architectures such as Highway
networks and Residual networks which, for the first time, enabled the training
of feedforward networks with dozens to hundreds of layers using simple gradient
descent. While depth of representation has been posited as a primary reason for
their success, there are indications that these architectures defy a popular
view of deep learning as a hierarchical computation of increasingly abstract
features at each layer.
  In this report, we argue that this view is incomplete and does not adequately
explain several recent findings. We propose an alternative viewpoint based on
unrolled iterative estimation -- a group of successive layers iteratively
refine their estimates of the same features instead of computing an entirely
new representation. We demonstrate that this viewpoint directly leads to the
construction of Highway and Residual networks. Finally we provide preliminary
experiments to discuss the similarities and differences between the two
architectures.



Single image super-resolution is the task of inferring a high-resolution
image from a single low-resolution input. Traditionally, the performance of
algorithms for this task is measured using pixel-wise reconstruction measures
such as peak signal-to-noise ratio (PSNR) which have been shown to correlate
poorly with the human perception of image quality. As a result, algorithms
minimizing these metrics tend to produce over-smoothed images that lack
high-frequency textures and do not look natural despite yielding high PSNR
values.
  We propose a novel application of automated texture synthesis in combination
with a perceptual loss focusing on creating realistic textures rather than
optimizing for a pixel-accurate reproduction of ground truth images during
training. By using feed-forward fully convolutional neural networks in an
adversarial training setting, we achieve a significant boost in image quality
at high magnification ratios. Extensive experiments on a number of datasets
show the effectiveness of our approach, yielding state-of-the-art results in
both quantitative and qualitative benchmarks.



The paper proposes an analysis of liquid democracy (or, delegable proxy
voting) from the perspective of binary aggregation and of binary diffusion
models. We show how liquid democracy on binary issues can be embedded into the
framework of binary aggregation with abstentions, enabling the transfer of
known results about the latter---such as impossibility theorems---to the
former. This embedding also sheds light on the relation between delegation
cycles in liquid democracy and the probability of collective abstentions, as
well as the issue of individual rationality in a delegable proxy voting
setting. We then show how liquid democracy on binary issues can be modeled and
analyzed also as a specific process of dynamics of binary opinions on networks.
These processes---called Boolean DeGroot processes---are a special case of the
DeGroot stochastic model of opinion diffusion. We establish the convergence
conditions of such processes and show they provide some novel insights on how
the effects of delegation cycles and individual rationality could be mitigated
within liquid democracy.
  The study is a first attempt to provide theoretical foundations to the
delgable proxy features of the liquid democracy voting system. Our analysis
suggests recommendations on how the system may be modified to make it more
resilient with respect to the handling of delegation cycles and of inconsistent
majorities.



Data science models, although successful in a number of commercial domains,
have had limited applicability in scientific problems involving complex
physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm
that aims to leverage the wealth of scientific knowledge for improving the
effectiveness of data science models in enabling scientific discovery. The
overarching vision of TGDS is to introduce scientific consistency as an
essential component for learning generalizable models. Further, by producing
scientifically interpretable models, TGDS aims to advance our scientific
understanding by discovering novel domain insights. Indeed, the paradigm of
TGDS has started to gain prominence in a number of scientific disciplines such
as turbulence modeling, material discovery, quantum chemistry, bio-medical
science, bio-marker discovery, climate science, and hydrology. In this paper,
we formally conceptualize the paradigm of TGDS and present a taxonomy of
research themes in TGDS. We describe several approaches for integrating domain
knowledge in different research themes using illustrative examples from
different disciplines. We also highlight some of the promising avenues of novel
research for realizing the full potential of theory-guided data science.



Algorithms which sort lists of real numbers into ascending order have been
studied for decades. They are typically based on a series of pairwise
comparisons and run entirely on chip. However people routinely sort lists which
depend on subjective or complex judgements that cannot be automated. Examples
include marketing research; where surveys are used to learn about customer
preferences for products, the recruiting process; where interviewers attempt to
rank potential employees, and sporting tournaments; where we infer team
rankings from a series of one on one matches. We develop a novel sorting
algorithm, where each pairwise comparison reflects a subjective human judgement
about which element is bigger or better. We introduce a finite and large error
rate to each judgement, and we take the cost of each comparison to
significantly exceed the cost of other computational steps. The algorithm must
request the most informative sequence of comparisons from the user; in order to
identify the correct sorted list with minimum human input. Our Discrete
Adiabatic Monte Carlo approach exploits the gradual acquisition of information
by tracking a set of plausible hypotheses which are updated after each
additional comparison.



AUC (Area under the ROC curve) is an important performance measure for
applications where the data is highly imbalanced. Learning to maximize AUC
performance is thus an important research problem. Using a max-margin based
surrogate loss function, AUC optimization problem can be approximated as a
pairwise rankSVM learning problem. Batch learning methods for solving the
kernelized version of this problem suffer from scalability and may not result
in sparse classifiers. Recent years have witnessed an increased interest in the
development of online or single-pass online learning algorithms that design a
classifier by maximizing the AUC performance. The AUC performance of nonlinear
classifiers, designed using online methods, is not comparable with that of
nonlinear classifiers designed using batch learning algorithms on many
real-world datasets. Motivated by these observations, we design a scalable
algorithm for maximizing AUC performance by greedily adding the required number
of basis functions into the classifier model. The resulting sparse classifiers
perform faster inference. Our experimental results show that the level of
sparsity achievable can be order of magnitude smaller than the Kernel RankSVM
model without affecting the AUC performance much.



One of the key challenges of artificial intelligence is to learn models that
are effective in the context of planning. In this document we introduce the
predictron architecture. The predictron consists of a fully abstract model,
represented by a Markov reward process, that can be rolled forward multiple
"imagined" planning steps. Each forward pass of the predictron accumulates
internal rewards and values over multiple planning depths. The predictron is
trained end-to-end so as to make these accumulated values accurately
approximate the true value function. We applied the predictron to procedurally
generated random mazes and a simulator for the game of pool. The predictron
yielded significantly more accurate predictions than conventional deep neural
network architectures.



Convolutions have long been regarded as fundamental to applied mathematics,
physics and engineering. Their mathematical elegance allows for common tasks
such as numerical differentiation to be computed efficiently on large data
sets. Efficient computation of convolutions is critical to artificial
intelligence in real-time applications, like machine vision, where convolutions
must be continuously and efficiently computed on tens to hundreds of kilobytes
per second. In this paper, we explore how convolutions are used in fundamental
machine vision applications. We present an accelerated n-dimensional
convolution package in the high performance computing language, Julia, and
demonstrate its efficacy in solving the time to contact problem for machine
vision. Results are measured against synthetically generated videos and
quantitatively assessed according to their mean squared error from the ground
truth. We achieve over an order of magnitude decrease in compute time and
allocated memory for comparable machine vision applications. All code is
packaged and integrated into the official Julia Package Manager to be used in
various other scenarios.



Defining various dishonest notions in a formal way is a key step to enable
intelligent agents to act in untrustworthy environments. This review evaluates
the literature for this topic by looking at formal definitions based on modal
logic as well as other formal approaches. Criteria from philosophical
groundwork is used to assess the definitions for correctness and completeness.
The key contribution of this review is to show that only a few definitions
fully comply with this gold standard and to point out the missing steps towards
a successful application of these definitions in an actual agent environment.



We tackle the issue of finding a good policy when the number of policy
updates is limited. This is done by approximating the expected policy reward as
a sequence of concave lower bounds which can be efficiently maximized,
drastically reducing the number of policy updates required to achieve good
performance. We also extend existing methods to negative rewards, enabling the
use of control variates.



We introduce a new paradigm to investigate unsupervised learning, reducing
unsupervised learning to supervised learning. Specifically, we mitigate the
subjectivity in unsupervised decision-making by leveraging knowledge acquired
from prior, possibly heterogeneous, supervised learning tasks. We demonstrate
the versatility of our framework via comprehensive expositions and detailed
experiments on several unsupervised problems such as (a) clustering, (b)
outlier detection, and (c) similarity prediction under a common umbrella of
meta-unsupervised-learning. We also provide rigorous PAC-agnostic bounds to
establish the theoretical foundations of our framework, and show that our
framing of meta-clustering circumvents Kleinberg's impossibility theorem for
clustering.



Despite of the pain and limited accuracy of blood tests for early recognition
of cardiovascular disease, they dominate risk screening and triage. On the
other hand, heart rate variability is non-invasive and cheap, but not
considered accurate enough for clinical practice. Here, we tackle heart beat
interval based classification with deep learning. We introduce an end to end
differentiable hybrid architecture, consisting of a layer of biological neuron
models of cardiac dynamics (modified FitzHugh Nagumo neurons) and several
layers of a standard feed-forward neural network. The proposed model is
evaluated on ECGs from 474 stable at-risk (coronary artery disease) patients,
and 1172 chest pain patients of an emergency department. We show that it can
significantly outperform models based on traditional heart rate variability
predictors, as well as approaching or in some cases outperforming clinical
blood tests, based only on 60 seconds of inter-beat intervals.



We propose a new formalism for specifying and reasoning about problems that
involve heterogeneous "pieces of information" -- large collections of data,
decision procedures of any kind and complexity and connections between them.
The essence of our proposal is to lift Codd's relational algebra from
operations on relational tables to operations on classes of structures (with
recursion), and to add a direction of information propagation. We observe the
presence of information propagation in several formalisms for efficient
reasoning and use it to express unary negation and operations used in graph
databases. We carefully analyze several reasoning tasks and establish a precise
connection between a generalized query evaluation and temporal logic model
checking. Our development allows us to reveal a general correspondence between
classical and modal logics and may shed a new light on the good computational
properties of modal logics and related formalisms.



Temporal Difference learning or TD($\lambda$) is a fundamental algorithm in
the field of reinforcement learning. However, setting TD's $\lambda$ parameter,
which controls the timescale of TD updates, is generally left up to the
practitioner. We formalize the $\lambda$ selection problem as a bias-variance
trade-off where the solution is the value of $\lambda$ that leads to the
smallest Mean Squared Value Error (MSVE). To solve this trade-off we suggest
applying Leave-One-Trajectory-Out Cross-Validation (LOTO-CV) to search the
space of $\lambda$ values. Unfortunately, this approach is too computationally
expensive for most practical applications. For Least Squares TD (LSTD) we show
that LOTO-CV can be implemented efficiently to automatically tune $\lambda$ and
apply function optimization methods to efficiently search the space of
$\lambda$ values. The resulting algorithm, ALLSTD, is parameter free and our
experiments demonstrate that ALLSTD is significantly computationally faster
than the na\"{i}ve LOTO-CV implementation while achieving similar performance.



Referring expressions are natural language constructions used to identify
particular objects within a scene. In this paper, we propose a unified
framework for the tasks of referring expression comprehension and generation.
Our model is composed of three modules: speaker, listener, and reinforcer. The
speaker generates referring expressions, the listener comprehends referring
expressions, and the reinforcer introduces a reward function to guide sampling
of more discriminative expressions. The listener-speaker modules are trained
jointly in an end-to-end learning framework, allowing the modules to be aware
of one another during learning while also benefiting from the discriminative
reinforcer's feedback. We demonstrate that this unified framework and training
achieves state-of-the-art results for both comprehension and generation on
three referring expression datasets. Project and demo page:
https://vision.cs.unc.edu/refer



The causal structure of any system can be analyzed at a multitude of spatial
and temporal scales. It has long been thought that while higher scale (macro)
descriptions of causal structure may be useful to observers, they are at best a
compressed description and at worse leave out critical information. However,
recent research applying information theory to causal analysis has shown that
the causal structure of some systems can actually come into focus (be more
informative) at a macroscale (Hoel et al. 2013). That is, a macro model of a
system (a map) can be more informative than a fully detailed model of the
system (the territory). This has been called causal emergence. While causal
emergence may at first glance seem counterintuitive, this paper grounds the
phenomenon in a classic concept from information theory: Shannon's discovery of
the channel capacity. I argue that systems have a particular causal capacity,
and that different causal models of those systems take advantage of that
capacity to various degrees. For some systems, only macroscale causal models
use the full causal capacity. Such macroscale causal models can either be
coarse-grains, or may leave variables and states out of the model (exogenous)
in various ways, which can improve the model's efficacy and its informativeness
via the same mathematical principles of how error-correcting codes take
advantage of an information channel's capacity. As model choice increase, the
causal capacity of a system approaches the channel capacity. Ultimately, this
provides a general framework for understanding how the causal structure of some
systems cannot be fully captured by even the most detailed microscopic model.



Non-negative matrix factorization (NMF) is a prob- lem with many
applications, ranging from facial recognition to document clustering. However,
due to the variety of algorithms that solve NMF, the randomness involved in
these algorithms, and the somewhat subjective nature of the problem, there is
no clear "correct answer" to any particular NMF problem, and as a result, it
can be hard to test new algorithms. This paper suggests some test cases for NMF
algorithms derived from matrices with enumerable exact non-negative
factorizations and perturbations of these matrices. Three algorithms using
widely divergent approaches to NMF all give similar solutions over these test
cases, suggesting that these test cases could be used as test cases for
implementations of these existing NMF algorithms as well as potentially new NMF
algorithms. This paper also describes how the proposed test cases could be used
in practice.



This paper tackles the reduction of redundant repeating generation that is
often observed in RNN-based encoder-decoder models. Our basic idea is to
jointly estimate the upper-bound frequency of each target vocabulary in the
encoder and control the output words based on the estimation in the decoder.
Our method shows significant improvement over a strong RNN-based
encoder-decoder baseline and achieved its best results on an abstractive
summarization benchmark.



Modern knowledge base systems frequently need to combine a collection of
databases in different formats: e.g., relational databases, XML databases, rule
bases, ontologies, etc. In the deductive database system DDBASE, we can manage
these different formats of knowledge and reason about them. Even the file
systems on different computers can be part of the knowledge base. Often, it is
necessary to handle different versions of a knowledge base. E.g., we might want
to find out common parts or differences of two versions of a relational
database.
  We will examine the use of abstractions of rule bases by predicate dependency
and rule predicate graphs. Also the proof trees of derived atoms can help to
compare different versions of a rule base. Moreover, it might be possible to
have derivations joining rules with other formalisms of knowledge
representation.
  Ontologies have shown their benefits in many applications of intelligent
systems, and there have been many proposals for rule languages compatible with
the semantic web stack, e.g., SWRL, the semantic web rule language. Recently,
ontologies are used in hybrid systems for specifying the provenance of the
different components.



We propose a swarm-based optimization algorithm inspired by air currents of a
tornado. Two main air currents - spiral and updraft - are mimicked. Spiral
motion is designed for exploration of new search areas and updraft movements is
deployed for exploitation of a promising candidate solution. Assignment of just
one search direction to each particle at each iteration, leads to low
computational complexity of the proposed algorithm respect to the conventional
algorithms. Regardless of the step size parameters, the only parameter of the
proposed algorithm, called tornado diameter, can be efficiently adjusted by
randomization. Numerical results over six different benchmark cost functions
indicate comparable and, in some cases, better performance of the proposed
algorithm respect to some other metaheuristics.



We revisit the notion of probably approximately correct implication bases
from the literature and present a first formulation in the language of formal
concept analysis, with the goal to investigate whether such bases represent a
suitable substitute for exact implication bases in practical use-cases. To this
end, we quantitatively examine the behavior of probably approximately correct
implication bases on artificial and real-world data sets and compare their
precision and recall with respect to their corresponding exact implication
bases. Using a small example, we also provide qualitative insight that
implications from probably approximately correct bases can still represent
meaningful knowledge from a given data set.



In this paper, we study learning generalized driving style representations
from automobile GPS trip data. We propose a novel Autoencoder Regularized deep
neural Network (ARNet) and a trip encoding framework trip2vec to learn drivers'
driving styles directly from GPS records, by combining supervised and
unsupervised feature learning in a unified architecture. Experiments on a
challenging driver number estimation problem and the driver identification
problem show that ARNet can learn a good generalized driving style
representation: It significantly outperforms existing methods and alternative
architectures by reaching the least estimation error on average (0.68, less
than one driver) and the highest identification accuracy (by at least 3%
improvement) compared with traditional supervised learning methods.



Existing multi-objective reinforcement learning (MORL) algorithms do not
account for objectives that arise from players with differing beliefs.
Concretely, consider two players with different beliefs and utility functions
who may cooperate to build a machine that takes actions on their behalf. A
representation is needed for how much the machine's policy will prioritize each
player's interests over time. Assuming the players have reached common
knowledge of their situation, this paper derives a recursion that any Pareto
optimal policy must satisfy. Two qualitative observations can be made from the
recursion: the machine must (1) use each player's own beliefs in evaluating how
well an action will serve that player's utility function, and (2) shift the
relative priority it assigns to each player's expected utilities over time, by
a factor proportional to how well that player's beliefs predict the machine's
inputs. Observation (2) represents a substantial divergence from na\"{i}ve
linear utility aggregation (as in Harsanyi's utilitarian theorem, and existing
MORL algorithms), which is shown here to be inadequate for Pareto optimal
sequential decision-making on behalf of players with different beliefs.



To ease the development of robot learning in industry, two conditions need to
be fulfilled. Manipulators must be able to learn high accuracy and precision
tasks while being safe for workers in the factory. In this paper, we extend
previously submitted work which consists in rapid learning of local high
accuracy behaviors. By exploration and regression, linear and quadratic models
are learnt for respectively the dynamics and cost function. Iterative Linear
Quadratic Gaussian Regulator combined with cost quadratic regression can
converge rapidly in the final stages towards high accuracy behavior as the cost
function is modelled quite precisely. In this paper, both a different cost
function and a second order improvement method are implemented within this
framework. We also propose an analysis of the algorithm parameters through
simulation for a positioning task. Finally, an experimental validation on a
KUKA LBR iiwa robot is carried out. This collaborative robot manipulator can be
easily programmed into safety mode, which makes it qualified for the second
industry constraint stated above.



We present a general framework, the coupled compound Poisson factorization
(CCPF), to capture the missing-data mechanism in extremely sparse data sets by
coupling a hierarchical Poisson factorization with an arbitrary data-generating
model. We derive a stochastic variational inference algorithm for the resulting
model and, as examples of our framework, implement three different
data-generating models---a mixture model, linear regression, and factor
analysis---to robustly model non-random missing data in the context of
clustering, prediction, and matrix factorization. In all three cases, we test
our framework against models that ignore the missing-data mechanism on large
scale studies with non-random missing data, and we show that explicitly
modeling the missing-data mechanism substantially improves the quality of the
results, as measured using data log likelihood on a held-out test set.



One of the main problems that emerges in the classic approach to semantics is
the difficulty in acquisition and maintenance of ontologies and semantic
annotations. On the other hand, the Internet explosion and the massive
diffusion of mobile smart devices lead to the creation of a worldwide system,
which information is daily checked and fueled by the contribution of millions
of users who interacts in a collaborative way. Search engines, continually
exploring the Web, are a natural source of information on which to base a
modern approach to semantic annotation. A promising idea is that it is possible
to generalize the semantic similarity, under the assumption that semantically
similar terms behave similarly, and define collaborative proximity measures
based on the indexing information returned by search engines. The PMING
Distance is a proximity measure used in data mining and information retrieval,
which collaborative information express the degree of relationship between two
terms, using only the number of documents returned as result for a query on a
search engine. In this work, the PMINIG Distance is updated, providing a novel
formal algebraic definition, which corrects previous works. The novel point of
view underlines the features of the PMING to be a locally normalized linear
combination of the Pointwise Mutual Information and Normalized Google Distance.
The analyzed measure dynamically reflects the collaborative change made on the
web resources.



Despite enormous progress in object detection and classification, the problem
of incorporating expected contextual relationships among object instances into
modern recognition systems remains a key challenge. In this work we propose
Information Pursuit, a Bayesian framework for scene parsing that combines prior
models for the geometry of the scene and the spatial arrangement of objects
instances with a data model for the output of high-level image classifiers
trained to answer specific questions about the scene. In the proposed
framework, the scene interpretation is progressively refined as evidence
accumulates from the answers to a sequence of questions. At each step, we
choose the question to maximize the mutual information between the new answer
and the full interpretation given the current evidence obtained from previous
inquiries. We also propose a method for learning the parameters of the model
from synthesized, annotated scenes obtained by top-down sampling from an
easy-to-learn generative scene model. Finally, we introduce a database of
annotated indoor scenes of dining room tables, which we use to evaluate the
proposed approach.



Maximizing product use is a central goal of many businesses, which makes
retention and monetization two central analytics metrics in games. Player
retention may refer to various duration variables quantifying product use:
total playtime or session playtime are popular research targets, and active
playtime is well-suited for subscription games. Such research often has the
goal of increasing player retention or conversely decreasing player churn.
Survival analysis is a framework of powerful tools well suited for retention
type data. This paper contributes new methods to game analytics on how playtime
can be analyzed using survival analysis without covariates. Survival and hazard
estimates provide both a visual and an analytic interpretation of the playtime
phenomena as a funnel type nonparametric estimate. Metrics based on the
survival curve can be used to aggregate this playtime information into a single
statistic. Comparison of survival curves between cohorts provides a scientific
AB-test. All these methods work on censored data and enable computation of
confidence intervals. This is especially important in time and sample limited
data which occurs during game development. Throughout this paper, we illustrate
the application of these methods to real world game development problems on the
Hipster Sheep mobile game.



This paper extends recent work in interactive machine learning (IML) focused
on effectively incorporating human feedback. We show how control and feedback
signals complement each other in systems which model human reward. We
demonstrate that simultaneously incorporating human control and feedback
signals can improve interactive robotic systems' performance on a self-mirrored
movement control task where an RL-agent controlled right arm attempts to match
the preprogrammed movement pattern of the left arm. We illustrate the impact of
varying human feedback parameters on task performance by investigating the
probability of giving feedback on each time step and the likelihood of given
feedback being correct. We further illustrate that varying the temporal decay
with which the agent incorporates human feedback has a significant impact on
task performance. We found that smearing human feedback over time steps
improves performance and we show varying the probability of feedback at each
time step, and an increased likelihood of those feedbacks being 'correct' can
impact agent performance. We conclude that understanding latent variables in
human feedback is crucial for learning algorithms acting in human-machine
interaction domains.



Multi-task learning (MTL) involves the simultaneous training of two or more
related tasks over shared representations. In this work, we apply MTL to
audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn
a mapping between audio-visual fused features and frame labels obtained from
acoustic GMM/HMM model. This is combined with an auxiliary task which maps
visual features to frame labels obtained from a separate visual GMM/HMM model.
The MTL model is tested at various levels of babble noise and the results are
compared with a base-line hybrid DNN-HMM AV-ASR model. Our results indicate
that MTL is especially useful at higher level of noise. Compared to base-line,
upto 7\% relative improvement in WER is reported at -3 SNR dB



The majority of online display ads are served through real-time bidding (RTB)
--- each ad display impression is auctioned off in real-time when it is just
being generated from a user visit. To place an ad automatically and optimally,
it is critical for advertisers to devise a learning algorithm to cleverly bid
an ad impression in real-time. Most previous works consider the bid decision as
a static optimization problem of either treating the value of each impression
independently or setting a bid price to each segment of ad volume. However, the
bidding for a given ad campaign would repeatedly happen during its life span
before the budget runs out. As such, each bid is strategically correlated by
the constrained budget and the overall effectiveness of the campaign (e.g., the
rewards from generated clicks), which is only observed after the campaign has
completed. Thus, it is of great interest to devise an optimal bidding strategy
sequentially so that the campaign budget can be dynamically allocated across
all the available impressions on the basis of both the immediate and future
rewards. In this paper, we formulate the bid decision process as a
reinforcement learning problem, where the state space is represented by the
auction information and the campaign's real-time parameters, while an action is
the bid price to set. By modeling the state transition via auction competition,
we build a Markov Decision Process framework for learning the optimal bidding
policy to optimize the advertising performance in the dynamic real-time bidding
environment. Furthermore, the scalability problem from the large real-world
auction volume and campaign budget is well handled by state value approximation
using neural networks.



We describe an open-source toolkit for neural machine translation (NMT). The
toolkit prioritizes efficiency, modularity, and extensibility with the goal of
supporting NMT research into model architectures, feature representations, and
source modalities, while maintaining competitive performance and reasonable
training requirements. The toolkit consists of modeling and translation
support, as well as detailed pedagogical documentation about the underlying
techniques.



Limited annotated data available for the recognition of facial expression and
action units embarrasses the training of deep networks, which can learn
disentangled invariant features. However, a linear model with just several
parameters normally is not demanding in terms of training data. In this paper,
we propose an elegant linear model to untangle confounding factors in
challenging realistic multichannel signals such as 2D face videos. The simple
yet powerful model does not rely on huge training data and is natural for
recognizing facial actions without explicitly disentangling the identity. Base
on well-understood intuitive linear models such as Sparse Representation based
Classification (SRC), previous attempts require a prepossessing of explicit
decoupling which is practically inexact. Instead, we exploit the low-rank
property across frames to subtract the underlying neutral faces which are
modeled jointly with sparse representation on the action components with group
sparsity enforced. On the extended Cohn-Kanade dataset (CK+), our one-shot
automatic method on raw face videos performs as competitive as SRC applied on
manually prepared action components and performs even better than SRC in terms
of true positive rate. We apply the model to the even more challenging task of
facial action unit recognition, verified on the MPI Face Video Database
(MPI-VDB) achieving a decent performance. All the programs and data have been
made publicly available.



In this paper, we try to predict the winning team of a match in the
multiplayer eSports game Dota 2. To address the weaknesses of previous work, we
consider more aspects of prior (pre-match) features from individual players'
match history, as well as real-time (during-match) features at each minute as
the match progresses. We use logistic regression, the proposed Attribute
Sequence Model, and their combinations as the prediction models. In a dataset
of 78362 matches where 20631 matches contain replay data, our experiments show
that adding more aspects of prior features improves accuracy from 58.69% to
71.49%, and introducing real-time features achieves up to 93.73% accuracy when
predicting at the 40th minute.



In this paper, a novel architecture for a deep recurrent neural network,
residual LSTM is introduced. A plain LSTM has an internal memory cell that can
learn long term dependencies of sequential data. It also provides a temporal
shortcut path to avoid vanishing or exploding gradients in the temporal domain.
The residual LSTM provides an additional spatial shortcut path from lower
layers for efficient training of deep networks with multiple LSTM layers.
Compared with the previous work, highway LSTM, residual LSTM separates a
spatial shortcut path with temporal one by using output layers, which can help
to avoid a conflict between spatial and temporal-domain gradient flows.
Furthermore, residual LSTM reuses the output projection matrix and the output
gate of LSTM to control the spatial information flow instead of additional gate
networks, which effectively reduces more than 10% of network parameters. An
experiment for distant speech recognition on the AMI SDM corpus shows that
10-layer plain and highway LSTM networks presented 13.7% and 6.2% increase in
WER over 3-layer aselines, respectively. On the contrary, 10-layer residual
LSTM networks provided the lowest WER 41.0%, which corresponds to 3.3% and 2.8%
WER reduction over plain and highway LSTM networks, respectively.



Credit assignment in traditional recurrent neural networks usually involves
back-propagating through a long chain of tied weight matrices. The length of
this chain scales linearly with the number of time-steps as the same network is
run at each time-step. This creates many problems, such as vanishing gradients,
that have been well studied. In contrast, a NNEM's architecture recurrent
activity doesn't involve a long chain of activity (though some architectures
such as the NTM do utilize a traditional recurrent architecture as a
controller). Rather, the externally stored embedding vectors are used at each
time-step, but no messages are passed from previous time-steps. This means that
vanishing gradients aren't a problem, as all of the necessary gradient paths
are short. However, these paths are extremely numerous (one per embedding
vector in memory) and reused for a very long time (until it leaves the memory).
Thus, the forward-pass information of each memory must be stored for the entire
duration of the memory. This is problematic as this additional storage far
surpasses that of the actual memories, to the extent that large memories on
infeasible to back-propagate through in high dimensional settings. One way to
get around the need to hold onto forward-pass information is to recalculate the
forward-pass whenever gradient information is available. However, if the
observations are too large to store in the domain of interest, direct
reinstatement of a forward pass cannot occur. Instead, we rely on a learned
autoencoder to reinstate the observation, and then use the embedding network to
recalculate the forward-pass. Since the recalculated embedding vector is
unlikely to perfectly match the one stored in memory, we try out 2
approximations to utilize error gradient w.r.t. the vector in memory.



We study characteristics of receptive fields of units in deep convolutional
networks. The receptive field size is a crucial issue in many visual tasks, as
the output must respond to large enough areas in the image to capture
information about large objects. We introduce the notion of an effective
receptive field, and show that it both has a Gaussian distribution and only
occupies a fraction of the full theoretical receptive field. We analyze the
effective receptive field in several architecture designs, and the effect of
nonlinear activations, dropout, sub-sampling and skip connections on it. This
leads to suggestions for ways to address its tendency to be too small.



In this paper, we improve the previously best known regret bound to achieve
$\epsilon$-differential privacy in oblivious adversarial bandits from
$\mathcal{O}{(T^{2/3}/\epsilon)}$ to $\mathcal{O}{(\sqrt{T} \ln T /\epsilon)}$.
This is achieved by combining a Laplace Mechanism with EXP3. We show that
though EXP3 is already differentially private, it leaks a linear amount of
information in $T$. However, we can improve this privacy by relying on its
intrinsic exponential mechanism for selecting actions. This allows us to reach
$\mathcal{O}{(\sqrt{\ln T})}$-DP, with a regret of $\mathcal{O}{(T^{2/3})}$
that holds against an adaptive adversary, an improvement from the best known of
$\mathcal{O}{(T^{3/4})}$. This is done by using an algorithm that run EXP3 in a
mini-batch loop. Finally, we run experiments that clearly demonstrate the
validity of our theoretical analysis.



For a social networking service to acquire and retain users, it must find
ways to keep them engaged. By accurately gauging their preferences, it is able
to serve them with the subset of available content that maximises revenue for
the site. Without the constraints of an appropriate regulatory framework, we
argue that a sufficiently sophisticated curator algorithm tasked with
performing this process may choose to explore curation strategies that are
detrimental to users. In particular, we suggest that such an algorithm is
capable of learning to manipulate its users, for several qualitative reasons:
1. Access to vast quantities of user data combined with ongoing breakthroughs
in the field of machine learning are leading to powerful but uninterpretable
strategies for decision making at scale. 2. The availability of an effective
feedback mechanism for assessing the short and long term user responses to
curation strategies. 3. Techniques from reinforcement learning have allowed
machines to learn automated and highly successful strategies at an abstract
level, often resulting in non-intuitive yet nonetheless highly appropriate
action selection. In this work, we consider the form that these strategies for
user manipulation might take and scrutinise the role that regulation should
play in the design of such systems.



Humans are not only adept in recognizing what class an input instance belongs
to (i.e., classification task), but perhaps more remarkably, they can imagine
(i.e., generate) plausible instances of a desired class with ease, when
prompted. Inspired by this, we propose a framework which allows transforming
Cascade-Correlation Neural Networks (CCNNs) into probabilistic generative
models, thereby enabling CCNNs to generate samples from a category of interest.
CCNNs are a well-known class of deterministic, discriminative NNs, which
autonomously construct their topology, and have been successful in giving
accounts for a variety of psychological phenomena. Our proposed framework is
based on a Markov Chain Monte Carlo (MCMC) method, called the
Metropolis-adjusted Langevin algorithm, which capitalizes on the gradient
information of the target distribution to direct its explorations towards
regions of high probability, thereby achieving good mixing properties. Through
extensive simulations, we demonstrate the efficacy of our proposed framework.



Artificial Neural Networks (ANNs) have received increasing attention in
recent years with applications that span a wide range of disciplines including
vital domains such as medicine, network security and autonomous transportation.
However, neural network architectures are becoming increasingly complex and
with an increasing need to obtain real-time results from such models, it has
become pivotal to use parallelization as a mechanism for speeding up network
training and deployment. In this work we propose an implementation of Network
Parallel Training through Cannon's Algorithm for matrix multiplication. We show
that increasing the number of processes speeds up training until the point
where process communication costs become prohibitive; this point varies by
network complexity. We also show through empirical efficiency calculations that
the speedup obtained is superlinear.



A new, radical CNN design approach is presented in this paper, considering
the reduction of the total computational load during inference. This is
achieved by a new holistic intervention on both the CNN architecture and the
training procedure, which targets to the parsimonious inference by learning to
exploit or remove the redundant capacity of a CNN architecture. This is
accomplished, by the introduction of a new structural element that can be
inserted as an add-on to any contemporary CNN architecture, whilst preserving
or even improving its recognition accuracy. Our approach formulates a
systematic and data-driven method for developing CNNs that are trained to
eventually change size and form in real-time during inference, targeting to the
smaller possible computational footprint. Results are provided for the optimal
implementation on a few modern, high-end mobile computing platforms indicating
a significant speed-up of up to x3 times.



In this work several semantic approaches to concept-based query expansion and
reranking schemes are studied and compared with different ontology-based
expansion methods in web document search and retrieval. In particular, we focus
on concept-based query expansion schemes, where, in order to effectively
increase the precision of web document retrieval and to decrease the users
browsing time, the main goal is to quickly provide users with the most suitable
query expansion. Two key tasks for query expansion in web document retrieval
are to find the expansion candidates, as the closest concepts in web document
domain, and to rank the expanded queries properly. The approach we propose aims
at improving the expansion phase for better web document retrieval and
precision. The basic idea is to measure the distance between candidate concepts
using the PMING distance, a collaborative semantic proximity measure, i.e. a
measure which can be computed by using statistical results from web search
engine. Experiments show that the proposed technique can provide users with
more satisfying expansion results and improve the quality of web document
retrieval.



We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e.
translation and rotation, of texture-less rigid objects. The dataset features
thirty industry-relevant objects with no significant texture and no
discriminative color or reflectance properties. The objects exhibit symmetries
and mutual similarities in shape and/or size. Compared to other datasets, a
unique property is that some of the objects are parts of others. The dataset
includes training and test images that were captured with three synchronized
sensors, specifically a structured-light and a time-of-flight RGB-D sensor and
a high-resolution RGB camera. There are approximately 39K training and 10K test
images from each sensor. Additionally, two types of 3D models are provided for
each object, i.e. a manually created CAD model and a semi-automatically
reconstructed one. Training images depict individual objects against a black
background. Test images originate from twenty test scenes having varying
complexity, which increases from simple scenes with several isolated objects to
very challenging ones with multiple instances of several objects and with a
high amount of clutter and occlusion. The images were captured from a
systematically sampled view sphere around the object/scene, and are annotated
with accurate ground truth 6D poses of all modeled objects. Initial evaluation
results indicate that the state of the art in 6D object pose estimation has
ample room for improvement, especially in difficult cases with significant
occlusion. The T-LESS dataset is available online at cmp.felk.cvut.cz/t-less.



In this paper, for the first time, we study label propagation in
heterogeneous graphs under heterophily assumption. Homophily label propagation
(i.e., two connected nodes share similar labels) in homogeneous graph (with
same types of vertices and relations) has been extensively studied before.
Unfortunately, real-life networks are heterogeneous, they contain different
types of vertices (e.g., users, images, texts) and relations (e.g.,
friendships, co-tagging) and allow for each node to propagate both the same and
opposite copy of labels to its neighbors. We propose a $\mathcal{K}$-partite
label propagation model to handle the mystifying combination of heterogeneous
nodes/relations and heterophily propagation. With this model, we develop a
novel label inference algorithm framework with update rules in near-linear time
complexity. Since real networks change over time, we devise an incremental
approach, which supports fast updates for both new data and evidence (e.g.,
ground truth labels) with guaranteed efficiency. We further provide a utility
function to automatically determine whether an incremental or a re-modeling
approach is favored. Extensive experiments on real datasets have verified the
effectiveness and efficiency of our approach, and its superiority over the
state-of-the-art label propagation methods.



Many aspects of people's lives are proven to be deeply connected to their
jobs. In this paper, we first investigate the distinct characteristics of major
occupation categories based on tweets. From multiple social media platforms, we
gather several types of user information. From users' LinkedIn webpages, we
learn their proficiencies. To overcome the ambiguity of self-reported
information, a soft clustering approach is applied to extract occupations from
crowd-sourced data. Eight job categories are extracted, including Marketing,
Administrator, Start-up, Editor, Software Engineer, Public Relation, Office
Clerk, and Designer. Meanwhile, users' posts on Twitter provide cues for
understanding their linguistic styles, interests, and personalities. Our
results suggest that people of different jobs have unique tendencies in certain
language styles and interests. Our results also clearly reveal distinctive
levels in terms of Big Five Traits for different jobs. Finally, a classifier is
built to predict job types based on the features extracted from tweets. A high
accuracy indicates a strong discrimination power of language features for job
prediction task.



The fifth Dialog State Tracking Challenge (DSTC5) introduces a new
cross-language dialog state tracking scenario, where the participants are asked
to build their trackers based on the English training corpus, while evaluating
them with the unlabeled Chinese corpus. Although the computer-generated
translations for both English and Chinese corpus are provided in the dataset,
these translations contain errors and careless use of them can easily hurt the
performance of the built trackers. To address this problem, we propose a
multichannel Convolutional Neural Networks (CNN) architecture, in which we
treat English and Chinese language as different input channels of one single
CNN model. In the evaluation of DSTC5, we found that such multichannel
architecture can effectively improve the robustness against translation errors.
Additionally, our method for DSTC5 is purely machine learning based and
requires no prior knowledge about the target language. We consider this a
desirable property for building a tracker in the cross-language context, as not
every developer will be familiar with both languages.



ENIGMA is a learning-based method for guiding given clause selection in
saturation-based theorem provers. Clauses from many proof searches are
classified as positive and negative based on their participation in the proofs.
An efficient classification model is trained on this data, using fast
feature-based characterization of the clauses . The learned model is then
tightly linked with the core prover and used as a basis of a new parameterized
evaluation heuristic that provides fast ranking of all generated clauses. The
approach is evaluated on the E prover and the CASC 2016 AIM benchmark, showing
a large increase of E's performance.



We develop a framework for rendering photographic images, taking into account
display limitations, so as to optimize perceptual similarity between the
rendered image and the original scene. We formulate this as a constrained
optimization problem, in which we minimize a measure of perceptual
dissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics
the early stage transformations of the human visual system. When rendering
images acquired with higher dynamic range than that of the display, we find
that the optimized solution boosts the contrast of low-contrast features
without introducing significant artifacts, yielding results of comparable
visual quality to current state-of-the art methods with no manual intervention
or parameter settings. We also examine a variety of other display constraints,
including limitations on minimum luminance (black point), mean luminance (as a
proxy for energy consumption), and quantized luminance levels (halftoning).
Finally, we show that the method may be used to enhance details and contrast of
images degraded by optical scattering (e.g. fog).



The UITP workshop series brings together researchers interested in designing,
developing and evaluating user interfaces for automated reasoning tools, such
as interactive proof assistants, automated theorem provers, model finders,
tools for formal methods, and tools for visualising and manipulating logical
formulas and proofs. The twelth edition of UITP took place in Coimbra,
Portugal, and was part of the International Joint Conference on Automated
Reasoning (IJCAR'16). The workshop consisted of an invited talk, six
presentations of submitted papers and lively hands-on session for reasoning
tools and their user-interface. These post-proceedings contain four contributed
papers accepted for publication after a second round of reviewing after the
workshop as well as the invited paper.



We consider an online version of the robust Principle Component Analysis
(PCA), which arises naturally in time-varying source separations such as video
foreground-background separation. This paper proposes a compressive online
robust PCA with prior information for recursively separating a sequences of
frames into sparse and low-rank components from a small set of measurements. In
contrast to conventional batch-based PCA, which processes all the frames
directly, the proposed method processes measurements taken from each frame.
Moreover, this method can efficiently incorporate multiple prior information,
namely previous reconstructed frames, to improve the separation and thereafter,
update the prior information for the next frame. We utilize multiple prior
information by solving $n\text{-}\ell_{1}$ minimization for incorporating the
previous sparse components and using incremental singular value decomposition
($\mathrm{SVD}$) for exploiting the previous low-rank components. We also
establish theoretical bounds on the number of measurements required to
guarantee successful separation under assumptions of static or slowly-changing
low-rank components. Using numerical experiments, we evaluate our bounds and
the performance of the proposed algorithm. In addition, we apply the proposed
algorithm to online video foreground and background separation from compressive
measurements. Experimental results show that the proposed method outperforms
the existing methods.



Deep learning techniques lie at the heart of several significant AI advances
in recent years including object recognition and detection, image captioning,
machine translation, speech recognition and synthesis, and playing the game of
Go. Automated first-order theorem provers can aid in the formalization and
verification of mathematical theorems and play a crucial role in program
analysis, theory reasoning, security, interpolation, and system verification.
Here we suggest deep learning based guidance in the proof search of the theorem
prover E. We train and compare several deep neural network models on the traces
of existing ATP proofs of Mizar statements and use them to select processed
clauses during proof search. We give experimental evidence that with a hybrid,
two-phase approach, deep learning based guidance can significantly reduce the
average number of proof search steps while increasing the number of theorems
proved. Using a few proof guidance strategies that leverage deep neural
networks, we have found first-order proofs of 7.36% of the first-order logic
translations of the Mizar Mathematical Library theorems that did not previously
have ATP generated proofs. This increases the ratio of statements in the corpus
with ATP generated proofs from 56% to 59%.



The $k$-Means clustering problem on $n$ points is NP-Hard for any dimension
$d\ge 2$, however, for the 1D case there exist exact polynomial time
algorithms. Previous literature reported an $O(kn^2)$ time dynamic programming
algorithm that uses $O(kn)$ space. We present a new algorithm computing the
optimal clustering in only $O(kn)$ time using linear space. For $k = \Omega(\lg
n)$, we improve this even further to $n 2^{O(\sqrt{ \lg \lg n \lg k})}$ time.
We generalize the new algorithm(s) to work for the absolute distance instead of
squared distance and to work for any Bregman Divergence as well.



Social messages classification is a research domain that has attracted the
attention of many researchers in these last years. Indeed, the social message
is different from ordinary text because it has some special characteristics
like its shortness. Then the development of new approaches for the processing
of the social message is now essential to make its classification more
efficient. In this paper, we are mainly interested in the classification of
social messages based on their spreading on online social networks (OSN). We
proposed a new distance metric based on the Dynamic Time Warping distance and
we use it with the probabilistic and the evidential k Nearest Neighbors (k-NN)
classifiers to classify propagation networks (PrNets) of messages. The
propagation network is a directed acyclic graph (DAG) that is used to record
propagation traces of the message, the traversed links and their types. We
tested the proposed metric with the chosen k-NN classifiers on real world
propagation traces that were collected from Twitter social network and we got
good classification accuracies.



The recent surge in interest in ethics in artificial intelligence may leave
many educators wondering how to address moral, ethical, and philosophical
issues in their AI courses. As instructors we want to develop curriculum that
not only prepares students to be artificial intelligence practitioners, but
also to understand the moral, ethical, and philosophical impacts that
artificial intelligence will have on society. In this article we provide
practical case studies and links to resources for use by AI educators. We also
provide concrete suggestions on how to integrate AI ethics into a general
artificial intelligence course and how to teach a stand-alone artificial
intelligence ethics course.



The Frame Problem (FP) is a puzzle in philosophy of mind and epistemology,
articulated by the Stanford Encyclopedia of Philosophy as follows: "How do we
account for our apparent ability to make decisions on the basis only of what is
relevant to an ongoing situation without having explicitly to consider all that
is not relevant?" In this work, we focus on the causal variant of the FP, the
Causal Frame Problem (CFP). Assuming that a reasoner's mental causal model can
be (implicitly) represented by a causal Bayes net, we first introduce a notion
called Potential Level (PL). PL, in essence, encodes the relative position of a
node with respect to its neighbors in a causal Bayes net. Drawing on the
psychological literature on causal judgment, we substantiate the claim that PL
may bear on how time is encoded in the mind. Using PL, we propose an inference
framework, called the PL-based Inference Framework (PLIF), which permits a
boundedly-rational approach to the CFP to be formally articulated at Marr's
algorithmic level of analysis. We show that our proposed framework, PLIF, is
consistent with a wide range of findings in causal judgment literature, and
that PL and PLIF make a number of predictions, some of which are already
supported by existing findings.



The popularity of image sharing on social media and the engagement it creates
between users reflects the important role that visual context plays in everyday
conversations. We present a novel task, Image-Grounded Conversations (IGC), in
which natural-sounding conversations are generated about a shared image. To
benchmark progress, we introduce a new multiple-reference dataset of
crowd-sourced, event-centric conversations on images. IGC falls on the
continuum between chit-chat and goal-directed conversation models, where visual
grounding constrains the topic of conversation to event-driven utterances.
Experiments with models trained on social media data show that the combination
of visual and textual context enhances the quality of generated conversational
turns. In human evaluation, the gap between human performance and that of both
neural and retrieval architectures suggests that multi-modal IGC presents an
interesting challenge for dialogue research.



We study the problem of identifying the causal relationship between two
discrete random variables from observational data. We recently proposed a novel
framework called entropic causality that works in a very general functional
model but makes the assumption that the unobserved exogenous variable has small
entropy in the true causal direction.
  This framework requires the solution of a minimum entropy coupling problem:
Given marginal distributions of m discrete random variables, each on n states,
find the joint distribution with minimum entropy, that respects the given
marginals. This corresponds to minimizing a concave function of nm variables
over a convex polytope defined by nm linear constraints, called a
transportation polytope. Unfortunately, it was recently shown that this minimum
entropy coupling problem is NP-hard, even for 2 variables with n states. Even
representing points (joint distributions) over this space can require
exponential complexity (in n, m) if done naively.
  In our recent work we introduced an efficient greedy algorithm to find an
approximate solution for this problem. In this paper we analyze this algorithm
and establish two results: that our algorithm always finds a local minimum and
also is within an additive approximation error from the unknown global optimum.



Natural-language-facilitated human-robot cooperation (NLC), in which natural
language (NL) is used to share knowledge between a human and a robot for
conducting intuitive human-robot cooperation (HRC), is continuously developing
in the recent decade. Currently, NLC is used in several robotic domains such as
manufacturing, daily assistance and health caregiving. It is necessary to
summarize current NLC-based robotic systems and discuss the future developing
trends, providing helpful information for future NLC research. In this review,
we first analyzed the driving forces behind the NLC research. Regarding to a
robot s cognition level during the cooperation, the NLC implementations then
were categorized into four types {NL-based control, NL-based robot training,
NL-based task execution, NL-based social companion} for comparison and
discussion. Last based on our perspective and comprehensive paper review, the
future research trends were discussed.



FOSS is an acronym for Free and Open Source Software. The FOSS 2013 survey
primarily targets FOSS contributors and relevant anonymized dataset is publicly
available under CC by SA license. In this study, the dataset is analyzed from a
critical perspective using statistical and clustering techniques (especially
multiple correspondence analysis) with a strong focus on women contributors
towards discovering hidden trends and facts. Important inferences are drawn
about development practices and other facets of the free software and OSS
worlds.



We introduce a new family of minmax rank aggregation problems under two
distance measures, the Kendall {\tau} and the Spearman footrule. As the
problems are NP-hard, we proceed to describe a number of constant-approximation
algorithms for solving them. We conclude with illustrative applications of the
aggregation methods on the Mallows model and genomic data.



Most of researches on image forensics have been mainly focused on detection
of artifacts introduced by a single processing tool. They lead in the
development of many specialized algorithms looking for one or more particular
footprints under specific settings. Naturally, the performance of such
algorithms are not perfect, and accordingly the provided output might be noisy,
inaccurate and only partially correct. Furthermore, a forged image in practical
scenarios is often the result of utilizing several tools available by
image-processing software systems. Therefore, reliable tamper detection
requires developing more poweful tools to deal with various tempering
scenarios. Fusion of forgery detection tools based on Fuzzy Inference System
has been used before for addressing this problem. Adjusting the membership
functions and defining proper fuzzy rules for attaining to better results are
time-consuming processes. This can be accounted as main disadvantage of fuzzy
inference systems. In this paper, a Neuro-Fuzzy inference system for fusion of
forgery detection tools is developed. The neural network characteristic of
these systems provides appropriate tool for automatically adjusting the
membership functions. Moreover, initial fuzzy inference system is generated
based on fuzzy clustering techniques. The proposed framework is implemented and
validated on a benchmark image splicing data set in which three forgery
detection tools are fused based on adaptive Neuro-Fuzzy inference system. The
outcome of the proposed method reveals that applying Neuro Fuzzy inference
systems could be a better approach for fusion of forgery detection tools.



This research presents an innovative and unique way of solving the
advertisement prediction problem which is considered as a learning problem over
the past several years. Online advertising is a multi-billion-dollar industry
and is growing every year with a rapid pace. The goal of this research is to
enhance click through rate of the contextual advertisements using Linear
Regression. In order to address this problem, a new technique propose in this
paper to predict the CTR which will increase the overall revenue of the system
by serving the advertisements more suitable to the viewers with the help of
feature extraction and displaying the advertisements based on context of the
publishers. The important steps include the data collection, feature
extraction, CTR prediction and advertisement serving. The statistical results
obtained from the dynamically used technique show an efficient outcome by
fitting the data close to perfection for the LR technique using optimized
feature selection.



Natural-language-facilitated human-robot cooperation (NLC) refers to using
natural language (NL) to facilitate interactive information sharing and task
executions with a common goal constraint between robots and humans. Recently,
NLC research has received increasing attention. Typical NLC scenarios include
robotic daily assistance, robotic health caregiving, intelligent manufacturing,
autonomous navigation, and robot social accompany. However, a thorough review,
that can reveal latest methodologies to use NL to facilitate human-robot
cooperation, is missing. In this review, a comprehensive summary about
methodologies for NLC is presented. NLC research includes three main research
focuses: NL instruction understanding, NL-based execution plan generation, and
knowledge-world mapping. In-depth analyses on theoretical methods,
applications, and model advantages and disadvantages are made. Based on our
paper review and perspective, potential research directions of NLC are
summarized.



This paper formalises the problem of online algorithm selection in the
context of Reinforcement Learning. The setup is as follows: given an episodic
task and a finite number of off-policy RL algorithms, a meta-algorithm has to
decide which RL algorithm is in control during the next episode so as to
maximize the expected return. The article presents a novel meta-algorithm,
called Epochal Stochastic Bandit Algorithm Selection (ESBAS). Its principle is
to freeze the policy updates at each epoch, and to leave a rebooted stochastic
bandit in charge of the algorithm selection. Under some assumptions, a thorough
theoretical analysis demonstrates its near-optimality considering the
structural sampling budget limitations. ESBAS is first empirically evaluated on
a dialogue task where it is shown to outperform each individual algorithm in
most configurations. ESBAS is then adapted to a true online setting where
algorithms update their policies after each transition, which we call SSBAS.
SSBAS is evaluated on a fruit collection task where it is shown to adapt the
stepsize parameter more efficiently than the classical hyperbolic decay, and on
an Atari game, where it improves the performance by a wide margin.



With machine learning successfully applied to new daunting problems almost
every day, general AI starts looking like an attainable goal. However, most
current research focuses instead on important but narrow applications, such as
image classification or machine translation. We believe this to be largely due
to the lack of objective ways to measure progress towards broad machine
intelligence. In order to fill this gap, we propose here a set of concrete
desiderata for general AI, together with a platform to test machines on how
well they satisfy such desiderata, while keeping all further complexities to a
minimum.



Retrosynthesis is a technique to plan the chemical synthesis of organic
molecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a
search tree is built by analysing molecules recursively and dissecting them
into simpler molecular building blocks until one obtains a set of known
building blocks. The search space is intractably large, and it is difficult to
determine the value of retrosynthetic positions. Here, we propose to model
retrosynthesis as a Markov Decision Process. In combination with a Deep Neural
Network policy learned from essentially the complete published knowledge of
chemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In
exploratory studies, we demonstrate that MCTS with neural network policies
outperforms the traditionally used best-first search with hand-coded
heuristics.



The problem of quantizing the activations of a deep neural network is
considered. An examination of the popular binary quantization approach shows
that this consists of approximating a classical non-linearity, the hyperbolic
tangent, by two functions: a piecewise constant sign function, which is used in
feedforward network computations, and a piecewise linear hard tanh function,
used in the backpropagation step during network learning. The problem of
approximating the ReLU non-linearity, widely used in the recent deep learning
literature, is then considered. An half-wave Gaussian quantizer (HWGQ) is
proposed for forward approximation and shown to have efficient implementation,
by exploiting the statistics of of network activations and batch normalization
operations commonly used in the literature. To overcome the problem of gradient
mismatch, due to the use of different forward and backward approximations,
several piece-wise backward approximators are then investigated. The
implementation of the resulting quantized network, denoted as HWGQ-Net, is
shown to achieve much closer performance to full precision networks, such as
AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision
networks, with 1-bit binary weights and 2-bit quantized activations.



Real-time optimization of traffic flow addresses important practical
problems: reducing a driver's wasted time, improving city-wide efficiency,
reducing gas emissions and improving air quality. Much of the current research
in traffic-light optimization relies on extending the capabilities of traffic
lights to either communicate with each other or communicate with vehicles.
However, before such capabilities become ubiquitous, opportunities exist to
improve traffic lights by being more responsive to current traffic situations
within the current, already deployed, infrastructure. In this paper, we
introduce a traffic light controller that employs bidding within micro-auctions
to efficiently incorporate traffic sensor information; no other outside sources
of information are assumed. We train and test traffic light controllers on
large-scale data collected from opted-in Android cell-phone users over a period
of several months in Mountain View, California and the River North neighborhood
of Chicago, Illinois. The learned auction-based controllers surpass (in both
the relevant metrics of road-capacity and mean travel time) the currently
deployed lights, optimized static-program lights, and longer-term planning
approaches, in both cities, measured using real user driving data.



Entity resolution (ER) is the task of identifying all records in a database
that refer to the same underlying entity, and are therefore duplicates of each
other. Due to inherent ambiguity of data representation and poor data quality,
ER is a challenging task for any automated process. As a remedy, human-powered
ER via crowdsourcing has become popular in recent years. Using crowd to answer
queries is costly and time consuming. Furthermore, crowd-answers can often be
faulty. Therefore, crowd-based ER methods aim to minimize human participation
without sacrificing the quality and use a computer generated similarity matrix
actively. While, some of these methods perform well in practice, no theoretical
analysis exists for them, and further their worst case performances do not
reflect the experimental findings. This creates a disparity in the
understanding of the popular heuristics for this problem. In this paper, we
make the first attempt to close this gap. We provide a thorough analysis of the
prominent heuristic algorithms for crowd-based ER. We justify experimental
observations with our analysis and information theoretic lower bounds.



Kriging or Gaussian Process Regression is applied in many fields as a
non-linear regression model as well as a surrogate model in the field of
evolutionary computation. However, the computational and space complexity of
Kriging, that is cubic and quadratic in the number of data points respectively,
becomes a major bottleneck with more and more data available nowadays. In this
paper, we propose a general methodology for the complexity reduction, called
cluster Kriging, where the whole data set is partitioned into smaller clusters
and multiple Kriging models are built on top of them. In addition, four Kriging
approximation algorithms are proposed as candidate algorithms within the new
framework. Each of these algorithms can be applied to much larger data sets
while maintaining the advantages and power of Kriging. The proposed algorithms
are explained in detail and compared empirically against a broad set of
existing state-of-the-art Kriging approximation methods on a well-defined
testing framework. According to the empirical study, the proposed algorithms
consistently outperform the existing algorithms. Moreover, some practical
suggestions are provided for using the proposed algorithms.



In the field of exploratory data mining, local structure in data can be
described by patterns and discovered by mining algorithms. Although many
solutions have been proposed to address the redundancy problems in pattern
mining, most of them either provide succinct pattern sets or take the interests
of the user into account-but not both. Consequently, the analyst has to invest
substantial effort in identifying those patterns that are relevant to her
specific interests and goals. To address this problem, we propose a novel
approach that combines pattern sampling with interactive data mining. In
particular, we introduce the LetSIP algorithm, which builds upon recent
advances in 1) weighted sampling in SAT and 2) learning to rank in interactive
pattern mining. Specifically, it exploits user feedback to directly learn the
parameters of the sampling distribution that represents the user's interests.
We compare the performance of the proposed algorithm to the state-of-the-art in
interactive pattern mining by emulating the interests of a user. The resulting
system allows efficient and interleaved learning and sampling, thus
user-specific anytime data exploration. Finally, LetSIP demonstrates favourable
trade-offs concerning both quality-diversity and exploitation-exploration when
compared to existing methods.



We present a visually grounded model of speech perception which projects
spoken utterances and images to a joint semantic space. We use a multi-layer
recurrent highway network to model the temporal nature of spoken speech, and
show that it learns to extract both form and meaning-based linguistic knowledge
from the input signal. We carry out an in-depth analysis of the representations
used by different components of the trained model and show that encoding of
semantic aspects tends to become richer as we go up the hierarchy of layers,
whereas encoding of form-related aspects of the language input tends to
initially increase and then plateau or decrease.



We propose a method to generate multiple diverse and valid human pose
hypotheses in 3D all consistent with the 2D detection of joints in a monocular
RGB image. We use a novel generative model uniform (unbiased) in the space of
anatomically plausible 3D poses. Our model is compositional (produces a pose by
combining parts) and since it is restricted only by anatomical constraints it
can generalize to every plausible human 3D pose. Removing the model bias
intrinsically helps to generate more diverse 3D pose hypotheses. We argue that
generating multiple pose hypotheses is more reasonable than generating only a
single 3D pose based on the 2D joint detection given the depth ambiguity and
the uncertainty due to occlusion and imperfect 2D joint detection. We hope that
the idea of generating multiple consistent pose hypotheses can give rise to a
new line of future work that has not received much attention in the literature.
We used the Human3.6M dataset for empirical evaluation.



We present Deep Generalized Canonical Correlation Analysis (DGCCA) -- a
method for learning nonlinear transformations of arbitrarily many views of
data, such that the resulting transformations are maximally informative of each
other. While methods for nonlinear two-view representation learning (Deep CCA,
(Andrew et al., 2013)) and linear many-view representation learning
(Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview
representation learning technique that combines the flexibility of nonlinear
(deep) representation learning with the statistical power of incorporating
information from many independent sources, or views. We present the DGCCA
formulation as well as an efficient stochastic optimization algorithm for
solving it. We learn DGCCA representations on two distinct datasets for three
downstream tasks: phonetic transcription from acoustic and articulatory
measurements, and recommending hashtags and friends on a dataset of Twitter
users. We find that DGCCA representations soundly beat existing methods at
phonetic transcription and hashtag recommendation, and in general perform no
worse than standard linear many-view techniques.



Although deep learning models have proven effective at solving problems in
natural language processing, the mechanism by which they come to their
conclusions is often unclear. As a result, these models are generally treated
as black boxes, yielding no insight of the underlying learned patterns. In this
paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new
approach for tracking the importance of a given input to the LSTM for a given
output. By identifying consistently important patterns of words, we are able to
distill state of the art LSTMs on sentiment analysis and question answering
into a set of representative phrases. This representation is then
quantitatively validated by using the extracted phrases to construct a simple,
rule-based classifier which approximates the output of the LSTM.



In application domains such as healthcare, we want accurate predictive models
that are also causally interpretable. In pursuit of such models, we propose a
causal regularizer to steer predictive models towards causally-interpretable
solutions and theoretically study its properties. In a large-scale analysis of
Electronic Health Records (EHR), our causally-regularized model outperforms its
L1-regularized counterpart in causal accuracy and is competitive in predictive
performance. We perform non-linear causality analysis by causally regularizing
a special neural network architecture. We also show that the proposed causal
regularizer can be used together with neural representation learning algorithms
to yield up to 20% improvement over multilayer perceptron in detecting
multivariate causation, a situation common in healthcare, where many causal
factors should occur simultaneously to have an effect on the target variable.



In recent years, machine learning techniques based on neural networks for
mobile computing become increasingly popular. Classical multi-layer neural
networks require matrix multiplications at each stage. Multiplication operation
is not an energy efficient operation and consequently it drains the battery of
the mobile device. In this paper, we propose a new energy efficient neural
network with the universal approximation property over space of Lebesgue
integrable functions. This network, called, additive neural network, is very
suitable for mobile computing. The neural structure is based on a novel vector
product definition, called ef-operator, that permits a multiplier-free
implementation. In ef-operation, the "product" of two real numbers is defined
as the sum of their absolute values, with the sign determined by the sign of
the product of the numbers. This "product" is used to construct a vector
product in $R^N$. The vector product induces the $l_1$ norm. The proposed
additive neural network successfully solves the XOR problem. The experiments on
MNIST dataset show that the classification performances of the proposed
additive neural networks are very similar to the corresponding multi-layer
perceptron and convolutional neural networks (LeNet).



Statistical Relational Learning (SRL) methods have shown that classification
accuracy can be improved by integrating relations between samples. Techniques
such as iterative classification or relaxation labeling achieve this by
propagating information between related samples during the inference process.
When only a few samples are labeled and connections between samples are sparse,
collective inference methods have shown large improvements over standard
feature-based ML methods. However, in contrast to feature based ML, collective
inference methods require complex inference procedures and often depend on the
strong assumption of label consistency among related samples. In this paper, we
introduce new relational features for standard ML methods by extracting
information from direct and indirect relations. We show empirically on three
standard benchmark datasets that our relational features yield results
comparable to collective inference methods. Finally we show that our proposal
outperforms these methods when additional information is available.



The Boolean Satisfiability problem asks if a Boolean formula is satisfiable
by some assignment of the variables or not. It belongs to the NP-complete
complexity class and hence no algorithm with polynomial time worst-case
complexity is known, i.e., the problem is hard. The K-SAT problem is the subset
of the Boolean Satisfiability problem, for which the Boolean formula has the
conjunctive normal form with K literals per clause. This problem is still
NP-complete for $K \ge 3$. Although the worst case complexity of NP-complete
problems is conjectured to be exponential, there might be subsets of the
realizations where solutions can typically be found in polynomial time. In
fact, random $K$-SAT, with the number of clauses to number of variables ratio
$\alpha$ as control parameter, shows a phase transition between a satisfiable
phase and an unsatisfiable phase, at which the hardest problems are located. We
use here several linear programming approaches to reveal further "easy-hard"
transition points at which the typical hardness of the problems increases which
means that such algorithms can solve the problem on one side efficiently but
not beyond this point. For one of these transitions, we observed a coincidence
with a structural transition of the literal factor graphs of the problem
instances. We also investigated cutting-plane approaches, which often increase
the computational efficiency. Also we tried out a mapping to another
NP-complete optimization problem using a specific algorithm for that problem.
In both cases, no improvement of the performance was observed, i.e., no shift
of the easy-hard transition to higher values of $\alpha$.



Parameterized algorithms are a way to solve hard problems more efficiently,
given that a specific parameter of the input is small. In this paper, we apply
this idea to the field of answer set programming (ASP). To this end, we propose
two kinds of graph representations of programs to exploit their treewidth as a
parameter. Treewidth roughly measures to which extent the internal structure of
a program resembles a tree. Our main contribution is the design of
parameterized dynamic programming algorithms, which run in linear time if the
treewidth and weights of the given program are bounded. Compared to previous
work, our algorithms handle the full syntax of ASP. Finally, we report on an
empirical evaluation that shows good runtime behaviour for benchmark instances
of low treewidth, especially for counting answer sets.



Matrix games like Prisoner's Dilemma have guided research on social dilemmas
for decades. However, they necessarily treat the choice to cooperate or defect
as an atomic action. In real-world social dilemmas these choices are temporally
extended. Cooperativeness is a property that applies to policies, not
elementary actions. We introduce sequential social dilemmas that share the
mixed incentive structure of matrix game social dilemmas but also require
agents to learn policies that implement their strategic intentions. We analyze
the dynamics of policies learned by multiple self-interested independent
learning agents, each using its own deep Q-network, on two Markov games we
introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We
characterize how learned behavior in each domain changes as a function of
environmental factors including resource abundance. Our experiments show how
conflict can emerge from competition over shared resources and shed light on
how the sequential nature of real world social dilemmas affects cooperation.



This paper presents incremental network quantization (INQ), a novel method,
targeting to efficiently convert any pre-trained full-precision convolutional
neural network (CNN) model into a low-precision version whose weights are
constrained to be either powers of two or zero. Unlike existing methods which
are struggled in noticeable accuracy loss, our INQ has the potential to resolve
this issue, as benefiting from two innovations. On one hand, we introduce three
interdependent operations, namely weight partition, group-wise quantization and
re-training. A well-proven measure is employed to divide the weights in each
layer of a pre-trained CNN model into two disjoint groups. The weights in the
first group are responsible to form a low-precision base, thus they are
quantized by a variable-length encoding method. The weights in the other group
are responsible to compensate for the accuracy loss from the quantization, thus
they are the ones to be re-trained. On the other hand, these three operations
are repeated on the latest re-trained group in an iterative manner until all
the weights are converted into low-precision ones, acting as an incremental
network quantization and accuracy enhancement procedure. Extensive experiments
on the ImageNet classification task using almost all known deep CNN
architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the
efficacy of the proposed method. Specifically, at 5-bit quantization, our
models have improved accuracy than the 32-bit floating-point references. Taking
ResNet-18 as an example, we further show that our quantized models with 4-bit,
3-bit and 2-bit ternary weights have improved or very similar accuracy against
its 32-bit floating-point baseline. Besides, impressive results with the
combination of network pruning and INQ are also reported. The code is available
at https://github.com/Zhouaojun/Incremental-Network-Quantization.



Recent research in psycholinguistics has provided increasing evidence that
humans predict upcoming content. Prediction also affects perception and might
be a key to robustness in human language processing. In this paper, we
investigate the factors that affect human prediction by building a
computational model that can predict upcoming discourse referents based on
linguistic knowledge alone vs. linguistic knowledge jointly with common-sense
knowledge in the form of scripts. We find that script knowledge significantly
improves model estimates of human predictions. In a second study, we test the
highly controversial hypothesis that predictability influences referring
expression type but do not find evidence for such an effect.



We present Octopus, an AI agent to jointly balance three conflicting task
objectives on a micro-crowdsourcing marketplace - the quality of work, total
cost incurred, and time to completion. Previous control agents have mostly
focused on cost-quality, or cost-time tradeoffs, but not on directly
controlling all three in concert. A naive formulation of three-objective
optimization is intractable; Octopus takes a hierarchical POMDP approach, with
three different components responsible for setting the pay per task, selecting
the next task, and controlling task-level quality. We demonstrate that Octopus
significantly outperforms existing state-of-the-art approaches on real
experiments. We also deploy Octopus on Amazon Mechanical Turk, showing its
ability to manage tasks in a real-world dynamic setting.



Usually bilingual word vectors are trained "online". Mikolov et al. showed
they can also be found "offline", whereby two pre-trained embeddings are
aligned with a linear transformation, using dictionaries compiled from expert
knowledge. In this work, we prove that the linear transformation between two
spaces should be orthogonal. This transformation can be obtained using the
singular value decomposition. We introduce a novel "inverted softmax" for
identifying translation pairs, with which we improve the precision @1 of
Mikolov's original mapping from 34% to 43%, when translating a test set
composed of both common and rare English words into Italian. Orthogonal
transformations are more robust to noise, enabling us to learn the
transformation without expert bilingual signal by constructing a
"pseudo-dictionary" from the identical character strings which appear in both
languages, achieving 40% precision on the same test set. Finally, we extend our
method to retrieve the true translations of English sentences from a corpus of
200k Italian sentences with a precision @1 of 68%.



We introduce a neural architecture for navigation in novel environments. Our
proposed architecture learns to map from first-person views and plans a
sequence of actions towards goals in the environment. The Cognitive Mapper and
Planner (CMP) is based on two key ideas: a) a unified joint architecture for
mapping and planning, such that the mapping is driven by the needs of the
planner, and b) a spatial memory with the ability to plan given an incomplete
set of observations about the world. CMP constructs a top-down belief map of
the world and applies a differentiable neural net planner to produce the next
action at each time step. The accumulated belief of the world enables the agent
to track visited regions of the environment. Our experiments demonstrate that
CMP outperforms both reactive strategies and standard memory-based
architectures and performs well in novel environments. Furthermore, we show
that CMP can also achieve semantically specified goals, such as "go to a
chair".



Machine learning and deep learning in particular has advanced tremendously on
perceptual tasks in recent years. However, it remains vulnerable against
adversarial perturbations of the input that have been crafted specifically to
fool the system while being quasi-imperceptible to a human. In this work, we
propose to augment deep neural networks with a small "detector" subnetwork
which is trained on the binary classification task of distinguishing genuine
data from data containing adversarial perturbations. Our method is orthogonal
to prior work on addressing adversarial perturbations, which has mostly focused
on making the classification network itself more robust. We show empirically
that adversarial perturbations can be detected surprisingly well even though
they are quasi-imperceptible to humans. Moreover, while the detectors have been
trained to detect only a specific adversary, they generalize to similar and
weaker adversaries. In addition, we propose an adversarial attack that fools
both the classifier and the detector and a novel training procedure for the
detector that counteracts this attack.



Neural language models predict the next token using a latent representation
of the immediate token history. Recently, various methods for augmenting neural
language models with an attention mechanism over a differentiable memory have
been proposed. For predicting the next token, these models query information
from a memory of the recent history which can facilitate learning mid- and
long-range dependencies. However, conventional attention mechanisms used in
memory-augmented neural language models produce a single output vector per time
step. This vector is used both for predicting the next token as well as for the
key and value of a differentiable memory of a token history. In this paper, we
propose a neural language model with a key-value attention mechanism that
outputs separate representations for the key and value of a differentiable
memory, as well as for encoding the next-word distribution. This model
outperforms existing memory-augmented neural language models on two corpora.
Yet, we found that our method mainly utilizes a memory of the five most recent
output representations. This led to the unexpected main finding that a much
simpler model based only on the concatenation of recent output representations
from previous time steps is on par with more sophisticated memory-augmented
neural language models.



Bipartite data is common in data engineering and brings unique challenges,
particularly when it comes to clustering tasks that impose on strong structural
assumptions. This work presents an unsupervised method for assessing similarity
in bipartite data. Similar to some co-clustering methods, the method is based
on regular equivalence in graphs. The algorithm uses spectral properties of a
bipartite adjacency matrix to estimate similarity in both dimensions. The
method is reflexive in that similarity in one dimension is used to inform
similarity in the other. Reflexive regular equivalence can also use the
structure of transitivities -- in a network sense -- the contribution of which
is controlled by the algorithm's only free-parameter, $\alpha$. The method is
completely unsupervised and can be used to validate assumptions of
co-similarity, which are required but often untested, in co-clustering
analyses. Three variants of the method with different normalizations are tested
on synthetic data. The method is found to be robust to noise and well-suited to
asymmetric co-similar structure, making it particularly informative for cluster
analysis and recommendation in bipartite data of unknown structure. In
experiments, the convergence and speed of the algorithm are found to be stable
for different levels of noise. Real-world data from a network of malaria genes
are analyzed, where the similarity produced by the reflexive method is shown to
out-perform other measures' ability to correctly classify genes.



We propose a direct estimation method for R\'{e}nyi and f-divergence measures
based on a new graph theoretical interpretation. Suppose that we are given two
sample sets $X$ and $Y$, respectively with $N$ and $M$ samples, where
$\eta:=M/N$ is a constant value. Considering the $k$-nearest neighbor ($k$-NN)
graph of $Y$ in the joint data set $(X,Y)$, we show that the average powered
ratio of the number of $X$ points to the number of $Y$ points among all $k$-NN
points is proportional to R\'{e}nyi divergence of $X$ and $Y$ densities. A
similar method can also be used to estimate f-divergence measures. We derive
bias and variance rates, and show that for the class of $\gamma$-H\"{o}lder
smooth functions, the estimator achieves the MSE rate of
$O(N^{-2\gamma/(\gamma+d)})$. Furthermore, by using a weighted ensemble
estimation technique, for density functions with continuous and bounded
derivatives of up to the order $d$, and some extra conditions at the support
set boundary, we derive an ensemble estimator that achieves the parametric MSE
rate of $O(1/N)$. Our estimators are more computationally tractable than other
competing estimators, which makes them appealing in many practical
applications.



People can refer to quantities in a visual scene by using either exact
cardinals (e.g. one, two, three) or natural language quantifiers (e.g. few,
most, all). In humans, these two processes underlie fairly different cognitive
and neural mechanisms. Inspired by this evidence, the present study proposes
two models for learning the objective meaning of cardinals and quantifiers from
visual scenes containing multiple objects. We show that a model capitalizing on
a 'fuzzy' measure of similarity is effective for learning quantifiers, whereas
the learning of exact cardinals is better accomplished when information about
number is provided.



Being an unsupervised machine learning and data mining technique,
biclustering and its multimodal extensions are becoming popular tools for
analysing object-attribute data in different domains. Apart from conventional
clustering techniques, biclustering is searching for homogeneous groups of
objects while keeping their common description, e.g., in binary setting, their
shared attributes. In bioinformatics, biclustering is used to find genes, which
are active in a subset of situations, thus being candidates for biomarkers.
However, the authors of those biclustering techniques that are popular in gene
expression analysis, may overlook the existing methods. For instance, BiMax
algorithm is aimed at finding biclusters, which are well-known for decades as
formal concepts. Moreover, even if bioinformatics classify the biclustering
methods according to reasonable domain-driven criteria, their classification
taxonomies may be different from survey to survey and not full as well. So, in
this paper we propose to use concept lattices as a tool for taxonomy building
(in the biclustering domain) and attribute exploration as means for
cross-domain taxonomy completion.



Multi-agent path finding (MAPF) is well-studied in artificial intelligence,
robotics, theoretical computer science and operations research. We discuss
issues that arise when generalizing MAPF methods to real-world scenarios and
four research directions that address them. We emphasize the importance of
addressing these issues as opposed to developing faster methods for the
standard formulation of the MAPF problem.



In this work we study the quantitative relation between the recursive
teaching dimension (RTD) and the VC dimension (VCD) of concept classes of
finite sizes. The RTD of a concept class $\mathcal C \subseteq \{0, 1\}^n$,
introduced by Zilles et al. (2011), is a combinatorial complexity measure
characterized by the worst-case number of examples necessary to identify a
concept in $\mathcal C$ according to the recursive teaching model.
  For any finite concept class $\mathcal C \subseteq \{0,1\}^n$ with
$\mathrm{VCD}(\mathcal C)=d$, Simon & Zilles (2015) posed an open problem
$\mathrm{RTD}(\mathcal C) = O(d)$, i.e., is RTD linearly upper bounded by VCD?
Previously, the best known result is an exponential upper bound
$\mathrm{RTD}(\mathcal C) = O(d \cdot 2^d)$, due to Chen et al. (2016). In this
paper, we show a quadratic upper bound: $\mathrm{RTD}(\mathcal C) = O(d^2)$,
much closer to an answer to the open problem. We also discuss the challenges in
fully solving the problem.



Distributed training of deep learning models on large-scale training data is
typically conducted with asynchronous stochastic optimization to maximize the
rate of updates, at the cost of additional noise introduced from asynchrony. In
contrast, the synchronous approach is often thought to be impractical due to
idle time wasted on waiting for straggling workers. We revisit these
conventional beliefs in this paper, and examine the weaknesses of both
approaches. We demonstrate that a third approach, synchronous optimization with
backup workers, can avoid asynchronous noise while mitigating for the worst
stragglers. Our approach is empirically validated and shown to converge faster
and to better test accuracies.



Distributed optimization algorithms are widely used in many industrial
machine learning applications. However choosing the appropriate algorithm and
cluster size is often difficult for users as the performance and convergence
rate of optimization algorithms vary with the size of the cluster. In this
paper we make the case for an ML-optimizer that can select the appropriate
algorithm and cluster size to use for a given problem. To do this we propose
building two models: one that captures the system level characteristics of how
computation, communication change as we increase cluster sizes and another that
captures how convergence rates change with cluster sizes. We present
preliminary results from our prototype implementation called Hemingway and
discuss some of the challenges involved in developing such a system.



Traditionally, multi-layer neural networks use dot product between the output
vector of previous layer and the incoming weight vector as the input to
activation function. The result of dot product is unbounded, thus increases the
risk of large variance. Large variance of neuron makes the model sensitive to
the change of input distribution, thus results in poor generalization, and
aggravates the internal covariate shift which slows down the training. To bound
dot product and decrease the variance, we propose to use cosine similarity or
centered cosine similarity (Pearson Correlation Coefficient) instead of dot
product in neural networks, which we call cosine normalization. We compare
cosine normalization with batch, weight and layer normalization in
fully-connected neural networks as well as convolutional networks on the data
sets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that
cosine normalization achieves better performance than other normalization
techniques.



Reinforcement Learning algorithms can learn complex behavioral patterns for
sequential decision making tasks wherein an agent interacts with an environment
and acquires feedback in the form of rewards sampled from it. Traditionally,
such algorithms make decisions, i.e., select actions to execute, at every
single time step of the agent-environment interactions. In this paper, we
propose a novel framework, Fine Grained Action Repetition (FiGAR), which
enables the agent to decide the action as well as the time scale of repeating
it. FiGAR can be used for improving any Deep Reinforcement Learning algorithm
which maintains an explicit policy estimate by enabling temporal abstractions
in the action space. We empirically demonstrate the efficacy of our framework
by showing performance improvements on top of three policy search algorithms in
different domains: Asynchronous Advantage Actor Critic in the Atari 2600
domain, Trust Region Policy Optimization in Mujoco domain and Deep
Deterministic Policy Gradients in the TORCS car racing domain.



Reason and inference require process as well as memory skills by humans.
Neural networks are able to process tasks like image recognition (better than
humans) but in memory aspects are still limited (by attention mechanism, size).
Recurrent Neural Network (RNN) and it's modified version LSTM are able to solve
small memory contexts, but as context becomes larger than a threshold, it is
difficult to use them. The Solution is to use large external memory. Still, it
poses many challenges like, how to train neural networks for discrete memory
representation, how to describe long term dependencies in sequential data etc.
Most prominent neural architectures for such tasks are Memory networks:
inference components combined with long term memory and Neural Turing Machines:
neural networks using external memory resources. Also, additional techniques
like attention mechanism, end to end gradient descent on discrete memory
representation are needed to support these solutions. Preliminary results of
above neural architectures on simple algorithms (sorting, copying) and Question
Answering (based on story, dialogs) application are comparable with the state
of the art. In this paper, I explain these architectures (in general), the
additional techniques used and the results of their application.



Mobile robots are increasingly being employed for performing complex tasks in
dynamic environments. Reinforcement learning (RL) methods are recognized to be
promising for specifying such tasks in a relatively simple manner. However, the
strong dependency between the learning method and the task to learn is a
well-known problem that restricts practical implementations of RL in robotics,
often requiring major modifications of parameters and adding other techniques
for each particular task. In this paper we present a practical core
implementation of RL which enables the learning process for multiple robotic
tasks with minimal per-task tuning or none. Based on value iteration methods,
this implementation includes a novel approach for action selection, called
Q-biased softmax regression (QBIASSR), which avoids poor performance of the
learning process when the robot reaches new unexplored states. Our approach
takes advantage of the structure of the state space by attending the physical
variables involved (e.g., distances to obstacles, X,Y,{\theta} pose, etc.),
thus experienced sets of states may favor the decision-making process of
unexplored or rarely-explored states. This improvement has a relevant role in
reducing the tuning of the algorithm for particular tasks. Experiments with
real and simulated robots, performed with the software framework also
introduced here, show that our implementation is effectively able to learn
different robotic tasks without tuning the learning method. Results also
suggest that the combination of true online SARSA({\lambda}) with QBIASSR can
outperform the existing RL core algorithms in low-dimensional robotic tasks.



Visual question answering (VQA) has witnessed great progress since May, 2015
as a classic problem unifying visual and textual data into a system. Many
enlightening VQA works explore deep into the image and question encodings and
fusing methods, of which attention is the most effective and infusive
mechanism. Current attention based methods focus on adequate fusion of visual
and textual features, but lack the attention to where people focus to ask
questions about the image. Traditional attention based methods attach a single
value to the feature at each spatial location, which losses many useful
information. To remedy these problems, we propose a general method to perform
saliency-like pre-selection on overlapped region features by the interrelation
of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication
based attention method to capture more competent correlation information
between visual and textual features. We conduct experiments on the large-scale
COCO-VQA dataset and analyze the effectiveness of our model demonstrated by
strong empirical results.



Recent studies have shown that deep neural networks (DNN) are vulnerable to
adversarial samples: maliciously-perturbed samples crafted to yield incorrect
model outputs. Such attacks can severely undermine DNN systems, particularly in
security-sensitive settings. It was observed that an adversary could easily
generate adversarial samples by making a small perturbation on irrelevant
feature dimensions that are unnecessary for the current classification task. To
overcome this problem, we introduce a defensive mechanism called DeepCloak. By
identifying and removing unnecessary features in a DNN model, DeepCloak limits
the capacity an attacker can use generating adversarial samples and therefore
increase the robustness against such inputs. Comparing with other defensive
approaches, DeepCloak is easy to implement and computationally efficient.
Experimental results show that DeepCloak can increase the performance of
state-of-the-art DNN models against adversarial samples.



We used redescription mining to find interpretable rules revealing
associations between those determinants that provide insights about the
Alzheimer's disease (AD). We extended the CLUS-RM redescription mining
algorithm to a constraint-based redescription mining (CBRM) setting, which
enables several modes of targeted exploration of specific, user-constrained
associations. Redescription mining enabled finding specific constructs of
clinical and biological attributes that describe many groups of subjects of
different size, homogeneity and levels of cognitive impairment. We confirmed
some previously known findings. However, in some instances, as with the
attributes: testosterone, the imaging attribute Spatial Pattern of
Abnormalities for Recognition of Early AD, as well as the levels of leptin and
angiopoietin-2 in plasma, we corroborated previously debatable findings or
provided additional information about these variables and their association
with AD pathogenesis. Applying redescription mining on ADNI data resulted with
the discovery of one largely unknown attribute: the Pregnancy-Associated
Protein-A (PAPP-A), which we found highly associated with cognitive impairment
in AD. Statistically significant correlations (p <= 0.01) were found between
PAPP-A and various different clinical tests. The high importance of this
finding lies in the fact that PAPP-A is a metalloproteinase, known to cleave
insulin-like growth factor binding proteins. Since it also shares similar
substrates with A Disintegrin and the Metalloproteinase family of enzymes that
act as {\alpha}-secretase to physiologically cleave amyloid precursor protein
(APP) in the non-amyloidogenic pathway, it could be directly involved in the
metabolism of APP very early during the disease course. Therefore, further
studies should investigate the role of PAPP-A in the development of AD more
thoroughly.



In statistical relational learning, knowledge graph completion deals with
automatically understanding the structure of large knowledge graphs---labeled
directed graphs---and predicting missing relationships---labeled edges.
State-of-the-art embedding models propose different trade-offs between modeling
expressiveness, and time and space complexity. We reconcile both expressiveness
and complexity through the use of complex-valued embeddings and explore the
link between such complex-valued embeddings and unitary diagonalization. We
corroborate our approach theoretically and show that all real square
matrices---thus all possible relation/adjacency matrices---are the real part of
some unitarily diagonalizable matrix. This results opens the door to a lot of
other applications of square matrices factorization. Our approach based on
complex embeddings is arguably simple, as it only involves a Hermitian dot
product, the complex counterpart of the standard dot product between real
vectors, whereas other methods resort to more and more complicated composition
functions to increase their expressiveness. The proposed complex embeddings are
scalable to large data sets as it remains linear in both space and time, while
consistently outperforming alternative approaches on standard link prediction
benchmarks.



Limited labeled data are available for the research of estimating facial
expression intensities. For instance, the ability to train deep networks for
automated pain assessment is limited by small datasets with labels of
patient-reported pain intensities. Fortunately, fine-tuning from a
data-extensive pre-trained domain, such as face verification, can alleviate
this problem. In this paper, we propose a network that fine-tunes a
state-of-the-art face verification network using a regularized regression loss
and additional data with expression labels. In this way, the expression
intensity regression task can benefit from the rich feature representations
trained on a huge amount of data for face verification. The proposed
regularized deep regressor is applied to estimate the pain expression intensity
and verified on the widely-used UNBC-McMaster Shoulder-Pain dataset, achieving
the state-of-the-art performance. A weighted evaluation metric is also proposed
to address the imbalance issue of different pain intensities.



LTE in unlicensed spectrum (LTE-U) is a promising approach to overcome the
wireless spectrum scarcity. However, to reap the benefits of LTE-U, a fair
coexistence mechanism with other incumbent WiFi deployments is required. In
this paper, a novel deep learning approach is proposed for modeling the
resource allocation problem of LTE-U small base stations (SBSs). The proposed
approach enables multiple SBSs to proactively perform dynamic channel
selection, carrier aggregation, and fractional spectrum access while
guaranteeing fairness with existing WiFi networks and other LTE-U operators.
Adopting a proactive coexistence mechanism enables future delay-intolerant
LTE-U data demands to be served within a given prediction window ahead of their
actual arrival time thus avoiding the underutilization of the unlicensed
spectrum during off-peak hours while maximizing the total served LTE-U traffic
load. To this end, a noncooperative game model is formulated in which SBSs are
modeled as Homo Egualis agents that aim at predicting a sequence of future
actions and thus achieving long-term equal weighted fairness with WLAN and
other LTE-U operators over a given time horizon. The proposed deep learning
algorithm is then shown to reach a mixed-strategy Nash equilibrium (NE), when
it converges. Simulation results using real data traces show that the proposed
scheme can yield up to 28% and 11% gains over a conventional reactive approach
and a proportional fair coexistence mechanism, respectively. The results also
show that the proposed framework prevents WiFi performance degradation for a
densely deployed LTE-U network.



As artificial agents proliferate, it is becoming increasingly important to
ensure that their interactions with one another are well-behaved. In this
paper, we formalize a common-sense notion of when algorithms are well-behaved:
an algorithm is safe if it does no harm. Motivated by recent progress in deep
learning, we focus on the specific case where agents update their actions
according to gradient descent. The first result is that gradient descent
converges to a Nash equilibrium in safe games.
  The paper provides sufficient conditions that guarantee safe interactions.
The main contribution is to define strongly-typed agents and show they are
guaranteed to interact safely. A series of examples show that strong-typing
generalizes certain key features of convexity and is closely related to blind
source separation. The analysis introduce a new perspective on classical
multilinear games based on tensor decomposition.



Apprenticeship learning has recently attracted a wide attention due to its
capability of allowing robots to learn physical tasks directly from
demonstrations provided by human experts. Most previous techniques assumed that
the state space is known a priori or employed simple state representations that
usually suffer from perceptual aliasing. Different from previous research, we
propose a novel approach named Sequence-based Multimodal Apprenticeship
Learning (SMAL), which is capable to simultaneously fusing temporal information
and multimodal data, and to integrate robot perception with decision making. To
evaluate the SMAL approach, experiments are performed using both simulations
and real-world robots in the challenging search and rescue scenarios. The
empirical study has validated that our SMAL approach can effectively learn
plans for robots to make decisions using sequence of multimodal observations.
Experimental results have also showed that SMAL outperforms the baseline
methods using individual images.



For robots to coexist with humans in a social world like ours, it is crucial
that they possess human-like social interaction skills. Programming a robot to
possess such skills is a challenging task. In this paper, we propose a
Multimodal Deep Q-Network (MDQN) to enable a robot to learn human-like
interaction skills through a trial and error method. This paper aims to develop
a robot that gathers data during its interaction with a human and learns human
interaction behaviour from the high-dimensional sensory information using
end-to-end reinforcement learning. This paper demonstrates that the robot was
able to learn basic interaction skills successfully, after 14 days of
interacting with people.



Over the past few years, online aggression and abusive behaviors have
occurred in many different forms and on a variety of platforms. In extreme
cases, these incidents have evolved into hate, discrimination, and bullying,
and even materialized into real-world threats and attacks against individuals
or groups. In this paper, we study the Gamergate controversy. Started in August
2014 in the online gaming world, it quickly spread across various social
networking platforms, ultimately leading to many incidents of cyberbullying and
cyberaggression. We focus on Twitter, presenting a measurement study of a
dataset of 340k unique users and 1.6M tweets to study the properties of these
users, the content they post, and how they differ from random Twitter users. We
find that users involved in this "Twitter war" tend to have more friends and
followers, are generally more engaged and post tweets with negative sentiment,
less joy, and more hate than random users. We also perform preliminary
measurements on how the Twitter suspension mechanism deals with such abusive
behaviors. While we focus on Gamergate, our methodology to collect and analyze
tweets related to aggressive and bullying activities is of independent
interest.



We introduce AI rationalization, an approach for generating explanations of
autonomous system behavior as if a human had performed the behavior. We
describe a rationalization technique that uses neural machine translation to
translate internal state-action representations of an autonomous agent into
natural language. We evaluate our technique in the Frogger game environment,
training an autonomous game playing agent to rationalize its action choices
using natural language. A natural language training corpus is collected from
human players thinking out loud as they play the game. We motivate the use of
rationalization as an approach to explanation generation and show the results
of two experiments evaluating the effectiveness of rationalization. Results of
these evaluations show that neural machine translation is able to accurately
generate rationalizations that describe agent behavior, and that
rationalizations are more satisfying to humans than other alternative methods
of explanation.



Despite the successes in capturing continuous distributions, the application
of generative adversarial networks (GANs) to discrete settings, like natural
language tasks, is rather restricted. The fundamental reason is the difficulty
of back-propagation through discrete random variables combined with the
inherent instability of the GAN training objective. To address these problems,
we propose Maximum-Likelihood Augmented Discrete Generative Adversarial
Networks. Instead of directly optimizing the GAN objective, we derive a novel
and low-variance objective using the discriminator's output that follows
corresponds to the log-likelihood. Compared with the original, the new
objective is proved to be consistent in theory and beneficial in practice. The
experimental results on various discrete datasets demonstrate the effectiveness
of the proposed approach.



We introduce DeepNAT, a 3D Deep convolutional neural network for the
automatic segmentation of NeuroAnaTomy in T1-weighted magnetic resonance
images. DeepNAT is an end-to-end learning-based approach to brain segmentation
that jointly learns an abstract feature representation and a multi-class
classification. We propose a 3D patch-based approach, where we do not only
predict the center voxel of the patch but also neighbors, which is formulated
as multi-task learning. To address a class imbalance problem, we arrange two
networks hierarchically, where the first one separates foreground from
background, and the second one identifies 25 brain structures on the
foreground. Since patches lack spatial context, we augment them with
coordinates. To this end, we introduce a novel intrinsic parameterization of
the brain volume, formed by eigenfunctions of the Laplace-Beltrami operator. As
network architecture, we use three convolutional layers with pooling, batch
normalization, and non-linearities, followed by fully connected layers with
dropout. The final segmentation is inferred from the probabilistic output of
the network with a 3D fully connected conditional random field, which ensures
label agreement between close voxels. The roughly 2.7 million parameters in the
network are learned with stochastic gradient descent. Our results show that
DeepNAT compares favorably to state-of-the-art methods. Finally, the purely
learning-based method may have a high potential for the adaptation to young,
old, or diseased brains by fine-tuning the pre-trained network with a small
training sample on the target application, where the availability of larger
datasets with manual annotations may boost the overall segmentation accuracy in
the future.



We propose a novel approach for using unsupervised boosting to create an
ensemble of generative models, where models are trained in sequence to correct
earlier mistakes. Our meta-algorithmic framework can leverage any existing base
learner that permits likelihood evaluation, including recent deep expressive
models. Further, our approach allows the ensemble to include discriminative
models trained to distinguish real data from model-generated data. We show
theoretical conditions under which incorporating a new model in the ensemble
will improve the fit and empirically demonstrate the effectiveness of our
black-box boosting algorithms on density estimation, classification, and sample
generation on benchmark datasets for a wide range of generative models.



We study the problem of causal structure learning over a set of random
variables when the experimenter is allowed to perform at most $M$ experiments
in a non-adaptive manner. We consider the optimal learning strategy in terms of
minimizing the portions of the structure that remains unknown given the limited
number of experiments in both Bayesian and minimax setting. We characterize the
theoretical optimal solution and propose an algorithm, which designs the
experiments efficiently in terms of time complexity. We show that for bounded
degree graphs, in the minimax case and in the Bayesian case with uniform
priors, our proposed algorithm is a $\rho$-approximation algorithm, where
$\rho$ is independent of the order of the underlying graph. Simulations on both
synthetic and real data show that the performance of our algorithm is very
close to the optimal solution.



As machine learning systems become ubiquitous, there has been a surge of
interest in interpretable machine learning: systems that provide explanation
for their outputs. These explanations are often used to qualitatively assess
other criteria such as safety or non-discrimination. However, despite the
interest in interpretability, there is very little consensus on what
interpretable machine learning is and how it should be measured. In this
position paper, we first define interpretability and describe when
interpretability is needed (and when it is not). Next, we suggest a taxonomy
for rigorous evaluation and expose open questions towards a more rigorous
science of interpretable machine learning.



For a safe, natural and effective human-robot social interaction, it is
essential to develop a system that allows a robot to demonstrate the
perceivable responsive behaviors to complex human behaviors. We introduce the
Multimodal Deep Attention Recurrent Q-Network using which the robot exhibits
human-like social interaction skills after 14 days of interacting with people
in an uncontrolled real world. Each and every day during the 14 days, the
system gathered robot interaction experiences with people through a
hit-and-trial method and then trained the MDARQN on these experiences using
end-to-end reinforcement learning approach. The results of interaction based
learning indicate that the robot has learned to respond to complex human
behaviors in a perceivable and socially acceptable manner.



Conventional reinforcement learning methods for Markov decision processes
rely on weakly-guided, stochastic searches to drive the learning process. It
can therefore be difficult to predict what agent behaviors might emerge. In
this paper, we consider an information-theoretic cost function for performing
constrained stochastic searches that promote the formation of risk-averse to
risk-favoring behaviors. This cost function is the value of information, which
provides the optimal trade-off between the expected return of a policy and the
policy's complexity; policy complexity is measured by number of bits and
controlled by a single hyperparameter on the cost function. As the policy
complexity is reduced, the agents will increasingly eschew risky actions. This
reduces the potential for high accrued rewards. As the policy complexity
increases, the agents will take actions, regardless of the risk, that can raise
the long-term rewards. The obtainable reward depends on a single, tunable
hyperparameter that regulates the degree of policy complexity.
  We evaluate the performance of value-of-information-based policies on a
stochastic version of Ms. Pac-Man. A major component of this paper is the
demonstration that ranges of policy complexity values yield different game-play
styles and explaining why this occurs. We also show that our
reinforcement-learning search mechanism is more efficient than the others we
utilize. This result implies that the value of information theory is
appropriate for framing the exploitation-exploration trade-off in reinforcement
learning.



Machine learning is essentially the sciences of playing with data. An
adaptive data selection strategy, enabling to dynamically choose different data
at various training stages, can reach a more effective model in a more
efficient way. In this paper, we propose a deep reinforcement learning
framework, which we call \emph{\textbf{N}eural \textbf{D}ata \textbf{F}ilter}
(\textbf{NDF}), to explore automatic and adaptive data selection in the
training process. In particular, NDF takes advantage of a deep neural network
to adaptively select and filter important data instances from a sequential
stream of training data, such that the future accumulative reward (e.g., the
convergence speed) is maximized. In contrast to previous studies in data
selection that is mainly based on heuristic strategies, NDF is quite generic
and thus can be widely suitable for many machine learning tasks. Taking neural
network training with stochastic gradient descent (SGD) as an example,
comprehensive experiments with respect to various neural network modeling
(e.g., multi-layer perceptron networks, convolutional neural networks and
recurrent neural networks) and several applications (e.g., image classification
and text understanding) demonstrate that NDF powered SGD can achieve comparable
accuracy with standard SGD process by using less data and fewer iterations.



Deep neural networks require a large amount of labeled training data during
supervised learning. However, collecting and labeling so much data might be
infeasible in many cases. In this paper, we introduce a source-target selective
joint fine-tuning scheme for improving the performance of deep learning tasks
with insufficient training data. In this scheme, a target learning task with
insufficient training data is carried out simultaneously with another source
learning task with abundant training data. However, the source learning task
does not use all existing training data. Our core idea is to identify and use a
subset of training images from the original source learning task whose
low-level characteristics are similar to those from the target learning task,
and jointly fine-tune shared convolutional layers for both tasks. Specifically,
we compute descriptors from linear or nonlinear filter bank responses on
training images from both tasks, and use such descriptors to search for a
desired subset of training samples for the source learning task.
  Experiments demonstrate that our selective joint fine-tuning scheme achieves
state-of-the-art performance on multiple visual classification tasks with
insufficient training data for deep learning. Such tasks include Caltech 256,
MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to
fine-tuning without a source domain, the proposed method can improve the
classification accuracy by 2% - 10% using a single model.



We introduce Stacked Thompson Bandits (STB) for efficiently generating plans
that are likely to satisfy a given bounded temporal logic requirement. STB uses
a simulation for evaluation of plans, and takes a Bayesian approach to using
the resulting information to guide its search. In particular, we show that
stacking multiarmed bandits and using Thompson sampling to guide the action
selection process for each bandit enables STB to generate plans that satisfy
requirements with a high probability while only searching a fraction of the
search space.



Many real-world problems, such as network packet routing and urban traffic
control, are naturally modeled as multi-agent reinforcement learning (RL)
problems. However, existing multi-agent RL methods typically scale poorly in
the problem size. Therefore, a key challenge is to translate the success of
deep learning on single-agent RL to the multi-agent setting. A major stumbling
block is that independent Q-learning, the most popular multi-agent RL method,
introduces nonstationarity that makes it incompatible with the experience
replay memory on which deep Q-learning relies. This paper proposes two methods
that address this problem: 1) using a multi-agent variant of importance
sampling to naturally decay obsolete data and 2) conditioning each agent's
value function on a fingerprint that disambiguates the age of the data sampled
from the replay memory. Results on a challenging decentralised variant of
StarCraft unit micromanagement confirm that these methods enable the successful
combination of experience replay with multi-agent RL.



We establish a new connection between value and policy based reinforcement
learning (RL) based on a relationship between softmax temporal value
consistency and policy optimality under entropy regularization. Specifically,
we show that softmax consistent action values correspond to optimal entropy
regularized policy probabilities along any action sequence, regardless of
provenance. From this observation, we develop a new RL algorithm, Path
Consistency Learning (PCL), that minimizes a notion of soft consistency error
along multi-step action sequences extracted from both on- and off-policy
traces. We examine the behavior of PCL in different scenarios and show that PCL
can be interpreted as generalizing both actor-critic and Q-learning algorithms.
We subsequently deepen the relationship by showing how a single model can be
used to represent both a policy and the corresponding softmax state values,
eliminating the need for a separate critic. The experimental evaluation
demonstrates that PCL significantly outperforms strong actor-critic and
Q-learning baselines across several benchmarks.



Contextual bandits are widely used in Internet services from news
recommendation to advertising, and to Web search. Generalized linear models
(logistical regression in particular) have demonstrated stronger performance
than linear models in many applications where rewards are binary. However, most
theoretical analyses on contextual bandits so far are on linear bandits. In
this work, we propose an upper confidence bound based algorithm for generalized
linear contextual bandits, which achieves an $\tilde{O}(\sqrt{dT})$ regret over
$T$ rounds with $d$ dimensional feature vectors. This regret matches the
minimax lower bound, up to logarithmic terms, and improves on the best previous
result by a $\sqrt{d}$ factor, assuming the number of arms is fixed. A key
component in our analysis is to establish a new, sharp finite-sample confidence
bound for maximum-likelihood estimates in generalized linear models, which may
be of independent interest. We also analyze a simpler upper confidence bound
algorithm, which is useful in practice, and prove it to have optimal regret for
certain cases.



Task-oriented dialog systems have been applied in various tasks, such as
automated personal assistants, customer service providers and tutors. These
systems work well when users have clear and explicit intentions that are
well-aligned to the systems' capabilities. However, they fail if users
intentions are not explicit. To address this shortcoming, we propose a
framework to interleave non-task content (i.e. everyday social conversation)
into task conversations. When the task content fails, the system can still keep
the user engaged with the non-task content. We trained a policy using
reinforcement learning algorithms to promote long-turn conversation coherence
and consistency, so that the system can have smooth transitions between task
and non-task content. To test the effectiveness of the proposed framework, we
developed a movie promotion dialog system. Experiments with human users
indicate that a system that interleaves social and task content achieves a
better task success rate and is also rated as more engaging compared to a pure
task-oriented system.



One-sided matching mechanisms are fundamental for assigning a set of
indivisible objects to a set of self-interested agents when monetary transfers
are not allowed. Two widely-studied randomized mechanisms in multiagent
settings are the Random Serial Dictatorship (RSD) and the Probabilistic Serial
Rule (PS). Both mechanisms require only that agents specify ordinal preferences
and have a number of desirable economic and computational properties. However,
the induced outcomes of the mechanisms are often incomparable and thus there
are challenges when it comes to deciding which mechanism to adopt in practice.
In this paper, we first consider the space of general ordinal preferences and
provide empirical results on the (in)comparability of RSD and PS. We analyze
their respective economic properties under general and lexicographic
preferences. We then instantiate utility functions with the goal of gaining
insights on the manipulability, efficiency, and envyfreeness of the mechanisms
under different risk-attitude models. Our results hold under various preference
distribution models, which further confirm the broad use of RSD in most
practical applications.



The principle of common cause asserts that positive correlations between
causally unrelated events ought to be explained through the action of some
shared causal factors. Reichenbachian common cause systems are probabilistic
structures aimed at accounting for cases where correlations of the aforesaid
sort cannot be explained through the action of a single common cause. The
existence of Reichenbachian common cause systems of arbitrary finite size for
each pair of non-causally correlated events was allegedly demonstrated by
Hofer-Szab\'o and R\'edei in 2006. This paper shows that their proof is
logically deficient, and we propose an improved proof.



Sophisticated gated recurrent neural network architectures like LSTMs and
GRUs have been shown to be highly effective in a myriad of applications. We
develop an un-gated unit, the statistical recurrent unit (SRU), that is able to
learn long term dependencies in data by only keeping moving averages of
statistics. The SRU's architecture is simple, un-gated, and contains a
comparable number of parameters to LSTMs; yet, SRUs perform favorably to more
sophisticated LSTM and GRU alternatives, often outperforming one or both in
various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an
unbiased manner by optimizing respective architectures' hyperparameters in a
Bayesian optimization scheme for both synthetic and real-world tasks.



We present a learning-based mapless motion planner by taking the sparse
10-dimensional range findings and the target position with respect to the
mobile robot coordinate frame as input and the continuous steering commands as
output. Traditional motion planners for mobile ground robots with a laser range
sensor mostly depend on the obstacle map of the navigation environment where
both the highly precise laser sensor and the obstacle map building work of the
environment are indispensable. We show that, through an asynchronous deep
reinforcement learning method, a mapless motion planner can be trained
end-to-end without any manually designed features and prior demonstrations. The
trained planner can be directly applied in unseen virtual and real
environments. The experiments show that the proposed mapless motion planner can
navigate the nonholonomic mobile robot to the desired targets without colliding
with any obstacles.



Learning to Optimize is a recently proposed framework for learning
optimization algorithms using reinforcement learning. In this paper, we explore
learning an optimization algorithm for training shallow neural nets. Such
high-dimensional stochastic optimization problems present interesting
challenges for existing reinforcement learning algorithms. We develop an
extension that is suited to learning optimization algorithms in this setting
and demonstrate that the learned optimization algorithm consistently
outperforms other known optimization algorithms even on unseen tasks and is
robust to changes in stochasticity of gradients and the neural net
architecture. More specifically, we show that an optimization algorithm trained
with the proposed method on the problem of training a neural net on MNIST
generalizes to the problems of training neural nets on the Toronto Faces
Dataset, CIFAR-10 and CIFAR-100.



This paper presents OptNet, a network architecture that integrates
optimization problems (here, specifically in the form of quadratic programs) as
individual layers in larger end-to-end trainable deep networks. These layers
encode constraints and complex dependencies between the hidden states that
traditional convolutional and fully-connected layers often cannot capture. In
this paper, we explore the foundations for such an architecture: we show how
techniques from sensitivity analysis, bilevel optimization, and implicit
differentiation can be used to exactly differentiate through these layers and
with respect to layer parameters; we develop a highly efficient solver for
these layers that exploits fast GPU-based batch solves within a primal-dual
interior point method, and which provides backpropagation gradients with
virtually no additional cost on top of the solve; and we highlight the
application of these approaches in several problems. In one notable example, we
show that the method is capable of learning to play mini-Sudoku (4x4) given
just input and output games, with no a priori information about the rules of
the game; this highlights the ability of our architecture to learn hard
constraints better than other neural architectures.



We consider a scheduling problem where a cloud service provider has multiple
units of a resource available over time. Selfish clients submit jobs, each with
an arrival time, deadline, length, and value. The service provider's goal is to
implement a truthful online mechanism for scheduling jobs so as to maximize the
social welfare of the schedule. Recent work shows that under a stochastic
assumption on job arrivals, there is a single-parameter family of mechanisms
that achieves near-optimal social welfare. We show that given any such family
of near-optimal online mechanisms, there exists an online mechanism that in the
worst case performs nearly as well as the best of the given mechanisms. Our
mechanism is truthful whenever the mechanisms in the given family are truthful
and prompt, and achieves optimal (within constant factors) regret.
  We model the problem of competing against a family of online scheduling
mechanisms as one of learning from expert advice. A primary challenge is that
any scheduling decisions we make affect not only the payoff at the current
step, but also the resource availability and payoffs in future steps.
Furthermore, switching from one algorithm (a.k.a. expert) to another in an
online fashion is challenging both because it requires synchronization with the
state of the latter algorithm as well as because it affects the incentive
structure of the algorithms. We further show how to adapt our algorithm to a
non-clairvoyant setting where job lengths are unknown until jobs are run to
completion. Once again, in this setting, we obtain truthfulness along with
asymptotically optimal regret (within poly-logarithmic factors).



In this paper, we present a general framework for learning social affordance
grammar as a spatiotemporal AND-OR graph (ST-AOG) from RGB-D videos of human
interactions, and transfer the grammar to humanoids to enable a real-time
motion inference for human-robot interaction (HRI). Based on Gibbs sampling,
our weakly supervised grammar learning can automatically construct a
hierarchical representation of an interaction with long-term joint sub-tasks of
both agents and short term atomic actions of individual agents. Based on a new
RGB-D video dataset with rich instances of human interactions, our experiments
of Baxter simulation, human evaluation, and real Baxter test demonstrate that
the model learned from limited training data successfully generates human-like
behaviors in unseen scenarios and outperforms both baselines.



Conversion optimization means designing a web interface so that as many users
as possible take a desired action on it, such as register or purchase. Such
design is usually done by hand, testing one change at a time through A/B
testing, or a limited number of combinations through multivariate testing,
making it possible to evaluate only a small fraction of designs in a vast
design space. This paper describes Sentient Ascend, an automatic conversion
optimization system that uses evolutionary optimization to create effective web
interface designs. Ascend makes it possible to discover and utilize
interactions between the design elements that are difficult to identify
otherwise. Moreover, evaluation of design candidates is done in parallel
online, i.e. with a large number of real users interacting with the system. A
case study on an existing media site shows that significant improvements (i.e.
over 43%) are possible beyond human design. Ascend can therefore be seen as an
approach to massively multivariate conversion optimization, based on a
massively parallel interactive evolution.



Online two-sided matching markets such as Q&A forums (e.g. StackOverflow,
Quora) and online labour platforms (e.g. Upwork) critically rely on the ability
to propose adequate matches based on imperfect knowledge of the two parties to
be matched. This prompts the following question: Which matching recommendation
algorithms can, in the presence of such uncertainty, lead to efficient platform
operation?
  To answer this question, we develop a model of a task / server matching
system. For this model, we give a necessary and sufficient condition for an
incoming stream of tasks to be manageable by the system. We further identify a
so-called back-pressure policy under which the throughput that the system can
handle is optimized. We show that this policy achieves strictly larger
throughput than a natural greedy policy. Finally, we validate our model and
confirm our theoretical findings with experiments based on logs of
Math.StackExchange, a StackOverflow forum dedicated to mathematics.



In this work, we open up the DAWT dataset - Densely Annotated Wikipedia Texts
across multiple languages. The annotations include labeled text mentions
mapping to entities (represented by their Freebase machine ids) as well as the
type of the entity. The data set contains total of 13.6M articles, 5.0B tokens,
13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text
to entity links than originally present in the Wikipedia markup. Moreover, it
spans several languages including English, Spanish, Italian, German, French and
Arabic. We also present the methodology used to generate the dataset which
enriches Wikipedia markup in order to increase number of links. In addition to
the main dataset, we open up several derived datasets including mention entity
co-occurrence counts and entity embeddings, as well as mappings between
Freebase ids and Wikidata item ids. We also discuss two applications of these
datasets and hope that opening them up would prove useful for the Natural
Language Processing and Information Retrieval communities, as well as
facilitate multi-lingual research.



Generic generation and manipulation of text is challenging and has limited
success compared to recent deep generative modeling in visual domain. This
paper aims at generating plausible natural language sentences, whose attributes
are dynamically controlled by learning disentangled latent representations with
designated semantics. We propose a new neural generative model which combines
variational auto-encoders and holistic attribute discriminators for effective
imposition of semantic structures. With differentiable approximation to
discrete text samples, explicit constraints on independent attribute controls,
and efficient collaborative learning of generator and discriminators, our model
learns highly interpretable representations from even only word annotations,
and produces realistic sentences with desired attributes. Quantitative
evaluation validates the accuracy of sentence and attribute generation.



When using reinforcement learning (RL) algorithms to evaluate a policy it is
common, given a large state space, to introduce some form of approximation
architecture for the value function (VF). The exact form of this architecture
can have a significant effect on the accuracy of the VF estimate, however, and
determining a suitable approximation architecture can often be a highly complex
task. Consequently there is a large amount of interest in the potential for
allowing RL algorithms to adaptively generate approximation architectures.
  We investigate a method of adapting approximation architectures which uses
feedback regarding the frequency with which an agent has visited certain states
to guide which areas of the state space to approximate with greater detail.
This method is "unsupervised" in the sense that it makes no direct reference to
reward or the VF estimate. We introduce an algorithm based upon this idea which
adapts a state aggregation approximation architecture on-line.
  A common method of scoring a VF estimate is to weight the squared Bellman
error of each state-action by the probability of that state-action occurring.
Adopting this scoring method, and assuming $S$ states, we demonstrate
theoretically that - provided (1) the number of cells $X$ in the state
aggregation architecture is of order $\sqrt{S}\log_2{S}\ln{S}$ or greater, (2)
the policy and transition function are close to deterministic, and (3) the
prior for the transition function is uniformly distributed - our algorithm,
used in conjunction with a suitable RL algorithm, can guarantee a score which
is arbitrarily close to zero as $S$ becomes large. It is able to do this
despite having only $O(X \log_2S)$ space complexity and negligible time
complexity. The results take advantage of certain properties of the stationary
distributions of Markov chains.



We design a new approach that allows robot learning of new activities from
unlabeled human example videos. Given videos of humans executing the same
activity from a human's viewpoint (i.e., first-person videos), our objective is
to make the robot learn the temporal structure of the activity as its future
regression network, and learn to transfer such model for its own motor
execution. We present a new deep learning model: We extend the state-of-the-art
convolutional object detection network for the representation/estimation of
human hands in training videos, and newly introduce the concept of using a
fully convolutional network to regress (i.e., predict) the intermediate scene
representation corresponding to the future frame (e.g., 1-2 seconds later).
Combining these allows direct prediction of future locations of human hands and
objects, which enables the robot to infer the motor control plan using our
manipulation network. We experimentally confirm that our approach makes
learning of robot activities from unlabeled human interaction videos possible,
and demonstrate that our robot is able to execute the learned collaborative
activities in real-time directly based on its camera input.



Neural networks have proven effective at solving difficult problems but
designing their architectures can be challenging, even for image classification
problems alone. Our goal is to minimize human participation, so we employ
evolutionary algorithms to discover such networks automatically. Despite
significant computational requirements, we show that it is now possible to
evolve models with accuracies within the range of those published in the last
year. Specifically, we employ simple evolutionary techniques at unprecedented
scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting
from trivial initial conditions and reaching accuracies of 94.6% (95.6% for
ensemble) and 77.0%, respectively. To do this, we use novel and intuitive
mutation operators that navigate large search spaces; we stress that no human
participation is required once evolution starts and that the output is a
fully-trained model. Throughout this work, we place special emphasis on the
repeatability of results, the variability in the outcomes and the computational
requirements.



Deep neural networks are representation learning techniques. During training,
a deep net is capable of generating a descriptive language of unprecedented
size and detail in machine learning. Extracting the descriptive language coded
within a trained CNN model (in the case of image data), and reusing it for
other purposes is a field of interest, as it provides access to the visual
descriptors previously learnt by the CNN after processing millions of images,
without requiring an expensive training phase. Contributions to this field
(commonly known as feature representation transfer or transfer learning) have
been purely empirical so far, extracting all CNN features from a single layer
close to the output and testing their performance by feeding them to a
classifier. This approach has provided consistent results, although its
relevance is limited to classification tasks. In a completely different
approach, in this paper we statistically measure the discriminative power of
every single feature found within a deep CNN, when used for characterizing
every class of 11 datasets. We seek to provide new insights into the behavior
of CNN features, particularly the ones from convolutional layers, as this can
be relevant for their application to knowledge representation and reasoning.
Our results confirm that low and middle level features may behave differently
to high level features, but only under certain conditions. We find that all CNN
features can be used for knowledge representation purposes both by their
presence or by their absence, doubling the information a single CNN feature may
provide. We also study how much noise these features may include, and propose a
thresholding approach to discard most of it. All these insights have a direct
application to the generation of CNN embedding spaces.



This paper contributes a first study into how different human users deliver
simultaneous control and feedback signals during human-robot interaction. As
part of this work, we formalize and present a general interactive learning
framework for online cooperation between humans and reinforcement learning
agents. In many human-machine interaction settings, there is a growing gap
between the degrees-of-freedom of complex semi-autonomous systems and the
number of human control channels. Simple human control and feedback mechanisms
are required to close this gap and allow for better collaboration between
humans and machines on complex tasks. To better inform the design of concurrent
control and feedback interfaces, we present experimental results from a
human-robot collaborative domain wherein the human must simultaneously deliver
both control and feedback signals to interactively train an actor-critic
reinforcement learning robot. We compare three experimental conditions: 1)
human delivered control signals, 2) reward-shaping feedback signals, and 3)
simultaneous control and feedback. Our results suggest that subjects provide
less feedback when simultaneously delivering feedback and control signals and
that control signal quality is not significantly diminished. Our data suggest
that subjects may also modify when and how they provide feedback. Through
algorithmic development and tuning informed by this study, we expect
semi-autonomous actions of robotic agents can be better shaped by human
feedback, allowing for seamless collaboration and improved performance in
difficult interactive domains.



We study contextual multi-armed bandit problems under linear realizability on
rewards and uncertainty (or noise) on features. For the case of identical noise
on features across actions, we propose an algorithm, coined {\em NLinRel},
having $O\left(T^{\frac{7}{8}} \left(\log{(dT)}+K\sqrt{d}\right)\right)$ regret
bound for $T$ rounds, $K$ actions, and $d$-dimensional feature vectors. Next,
for the case of non-identical noise, we observe that popular linear hypotheses
including {\em NLinRel} are impossible to achieve such sub-linear regret.
Instead, under assumption of Gaussian feature vectors, we prove that a greedy
algorithm has $O\left(T^{\frac23}\sqrt{\log d}\right)$ regret bound with
respect to the optimal linear hypothesis. Utilizing our theoretical
understanding on the Gaussian case, we also design a practical variant of {\em
NLinRel}, coined {\em Universal-NLinRel}, for arbitrary feature distributions.
It first runs {\em NLinRel} for finding the `true' coefficient vector using
feature uncertainties and then adjust it to minimize its regret using the
statistical feature information. We justify the performance of {\em
Universal-NLinRel} on both synthetic and real-world datasets.



We consider the case in which a robot has to navigate in an unknown
environment but does not have enough on-board power or payload to carry a
traditional depth sensor (e.g., a 3D lidar) and thus can only acquire a few
(point-wise) depth measurements. We address the following question: is it
possible to reconstruct the geometry of an unknown environment using sparse and
incomplete depth measurements? Reconstruction from incomplete data is not
possible in general, but when the robot operates in man-made environments, the
depth exhibits some regularity (e.g., many planar surfaces with only a few
edges); we leverage this regularity to infer depth from a small number of
measurements. Our first contribution is a formulation of the depth
reconstruction problem that bridges robot perception with the compressive
sensing literature in signal processing. The second contribution includes a set
of formal results that ascertain the exactness and stability of the depth
reconstruction in 2D and 3D problems, and completely characterize the geometry
of the profiles that we can reconstruct. Our third contribution is a set of
practical algorithms for depth reconstruction: our formulation directly
translates into algorithms for depth estimation based on convex programming. In
real-world problems, these convex programs are very large and general-purpose
solvers are relatively slow. For this reason, we discuss ad-hoc solvers that
enable fast depth reconstruction in real problems. The last contribution is an
extensive experimental evaluation in 2D and 3D problems, including Monte Carlo
runs on simulated instances and testing on multiple real datasets. Empirical
results confirm that the proposed approach ensures accurate depth
reconstruction, outperforms interpolation-based strategies, and performs well
even when the assumption of structured environment is violated.



To be able to interact better with humans, it is crucial for machines to
understand sound - a primary modality of human perception. Previous works have
used sound to learn embeddings for improved generic textual similarity
assessment. In this work, we treat sound as a first-class citizen, studying
downstream textual tasks which require aural grounding. To this end, we propose
sound-word2vec - a new embedding scheme that learns specialized word embeddings
grounded in sounds. For example, we learn that two seemingly (semantically)
unrelated concepts, like leaves and paper are similar due to the similar
rustling sounds they make. Our embeddings prove useful in textual tasks
requiring aural reasoning like text-based sound retrieval and discovering foley
sound effects (used in movies). Moreover, our embedding space captures
interesting dependencies between words and onomatopoeia and outperforms prior
work on aurally-relevant word relatedness datasets such as AMEN and ASLex.



Human-centered environments are rich with a wide variety of spatial relations
between everyday objects. For autonomous robots to operate effectively in such
environments, they should be able to reason about these relations and
generalize them to objects with different shapes and sizes. For example, having
learned to place a toy inside a basket, a robot should be able to generalize
this concept using a spoon and a cup. This requires a robot to have the
flexibility to learn arbitrary relations in a lifelong manner, making it
challenging for an expert to pre-program it with sufficient knowledge to do so
beforehand. In this paper, we address the problem of learning spatial relations
by introducing a novel method from the perspective of distance metric learning.
Our approach enables a robot to reason about the similarity between pairwise
spatial relations, thereby enabling it to use its previous knowledge when
presented with a new relation to imitate. We show how this makes it possible to
learn arbitrary spatial relations from non-expert users using a small number of
examples and in an interactive manner. Our extensive evaluation with real-world
data demonstrates the effectiveness of our method in reasoning about a
continuous spectrum of spatial relations and generalizing them to new objects.



Class labels have been empirically shown useful in improving the sample
quality of generative adversarial nets (GANs). In this paper, we mathematically
study the properties of the current variants of GANs that make use of class
label information. With class aware gradient and cross-entropy decomposition,
we reveal how class labels and associated losses influence GAN's training.
Based on that, we propose Activation Maximization Generative Adversarial
Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been
conducted to validate our analysis and evaluate the effectiveness of our
solution, where AM-GAN outperforms other strong baselines and achieves
state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we
demonstrate that, with the Inception ImageNet classifier, Inception Score
mainly tracks the diversity of the generator, and there is, however, no
reliable evidence that it can reflect the true sample quality. We thus propose
a new metric, called AM Score, to provide more accurate estimation on the
sample quality. Our proposed model also outperforms the baseline methods in the
new metric.



Advances in neural network based classifiers have transformed automatic
feature learning from a pipe dream of stronger AI to a routine and expected
property of practical systems. Since the emergence of AlexNet every winning
submission of the ImageNet challenge has employed end-to-end representation
learning, and due to the utility of good representations for transfer learning,
representation learning has become as an important and distinct task from
supervised learning. At present, this distinction is inconsequential, as
supervised methods are state-of-the-art in learning transferable
representations. But recent work has shown that generative models can also be
powerful agents of representation learning. Will the representations learned
from these generative methods ever rival the quality of those from their
supervised competitors? In this work, we argue in the affirmative, that from an
information theoretic perspective, generative models have greater potential for
representation learning. Based on several experimentally validated assumptions,
we show that supervised learning is upper bounded in its capacity for
representation learning in ways that certain generative models, such as
Generative Adversarial Networks (GANs) are not. We hope that our analysis will
provide a rigorous motivation for further exploration of generative
representation learning.



Epistemic planning can be used for decision making in multi-agent situations
with distributed knowledge and capabilities. Dynamic Epistemic Logic (DEL) has
been shown to provide a very natural and expressive framework for epistemic
planning. In this paper, we aim to give an accessible introduction to DEL-based
epistemic planning. The paper starts with the most classical framework for
planning, STRIPS, and then moves towards epistemic planning in a number of
smaller steps, where each step is motivated by the need to be able to model
more complex planning scenarios.



Epistemic planning can be used for decision making in multi-agent situations
with distributed knowledge and capabilities. Recently, Dynamic Epistemic Logic
(DEL) has been shown to provide a very natural and expressive framework for
epistemic planning. We extend the DEL-based epistemic planning framework to
include perspective shifts, allowing us to define new notions of sequential and
conditional planning with implicit coordination. With these, it is possible to
solve planning tasks with joint goals in a decentralized manner without the
agents having to negotiate about and commit to a joint policy at plan time.
First we define the central planning notions and sketch the implementation of a
planning system built on those notions. Afterwards we provide some case studies
in order to evaluate the planner empirically and to show that the concept is
useful for multi-agent systems in practice.



Our desire and fascination with intelligent machines dates back to the
antiquity's mythical automaton Talos, Aristotle's mode of mechanical thought
(syllogism) and Heron of Alexandria's mechanical machines and automata.
However, the quest for Artificial General Intelligence (AGI) is troubled with
repeated failures of strategies and approaches throughout the history. This
decade has seen a shift in interest towards bio-inspired software and hardware,
with the assumption that such mimicry entails intelligence. Though these steps
are fruitful in certain directions and have advanced automation, their singular
design focus renders them highly inefficient in achieving AGI. Which set of
requirements have to be met in the design of AGI? What are the limits in the
design of the artificial? Here, a careful examination of computation in
biological systems hints that evolutionary tinkering of contextual processing
of information enabled by a hierarchical architecture is the key to build AGI.



A Robust Markov Decision Process (RMDP) is a sequential decision making model
that accounts for uncertainty in the parameters of dynamic systems. This
uncertainty introduces difficulties in learning an optimal policy, especially
for environments with large state spaces. We propose two algorithms, RTD-DQN
and Deep-RoK, for solving large-scale RMDPs using nonlinear approximation
schemes such as deep neural networks. The RTD-DQN algorithm incorporates the
robust Bellman temporal difference error into a robust loss function, yielding
robust policies for the agent. The Deep-RoK algorithm is a robust Bayesian
method, based on the Extended Kalman Filter (EKF), that accounts for both the
uncertainty in the weights of the approximated value function and the
uncertainty in the transition probabilities, improving the robustness of the
agent. We provide theoretical results for our approach and test the proposed
algorithms on a continuous state domain.



The sure thing principle and the law of total probability are basic laws in
classic probability theory. A disjunction fallacy leads to the violation of
these two classical probability laws. In this paper, a new quantum dynamic
belief decision making model based on quantum dynamic modelling and
Dempster-Shafer (D-S) evidence theory is proposed to address this issue and
model the real human decision-making process. Some mathematical techniques are
borrowed from quantum mathematics. Generally, belief and action are two parts
in a decision making process. The uncertainty in belief part is represented by
a superposition of certain states. The uncertainty in actions is represented as
an extra uncertainty state. The interference effect is produced due to the
entanglement between beliefs and actions. Basic probability assignment (BPA) of
decisions is generated by quantum dynamic modelling. Then BPA of the extra
uncertain state and an entanglement degree defined by an entropy function named
Deng entropy are used to measure the interference effect. Compared the existing
model, the number of free parameters is less in our model. Finally, a classical
categorization decision-making experiment is illustrated to show the
effectiveness of our model.



The recent tremendous success of unsupervised word embeddings in a multitude
of applications raises the obvious question if similar methods could be derived
to improve embeddings (i.e. semantic representations) of word sequences as
well. We present a simple but efficient unsupervised objective to train
distributed representations of sentences. Our method outperforms the
state-of-the-art unsupervised models on most benchmark tasks, highlighting the
robustness of the produced general-purpose sentence embeddings.



We consider the problem of learning a causal graph over a set of variables
with interventions. We study the cost-optimal causal graph learning problem:
For a given skeleton (undirected version of the causal graph), design the set
of interventions with minimum total cost, that can uniquely identify any causal
graph with the given skeleton. We show that this problem is solvable in
polynomial time. Later, we consider the case when the number of interventions
is limited. For this case, we provide polynomial time algorithms when the
skeleton is a tree or a clique tree. For a general chordal skeleton, we develop
an efficient greedy algorithm, which can be improved when the causal graph
skeleton is an interval graph.



This work shows that policies with simple linear and RBF parameterizations
can be trained to solve a variety of continuous control tasks, including the
OpenAI gym benchmarks. The performance of these trained policies are
competitive with state of the art results, obtained with more elaborate
parameterizations such as fully connected neural networks. Furthermore,
existing training and testing scenarios are shown to be very limited and prone
to over-fitting, thus giving rise to only trajectory-centric policies. Training
with a diverse initial state distribution is shown to produce more global
policies with better generalization. This allows for interactive control
scenarios where the system recovers from large on-line perturbations; as shown
in the supplementary video.



Deep neural networks coupled with fast simulation and improved computation
have led to recent successes in the field of reinforcement learning (RL).
However, most current RL-based approaches fail to generalize since: (a) the gap
between simulation and real world is so large that policy-learning approaches
fail to transfer; (b) even if policy learning is done in real world, the data
scarcity leads to failed generalization from training to test scenarios (e.g.,
due to different friction or object masses). Inspired from H-infinity control
methods, we note that both modeling errors and differences in training and test
scenarios can be viewed as extra forces/disturbances in the system. This paper
proposes the idea of robust adversarial reinforcement learning (RARL), where we
train an agent to operate in the presence of a destabilizing adversary that
applies disturbance forces to the system. The jointly trained adversary is
reinforced -- that is, it learns an optimal destabilization policy. We
formulate the policy learning as a zero-sum, minimax objective function.
Extensive experiments in multiple environments (InvertedPendulum, HalfCheetah,
Swimmer, Hopper and Walker2d) conclusively demonstrate that our method (a)
improves training stability; (b) is robust to differences in training/test
conditions; and c) outperform the baseline even in the absence of the
adversary.



Traffic on freeways can be managed by means of ramp meters from Road Traffic
Control rooms. Human operators cannot efficiently manage a network of ramp
meters. To support them, we present an intelligent platform for traffic
management which includes a new ramp metering coordination scheme in the
decision making module, an efficient dashboard for interacting with human
operators, machine learning tools for learning event definitions and Complex
Event Processing tools able to deal with uncertainties inherent to the traffic
use case. Unlike the usual approach, the devised event-driven platform is able
to predict a congestion up to 4 minutes before it really happens. Proactive
decision making can then be established leading to significant improvement of
traffic conditions.



Being able to fall safely is a necessary motor skill for humanoids performing
highly dynamic tasks, such as running and jumping. We propose a new method to
learn a policy that minimizes the maximal impulse during the fall. The
optimization solves for both a discrete contact planning problem and a
continuous optimal control problem. Once trained, the policy can compute the
optimal next contacting body part (e.g. left foot, right foot, or hands),
contact location and timing, and the required joint actuation. We represent the
policy as a mixture of actor-critic neural network, which consists of n control
policies and the corresponding value functions. Each pair of actor-critic is
associated with one of the n possible contacting body parts. During execution,
the policy corresponding to the highest value function will be executed while
the associated body part will be the next contact with the ground. With this
mixture of actor-critic architecture, the discrete contact sequence planning is
solved through the selection of the best critics while the continuous control
problem is solved by the optimization of actors. We show that our policy can
achieve comparable, sometimes even higher, rewards than a recursive search of
the action space using dynamic programming, while enjoying 50 to 400 times of
speed gain during online execution.



Despite progress in visual perception tasks such as image classification and
detection, computers still struggle to understand the interdependency of
objects in the scene as a whole, e.g., relations between objects or their
attributes. Existing methods often ignore global context cues capturing the
interactions among different object instances, and can only recognize a handful
of types by exhaustively training individual detectors for all possible
relationships. To capture such global interdependency, we propose a deep
Variation-structured Reinforcement Learning (VRL) framework to sequentially
discover object relationships and attributes in the whole image. First, a
directed semantic action graph is built using language priors to provide a rich
and compact representation of semantic correlations between object categories,
predicates, and attributes. Next, we use a variation-structured traversal over
the action graph to construct a small, adaptive action set for each step based
on the current state and historical actions. In particular, an ambiguity-aware
object mining scheme is used to resolve semantic ambiguity among object
categories that the object detector fails to distinguish. We then make
sequential predictions using a deep RL framework, incorporating global context
cues and semantic embeddings of previously extracted phrases in the state
vector. Our experiments on the Visual Relationship Detection (VRD) dataset and
the large-scale Visual Genome dataset validate the superiority of VRL, which
can achieve significantly better detection results on datasets involving
thousands of relationship and attribute types. We also demonstrate that VRL is
able to predict unseen types embedded in our action graph by learning
correlations on shared graph nodes.



This paper develops a general framework for learning interpretable data
representation via Long Short-Term Memory (LSTM) recurrent neural networks over
hierarchal graph structures. Instead of learning LSTM models over the pre-fixed
structures, we propose to further learn the intermediate interpretable
multi-level graph structures in a progressive and stochastic way from data
during the LSTM network optimization. We thus call this model the
structure-evolving LSTM. In particular, starting with an initial element-level
graph representation where each node is a small data element, the
structure-evolving LSTM gradually evolves the multi-level graph representations
by stochastically merging the graph nodes with high compatibilities along the
stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two
connected nodes from their corresponding LSTM gate outputs, which is used to
generate a merging probability. The candidate graph structures are accordingly
generated where the nodes are grouped into cliques with their merging
probabilities. We then produce the new graph structure with a
Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in
local optimums by stochastic sampling with an acceptance probability. Once a
graph structure is accepted, a higher-level graph is then constructed by taking
the partitioned cliques as its nodes. During the evolving process,
representation becomes more abstracted in higher-levels where redundant
information is filtered out, allowing more efficient propagation of long-range
data dependencies. We evaluate the effectiveness of structure-evolving LSTM in
the application of semantic object parsing and demonstrate its advantage over
state-of-the-art LSTM models on standard benchmarks.



The most recent financial upheavals have cast doubt on the adequacy of some
of the conventional quantitative risk management strategies, such as VaR (Value
at Risk), in many common situations. Consequently, there has been an increasing
need for verisimilar financial stress testings, namely simulating and analyzing
financial portfolios in extreme, albeit rare scenarios. Unlike conventional
risk management which exploits statistical correlations among financial
instruments, here we focus our analysis on the notion of probabilistic
causation, which is embodied by Suppes-Bayes Causal Networks (SBCNs), SBCNs are
probabilistic graphical models that have many attractive features in terms of
more accurate causal analysis for generating financial stress scenarios. In
this paper, we present a novel approach for conducting stress testing of
financial portfolios based on SBCNs in combination with classical machine
learning classification tools. The resulting method is shown to be capable of
correctly discovering the causal relationships among financial factors that
affect the portfolios and thus, simulating stress testing scenarios with a
higher accuracy and lower computational complexity than conventional Monte
Carlo Simulations.



This paper proposes a new model for extracting an interpretable sentence
embedding by introducing self-attention. Instead of using a vector, we use a
2-D matrix to represent the embedding, with each row of the matrix attending on
a different part of the sentence. We also propose a self-attention mechanism
and a special regularization term for the model. As a side effect, the
embedding comes with an easy way of visualizing what specific parts of the
sentence are encoded into the embedding. We evaluate our model on 3 different
tasks: author profiling, sentiment classification, and textual entailment.
Results show that our model yields a significant performance gain compared to
other sentence embedding methods in all of the 3 tasks.



This study proposes behavior-based navigation architecture, named BBFM, to
deal with the problem of navigating the mobile robot in unknown environments in
the presence of obstacles and local minimum regions. In the architecture, the
complex navigation task is split into principal sub-tasks or behaviors. Each
behavior is implemented by a fuzzy controller and executed independently to
deal with a specific problem of navigation. The fuzzy controller is modified to
contain only the fuzzification and inference procedures so that its output is a
membership function representing the behavior's objective. The membership
functions of all controllers are then used as the objective functions for a
multi-objective optimization process to coordinate all behaviors. The result of
this process is an overall control signal, which is Pareto-optimal, used to
control the robot. A number of simulations, comparisons, and experiments were
conducted. The results show that the proposed architecture outperforms some
popular behavior-based architectures in term of accuracy, smoothness, traveled
distance, and time response.



We present a formal measure of argument strength, which combines the ideas
that conclusions of strong arguments are (i) highly probable and (ii) their
uncertainty is relatively precise. Likewise, arguments are weak when their
conclusion probability is low or when it is highly imprecise. We show how the
proposed measure provides a new model of the Ellsberg paradox. Moreover, we
further substantiate the psychological plausibility of our approach by an
experiment (N = 60). The data show that the proposed measure predicts human
inferences in the original Ellsberg task and in corresponding argument strength
tasks. Finally, we report qualitative data taken from structured interviews on
folk psychological conceptions on what argument strength means.



In this paper we study selected argument forms involving counterfactuals and
indicative conditionals under uncertainty. We selected argument forms to
explore whether people with an Eastern cultural background reason differently
about conditionals compared to Westerners, because of the differences in the
location of negations. In a 2x2 between-participants design, 63 Japanese
university students were allocated to four groups, crossing indicative
conditionals and counterfactuals, and each presented in two random task orders.
The data show close agreement between the responses of Easterners and
Westerners. The modal responses provide strong support for the hypothesis that
conditional probability is the best predictor for counterfactuals and
indicative conditionals. Finally, the grand majority of the responses are
probabilistically coherent, which endorses the psychological plausibility of
choosing coherence-based probability logic as a rationality framework for
psychological reasoning research.



We present a method for skin lesion segmentation for the ISIC 2017 Skin
Lesion Segmentation Challenge. Our approach is based on a Fully Convolutional
Network architecture which is trained end to end, from scratch, on a limited
dataset. Our semantic segmentation architecture utilizes several recent
innovations in particularly in the combined use of (i) use of atrous
convolutions to increase the effective field of view of the network's receptive
field without increasing the number of parameters, (ii) the use of
network-in-network $1\times1$ convolution layers to add capacity to the network
and (iii) state-of-art super-resolution upsampling of predictions using
subpixel CNN layers. We reported a mean IOU score of 0.642 on the validation
set provided by the organisers.



We propose an algorithm for meta-learning that is model-agnostic, in the
sense that it is compatible with any model trained with gradient descent and
applicable to a variety of different learning problems, including
classification, regression, and reinforcement learning. The goal of
meta-learning is to train a model on a variety of learning tasks, such that it
can solve new learning tasks using only a small number of training samples. In
our approach, the parameters of the model are explicitly trained such that a
small number of gradient steps with a small amount of training data from a new
task will produce good generalization performance on that task. In effect, our
method trains the model to be easy to fine-tune. We demonstrate that this
approach leads to state-of-the-art performance on two few-shot image
classification benchmarks, produces good results on few-shot regression, and
accelerates fine-tuning for policy gradient reinforcement learning with neural
network policies.



Our overall program objective is to provide more natural ways for soldiers to
interact and communicate with robots, much like how soldiers communicate with
other soldiers today. We describe how the Wizard-of-Oz (WOz) method can be
applied to multimodal human-robot dialogue in a collaborative exploration task.
While the WOz method can help design robot behaviors, traditional approaches
place the burden of decisions on a single wizard. In this work, we consider two
wizards to stand in for robot navigation and dialogue management software
components. The scenario used to elicit data is one in which a human-robot team
is tasked with exploring an unknown environment: a human gives verbal
instructions from a remote location and the robot follows them, clarifying
possible misunderstandings as needed via dialogue. We found the division of
labor between wizards to be workable, which holds promise for future software
development.



Neural networks are among the most accurate supervised learning methods in
use today, but their opacity makes them difficult to trust in critical
applications, especially when conditions in training differ from those in test.
Recent work on explanations for black-box models has produced tools (e.g. LIME)
to show the implicit rules behind predictions, which can help us identify when
models are right for the wrong reasons. However, these methods do not scale to
explaining entire datasets and cannot correct the problems they reveal. We
introduce a method for efficiently explaining and regularizing differentiable
models by examining and selectively penalizing their input gradients, which
provide a normal to the decision boundary. We apply these penalties both based
on expert annotation and in an unsupervised fashion that encourages diverse
models with qualitatively different decision boundaries for the same
classification problem. On multiple datasets, we show our approach generates
faithful explanations and models that generalize much better when conditions
differ between training and test.



Brain-inspired learning models attempt to mimic the cortical architecture and
computations performed in the neurons and synapses constituting the human brain
to achieve its efficiency in cognitive tasks. In this work, we present
convolutional spike timing dependent plasticity based feature learning with
biologically plausible leaky-integrate-and-fire neurons in Spiking Neural
Networks (SNNs). We use shared weight kernels that are trained to encode
representative features underlying the input patterns thereby improving the
sparsity as well as the robustness of the learning model. We demonstrate that
the proposed unsupervised learning methodology learns several visual categories
for object recognition with fewer number of examples and outperforms
traditional fully-connected SNN architectures while yielding competitive
accuracy. Additionally, we observe that the learning model performs out-of-set
generalization further making the proposed biologically plausible framework a
viable and efficient architecture for future neuromorphic applications.



We explore the use of Evolution Strategies (ES), a class of black box
optimization algorithms, as an alternative to popular MDP-based RL techniques
such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show
that ES is a viable solution strategy that scales extremely well with the
number of CPUs available: By using a novel communication strategy based on
common random numbers, our ES implementation only needs to communicate scalars,
making it possible to scale to over a thousand parallel workers. This allows us
to solve 3D humanoid walking in 10 minutes and obtain competitive results on
most Atari games after one hour of training. In addition, we highlight several
advantages of ES as a black box optimization technique: it is invariant to
action frequency and delayed rewards, tolerant of extremely long horizons, and
does not need temporal discounting or value function approximation.



This paper proposes an innovative method for segmentation of skin lesions in
dermoscopy images developed by the authors, based on fuzzy classification of
pixels and histogram thresholding.



In Stackelberg security games, a defender seeks to randomly allocate limited
security resources to protect critical targets from an attack. In this paper,
we study a fundamental, yet underexplored, phenomenon in security games, which
we term the \emph{Curse of Correlation} (CoC). Specifically, we observe that
there are inevitable correlations among the protection status of different
targets. Such correlation is a crucial concern, especially in
\emph{spatio-temporal} domains like conservation area patrolling, where
attackers can surveil patrollers at certain areas and then infer their
patrolling routes using such correlations. To mitigate this issue, we propose
to design entropy-maximizing defending strategies for spatio-temporal security
games, which frequently suffer from CoC. We prove that the problem is \#P-hard
in general. However, it admits efficient algorithms in well-motivated special
settings. Our experiments show significant advantages of max-entropy algorithms
over previous algorithms. A scalable implementation of our algorithm is
currently under pre-deployment testing for integration into FAMS software to
improve the scheduling of US federal air marshals.



Machine learning applications are increasingly deployed not only to serve
predictions using static models, but also as tightly-integrated components of
feedback loops involving dynamic, real-time decision making. These applications
pose a new set of requirements, none of which are difficult to achieve in
isolation, but the combination of which creates a challenge for existing
distributed execution frameworks: computation with millisecond latency at high
throughput, adaptive construction of arbitrary task graphs, and execution of
heterogeneous kernels over diverse sets of resources. We assert that a new
distributed execution framework is needed for such ML applications and propose
a candidate approach with a proof-of-concept architecture that achieves a 63x
performance improvement over a state-of-the-art execution framework for a
representative application.



We introduce a method for learning the dynamics of complex nonlinear systems
based on deep generative models over temporal segments of states and actions.
Unlike dynamics models that operate over individual discrete timesteps, we
learn the distribution over future state trajectories conditioned on past
state, past action, and planned future action trajectories, as well as a latent
prior over action trajectories. Our approach is based on convolutional
autoregressive models and variational autoencoders. It makes stable and
accurate predictions over long horizons for complex, stochastic systems,
effectively expressing uncertainty and modeling the effects of collisions,
sensory noise, and action delays. The learned dynamics model and action prior
can be used for end-to-end, fully differentiable trajectory optimization and
model-based policy optimization, which we use to evaluate the performance and
sample-efficiency of our method.



Recently, DNN model compression based on network architecture design, e.g.,
SqueezeNet, attracted a lot attention. No accuracy drop on image classification
is observed on these extremely compact networks, compared to well-known models.
An emerging question, however, is whether these model compression techniques
hurt DNN's learning ability other than classifying images on a single dataset.
Our preliminary experiment shows that these compression methods could degrade
domain adaptation (DA) ability, though the classification performance is
preserved. Therefore, we propose a new compact network architecture and
unsupervised DA method in this paper. The DNN is built on a new basic module
Conv-M which provides more diverse feature extractors without significantly
increasing parameters. The unified framework of our DA method will
simultaneously learn invariance across domains, reduce divergence of feature
representations, and adapt label prediction. Our DNN has 4.1M parameters, which
is only 6.7% of AlexNet or 59% of GoogLeNet. Experiments show that our DNN
obtains GoogLeNet-level accuracy both on classification and DA, and our DA
method slightly outperforms previous competitive ones. Put all together, our DA
strategy based on our DNN achieves state-of-the-art on sixteen of total
eighteen DA tasks on popular Office-31 and Office-Caltech datasets.



We approach structured output prediction by optimizing a deep value network
(DVN) to precisely estimate the task loss on different output configurations
for a given input. Once the model is trained, we perform inference by gradient
descent on the continuous relaxations of the output variables to find outputs
with promising scores from the value network. When applied to image
segmentation, the value network takes an image and a segmentation mask as
inputs and predicts a scalar estimating the intersection over union between the
input and ground truth masks. For multi-label classification, the DVN's
objective is to correctly predict the F1 score for any potential label
configuration. The DVN framework achieves the state-of-the-art results on
multi-label prediction and image segmentation benchmarks.



Bayesian optimization has been successful at global optimization of
expensive-to-evaluate multimodal objective functions. However, unlike most
optimization methods, Bayesian optimization typically does not use derivative
information. In this paper we show how Bayesian optimization can exploit
derivative information to decrease the number of objective function evaluations
required for good performance. In particular, we develop a novel Bayesian
optimization algorithm, the derivative-enabled knowledge-gradient (dKG), for
which we show one-step Bayes-optimality, asymptotic consistency, and greater
one-step value of information than is possible in the derivative-free setting.
Our procedure accommodates noisy and incomplete derivative information, comes
in both sequential and batch forms, and can optionally reduce the computational
cost of inference through automatically selected retention of a single
directional derivative. We also compute the d-KG acquisition function and its
gradient using a novel fast discretization-free technique. We show d-KG
provides state-of-the-art performance compared to a wide range of optimization
procedures with and without gradients, on benchmarks including logistic
regression, deep learning, kernel learning, and k-nearest neighbors.



The Entity Disambiguation and Linking (EDL) task matches entity mentions in
text to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase
id. It plays a critical role in the construction of a high quality information
network, and can be further leveraged for a variety of information retrieval
and NLP tasks such as text categorization and document tagging. EDL is a
complex and challenging problem due to ambiguity of the mentions and real world
text being multi-lingual. Moreover, EDL systems need to have high throughput
and should be lightweight in order to scale to large datasets and run on
off-the-shelf machines. More importantly, these systems need to be able to
extract and disambiguate dense annotations from the data in order to enable an
Information Retrieval or Extraction task running on the data to be more
efficient and accurate. In order to address all these challenges, we present
the Lithium EDL system and algorithm - a high-throughput, lightweight,
language-agnostic EDL system that extracts and correctly disambiguates 75% more
entities than state-of-the-art EDL systems and is significantly faster than
them.



How can we explain the predictions of a black-box model? In this paper, we
use influence functions -- a classic technique from robust statistics -- to
trace a model's prediction through the learning algorithm and back to its
training data, thereby identifying training points most responsible for a given
prediction. To scale up influence functions to modern machine learning
settings, we develop a simple, efficient implementation that requires only
oracle access to gradients and Hessian-vector products. We show that even on
non-convex and non-differentiable models where the theory breaks down,
approximations to influence functions can still provide valuable information.
On linear models and convolutional neural networks, we demonstrate that
influence functions are useful for multiple purposes: understanding model
behavior, debugging models, detecting dataset errors, and even creating
visually-indistinguishable training-set attacks.



Voting systems typically treat all voters equally. We argue that perhaps they
should not: Voters who have supported good choices in the past should be given
higher weight than voters who have supported bad ones. To develop a formal
framework for desirable weighting schemes, we draw on no-regret learning.
Specifically, given a voting rule, we wish to design a weighting scheme such
that applying the voting rule, with voters weighted by the scheme, leads to
choices that are almost as good as those endorsed by the best voter in
hindsight. We derive possibility and impossibility results for the existence of
such weighting schemes, depending on whether the voting rule and the weighting
scheme are deterministic or randomized, as well as on the social choice axioms
satisfied by the voting rule.



Recent development of large-scale question answering (QA) datasets triggered
a substantial amount of research into end-to-end neural architectures for QA.
Increasingly complex systems have been conceived without comparison to simpler
neural baseline systems that would justify their complexity. In this work, we
propose a simple heuristic that guides the development of neural baseline
systems for the extractive QA task. We find that there are two ingredients
necessary for building a high-performing neural QA system: first, the awareness
of question words while processing the context and second, a composition
function that goes beyond simple bag-of-words modeling, such as recurrent
neural networks. Our results show that FastQA, a system that meets these two
requirements, can achieve very competitive performance compared with existing
models. We argue that this surprising finding puts results of previous systems
and the complexity of recent QA datasets into perspective.



Programming by Example (PBE) targets at automatically inferring a computer
program for accomplishing a certain task from sample input and output. In this
paper, we propose a deep neural networks (DNN) based PBE model called Neural
Programming by Example (NPBE), which can learn from input-output strings and
induce programs that solve the string manipulation problems. Our NPBE model has
four neural network based components: a string encoder, an input-output
analyzer, a program generator, and a symbol selector. We demonstrate the
effectiveness of NPBE by training it end-to-end to solve some common string
manipulation problems in spreadsheet systems. The results show that our model
can induce string manipulation programs effectively. Our work is one step
towards teaching DNN to generate computer programs.



Keyword spotting (KWS) constitutes a major component of human-technology
interfaces. Maximizing the detection accuracy at a low false alarm (FA) rate,
while minimizing the footprint size, latency and complexity are the goals for
KWS. Towards achieving them, we study Convolutional Recurrent Neural Networks
(CRNNs). Inspired by large-scale state-of-the-art speech recognition systems,
we combine the strengths of convolutional layers and recurrent layers to
exploit local structure and long-range context. We analyze the effect of
architecture parameters, and propose training strategies to improve
performance. With only ~230k parameters, our CRNN model yields acceptably low
latency, and achieves 97.71% accuracy at 0.5 FA/hour for 5 dB signal-to-noise
ratio.



Human parsing has recently attracted a lot of research interests due to its
huge application potentials. However existing datasets have limited number of
images and annotations, and lack the variety of human appearances and the
coverage of challenging cases in unconstrained environment. In this paper, we
introduce a new benchmark "Look into Person (LIP)" that makes a significant
advance in terms of scalability, diversity and difficulty, a contribution that
we feel is crucial for future developments in human-centric analysis. This
comprehensive dataset contains over 50,000 elaborately annotated images with 19
semantic part labels, which are captured from a wider range of viewpoints,
occlusions and background complexity. Given these rich annotations we perform
detailed analyses of the leading human parsing approaches, gaining insights
into the success and failures of these methods. Furthermore, in contrast to the
existing efforts on improving the feature discriminative capability, we solve
human parsing by exploring a novel self-supervised structure-sensitive learning
approach, which imposes human pose structures into parsing results without
resorting to extra supervision (i.e., no need for specifically labeling human
joints in model training). Our self-supervised learning framework can be
injected into any advanced neural networks to help incorporate rich high-level
knowledge regarding human joints from a global perspective and improve the
parsing results. Extensive evaluations on our LIP and the public
PASCAL-Person-Part dataset demonstrate the superiority of our method.



We consider the problem of provably optimal exploration in reinforcement
learning for finite horizon MDPs. We show that an optimistic modification to
value iteration achieves a regret bound of $\tilde{O}( \sqrt{HSAT} +
H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states,
$A$ the number of actions and $T$ the number of time-steps. This result
improves over the best previous known bound $\tilde{O}(HS \sqrt{AT})$ achieved
by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new
results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of
$\tilde{O}(\sqrt{HSAT})$ that matches the established lower bound of
$\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contains two key
insights. We use careful application of concentration inequalities to the
optimal value function as a whole, rather than to the transitions probabilities
(to improve scaling in $S$), and we define Bernstein-based "exploration
bonuses" that use the empirical variance of the estimated values at the next
states (to improve scaling in $H$).



We consider the optimal value of information (VoI) problem, where the goal is
to sequentially select a set of tests with a minimal cost, so that one can
efficiently make the best decision based on the observed outcomes. Existing
algorithms are either heuristics with no guarantees, or scale poorly (with
exponential run time in terms of the number of available tests). Moreover,
these methods assume a known distribution over the test outcomes, which is
often not the case in practice. We propose an efficient sampling-based online
learning framework to address the above issues. First, assuming the
distribution over hypotheses is known, we propose a dynamic hypothesis
enumeration strategy, which allows efficient information gathering with strong
theoretical guarantees. We show that with sufficient amount of samples, one can
identify a near-optimal decision with high probability. Second, when the
parameters of the hypotheses distribution are unknown, we propose an algorithm
which learns the parameters progressively via posterior sampling in an online
fashion. We further establish a rigorous bound on the expected regret. We
demonstrate the effectiveness of our approach on a real-world interactive
troubleshooting application and show that one can efficiently make high-quality
decisions with low cost.



Knowledge graphs enable a wide variety of applications, including question
answering and information retrieval. Despite the great effort invested in their
creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata)
remain incomplete. We introduce Relational Graph Convolutional Networks
(R-GCNs) and apply them to two standard knowledge base completion tasks: Link
prediction (recovery of missing facts, i.e. subject-predicate-object triples)
and entity classification (recovery of missing entity attributes). R-GCNs are
related to a recent class of neural networks operating on graphs, and are
developed specifically to deal with the highly multi-relational data
characteristic of realistic knowledge bases. We demonstrate the effectiveness
of R-GCNs as a stand-alone model for entity classification. We further show
that factorization models for link prediction such as DistMult can be
significantly improved by enriching them with an encoder model to accumulate
evidence over multiple inference steps in the relational graph, demonstrating a
large improvement of 29.8% on FB15k-237 over a decoder-only baseline.



Many real-world tasks involve multiple agents with partial observability and
limited communication. Learning is challenging in these settings due to local
viewpoints of agents, which perceive the world as non-stationary due to
concurrently-exploring teammates. Approaches that learn specialized policies
for individual tasks face problems when applied to the real world: not only do
agents have to learn and store distinct policies for each task, but in practice
identities of tasks are often non-observable, making these approaches
inapplicable. This paper formalizes and addresses the problem of multi-task
multi-agent reinforcement learning under partial observability. We introduce a
decentralized single-task learning approach that is robust to concurrent
interactions of teammates, and present an approach for distilling single-task
policies into a unified policy that performs well across multiple related
tasks, without explicit provision of task identity.



This work presents CLTune, an auto-tuner for OpenCL kernels. It evaluates and
tunes kernel performance of a generic, user-defined search space of possible
parameter-value combinations. Example parameters include the OpenCL workgroup
size, vector data-types, tile sizes, and loop unrolling factors. CLTune can be
used in the following scenarios: 1) when there are too many tunable parameters
to explore manually, 2) when performance portability across OpenCL devices is
desired, or 3) when the optimal parameters change based on input argument
values (e.g. matrix dimensions). The auto-tuner is generic, easy to use,
open-source, and supports multiple search strategies including simulated
annealing and particle swarm optimisation. CLTune is evaluated on two GPU
case-studies inspired by the recent successes in deep learning: 2D convolution
and matrix-multiplication (GEMM). For 2D convolution, we demonstrate the need
for auto-tuning by optimizing for different filter sizes, achieving performance
on-par or better than the state-of-the-art. For matrix-multiplication, we use
CLTune to explore a parameter space of more than two-hundred thousand
configurations, we show the need for device-specific tuning, and outperform the
clBLAS library on NVIDIA, AMD and Intel GPUs.



We introduce the first goal-driven training for visual question answering and
dialog agents. Specifically, we pose a cooperative 'image guessing' game
between two agents -- Qbot and Abot -- who communicate in natural language
dialog so that Qbot can select an unseen image from a lineup of images. We use
deep reinforcement learning (RL) to learn the policies of these agents
end-to-end -- from pixels to multi-agent multi-round dialog to game reward.
  We demonstrate two experimental results.
  First, as a 'sanity check' demonstration of pure RL (from scratch), we show
results on a synthetic world, where the agents communicate in ungrounded
vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find
that two bots invent their own communication protocol and start using certain
symbols to ask/answer about certain visual attributes (shape/color/style).
Thus, we demonstrate the emergence of grounded language and communication among
'visual' dialog agents with no human supervision.
  Second, we conduct large-scale real-image experiments on the VisDial dataset,
where we pretrain with supervised dialog data and show that the RL 'fine-tuned'
agents significantly outperform SL agents. Interestingly, the RL Qbot learns to
ask questions that Abot is good at, ultimately resulting in more informative
dialog and a better team.



We elaborate a quantum model for the meaning associated with corpora of
written documents, like the pages forming the World Wide Web. To that end, we
are guided by how physicists constructed quantum theory for microscopic
entities, which unlike classical objects cannot be fully represented in our
spatial theater. We suggest that a similar construction needs to be carried out
by linguists and computational scientists, to capture the full meaning carried
by collections of documental entities. More precisely, we show how to associate
a quantum-like 'entity of meaning' to a 'language entity formed by printed
documents', considering the latter as the collection of traces that are left by
the former, in specific results of search actions that we describe as
measurements. In other words, we offer a perspective where a collection of
documents, like the Web, is described as the space of manifestation of a more
complex entity - the QWeb - which is the object of our modeling, drawing its
inspiration from previous studies on operational-realistic approaches to
quantum physics and quantum modeling of human cognition and decision-making. We
emphasize that a consistent QWeb model needs to account for the observed
correlations between words appearing in printed documents, e.g.,
co-occurrences, as the latter would depend on the 'meaning connections'
existing between the concepts that are associated with these words. In that
respect, we show that both 'context and interference (quantum) effects' are
required to explain the probabilities calculated by counting the relative
number of documents containing certain words and co-ocurrrences of words.



This paper introduces the QMDP-net, a neural network architecture for
planning under partial observability. The QMDP-net combines the strengths of
model-free learning and model-based planning. It is a recurrent policy network,
but it represents a policy for a parameterized set of tasks by connecting a
model with a planning algorithm that solves the model, thus embedding the
solution structure of planning in a network learning architecture. The QMDP-net
is fully differentiable and allows for end-to-end training. We train a QMDP-net
on different tasks so that it can generalize to new ones in the parameterized
task set and "transfer" to other similar tasks beyond the set. In preliminary
experiments, QMDP-net showed strong performance on several robotic tasks in
simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it
sometimes outperforms the QMDP algorithm in the experiments, as a result of
end-to-end learning.



This paper addresses the problem of handling spatial misalignments due to
camera-view changes or human-pose variations in person re-identification. We
first introduce a boosting-based approach to learn a correspondence structure
which indicates the patch-wise matching probabilities between images from a
target camera pair. The learned correspondence structure can not only capture
the spatial correspondence pattern between cameras but also handle the
viewpoint or human-pose variation in individual images. We further introduce a
global constraint-based matching process. It integrates a global matching
constraint over the learned correspondence structure to exclude cross-view
misalignments during the image patch matching process, hence achieving a more
reliable matching score between images. Finally, we also extend our approach by
introducing a multi-structure scheme, which learns a set of local
correspondence structures to capture the spatial correspondence sub-patterns
between a camera pair, so as to handle the spatial misalignments between
individual images in a more precise way. Experimental results on various
datasets demonstrate the effectiveness of our approach.



A natural image usually conveys rich semantic content and can be viewed from
different angles. Existing image description methods are largely restricted by
small sets of biased visual paragraph annotations, and fail to cover rich
underlying semantics. In this paper, we investigate a semi-supervised paragraph
generative framework that is able to synthesize diverse and semantically
coherent paragraph descriptions by reasoning over local semantic regions and
exploiting linguistic knowledge. The proposed Recurrent Topic-Transition
Generative Adversarial Network (RTT-GAN) builds an adversarial framework
between a structured paragraph generator and multi-level paragraph
discriminators. The paragraph generator generates sentences recurrently by
incorporating region-based visual and language attention mechanisms at each
step. The quality of generated paragraph sentences is assessed by multi-level
adversarial discriminators from two aspects, namely, plausibility at sentence
level and topic-transition coherence at paragraph level. The joint adversarial
training of RTT-GAN drives the model to generate realistic paragraphs with
smooth logical transition between sentence topics. Extensive quantitative
experiments on image and video paragraph datasets demonstrate the effectiveness
of our RTT-GAN in both supervised and semi-supervised settings. Qualitative
results on telling diverse stories for an image also verify the
interpretability of RTT-GAN.



Language understanding is a key component in a spoken dialogue system. In
this paper, we investigate how the language understanding module influences the
dialogue system performance by conducting a series of systematic experiments on
a task-oriented neural dialogue system in a reinforcement learning based
setting. The empirical study shows that among different types of language
understanding errors, slot-level errors can have more impact on the overall
performance of a dialogue system compared to intent-level errors. In addition,
our experiments demonstrate that the reinforcement learning based dialogue
system is able to learn when and what to confirm in order to achieve better
performance and greater robustness.



Imitation learning has been commonly applied to solve different tasks in
isolation. This usually requires either careful feature engineering, or a
significant number of samples. This is far from what we desire: ideally, robots
should be able to learn from very few demonstrations of any given task, and
instantly generalize to new situations of the same task, without requiring
task-specific engineering. In this paper, we propose a meta-learning framework
for achieving such capability, which we call one-shot imitation learning.
  Specifically, we consider the setting where there is a very large set of
tasks, and each task has many instantiations. For example, a task could be to
stack all blocks on a table into a single tower, another task could be to place
all blocks on a table into two-block towers, etc. In each case, different
instances of the task would consist of different sets of blocks with different
initial states. At training time, our algorithm is presented with pairs of
demonstrations for a subset of all tasks. A neural net is trained that takes as
input one demonstration and the current state (which initially is the initial
state of the other demonstration of the pair), and outputs an action with the
goal that the resulting sequence of states and actions matches as closely as
possible with the second demonstration. At test time, a demonstration of a
single instance of a new task is presented, and the neural net is expected to
perform well on new instances of this new task. The use of soft attention
allows the model to generalize to conditions and tasks unseen in the training
data. We anticipate that by training this model on a much greater variety of
tasks and settings, we will obtain a general system that can turn any
demonstrations into robust policies that can accomplish an overwhelming variety
of tasks.
  Videos available at https://bit.ly/nips2017-oneshot .



In all but the most trivial optimization problems, the structure of the
solutions exhibit complex interdependencies between the input parameters.
Decades of research with stochastic search techniques has shown the benefit of
explicitly modeling the interactions between sets of parameters and the overall
quality of the solutions discovered. We demonstrate a novel method, based on
learning deep networks, to model the global landscapes of optimization
problems. To represent the search space concisely and accurately, the deep
networks must encode information about the underlying parameter interactions
and their contributions to the quality of the solution. Once the networks are
trained, the networks are probed to reveal parameter combinations with high
expected performance with respect to the optimization task. These estimates are
used to initialize fast, randomized, local search algorithms, which in turn
expose more information about the search space that is subsequently used to
refine the models. We demonstrate the technique on multiple optimization
problems that have arisen in a variety of real-world domains, including:
packing, graphics, job scheduling, layout and compression. The problems include
combinatoric search spaces, discontinuous and highly non-linear spaces, and
span binary, higher-cardinality discrete, as well as continuous parameters.
Strengths, limitations, and extensions of the approach are extensively
discussed and demonstrated.



We study the use of randomized value functions to guide deep exploration in
reinforcement learning. This offers an elegant means for synthesizing
statistically and computationally efficient exploration with common practical
approaches to value function learning. We present several reinforcement
learning algorithms that leverage randomized value functions and demonstrate
their efficacy through computational studies. We also prove a regret bound that
establishes statistical efficiency with a tabular representation.



Statistical performance bounds for reinforcement learning (RL) algorithms can
be critical for high-stakes applications like healthcare. This paper introduces
a new framework for theoretically measuring the performance of such algorithms
called Uniform-PAC, which is a strengthening of the classical Probably
Approximately Correct (PAC) framework. In contrast to the PAC framework, the
uniform version may be used to derive high probability regret guarantees and so
forms a bridge between the two setups that has been missing in the literature.
We demonstrate the benefits of the new framework for finite-state episodic MDPs
with a new algorithm that is Uniform-PAC and simultaneously achieves optimal
regret and PAC guarantees except for a factor of the horizon.



In economics and psychology, delay discounting is often used to characterize
how individuals choose between a smaller immediate reward and a larger delayed
reward. People with higher delay discounting rate (DDR) often choose smaller
but more immediate rewards (a "today person"). In contrast, people with a lower
discounting rate often choose a larger future rewards (a "tomorrow person").
Since the ability to modulate the desire of immediate gratification for long
term rewards plays an important role in our decision-making, the lower
discounting rate often predicts better social, academic and health outcomes. In
contrast, the higher discounting rate is often associated with problematic
behaviors such as alcohol/drug abuse, pathological gambling and credit card
default. Thus, research on understanding and moderating delay discounting has
the potential to produce substantial societal benefits.



We provide new results for noise-tolerant and sample-efficient learning
algorithms under $s$-concave distributions. The new class of $s$-concave
distributions is a broad and natural generalization of log-concavity, and
includes many important additional distributions, e.g., the Pareto distribution
and $t$-distribution. This class has been studied in the context of efficient
sampling, integration, and optimization, but much remains unknown about the
geometry of this class of distributions and their applications in the context
of learning. The challenge is that unlike the commonly used distributions in
learning (uniform or more generally log-concave distributions), this broader
class is not closed under the marginalization operator and many such
distributions are fat-tailed. In this work, we introduce new convex geometry
tools to study the properties of $s$-concave distributions and use these
properties to provide bounds on quantities of interest to learning including
the probability of disagreement between two halfspaces, disagreement outside a
band, and the disagreement coefficient. We use these results to significantly
generalize prior results for margin-based active learning, disagreement-based
active learning, and passive learning of intersections of halfspaces. Our
analysis of geometric properties of $s$-concave distributions might be of
independent interest to optimization more broadly.



We consider the problem of a robot learning the mechanical properties of
objects through physical interaction with the object, and introduce a
practical, data-efficient approach for identifying the motion models of these
objects. The proposed method utilizes a physics engine, where the robot seeks
to identify the inertial and friction parameters of the object by simulating
its motion under different values of the parameters and identifying those that
result in a simulation which matches the observed real motions. The problem is
solved in a Bayesian optimization framework. The same framework is used for
both identifying the model of an object online and searching for a policy that
would minimize a given cost function according to the identified model.
Experimental results both in simulation and using a real robot indicate that
the proposed method outperforms state-of-the-art model-free reinforcement
learning approaches.



Convolutional Neural Networks have been a subject of great importance over
the past decade and great strides have been made in their utility for producing
state of the art performance in many computer vision problems. However, the
behavior of deep networks is yet to be fully understood and is still an active
area of research. In this work, we present an intriguing behavior: pre-trained
CNNs can be made to improve their predictions by structurally perturbing the
input. We observe that these perturbations - referred as Guided Perturbations -
enable a trained network to improve its prediction performance without any
learning or change in network weights. We perform various ablative experiments
to understand how these perturbations affect the local context and feature
representations. Furthermore, we demonstrate that this idea can improve
performance of several existing approaches on semantic segmentation and scene
labeling tasks on the PASCAL VOC dataset and supervised classification tasks on
MNIST and CIFAR10 datasets.



When using reinforcement learning (RL) algorithms to evaluate a policy it is
common, given a large state space, to introduce some form of approximation
architecture for the value function (VF). The exact form of this architecture
can have a significant effect on the accuracy of the VF estimate, however, and
determining a suitable approximation architecture can often be a highly complex
task. Consequently there is a large amount of interest in the potential for
allowing RL algorithms to adaptively generate (i.e. to learn) approximation
architectures.
  We investigate a method of adapting approximation architectures which uses
feedback regarding the frequency with which an agent has visited certain states
to guide which areas of the state space to approximate with greater detail. We
introduce an algorithm based upon this idea which adapts a state aggregation
approximation architecture on-line.
  Assuming $S$ states, we demonstrate theoretically that - provided the
following relatively non-restrictive assumptions are satisfied: (a) the number
of cells $X$ in the state aggregation architecture is of order
$\sqrt{S}\ln{S}\log_2{S}$ or greater, (b) the policy and transition function
are close to deterministic, and (c) the prior for the transition function is
uniformly distributed - our algorithm can guarantee, assuming we use an
appropriate scoring function to measure VF error, error which is arbitrarily
close to zero as $S$ becomes large. It is able to do this despite having only
$O(X\log_2{S})$ space complexity (and negligible time complexity). We conclude
by generating a set of empirical results which support the theoretical results.



Recently, research on accelerated stochastic gradient descent methods (e.g.,
SVRG) has made exciting progress (e.g., linear convergence for strongly convex
problems). However, the best-known methods (e.g., Katyusha) requires at least
two auxiliary variables and two momentum parameters. In this paper, we propose
a fast stochastic variance reduction gradient (FSVRG) method, in which we
design a novel update rule with the Nesterov's momentum and incorporate the
technique of growing epoch size. FSVRG has only one auxiliary variable and one
momentum weight, and thus it is much simpler and has much lower per-iteration
complexity. We prove that FSVRG achieves linear convergence for strongly convex
problems and the optimal $\mathcal{O}(1/T^2)$ convergence rate for non-strongly
convex problems, where $T$ is the number of outer-iterations. We also extend
FSVRG to directly solve the problems with non-smooth component functions, such
as SVM. Finally, we empirically study the performance of FSVRG for solving
various machine learning problems such as logistic regression, ridge
regression, Lasso and SVM. Our results show that FSVRG outperforms the
state-of-the-art stochastic methods, including Katyusha.



Many efforts have been dedicated to identifying restrictions on ontologies
expressed as tuple-generating dependencies (tgds), a.k.a. existential rules,
that lead to the decidability for the problem of answering ontology-mediated
queries (OMQs). This has given rise to three families of formalisms: guarded,
non-recursive, and sticky sets of tgds. In this work, we study the containment
problem for OMQs expressed in such formalisms, which is a key ingredient for
solving static analysis tasks associated with them. Our main contribution is
the development of specially tailored techniques for OMQ containment under the
classes of tgds stated above. This enables us to obtain sharp complexity bounds
for the problems at hand, which in turn allow us to delimitate its practical
applicability. We also apply our techniques to pinpoint the complexity of
problems associated with two emerging applications of OMQ containment:
distribution over components and UCQ rewritability of OMQs.



This thesis is in the area called computational social choice which is an
intersection area of algorithms and social choice theory.



Knowledge bases (KBs) of real-world facts about entities and their
relationships are useful resources for a variety of natural language processing
tasks. However, because knowledge bases are typically incomplete, it is useful
to be able to perform knowledge base completion or link prediction, i.e.,
predict whether a relationship not in the knowledge base is likely to be true.
This article serves as a brief overview of embedding models of entities and
relationships for knowledge base completion, summarizing up-to-date
experimental results on standard benchmark datasets FB15k, WN18, FB15k-237,
WN18RR, FB13 and WN11.



As a general and thus popular model for autonomous systems, partially
observable Markov decision process (POMDP) can capture uncertainties from
different sources like sensing noises, actuation errors, and uncertain
environments. However, its comprehensiveness makes the planning and control in
POMDP difficult. Traditional POMDP planning problems target to find the optimal
policy to maximize the expectation of accumulated rewards. But for safety
critical applications, guarantees of system performance described by formal
specifications are desired, which motivates us to consider formal methods to
synthesize supervisor for POMDP. With system specifications given by
Probabilistic Computation Tree Logic (PCTL), we propose a supervisory control
framework with a type of deterministic finite automata (DFA), za-DFA, as the
controller form. While the existing work mainly relies on optimization
techniques to learn fixed-size finite state controllers (FSCs), we develop an
$L^*$ learning based algorithm to determine both space and transitions of
za-DFA. Membership queries and different oracles for conjectures are defined.
The learning algorithm is sound and complete. An example is given in detailed
steps to illustrate the supervisor synthesis algorithm.



A recurring problem faced when training neural networks is that there is
typically not enough data to maximize the generalization capability of deep
neural networks(DNN). There are many techniques to address this, including data
augmentation, dropout, and transfer learning. In this paper, we introduce an
additional method which we call Smart Augmentation and we show how to use it to
increase the accuracy and reduce overfitting on a target network. Smart
Augmentation works by creating a network that learns how to generate augmented
data during the training process of a target network in a way that reduces that
networks loss. This allows us to learn augmentations that minimize the error of
that network.
  Smart Augmentation has shown the potential to increase accuracy by
demonstrably significant measures on all datasets tested. In addition, it has
shown potential to achieve similar or improved performance levels with
significantly smaller network sizes in a number of tested cases.



Although information workers may complain about meetings, they are an
essential part of their work life. Consequently, busy people spend a
significant amount of time scheduling meetings. We present Calendar.help, a
system that provides fast, efficient scheduling through structured workflows.
Users interact with the system via email, delegating their scheduling needs to
the system as if it were a human personal assistant. Common scheduling
scenarios are broken down using well-defined workflows and completed as a
series of microtasks that are automated when possible and executed by a human
otherwise. Unusual scenarios fall back to a trained human assistant who
executes them as unstructured macrotasks. We describe the iterative approach we
used to develop Calendar.help, and share the lessons learned from scheduling
thousands of meetings during a year of real-world deployments. Our findings
provide insight into how complex information tasks can be broken down into
repeatable components that can be executed efficiently to improve productivity.



Objective: We investigate whether deep learning techniques for natural
language processing (NLP) can be used efficiently for patient phenotyping.
Patient phenotyping is a classification task for determining whether a patient
has a medical condition, and is a crucial part of secondary analysis of
healthcare data. We assess the performance of deep learning algorithms and
compare them with classical NLP approaches.
  Materials and Methods: We compare convolutional neural networks (CNNs),
n-gram models, and approaches based on cTAKES that extract pre-defined medical
concepts from clinical notes and use them to predict patient phenotypes. The
performance is tested on 10 different phenotyping tasks using 1,610 discharge
summaries extracted from the MIMIC-III database.
  Results: CNNs outperform other phenotyping algorithms in all 10 tasks. The
average F1-score of our model is 76 (PPV of 83, and sensitivity of 71) with our
model having an F1-score up to 37 points higher than alternative approaches. We
additionally assess the interpretability of our model by presenting a method
that extracts the most salient phrases for a particular prediction.
  Conclusion: We show that NLP methods based on deep learning improve the
performance of patient phenotyping. Our CNN-based algorithm automatically
learns the phrases associated with each patient phenotype. As such, it reduces
the annotation complexity for clinical domain experts, who are normally
required to develop task-specific annotation rules and identify relevant
phrases. Our method performs well in terms of both performance and
interpretability, which indicates that deep learning is an effective approach
to patient phenotyping based on clinicians' notes.



The goal of imitation learning is to mimic expert behavior without access to
an explicit reward signal. Expert demonstrations provided by humans, however,
often show significant variability due to latent factors that are typically not
explicitly modeled. In this paper, we propose a new algorithm that can infer
the latent structure of expert demonstrations in an unsupervised way. Our
method, built on top of Generative Adversarial Imitation Learning, can not only
imitate complex behaviors, but also learn interpretable and meaningful
representations of complex behavioral data, including visual demonstrations. In
the driving domain, we show that a model learned from human demonstrations is
able to both accurately reproduce a variety of behaviors and accurately
anticipate human actions using raw visual inputs. Compared with various
baselines, our method can better capture the latent structure underlying expert
demonstrations, often recovering semantically meaningful factors of variation
in the data.



For robotic vehicles to navigate safely and efficiently in pedestrian-rich
environments, it is important to model subtle human behaviors and navigation
rules. However, while instinctive to humans, socially compliant navigation is
still difficult to quantify due to the stochasticity in people's behaviors.
Existing works are mostly focused on using feature-matching techniques to
describe and imitate human paths, but often do not generalize well since the
feature values can vary from person to person, and even run to run. This work
notes that while it is challenging to directly specify the details of what to
do (precise mechanisms of human navigation), it is straightforward to specify
what not to do (violations of social norms). Specifically, using deep
reinforcement learning, this work develops a time-efficient navigation policy
that respects common social norms. The proposed method is shown to enable fully
autonomous navigation of a robotic vehicle moving at human walking speed in an
environment with many pedestrians.



The doctrine of double effect ($\mathcal{DDE}$) is a long-studied ethical
principle that governs when actions that have both positive and negative
effects are to be allowed. The goal in this paper is to automate
$\mathcal{DDE}$. We briefly present $\mathcal{DDE}$, and use a first-order
modal logic, the deontic cognitive event calculus, as our framework to
formalize the doctrine. We present formalizations of increasingly stronger
versions of the principle, including what is known as the doctrine of triple
effect. We then use our framework to simulate successfully scenarios that have
been used to test for the presence of the principle in human subjects. Our
framework can be used in two different modes: One can use it to build
$\mathcal{DDE}$-compliant autonomous systems from scratch, or one can use it to
verify that a given AI system is $\mathcal{DDE}$-compliant, by applying a
$\mathcal{DDE}$ layer on an existing system or model. For the latter mode, the
underlying AI system can be built using any architecture (planners, deep neural
networks, bayesian networks, knowledge-representation systems, or a hybrid); as
long as the system exposes a few parameters in its model, such verification is
possible. The role of the $\mathcal{DDE}$ layer here is akin to a (dynamic or
static) software verifier that examines existing software modules. Finally, we
end by presenting initial work on how one can apply our $\mathcal{DDE}$ layer
to the STRIPS-style planning model, and to a modified POMDP model.This is
preliminary work to illustrate the feasibility of the second mode, and we hope
that our initial sketches can be useful for other researchers in incorporating
DDE in their own frameworks.



In this paper, we present a transfer learning approach for music
classification and regression tasks. We propose to use a pre-trained convnet
feature, a concatenated feature vector using the activations of feature maps of
multiple layers in a trained convolutional network. We show how this convnet
feature can serve as general-purpose music representation. In the experiments,
a convnet is trained for music tagging and then transferred to other
music-related classification and regression tasks. The convnet feature
outperforms the baseline MFCC feature in all the considered tasks and several
previous approaches that are aggregating MFCCs as well as low- and high-level
music features.



This work studies how an AI-controlled dog-fighting agent with tunable
decision-making parameters can learn to optimize performance against an
intelligent adversary, as measured by a stochastic objective function evaluated
on simulated combat engagements. Gaussian process Bayesian optimization (GPBO)
techniques are developed to automatically learn global Gaussian Process (GP)
surrogate models, which provide statistical performance predictions in both
explored and unexplored areas of the parameter space. This allows a learning
engine to sample full-combat simulations at parameter values that are most
likely to optimize performance and also provide highly informative data points
for improving future predictions. However, standard GPBO methods do not provide
a reliable surrogate model for the highly volatile objective functions found in
aerial combat, and thus do not reliably identify global maxima. These issues
are addressed by novel Repeat Sampling (RS) and Hybrid Repeat/Multi-point
Sampling (HRMS) techniques. Simulation studies show that HRMS improves the
accuracy of GP surrogate models, allowing AI decision-makers to more accurately
predict performance and efficiently tune parameters.



Recently, deep learning (DL) methods have been introduced very successfully
into human activity recognition (HAR) scenarios in ubiquitous and wearable
computing. Especially the prospect of overcoming the need for manual feature
design combined with superior classification capabilities render deep neural
networks very attractive for real-life HAR application. Even though DL-based
approaches now outperform the state-of-the-art in a number of recognitions
tasks of the field, yet substantial challenges remain. Most prominently, issues
with real-life datasets, typically including imbalanced datasets and
problematic data quality, still limit the effectiveness of activity recognition
using wearables. In this paper we tackle such challenges through Ensembles of
deep Long Short Term Memory (LSTM) networks. We have developed modified
training procedures for LSTM networks and combine sets of diverse LSTM learners
into classifier collectives. We demonstrate, both formally and empirically,
that Ensembles of deep LSTM learners outperform the individual LSTM networks.
Through an extensive experimental evaluation on three standard benchmarks
(Opportunity, PAMAP2, Skoda) we demonstrate the excellent recognition
capabilities of our approach and its potential for real-life applications of
human activity recognition.



Multiple different approaches of generating adversarial examples have been
proposed to attack deep neural networks. These approaches involve either
directly computing gradients with respect to the image pixels, or directly
solving an optimization on the image pixels. In this work, we present a
fundamentally new method for generating adversarial examples that is fast to
execute and provides exceptional diversity of output. We efficiently train
feed-forward neural networks in a self-supervised manner to generate
adversarial examples against a target network or set of networks. We call such
a network an Adversarial Transformation Network (ATN). ATNs are trained to
generate adversarial examples that minimally modify the classifier's outputs
given the original input, while constraining the new classification to match an
adversarial target class. We present methods to train ATNs and analyze their
effectiveness targeting a variety of MNIST classifiers as well as the latest
state-of-the-art ImageNet classifier Inception ResNet v2.



In visual question answering (VQA), an algorithm must answer text-based
questions about images. While multiple datasets for VQA have been created since
late 2014, they all have flaws in both their content and the way algorithms are
evaluated on them. As a result, evaluation scores are inflated and
predominantly determined by answering easier questions, making it difficult to
compare different methods. In this paper, we analyze existing VQA algorithms
using a new dataset. It contains over 1.6 million questions organized into 12
different categories. We also introduce questions that are meaningless for a
given image to force a VQA system to reason about image content. We propose new
evaluation schemes that compensate for over-represented question-types and make
it easier to study the strengths and weaknesses of algorithms. We analyze the
performance of both baseline and state-of-the-art VQA models, including
multi-modal compact bilinear pooling (MCB), neural module networks, and
recurrent answering units. Our experiments establish how attention helps
certain categories more than others, determine which models work better than
others, and explain how simple models (e.g. MLP) can surpass more complex
models (MCB) by simply learning to answer large, easy question categories.



Inverse reinforcement learning (IRL) aims to explain observed strategic
behavior by fitting reinforcement learning models to behavioral data. However,
traditional IRL methods are only applicable when the observations are in the
form of state-action paths. This assumption may not hold in many real-world
modelling settings, where only partial observations are available. In general,
we may assume that there is a summarizing function $\sigma$, which acts as a
filter between us and the true state-action paths that constitute the
demonstration. Some initial approaches to extending IRL to such situations have
been presented, but with very specific assumptions about the structure of
$\sigma$, such as that only certain state observations are missing. This paper
instead focuses on the most general case of the problem, where no assumptions
are made about the summarizing function, except that it can be evaluated. We
demonstrate that inference is still possible. The paper presents exact and
approximate inference algorithms that allow full posterior inference, which is
particularly important for assessing parameter uncertainty in this challenging
inference situation. Empirical scalability is demonstrated to reasonably sized
problems, and practical applicability is demonstrated by estimating the
posterior for a cognitive science RL model based on observed user's task
completion time only.



This paper investigates a novel task of generating texture images from
perceptual descriptions. Previous work on texture generation focused on either
synthesis from examples or generation from procedural models. Generating
textures from perceptual attributes have not been well studied yet. Meanwhile,
perceptual attributes, such as directionality, regularity and roughness are
important factors for human observers to describe a texture. In this paper, we
propose a joint deep network model that combines adversarial training and
perceptual feature regression for texture generation, while only random noise
and user-defined perceptual attributes are required as input. In this model, a
preliminary trained convolutional neural network is essentially integrated with
the adversarial framework, which can drive the generated textures to possess
given perceptual attributes. An important aspect of the proposed model is that,
if we change one of the input perceptual features, the corresponding appearance
of the generated textures will also be changed. We design several experiments
to validate the effectiveness of the proposed method. The results show that the
proposed method can produce high quality texture images with desired perceptual
properties.



The recently launched LinkedIn Salary product has been designed with the goal
of providing compensation insights to the world's professionals and thereby
helping them optimize their earning potential. We describe the overall design
and architecture of the statistical modeling system underlying this product. We
focus on the unique data mining challenges while designing and implementing the
system, and describe the modeling components such as Bayesian hierarchical
smoothing that help to compute and present robust compensation insights to
users. We report on extensive evaluation with nearly one year of de-identified
compensation data collected from over one million LinkedIn users, thereby
demonstrating the efficacy of the statistical models. We also highlight the
lessons learned through the deployment of our system at LinkedIn.



Semantic segmentation requires a detailed labeling of image pixels by object
category. Information derived from local image patches is necessary to describe
the detailed shape of individual objects. However, this information is
ambiguous and can result in noisy labels. Global inference of image content can
instead capture the general semantic concepts present. We advocate that
holistic inference of image concepts provides valuable information for detailed
pixel labeling. We propose a generic framework to leverage holistic information
in the form of a LabelBank for pixel-level segmentation.
  We show the ability of our framework to improve semantic segmentation
performance in a variety of settings. We learn models for extracting a holistic
LabelBank from visual cues, attributes, and/or textual descriptions. We
demonstrate improvements in semantic segmentation accuracy on standard datasets
across a range of state-of-the-art segmentation architectures and holistic
inference approaches.



This paper surveys the current state of the art in Natural Language
Generation (NLG), defined as the task of generating text or speech from
non-linguistic input. A survey of NLG is timely in view of the changes that the
field has undergone over the past decade or so, especially in relation to new
(usually data-driven) methods, as well as new applications of NLG technology.
This survey therefore aims to (a) give an up-to-date synthesis of research on
the core tasks in NLG and the architectures adopted in which such tasks are
organised; (b) highlight a number of relatively recent research topics that
have arisen partly as a result of growing synergies between NLG and other areas
of artificial intelligence; (c) draw attention to the challenges in NLG
evaluation, relating them to similar challenges faced in other areas of Natural
Language Processing, with an emphasis on different evaluation methods and the
relationships between them.



Which topics of machine learning are most commonly addressed in research?
This question was initially answered in 2007 by doing a qualitative survey
among distinguished researchers. In our study, we revisit this question from a
quantitative perspective. Concretely, we collect 54K abstracts of papers
published between 2007 and 2016 in leading machine learning journals and
conferences. We then use machine learning in order to determine the top 10
topics in machine learning. We not only include models, but provide a holistic
view across optimization, data, features, etc. This quantitative approach
allows reducing the bias of surveys. It reveals new and up-to-date insights
into what the 10 most prolific topics in machine learning research are. This
allows researchers to identify popular topics as well as new and rising topics
for their research.



While strong progress has been made in image captioning over the last years,
machine and human captions are still quite distinct. A closer look reveals that
this is due to the deficiencies in the generated word distribution, vocabulary
size, and strong bias in the generators towards frequent captions. Furthermore,
humans -- rightfully so -- generate multiple, diverse captions, due to the
inherent ambiguity in the captioning task which is not considered in today's
systems.
  To address these challenges, we change the training objective of the caption
generator from reproducing groundtruth captions to generating a set of captions
that is indistinguishable from human generated captions. Instead of
handcrafting such a learning target, we employ adversarial training in
combination with an approximate Gumbel sampler to implicitly match the
generated distribution to the human one. While our method achieves comparable
performance to the state-of-the-art in terms of the correctness of the
captions, we generate a set of diverse captions, that are significantly less
biased and match the word statistics better in several aspects.



Rating platforms enable large-scale collection of user opinion about items
(products, other users, etc.). However, many untrustworthy users give
fraudulent ratings for excessive monetary gains. In the paper, we present
FairJudge, a system to identify such fraudulent users. We propose three
metrics: (i) the fairness of a user that quantifies how trustworthy the user is
in rating the products, (ii) the reliability of a rating that measures how
reliable the rating is, and (iii) the goodness of a product that measures the
quality of the product. Intuitively, a user is fair if it provides reliable
ratings that are close to the goodness of the product. We formulate a mutually
recursive definition of these metrics, and further address cold start problems
and incorporate behavioral properties of users and products in the formulation.
We propose an iterative algorithm, FairJudge, to predict the values of the
three metrics. We prove that FairJudge is guaranteed to converge in a bounded
number of iterations, with linear time complexity. By conducting five different
experiments on five rating platforms, we show that FairJudge significantly
outperforms nine existing algorithms in predicting fair and unfair users. We
reported the 100 most unfair users in the Flipkart network to their review
fraud investigators, and 80 users were correctly identified (80% accuracy). The
FairJudge algorithm is already being deployed at Flipkart.



This paper introduces a new approach to the long-term tracking of an object
in a challenging environment. The object is a cow and the environment is an
enclosure in a cowshed. Some of the key challenges in this domain are a
cluttered background, low contrast and high similarity between moving objects
which greatly reduces the efficiency of most existing approaches, including
those based on background subtraction. Our approach is split into object
localization, instance segmentation, learning and tracking stages. Our solution
is compared to a range of semi-supervised object tracking algorithms and we
show that the performance is strong and well suited to subsequent analysis. We
present our solution as a first step towards broader tracking and behavior
monitoring for cows in precision agriculture with the ultimate objective of
early detection of lameness.



Decision-makers are faced with the challenge of estimating what is likely to
happen when they take an action. For instance, if I choose not to treat this
patient, are they likely to die? Practitioners commonly use supervised learning
algorithms to fit predictive models that help decision-makers reason about
likely future outcomes, but we show that this approach is unreliable, and
sometimes even dangerous. The key issue is that supervised learning algorithms
are highly sensitive to the policy used to choose actions in the training data,
which causes the model to capture relationships that do not generalize. We
propose using a different learning objective that predicts counterfactuals
instead of predicting outcomes under an existing action policy as in supervised
learning. To support decision-making in temporal settings, we introduce the
Counterfactual Gaussian Process (CGP) to predict the counterfactual future
progression of continuous-time trajectories under sequences of future actions.
We demonstrate the benefits of the CGP on two important decision-support tasks:
risk prediction and "what if?" reasoning for individualized treatment planning.



Visual servoing involves choosing actions that move a robot in response to
observations from a camera, in order to reach a goal configuration in the
world. Standard visual servoing approaches typically rely on manually designed
features and analytical dynamics models, which limits their generalization
capability and often requires extensive application-specific feature and model
engineering. In this work, we study how learned visual features, learned
predictive dynamics models, and reinforcement learning can be combined to learn
visual servoing mechanisms. We focus on target following, with the goal of
designing algorithms that can learn a visual servo using low amounts of data of
the target in question, to enable quick adaptation to new targets. Our approach
is based on servoing the camera in the space of learned visual features, rather
than image pixels or manually-designed keypoints. We demonstrate that standard
deep features, in our case taken from a model trained for object
classification, can be used together with a bilinear predictive model to learn
an effective visual servo that is robust to visual variation, changes in
viewing angle and appearance, and occlusions. A key component of our approach
is to use a sample-efficient fitted Q-iteration algorithm to learn which
features are best suited for the task at hand. We show that we can learn an
effective visual servo on a complex synthetic car following benchmark using
just 20 training trajectory samples for reinforcement learning. We demonstrate
substantial improvement over a conventional approach based on image pixels or
hand-designed keypoints, and we show an improvement in sample-efficiency of
more than two orders of magnitude over standard model-free deep reinforcement
learning algorithms. Videos are available at
http://rll.berkeley.edu/visual_servoing .



Classifiers deployed in the real world operate in a dynamic environment,
where the data distribution can change over time. These changes, referred to as
concept drift, can cause the predictive performance of the classifier to drop
over time, thereby making it obsolete. To be of any real use, these classifiers
need to detect drifts and be able to adapt to them, over time. Detecting drifts
has traditionally been approached as a supervised task, with labeled data
constantly being used for validating the learned model. Although effective in
detecting drifts, these techniques are impractical, as labeling is a difficult,
costly and time consuming activity. On the other hand, unsupervised change
detection techniques are unreliable, as they produce a large number of false
alarms. The inefficacy of the unsupervised techniques stems from the exclusion
of the characteristics of the learned classifier, from the detection process.
In this paper, we propose the Margin Density Drift Detection (MD3) algorithm,
which tracks the number of samples in the uncertainty region of a classifier,
as a metric to detect drift. The MD3 algorithm is a distribution independent,
application independent, model independent, unsupervised and incremental
algorithm for reliably detecting drifts from data streams. Experimental
evaluation on 6 drift induced datasets and 4 additional datasets from the
cybersecurity domain demonstrates that the MD3 approach can reliably detect
drifts, with significantly fewer false alarms compared to unsupervised feature
based drift detectors. The reduced false alarms enables the signaling of drifts
only when they are most likely to affect classification performance. As such,
the MD3 approach leads to a detection scheme which is credible, label efficient
and general in its applicability.



Implicit discourse relation classification is of great challenge due to the
lack of connectives as strong linguistic cues, which motivates the use of
annotated implicit connectives to improve the recognition. We propose a feature
imitation framework in which an implicit relation network is driven to learn
from another neural network with access to connectives, and thus encouraged to
extract similarly salient features for accurate classification. We develop an
adversarial model to enable an adaptive imitation scheme through competition
between the implicit network and a rival feature discriminator. Our method
effectively transfers discriminability of connectives to the implicit features,
and achieves state-of-the-art performance on the PDTB benchmark.



Keyphrase boundary classification (KBC) is the task of detecting keyphrases
in scientific articles and labelling them with respect to predefined types.
Although important in practice, this task is so far underexplored, partly due
to the lack of labelled data. To overcome this, we explore several auxiliary
tasks, including semantic super-sense tagging and identification of multi-word
expressions, and cast the task as a multi-task learning problem with deep
recurrent neural networks. Our multi-task models perform significantly better
than previous state of the art approaches on two scientific KBC datasets,
particularly for long keyphrases.



Deep generative models trained with large amounts of unlabelled data have
proven to be powerful within the domain of unsupervised learning. Many real
life data sets contain a small amount of labelled data points, that are
typically disregarded when training generative models. We propose the
Cluster-aware Generative Model, that uses unlabelled information to infer a
latent representation that models the natural clustering of the data, and
additional labelled data points to refine this clustering. The generative
performances of the model significantly improve when labelled information is
exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant
MNIST, while also achieving competitive semi-supervised classification
accuracies. The model can also be trained fully unsupervised, and still improve
the log-likelihood performance with respect to related methods.



Theory of Mind is the ability to attribute mental states (beliefs, intents,
knowledge, perspectives, etc.) to others and recognize that these mental states
may differ from one's own. Theory of Mind is critical to effective
communication and to teams demonstrating higher collective performance. To
effectively leverage the progress in Artificial Intelligence (AI) to make our
lives more productive, it is important for humans and AI to work well together
in a team. Traditionally, there has been much emphasis on research to make AI
more accurate, and (to a lesser extent) on having it better understand human
intentions, tendencies, beliefs, and contexts. The latter involves making AI
more human-like and having it develop a theory of our minds. In this work, we
argue that for human-AI teams to be effective, humans must also develop a
theory of AI's mind (ToAIM) - get to know its strengths, weaknesses, beliefs,
and quirks. We instantiate these ideas within the domain of Visual Question
Answering (VQA). We find that using just a few examples (50), lay people can be
trained to better predict responses and oncoming failures of a complex VQA
model. We further evaluate the role existing explanation (or interpretability)
modalities play in helping humans build ToAIM. Explainable AI has received
considerable scientific and popular attention in recent times. Surprisingly, we
find that having access to the model's internal states - its confidence in its
top-k predictions, explicit or implicit attention maps which highlight regions
in the image (and words in the question) the model is looking at (and listening
to) while answering a question about an image - do not help people better
predict its behavior.



We consider tackling a single-agent RL problem by distributing it to $n$
learners. These learners, called advisors, endeavour to solve the problem from
a different focus. Their advice, taking the form of action values, is then
communicated to an aggregator, which is in control of the system. We show that
the local planning method for the advisors is critical and that none of the
ones found in the literature is flawless: the egocentric planning overestimates
values of states where the other advisors disagree, and the agnostic planning
is inefficient around danger zones. We introduce a novel approach called
empathic and discuss its theoretical aspects. We empirically examine and
validate our theoretical findings on a fruit collection task.



I make some basic observations about hard takeoff, value alignment, and
coherent extrapolated volition, concepts which have been central in analyses of
superintelligent AI systems.



This paper presents a design of a non-player character (AI) for promoting
balancedness in use of body segments when engaging in full-body motion gaming.
In our experiment, we settle a battle between the proposed AI and a player by
using FightingICE, a fighting game platform for AI development. A middleware
called UKI is used to allow the player to control the game by using body motion
instead of the keyboard and mouse. During gameplay, the proposed AI analyze
health states of the player; it determines its next action by predicting how
each candidate action, recommended by a Monte-Carlo tree search algorithm, will
induce the player to move, and how the player's health tends to be affected.
Our result demonstrates successful improvement in balancedness in use of body
segments on 4 out of 5 subjects.



Databases are widespread, yet extracting relevant data can be difficult.
Without substantial domain knowledge, multivariate search queries often return
sparse or uninformative results. This paper introduces an approach for
searching structured data based on probabilistic programming and nonparametric
Bayes. Users specify queries in a probabilistic language that combines standard
SQL database search operators with an information theoretic ranking function
called predictive relevance. Predictive relevance can be calculated by a fast
sparse matrix algorithm based on posterior samples from CrossCat, a
nonparametric Bayesian model for high-dimensional, heterogeneously-typed data
tables. The result is a flexible search technique that applies to a broad class
of information retrieval problems, which we integrate into BayesDB, a
probabilistic programming platform for probabilistic data analysis. This paper
demonstrates applications to databases of US colleges, global macroeconomic
indicators of public health, and classic cars. We found that human evaluators
often prefer the results from probabilistic search to results from a standard
baseline.



Generative models in vision have seen rapid progress due to algorithmic
improvements and the availability of high-quality image datasets. In this
paper, we offer contributions in both these areas to enable similar progress in
audio modeling. First, we detail a powerful new WaveNet-style autoencoder model
that conditions an autoregressive decoder on temporal codes learned from the
raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality
dataset of musical notes that is an order of magnitude larger than comparable
public datasets. Using NSynth, we demonstrate improved qualitative and
quantitative performance of the WaveNet autoencoder over a well-tuned spectral
autoencoder baseline. Finally, we show that the model learns a manifold of
embeddings that allows for morphing between instruments, meaningfully
interpolating in timbre to create new types of sounds that are realistic and
expressive.



This communication presents a longitudinal model-free control approach for
computing the wheel torque command to be applied on a vehicle. This setting
enables us to overcome the problem of unknown vehicle parameters for generating
a suitable control law. An important parameter in this control setting is made
time-varying for ensuring finite-time stability. Several convincing computer
simulations are displayed and discussed. Overshoots become therefore smaller.
The driving comfort is increased and the robustness to time-delays is improved.



In this paper, we argue that the future of Artificial Intelligence research
resides in two keywords: integration and embodiment. We support this claim by
analyzing the recent advances of the field. Regarding integration, we note that
the most impactful recent contributions have been made possible through the
integration of recent Machine Learning methods (based in particular on Deep
Learning and Recurrent Neural Networks) with more traditional ones (e.g.
Monte-Carlo tree search, goal babbling exploration or addressable memory
systems). Regarding embodiment, we note that the traditional benchmark tasks
(e.g. visual classification or board games) are becoming obsolete as
state-of-the-art learning algorithms approach or even surpass human performance
in most of them, having recently encouraged the development of first-person 3D
game platforms embedding realistic physics. Building upon this analysis, we
first propose an embodied cognitive architecture integrating heterogenous
sub-fields of Artificial Intelligence into a unified framework. We demonstrate
the utility of our approach by showing how major contributions of the field can
be expressed within the proposed framework. We then claim that benchmarking
environments need to reproduce ecologically-valid conditions for bootstrapping
the acquisition of increasingly complex cognitive skills through the concept of
a cognitive arms race between embodied agents.



Over 50 million scholarly articles have been published: they constitute a
unique repository of knowledge. In particular, one may infer from them
relations between scientific concepts, such as synonyms and hyponyms.
Artificial neural networks have been recently explored for relation extraction.
In this work, we continue this line of work and present a system based on a
convolutional neural network to extract relations. Our model ranked first in
the SemEval-2017 task 10 (ScienceIE) for relation extraction in scientific
articles (subtask C).



This report is targeted to groups who are subject matter experts in their
application but deep learning novices. It contains practical advice for those
interested in testing the use of deep neural networks on applications that are
novel for deep learning. We suggest making your project more manageable by
dividing it into phases. For each phase this report contains numerous
recommendations and insights to assist novice practitioners.



Existing Android malware detection approaches use a variety of features such
as security sensitive APIs, system calls, control-flow structures and
information flows in conjunction with Machine Learning classifiers to achieve
accurate detection. Each of these feature sets provides a unique semantic
perspective (or view) of apps' behaviours with inherent strengths and
limitations. Meaning, some views are more amenable to detect certain attacks
but may not be suitable to characterise several other attacks. Most of the
existing malware detection approaches use only one (or a selected few) of the
aforementioned feature sets which prevent them from detecting a vast majority
of attacks. Addressing this limitation, we propose MKLDroid, a unified
framework that systematically integrates multiple views of apps for performing
comprehensive malware detection and malicious code localisation. The rationale
is that, while a malware app can disguise itself in some views, disguising in
every view while maintaining malicious intent will be much harder.
  MKLDroid uses a graph kernel to capture structural and contextual information
from apps' dependency graphs and identify malice code patterns in each view.
Subsequently, it employs Multiple Kernel Learning (MKL) to find a weighted
combination of the views which yields the best detection accuracy. Besides
multi-view learning, MKLDroid's unique and salient trait is its ability to
locate fine-grained malice code portions in dependency graphs (e.g.,
methods/classes). Through our large-scale experiments on several datasets
(incl. wild apps), we demonstrate that MKLDroid outperforms three
state-of-the-art techniques consistently, in terms of accuracy while
maintaining comparable efficiency. In our malicious code localisation
experiments on a dataset of repackaged malware, MKLDroid was able to identify
all the malice classes with 94% average recall.



Causal effect estimation from observational data is an important and much
studied research topic. The instrumental variable (IV) and local causal
discovery (LCD) patterns are canonical examples of settings where a closed-form
expression exists for the causal effect of one variable on another, given the
presence of a third variable. Both rely on faithfulness to infer that the
latter only influences the target effect via the cause variable. In reality, it
is likely that this assumption only holds approximately and that there will be
at least some form of weak interaction. This brings about the paradoxical
situation that, in the large-sample limit, no predictions are made, as
detecting the weak edge invalidates the setting. We introduce an alternative
approach by replacing strict faithfulness with a prior that reflects the
existence of many 'weak' (irrelevant) and 'strong' interactions. We obtain a
posterior distribution over the target causal effect estimator which shows
that, in many cases, we can still make good estimates. We demonstrate the
approach in an application on a simple linear-Gaussian setting, using the
MultiNest sampling algorithm, and compare it with established techniques to
show our method is robust even when strict faithfulness is violated.



This paper introduces a new lifelong learning solution where a single model
is trained for a sequence of tasks. The main challenge that vision systems face
in this context is catastrophic forgetting: as they tend to adapt to the most
recently seen task, they lose performance on the tasks that were learned
previously. Our method aims at preserving the knowledge of the previous tasks
while learning a new one by using autoencoders. For each task, an
under-complete autoencoder is learned, capturing the features that are crucial
for its achievement. When a new task is presented to the system, we prevent the
reconstructions of the features with these autoencoders from changing, which
has the effect of preserving the information on which the previous tasks are
mainly relying. At the same time, the features are given space to adjust to the
most recent environment as only their projection into a low dimension
submanifold is controlled. The proposed system is evaluated on image
classification tasks and shows a reduction of forgetting over the
state-of-the-art



A great variety of text tasks such as topic or spam identification, user
profiling, and sentiment analysis can be posed as a supervised learning problem
and tackle using a text classifier. A text classifier consists of several
subprocesses, some of them are general enough to be applied to any supervised
learning problem, whereas others are specifically designed to tackle a
particular task, using complex and computational expensive processes such as
lemmatization, syntactic analysis, etc. Contrary to traditional approaches, we
propose a minimalistic and wide system able to tackle text classification tasks
independent of domain and language, namely microTC. It is composed by some easy
to implement text transformations, text representations, and a supervised
learning algorithm. These pieces produce a competitive classifier even in the
domain of informally written text. We provide a detailed description of microTC
along with an extensive experimental comparison with relevant state-of-the-art
methods. mircoTC was compared on 30 different datasets. Regarding accuracy,
microTC obtained the best performance in 20 datasets while achieves competitive
results in the remaining 10. The compared datasets include several problems
like topic and polarity classification, spam detection, user profiling and
authorship attribution. Furthermore, it is important to state that our approach
allows the usage of the technology even without knowledge of machine learning
and natural language processing.



Treatment effects can be estimated from observational data as the difference
in potential outcomes. In this paper, we address the challenge of estimating
the potential outcome when treatment-dose levels can vary continuously over
time. Further, the outcome variable may not be measured at a regular frequency.
Our proposed solution represents the treatment response curves using linear
time-invariant dynamical systems---this provides a flexible means for modeling
response over time to highly variable dose curves. Moreover, for multivariate
data, the proposed method: uncovers shared structure in treatment response and
the baseline across multiple markers; and, flexibly models challenging
correlation structure both across and within signals over time. For this, we
build upon the framework of multiple-output Gaussian Processes. On simulated
and a challenging clinical dataset, we show significant gains in accuracy over
state-of-the-art models.



A promising paradigm for achieving highly efficient deep neural networks is
the idea of evolutionary deep intelligence, which mimics biological evolution
processes to progressively synthesize more efficient networks. A crucial design
factor in evolutionary deep intelligence is the genetic encoding scheme used to
simulate heredity and determine the architectures of offspring networks. In
this study, we take a deeper look at the notion of synaptic cluster-driven
evolution of deep neural networks which guides the evolution process towards
the formation of a highly sparse set of synaptic clusters in offspring
networks. Utilizing a synaptic cluster-driven genetic encoding, the
probabilistic encoding of synaptic traits considers not only individual
synaptic properties but also inter-synaptic relationships within a deep neural
network. This process results in highly sparse offspring networks which are
particularly tailored for parallel computational devices such as GPUs and deep
neural network accelerator chips. Comprehensive experimental results using four
well-known deep neural network architectures (LeNet-5, AlexNet, ResNet-56, and
DetectNet) on two different tasks (object categorization and object detection)
demonstrate the efficiency of the proposed method. Cluster-driven genetic
encoding scheme synthesizes networks that can achieve state-of-the-art
performance with significantly smaller number of synapses than that of the
original ancestor network. ($\sim$125-fold decrease in synapses for MNIST).
Furthermore, the improved cluster efficiency in the generated offspring
networks ($\sim$9.71-fold decrease in clusters for MNIST and a $\sim$8.16-fold
decrease in clusters for KITTI) is particularly useful for accelerated
performance on parallel computing hardware architectures such as those in GPUs
and deep neural network accelerator chips.



Models that can simulate how environments change in response to actions can
be used by agents to plan and act efficiently. We improve on previous
environment simulators from high-dimensional pixel observations by introducing
recurrent neural networks that are able to make temporally and spatially
coherent predictions for hundreds of time-steps into the future. We present an
in-depth analysis of the factors affecting performance, providing the most
extensive attempt to advance the understanding of the properties of these
models. We address the issue of computationally inefficiency with a model that
does not need to generate a high-dimensional image at each time-step. We show
that our approach can be used to improve exploration and is adaptable to many
diverse environments, namely 10 Atari games, a 3D car racing environment, and
complex 3D mazes.



Sentence simplification reduces semantic complexity to benefit people with
language impairments. Previous simplification studies on the sentence level and
word level have achieved promising results but also meet great challenges. For
sentence-level studies, sentences after simplification are fluent but sometimes
are not really simplified. For word-level studies, words are simplified but
also have potential grammar errors due to different usages of words before and
after simplification. In this paper, we propose a two-step simplification
framework by combining both the word-level and the sentence-level
simplifications, making use of their corresponding advantages. Based on the
two-step framework, we implement a novel constrained neural generation model to
simplify sentences given simplified words. The final results on Wikipedia and
Simple Wikipedia aligned datasets indicate that our method yields better
performance than various baselines.



This document describes the contributions of the 2016 Applications of Logic
Programming Workshop (AppLP), which was held on October 17 and associated with
the International Conference on Logic Programming (ICLP) in Flushing, New York
City.



Many modern computer vision and machine learning applications rely on solving
difficult optimization problems that involve non-differentiable objective
functions and constraints. The alternating direction method of multipliers
(ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a
generalization of ADMM that often achieves better performance, but its
efficiency depends strongly on algorithm parameters that must be chosen by an
expert user. We propose an adaptive method that automatically tunes the key
algorithm parameters to achieve optimal performance without user oversight.
Inspired by recent work on adaptivity, the proposed adaptive relaxed ADMM
(ARADMM) is derived by assuming a Barzilai-Borwein style linear gradient. A
detailed convergence analysis of ARADMM is provided, and numerical results on
several applications demonstrate fast practical convergence.



This thesis contributes to the formalisation of the notion of an agent within
the class of finite multivariate Markov chains. Agents are seen as entities
that act, perceive, and are goal-directed.
  We present a new measure that can be used to identify entities (called
$\iota$-entities), some general requirements for entities in multivariate
Markov chains, as well as formal definitions of actions and perceptions
suitable for such entities.
  The intuition behind $\iota$-entities is that entities are spatiotemporal
patterns for which every part makes every other part more probable. The
measure, complete local integration (CLI), is formally investigated in general
Bayesian networks. It is based on the specific local integration (SLI) which is
measured with respect to a partition. CLI is the minimum value of SLI over all
partitions. We prove that $\iota$-entities are blocks in specific partitions of
the global trajectory. These partitions are the finest partitions that achieve
a given SLI value. We also establish the transformation behaviour of SLI under
permutations of nodes in the network.
  We go on to present three conditions on general definitions of entities.
These are not fulfilled by sets of random variables i.e.\ the perception-action
loop, which is often used to model agents, is too restrictive. We propose that
any general entity definition should in effect specify a subset (called an an
entity-set) of the set of all spatiotemporal patterns of a given multivariate
Markov chain. The set of $\iota$-entities is such a set. Importantly the
perception-action loop also induces an entity-set.
  We then propose formal definitions of actions and perceptions for arbitrary
entity-sets. These specialise to standard notions in case of the
perception-action loop entity-set.
  Finally we look at some very simple examples.



We describe the SemEval task of extracting keyphrases and relations between
them from scientific documents, which is crucial for understanding which
publications describe which processes, tasks and materials. Although this was a
new task, we had a total of 26 submissions across 3 evaluation scenarios. We
expect the task and the findings reported in this paper to be relevant for
researchers working on understanding scientific content, as well as the broader
knowledge base population and information extraction communities.



In reinforcement learning, agents learn by performing actions and observing
their outcomes. Sometimes, it is desirable for a human operator to
\textit{interrupt} an agent in order to prevent dangerous situations from
happening. Yet, as part of their learning process, agents may link these
interruptions, that impact their reward, to specific states and deliberately
avoid them. The situation is particularly challenging in a multi-agent context
because agents might not only learn from their own past interruptions, but also
from those of other agents. Orseau and Armstrong defined \emph{safe
interruptibility} for one learner, but their work does not naturally extend to
multi-agent systems. This paper introduces \textit{dynamic safe
interruptibility}, an alternative definition more suited to decentralized
learning problems, and studies this notion in two learning frameworks:
\textit{joint action learners} and \textit{independent learners}. We give
realistic sufficient conditions on the learning algorithm to enable dynamic
safe interruptibility in the case of joint action learners, yet show that these
conditions are not sufficient for independent learners. We show however that if
agents can detect interruptions, it is possible to prune the observations to
ensure dynamic safe interruptibility even for independent learners.



Major advances have recently been made in merging language and vision
representations. But most tasks considered so far have confined themselves to
the processing of objects and lexicalised relations amongst objects (content
words). We know, however, that humans (even pre-school children) can abstract
over raw data to perform certain types of higher-level reasoning, expressed in
natural language by function words. A case in point is given by their ability
to learn quantifiers, i.e. expressions like 'few', 'some' and 'all'. From
formal semantics and cognitive linguistics, we know that quantifiers are
relations over sets which, as a simplification, we can see as proportions. For
instance, in 'most fish are red', most encodes the proportion of fish which are
red fish. In this paper, we study how well current language and vision
strategies model such relations. We show that state-of-the-art attention
mechanisms coupled with a traditional linguistic formalisation of quantifiers
gives best performance on the task. Additionally, we provide insights on the
role of 'gist' representations in quantification. A 'logical' strategy to
tackle the task would be to first obtain a numerosity estimation for the two
involved sets and then compare their cardinalities. We however argue that
precisely identifying the composition of the sets is not only beyond current
state-of-the-art models but perhaps even detrimental to a task that is most
efficiently performed by refining the approximate numerosity estimator of the
system.



Deep reinforcement learning has achieved many impressive results in recent
years. However, tasks with sparse rewards or long horizons continue to pose
significant challenges. To tackle these important problems, we propose a
general framework that first learns useful skills in a pre-training
environment, and then leverages the acquired skills for learning faster in
downstream tasks. Our approach brings together some of the strengths of
intrinsic motivation and hierarchical methods: the learning of useful skill is
guided by a single proxy reward, the design of which requires very minimal
domain knowledge about the downstream tasks. Then a high-level policy is
trained on top of these skills, providing a significant improvement of the
exploration and allowing to tackle sparse rewards in the downstream tasks. To
efficiently pre-train a large span of skills, we use Stochastic Neural Networks
combined with an information-theoretic regularizer. Our experiments show that
this combination is effective in learning a wide span of interpretable skills
in a sample-efficient way, and can significantly boost the learning performance
uniformly across a wide range of downstream tasks.



The role of semantics in zero-shot learning is considered. The effectiveness
of previous approaches is analyzed according to the form of supervision
provided. While some learn semantics independently, others only supervise the
semantic subspace explained by training classes. Thus, the former is able to
constrain the whole space but lacks the ability to model semantic correlations.
The latter addresses this issue but leaves part of the semantic space
unsupervised. This complementarity is exploited in a new convolutional neural
network (CNN) framework, which proposes the use of semantics as constraints for
recognition.Although a CNN trained for classification has no transfer ability,
this can be encouraged by learning an hidden semantic layer together with a
semantic code for classification. Two forms of semantic constraints are then
introduced. The first is a loss-based regularizer that introduces a
generalization constraint on each semantic predictor. The second is a codeword
regularizer that favors semantic-to-class mappings consistent with prior
semantic knowledge while allowing these to be learned from data. Significant
improvements over the state-of-the-art are achieved on several datasets.



For computer vision applications, prior works have shown the efficacy of
reducing the numeric precision of model parameters (network weights) in deep
neural networks but also that reducing the precision of activations hurts model
accuracy much more than reducing the precision of model parameters. We study
schemes to train networks from scratch using reduced-precision activations
without hurting the model accuracy. We reduce the precision of activation maps
(along with model parameters) using a novel quantization scheme and increase
the number of filter maps in a layer, and find that this scheme compensates or
surpasses the accuracy of the baseline full-precision network. As a result, one
can significantly reduce the dynamic memory footprint, memory bandwidth,
computational energy and speed up the training and inference process with
appropriate hardware support. We call our scheme WRPN - wide reduced-precision
networks. We report results using our proposed schemes and show that our
results are better than previously reported accuracies on ILSVRC-12 dataset
while being computationally less expensive compared to previously reported
reduced-precision networks.



Building a dialogue agent to fulfill complex tasks, such as travel planning,
is challenging because the agent has to learn to collectively complete multiple
subtasks. For example, the agent needs to reserve a hotel and book a flight so
that there leaves enough time for commute between arrival and hotel check-in.
This paper addresses this challenge by formulating the task in the mathematical
framework of options over Markov Decision Processes (MDPs), and proposing a
hierarchical deep reinforcement learning approach to learning a dialogue
manager that operates at different temporal scales. The dialogue manager
consists of: (1) a top-level dialogue policy that selects among subtasks or
options, (2) a low-level dialogue policy that selects primitive actions to
complete the subtask given by the top-level policy, and (3) a global state
tracker that helps ensure all cross-subtask constraints be satisfied.
Experiments on a travel planning task with simulated and real users show that
our approach leads to significant improvements over three baselines, two based
on handcrafted rules and the other based on flat deep reinforcement learning.



This papers shows that using separators, which is a pair of two complementary
contractors, we can easily and efficiently solve the localization problem of a
robot with sonar measurements in an unstructured environment. We introduce
separators associated with the Minkowski sum and the Minkowski difference in
order to facilitate the resolution. A test-case is given in order to illustrate
the principle of the approach.



This paper introduces Scavenger, the first theorem prover for pure
first-order logic without equality based on the new conflict resolution
calculus. Conflict resolution has a restricted resolution inference rule that
resembles (a first-order generalization of) unit propagation as well as a rule
for assuming decision literals and a rule for deriving new clauses by (a
first-order generalization of) conflict-driven clause learning.



As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, such as medical diagnosis or autonomous driving, it is
critical that researchers can explain how such algorithms arrived at their
predictions. In recent years, a number of image saliency methods have been
developed to summarize where highly complex neural networks "look" in an image
for evidence for their predictions. However, these techniques are limited by
their heuristic nature and architectural constraints. In this paper, we make
two main contributions: First, we propose a general framework for learning
different kinds of explanations for any black box algorithm. Second, we
specialise the framework to find the part of an image most responsible for a
classifier decision. Unlike previous works, our method is model-agnostic and
testable because it is grounded in explicit and interpretable image
perturbations.



Process mining analyzes business processes based on events stored in event
logs. However, some recorded events may correspond to activities on a very low
level of abstraction. When events are recorded on a too low level of
granularity, process discovery methods tend to generate overgeneralizing
process models. Grouping low-level events to higher level activities, i.e.,
event abstraction, can be used to discover better process models. Existing
event abstraction methods are mainly based on common sub-sequences and
clustering techniques. In this paper, we propose to first discover local
process models and then use those models to lift the event log to a higher
level of abstraction. Our conjecture is that process models discovered on the
obtained high-level event log return process models of higher quality: their
fitness and precision scores are more balanced. We show this with preliminary
results on several real-life event logs.



Output-agreement mechanisms such as ESP Game have been widely used in human
computation to obtain reliable human-generated labels. In this paper, we argue
that a "time-limited" output-agreement mechanism can be used to create a fast
and robust crowd-powered component in interactive systems, particularly
dialogue systems, to extract key information from user utterances on the fly.
Our experiments on Amazon Mechanical Turk using the Airline Travel Information
System (ATIS) dataset showed that the proposed approach achieves high-quality
results with an average response time shorter than 9 seconds.



Positioning data offer a remarkable source of information to analyze crowds
urban dynamics. However, discovering urban activity patterns from the emergent
behavior of crowds involves complex system modeling. An alternative approach is
to adopt computational techniques belonging to the emergent paradigm, which
enables self-organization of data and allows adaptive analysis. Specifically,
our approach is based on stigmergy. By using stigmergy each sample position is
associated with a digital pheromone deposit, which progressively evaporates and
aggregates with other deposits according to their spatiotemporal proximity.
Based on this principle, we exploit positioning data to identify high density
areas (hotspots) and characterize their activity over time. This
characterization allows the comparison of dynamics occurring in different days,
providing a similarity measure exploitable by clustering techniques. Thus, we
cluster days according to their activity behavior, discovering unexpected urban
activity patterns. As a case study, we analyze taxi traces in New York City
during 2015.



Multi-armed bandits are a quintessential machine learning problem requiring
the balancing of exploration and exploitation. While there has been progress in
developing algorithms with strong theoretical guarantees, there has been less
focus on practical near-optimal finite-time performance. In this paper, we
propose an algorithm for Bayesian multi-armed bandits that utilizes
value-function-driven online planning techniques. Building on previous work on
UCB and Gittins index, we introduce linearly-separable value functions that
take both the expected return and the benefit of exploration into consideration
to perform n-step lookahead. The algorithm enjoys a sub-linear performance
guarantee and we present simulation results that confirm its strength in
problems with structured priors. The simplicity and generality of our approach
makes it a strong candidate for analyzing more complex multi-armed bandit
problems.



This paper considers a general data-fitting problem over a networked system,
in which many computing nodes are connected by an undirected graph. This kind
of problem can find many real-world applications and has been studied
extensively in the literature. However, existing solutions either need a
central controller for information sharing or requires slot synchronization
among different nodes, which increases the difficulty of practical
implementations, especially for a very large and heterogeneous system.
  As a contrast, in this paper, we treat the data-fitting problem over the
network as a stochastic programming problem with many constraints. By adapting
the results in a recent paper, we design a fully distributed and asynchronized
stochastic gradient descent (SGD) algorithm. We show that our algorithm can
achieve global optimality and consensus asymptotically by only local
computations and communications. Additionally, we provide a sharp lower bound
for the convergence speed in the regular graph case. This result fits the
intuition and provides guidance to design a `good' network topology to speed up
the convergence. Also, the merit of our design is validated by experiments on
both synthetic and real-world datasets.



We propose a partially learned approach for the solution of ill posed inverse
problems with not necessarily linear forward operators. The method builds on
ideas from classical regularization theory and recent advances in deep learning
to perform learning while making use of prior information about the inverse
problem encoded in the forward operator, noise model and a regularizing
functional. The method results in a gradient-like iterative scheme, where the
"gradient" component is learned using a convolutional network that includes the
gradients of the data discrepancy and regularizer as input in each iteration.
We present results of such a partially learned gradient scheme on a non-linear
tomographic inversion problem with simulated data from both the Sheep-Logan
phantom as well as a head CT. The outcome is compared against FBP and TV
reconstruction and the proposed method provides a 5.4 dB PSNR improvement over
the TV reconstruction while being significantly faster, giving reconstructions
of 512 x 512 volumes in about 0.4 seconds using a single GPU.



A key enabler for optimizing business processes is accurately estimating the
probability distribution of a time series future given its past. Such
probabilistic forecasts are crucial for example for reducing excess inventory
in supply chains. In this paper we propose DeepAR, a novel methodology for
producing accurate probabilistic forecasts, based on training an
auto-regressive recurrent network model on a large number of related time
series. We show through extensive empirical evaluation on several real-world
forecasting data sets that our methodology is more accurate than
state-of-the-art models, while requiring minimal feature engineering.



In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an
approach to visualize and understand the decisions made by deep neural networks
(DNNs) given a specific input. CLEAR facilitates the visualization of attentive
regions and levels of interest of DNNs during the decision-making process. It
also enables the visualization of the most dominant classes associated with
these attentive regions of interest. As such, CLEAR can mitigate some of the
shortcomings of heatmap-based methods associated with decision ambiguity, and
allows for better insights into the decision-making process of DNNs.
Quantitative and qualitative experiments across three different datasets
demonstrate the efficacy of CLEAR for gaining a better understanding of the
inner workings of DNNs during the decision-making process.



Reinforcement learning involves decision making in dynamic and uncertain
environments, and constitutes one important element of artificial intelligence
(AI). In this paper, we experimentally demonstrate that the ultrafast chaotic
oscillatory dynamics of lasers efficiently solve the multi-armed bandit problem
(MAB), which requires decision making concerning a class of difficult
trade-offs called the exploration-exploitation dilemma. To solve the MAB, a
certain degree of randomness is required for exploration purposes. However,
pseudo-random numbers generated using conventional electronic circuitry
encounter severe limitations in terms of their data rate and the quality of
randomness due to their algorithmic foundations. We generate laser chaos
signals using a semiconductor laser sampled at a maximum rate of 100 GSample/s,
and combine it with a simple decision-making principle called tug-of-war with a
variable threshold, to ensure ultrafast, adaptive and accurate decision making
at a maximum adaptation speed of 1 GHz. We found that decision-making
performance was maximized with an optimal sampling interval, and we highlight
the exact coincidence between the negative autocorrelation inherent in laser
chaos and decision-making performance. This study paves the way for a new realm
of ultrafast photonics in the age of AI, where the ultrahigh bandwidth of
photons can provide new value.



Coreference evaluation metrics are hard to optimize directly as they are
non-differentiable functions, not easily decomposable into elementary
decisions. Consequently, most approaches optimize objectives only indirectly
related to the end goal, resulting in suboptimal performance. Instead, we
propose a differentiable relaxation that lends itself to gradient-based
optimisation, thus bypassing the need for reinforcement learning or heuristic
modification of cross-entropy. We show that by modifying the training objective
of a competitive neural coreference system, we obtain a substantial gain in
performance. This suggests that our approach can be regarded as a viable
alternative to using reinforcement learning or more computationally expensive
imitation learning.



In this work, we consider an extension of graphical models to random graphs,
trees, and other objects. To do this, many fundamental concepts for
multivariate random variables (e.g., marginal variables, Gibbs distribution,
Markov properties) must be extended to other mathematical objects; it turns out
that this extension is possible, as we will discuss, if we have a consistent,
complete system of projections on a given object. Each projection defines a
marginal random variable, allowing one to specify independence assumptions
between them. Furthermore, these independencies can be specified in terms of a
small subset of these marginal variables (which we call the atomic variables),
allowing the compact representation of independencies by a directed graph.
Projections also define factors, functions on the projected object space, and
hence a projection family defines a set of possible factorizations for a
distribution; these can be compactly represented by an undirected graph.
  The invariances used in graphical models are essential for learning
distributions, not just on multivariate random variables, but also on other
objects. When they are applied to random graphs and random trees, the result is
a general class of models that is applicable to a broad range of problems,
including those in which the graphs and trees have complicated edge structures.
These models need not be conditioned on a fixed number of vertices, as is often
the case in the literature for random graphs, and can be used for problems in
which attributes are associated with vertices and edges. For graphs,
applications include the modeling of molecules, neural networks, and relational
real-world scenes; for trees, applications include the modeling of infectious
diseases, cell fusion, the structure of language, and the structure of objects
in visual scenes. Many classic models are particular instances of this
framework.



We introduce a novel framework for evaluating multimodal deep learning models
with respect to their language understanding and generalization abilities. In
this approach, artificial data is automatically generated according to the
experimenter's specifications. The content of the data, both during training
and evaluation, can be controlled in detail, which enables tasks to be created
that require true generalization abilities, in particular the combination of
previously introduced concepts in novel ways. We demonstrate the potential of
our methodology by evaluating various visual question answering models on four
different tasks, and show how our framework gives us detailed insights into
their capabilities and limitations. By open-sourcing our framework, we hope to
stimulate progress in the field of multimodal language understanding.



In this paper, we propose an online learning algorithm based on a
Rao-Blackwellized particle filter for spatial concept acquisition and mapping.
We have proposed a nonparametric Bayesian spatial concept acquisition model
(SpCoA). We propose a novel method (SpCoSLAM) integrating SpCoA and FastSLAM in
the theoretical framework of the Bayesian generative model. The proposed method
can simultaneously learn place categories and lexicons while incrementally
generating an environmental map. Furthermore, the proposed method has scene
image features and a language model added to SpCoA. In the experiments, we
tested online learning of spatial concepts and environmental maps in a novel
environment of which the robot did not have a map. Then, we evaluated the
results of online learning of spatial concepts and lexical acquisition. The
experimental results demonstrated that the robot was able to more accurately
learn the relationships between words and the place in the environmental map
incrementally by using the proposed method.



We present RACE, a new dataset for benchmark evaluation of methods in the
reading comprehension task. Collected from the English exams for middle and
high school Chinese students in the age range between 12 to 18, RACE consists
of near 28,000 passages and near 100,000 questions generated by human experts
(English instructors), and covers a variety of topics which are carefully
designed for evaluating the students' ability in understanding and reasoning.
In particular, the proportion of questions that requires reasoning is much
larger in RACE than that in other benchmark datasets for reading comprehension,
and there is a significant gap between the performance of the state-of-the-art
models (43%) and the ceiling human performance (95%). We hope this new dataset
can serve as a valuable resource for research and evaluation in machine
comprehension. The dataset is freely available at
http://www.cs.cmu.edu/~glai1/data/race/ and the code is available at
https://github.com/qizhex/RACE_AR_baselines.



In this paper we propose the creation of generic LSH families for the angular
distance based on Johnson-Lindenstrauss projections. We show that feature
hashing is a valid J-L projection and propose two new LSH families based on
feature hashing. These new LSH families are tested on both synthetic and real
datasets with very good results and a considerable performance improvement over
other LSH families. While the theoretical analysis is done for the angular
distance, these families can also be used in practice for the euclidean
distance with excellent results [2]. Our tests using real datasets show that
the proposed LSH functions work well for the euclidean distance.



The rise of robotic applications has led to the generation of a huge volume
of unstructured data, whereas the current cloud infrastructure was designed to
process limited amounts of structured data. To address this problem, we propose
a learn-memorize-recall-reduce paradigm for robotic cloud computing. The
learning stage converts incoming unstructured data into structured data; the
memorization stage provides effective storage for the massive amount of data;
the recall stage provides efficient means to retrieve the raw data; while the
reduction stage provides means to make sense of this massive amount of
unstructured data with limited computing resources.



High Energy Physics (HEP) distributed computing infrastructures require
automatic tools to monitor, analyze and react to potential security incidents.
These tools should collect and inspect data such as resource consumption, logs
and sequence of system calls for detecting anomalies that indicate the presence
of a malicious agent. They should also be able to perform automated reactions
to attacks without administrator intervention. We describe a novel framework
that accomplishes these requirements, with a proof of concept implementation
for the ALICE experiment at CERN. We show how we achieve a fully virtualized
environment that improves the security by isolating services and Jobs without a
significant performance impact. We also describe a collected dataset for
Machine Learning based Intrusion Prevention and Detection Systems on Grid
computing. This dataset is composed of resource consumption measurements (such
as CPU, RAM and network traffic), logfiles from operating system services, and
system call data collected from production Jobs running in an ALICE Grid test
site and a big set of malware. This malware was collected from security
research sites. Based on this dataset, we will proceed to develop Machine
Learning algorithms able to detect malicious Jobs.



In this paper, we propose a simple variant of the original stochastic
variance reduction gradient (SVRG), where hereafter we refer to as the variance
reduced stochastic gradient descent (VR-SGD). Different from the choices of the
snapshot point and starting point in SVRG and its proximal variant, Prox-SVRG,
the two vectors of each epoch in VR-SGD are set to the average and last iterate
of the previous epoch, respectively. This setting allows us to use much larger
learning rates or step sizes than SVRG, e.g., 3/(7L) for VR-SGD vs 1/(10L) for
SVRG, and also makes our convergence analysis more challenging. In fact, a
larger learning rate enjoyed by VR-SGD means that the variance of its
stochastic gradient estimator asymptotically approaches zero more rapidly.
Unlike common stochastic methods such as SVRG and proximal stochastic methods
such as Prox-SVRG, we design two different update rules for smooth and
non-smooth objective functions, respectively. In other words, VR-SGD can tackle
non-smooth and/or non-strongly convex problems directly without using any
reduction techniques such as quadratic regularizers. Moreover, we analyze the
convergence properties of VR-SGD for strongly convex problems, which show that
VR-SGD attains a linear convergence rate. We also provide the convergence
guarantees of VR-SGD for non-strongly convex problems. Experimental results
show that the performance of VR-SGD is significantly better than its
counterparts, SVRG and Prox-SVRG, and it is also much better than the best
known stochastic method, Katyusha.



Morpheo is a transparent and secure machine learning platform collecting and
analysing large datasets. It aims at building state-of-the art prediction
models in various fields where data are sensitive. Indeed, it offers strong
privacy of data and algorithm, by preventing anyone to read the data, apart
from the owner and the chosen algorithms. Computations in Morpheo are
orchestrated by a blockchain infrastructure, thus offering total traceability
of operations. Morpheo aims at building an attractive economic ecosystem around
data prediction by channelling crypto-money from prediction requests to useful
data and algorithms providers. Morpheo is designed to handle multiple data
sources in a transfer learning approach in order to mutualize knowledge
acquired from large datasets for applications with smaller but similar
datasets.



Information systems experience an ever-growing volume of unstructured data,
particularly in the form of textual materials. This represents a rich source of
information from which one can create value for people, organizations and
businesses. For instance, recommender systems can benefit from automatically
understanding preferences based on user reviews or social media. However, it is
difficult for computer programs to correctly infer meaning from narrative
content. One major challenge is negations that invert the interpretation of
words and sentences. As a remedy, this paper proposes a novel learning strategy
to detect negations: we apply reinforcement learning to find a policy that
replicates the human perception of negations based on an exogenous response,
such as a user rating for reviews. Our method yields several benefits, as it
eliminates the former need for expensive and subjective manual labeling in an
intermediate stage. Moreover, the inferred policy can be used to derive
statistical inferences and implications regarding how humans process and act on
negations.



Predicting personality is essential for social applications supporting
human-centered activities, yet prior modeling methods with users written text
require too much input data to be realistically used in the context of social
media. In this work, we aim to drastically reduce the data requirement for
personality modeling and develop a model that is applicable to most users on
Twitter. Our model integrates Word Embedding features with Gaussian Processes
regression. Based on the evaluation of over 1.3K users on Twitter, we find that
our model achieves comparable or better accuracy than state of the art
techniques with 8 times fewer data.



In this work, we propose a method for learning driver models that account for
variables that cannot be observed directly. When trained on a synthetic
dataset, our models are able to learn encodings for vehicle trajectories that
distinguish between four distinct classes of driver behavior. Such encodings
are learned without any knowledge of the number of driver classes or any
objective that directly requires the models to learn encodings for each class.
We show that driving policies trained with knowledge of latent variables are
more effective than baseline methods at imitating the driver behavior that they
are trained to replicate. Furthermore, we demonstrate that the actions chosen
by our policy are heavily influenced by the latent variable settings that are
provided to them.



We introduce the Self-Annotated Reddit Corpus (SARC), a large corpus for
sarcasm research and for training and evaluating systems for sarcasm detection.
The corpus has 1.3 million sarcastic statements -- 10 times more than any
previous dataset -- and many times more instances of non-sarcastic statements,
allowing for learning in regimes of both balanced and unbalanced labels. Each
statement is furthermore self-annotated -- sarcasm is labeled by the author and
not an independent annotator -- and provided with user, topic, and conversation
context. We evaluate the corpus for accuracy, compare it to previous related
corpora, and provide baselines for the task of sarcasm detection.



Knowledge bases are important resources for a variety of natural language
processing tasks but suffer from incompleteness. We propose a novel embedding
model, \emph{ITransF}, to perform knowledge base completion. Equipped with a
sparse attention mechanism, ITransF discovers hidden concepts of relations and
transfer statistical strength through the sharing of concepts. Moreover, the
learned associations between relations and concepts, which are represented by
sparse attention vectors, can be interpreted easily. We evaluate ITransF on two
benchmark datasets---WN18 and FB15k for knowledge base completion and obtains
improvements on both the mean rank and Hits@10 metrics, over all baselines that
do not use additional information.



The analysis of the current integration attempts of some modes and use cases
of user-machine interaction is presented. The new concept of the user-driven
intelligent interface is proposed on the basis of multimodal augmented reality
and brain-computer interaction for various applications: in disabilities
studies, education, home care, health care, etc. The several use cases of
multimodal augmentation are presented. The perspectives of the better human
comprehension by the immediate feedback through neurophysical channels by means
of brain-computer interaction are outlined. It is shown that brain-computer
interface (BCI) technology provides new strategies to overcome limits of the
currently available user interfaces, especially for people with functional
disabilities. The results of the previous studies of the low end consumer and
open-source BCI-devices allow us to conclude that combination of machine
learning (ML), multimodal interactions (visual, sound, tactile) with BCI will
profit from the immediate feedback from the actual neurophysical reactions
classified by ML methods. In general, BCI in combination with other modes of AR
interaction can deliver much more information than these types of interaction
themselves. Even in the current state the combined AR-BCI interfaces could
provide the highly adaptable and personal services, especially for people with
functional disabilities.



Monte Carlo Tree Search (MCTS), most famously used in game-play artificial
intelligence (e.g., the game of Go), is a well-known strategy for constructing
approximate solutions to sequential decision problems. Its primary innovation
is the use of a heuristic, known as a default policy, to obtain Monte Carlo
estimates of downstream values for states in a decision tree. This information
is used to iteratively expand the tree towards regions of states and actions
that an optimal policy might visit. However, to guarantee convergence to the
optimal action, MCTS requires the entire tree to be expanded asymptotically. In
this paper, we propose a new technique called Primal-Dual MCTS that utilizes
sampled information relaxation upper bounds on potential actions, creating the
possibility of "ignoring" parts of the tree that stem from highly suboptimal
choices. This allows us to prove that despite converging to a partial decision
tree in the limit, the recommended action from Primal-Dual MCTS is optimal. The
new approach shows significant promise when used to optimize the behavior of a
single driver navigating a graph while operating on a ride-sharing platform.
Numerical experiments on a real dataset of 7,000 trips in New Jersey suggest
that Primal-Dual MCTS improves upon standard MCTS by producing deeper decision
trees and exhibits a reduced sensitivity to the size of the action space.



For effective treatment of Alzheimer disease (AD), it is important to
identify subjects who are most likely to exhibit rapid cognitive decline.
Herein, we developed a novel framework based on a deep convolutional neural
network which can predict future cognitive decline in mild cognitive impairment
(MCI) patients using flurodeoxyglucose and florbetapir positron emission
tomography (PET). The architecture of the network only relies on baseline PET
studies of AD and normal subjects as the training dataset. Feature extraction
and complicated image preprocessing including nonlinear warping are unnecessary
for our approach. Accuracy of prediction (84.2%) for conversion to AD in MCI
patients outperformed conventional feature-based quantification approaches. ROC
analyses revealed that performance of CNN-based approach was significantly
higher than that of the conventional quantification methods (p < 0.05). Output
scores of the network were strongly correlated with the longitudinal change in
cognitive measurements. These results show the feasibility of deep learning as
a tool for predicting disease outcome using brain images.



We present a baseline approach for cross-modal knowledge fusion. Different
basic fusion methods are evaluated on existing embedding approaches to show the
potential of joining knowledge about certain concepts across modalities in a
fused concept representation.



We consider the problem of diagnosis where a set of simple observations are
used to infer a potentially complex hidden hypothesis. Finding the optimal
subset of observations is intractable in general, thus we focus on the problem
of active diagnosis, where the agent selects the next most-informative
observation based on the results of previous observations. We show that under
the assumption of uniform observation entropy, one can build an implication
model which directly predicts the outcome of the potential next observation
conditioned on the results of past observations, and selects the observation
with the maximum entropy. This approach enjoys reduced computation complexity
by bypassing the complicated hypothesis space, and can be trained on
observation data alone, learning how to query without knowledge of the hidden
hypothesis.



Grids allow users flexible on-demand usage of computing resources through
remote communication networks. A remarkable example of a Grid in High Energy
Physics (HEP) research is used in the ALICE experiment at European Organization
for Nuclear Research CERN. Physicists can submit jobs used to process the huge
amount of particle collision data produced by the Large Hadron Collider (LHC).
Grids face complex security challenges. They are interesting targets for
attackers seeking for huge computational resources. Since users can execute
arbitrary code in the worker nodes on the Grid sites, special care should be
put in this environment. Automatic tools to harden and monitor this scenario
are required. Currently, there is no integrated solution for such requirement.
This paper describes a new security framework to allow execution of job
payloads in a sandboxed context. It also allows process behavior monitoring to
detect intrusions, even when new attack methods or zero day vulnerabilities are
exploited, by a Machine Learning approach. We plan to implement the proposed
framework as a software prototype that will be tested as a component of the
ALICE Grid middleware.



Relation detection is a core component for many NLP applications including
Knowledge Base Question Answering (KBQA). In this paper, we propose a
hierarchical recurrent neural network enhanced by residual learning that
detects KB relations given an input question. Our method uses deep residual
bidirectional LSTMs to compare questions and relation names via different
hierarchies of abstraction. Additionally, we propose a simple KBQA system that
integrates entity linking and our proposed relation detector to enable one
enhance another. Experimental results evidence that our approach achieves not
only outstanding relation detection performance, but more importantly, it helps
our KBQA system to achieve state-of-the-art accuracy for both single-relation
(SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.



Singleton arc consistency is an important type of local consistency which has
been recently shown to solve all constraint satisfaction problems (CSPs) over
constraint languages of bounded width. We aim to characterise all classes of
CSPs defined by a forbidden pattern that are solved by singleton arc
consistency and closed under removing constraints. We identify five new
patterns whose absence ensures solvability by singleton arc consistency, four
of which are provably maximal and three of which generalise 2-SAT. Combined
with simple counter-examples for other patterns, we make significant progress
towards a complete classification.



Though the deep learning is pushing the machine learning to a new stage,
basic theories of machine learning are still limited. The principle of
learning, the role of the a prior knowledge, the role of neuron bias, and the
basis for choosing neural transfer function and cost function, etc., are still
far from clear. In this paper, we present a general theoretical framework for
machine learning. We classify the prior knowledge into common and
problem-dependent parts, and consider that the aim of learning is to maximally
incorporate them. The principle we suggested for maximizing the former is the
design risk minimization principle, while the neural transfer function, the
cost function, as well as pretreatment of samples, are endowed with the role
for maximizing the latter. The role of the neuron bias is explained from a
different angle. We develop a Monte Carlo algorithm to establish the
input-output responses, and we control the input-output sensitivity of a
learning machine by controlling that of individual neurons. Applications of
function approaching and smoothing, pattern recognition and classification, are
provided to illustrate how to train general learning machines based on our
theory and algorithm. Our method may in addition induce new applications, such
as the transductive inference.



Our goal is to create a convenient natural language interface for performing
well-specified but complex actions such as analyzing data, manipulating text,
and querying databases. However, existing natural language interfaces for such
tasks are quite primitive compared to the power one wields with a programming
language. To bridge this gap, we start with a core programming language and
allow users to "naturalize" the core language incrementally by defining
alternative, more natural syntax and increasingly complex concepts in terms of
compositions of simpler ones. In a voxel world, we show that a community of
users can simultaneously teach a common system a diverse language and use it to
build hundreds of complex voxel structures. Over the course of three days,
these users went from using only the core language to using the naturalized
language in 85.9\% of the last 10K utterances.



Visual question answering (QA) has attracted a lot of attention lately, seen
essentially as a form of (visual) Turing test that artificial intelligence
should strive to achieve. In this paper, we study a crucial component of this
task: how can we design good datasets for the task? We focus on the design of
multiple-choice based datasets where the learner has to select the right answer
from a set of candidate ones including the target (i.e. the correct one) and
the decoys (i.e. the incorrect ones). Through careful analysis of the results
attained by state-of-the-art learning models and human annotators on existing
datasets, we show the design of the decoy answers has a significant impact on
how and what the learning models learn from the datasets. In particular, the
resulting learner can ignore the visual information, the question, or the both
while still doing well on the task. Inspired by this, we propose automatic
procedures to remedy such design deficiencies. We apply the procedures to
re-construct decoy answers for two popular visual QA datasets as well as to
create a new visual QA dataset from the Visual Genome project, resulting in the
largest dataset for this task. Extensive empirical studies show that the design
deficiencies have been alleviated in the remedied datasets and the performance
on them is likely a more faithful indicator of the difference among learning
models. The datasets are released and publicly available via
http://www.teds.usc.edu/website_vqa/.



Video captioning, the task of describing the content of a video, has seen
some promising improvements in recent years with sequence-to-sequence models,
but accurately learning the temporal and logical dynamics involved in the task
still remains a challenge, especially given the lack of sufficient annotated
data. We improve video captioning by sharing knowledge with two related
directed-generation tasks: a temporally-directed unsupervised video prediction
task to learn richer context-aware video encoder representations, and a
logically-directed language entailment generation task to learn better
video-entailed caption decoder representations. For this, we present a
many-to-many multi-task learning model that shares parameters across the
encoders and decoders of the three tasks. We achieve significant improvements
and the new state-of-the-art on several standard video captioning datasets
using diverse automatic and human evaluations. We also show mutual multi-task
improvements on the entailment generation task.



There is a wide gap between symbolic reasoning and deep learning. In this
research, we explore the possibility of using deep learning to improve symbolic
reasoning. Briefly, in a reasoning system, a deep feedforward neural network is
used to guide rewriting processes after learning from algebraic reasoning
examples produced by humans. To enable the neural network to recognise patterns
of algebraic expressions with non-deterministic sizes, reduced partial trees
are used to represent the expressions. Also, to represent both top-down and
bottom-up information of the expressions, a centralisation technique is used to
improve the reduced partial trees. Besides, symbolic association vectors and
rule application records are used to improve the rewriting processes.
Experimental results reveal that the algebraic reasoning examples can be
accurately learnt only if the feedforward neural network has enough hidden
layers. Also, the centralisation technique, the symbolic association vectors
and the rule application records can reduce error rates of reasoning. In
particular, the above approaches have led to 4.6% error rate of reasoning on a
dataset of linear equations, differentials and integrals.



Tasks like code generation and semantic parsing require mapping unstructured
(or partially structured) inputs to well-formed, executable outputs. We
introduce abstract syntax networks, a modeling framework for these problems.
The outputs are represented as abstract syntax trees (ASTs) and constructed by
a decoder with a dynamically-determined modular structure paralleling the
structure of the output tree. On the benchmark Hearthstone dataset for code
generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy,
compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we
perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with
no task-specific engineering.



In emotion recognition, it is difficult to recognize human's emotional states
using just a single modality. Besides, the annotation of physiological
emotional data is particularly expensive. These two aspects make the building
of effective emotion recognition model challenging. In this paper, we first
build a multi-view deep generative model to simulate the generative process of
multi-modality emotional data. By imposing a mixture of Gaussians assumption on
the posterior approximation of the latent variables, our model can learn the
shared deep representation from multiple modalities. To solve the
labeled-data-scarcity problem, we further extend our multi-view model to
semi-supervised learning scenario by casting the semi-supervised classification
problem as a specialized missing data imputation task. Our semi-supervised
multi-view deep generative framework can leverage both labeled and unlabeled
data from multiple modalities, where the weight factor for each modality can be
learned automatically. Compared with previous emotion recognition methods, our
method is more robust and flexible. The experiments conducted on two real
multi-modal emotion datasets have demonstrated the superiority of our framework
over a number of competitors.



Decoding human brain activities via functional magnetic resonance imaging
(fMRI) has gained increasing attention in recent years. While encouraging
results have been reported in brain states classification tasks, reconstructing
the details of human visual experience still remains difficult. Two main
challenges that hinder the development of effective models are the perplexing
fMRI measurement noise and the high dimensionality of limited data instances.
Existing methods generally suffer from one or both of these issues and yield
dissatisfactory results. In this paper, we tackle this problem by casting the
reconstruction of visual stimulus as the Bayesian inference of missing view in
a multiview latent variable model. Sharing a common latent representation, our
joint generative model of external stimulus and brain response is not only
"deep" in extracting nonlinear features from visual images, but also powerful
in capturing correlations among voxel activities of fMRI recordings. The
nonlinearity and deep structure endow our model with strong representation
ability, while the correlations of voxel activities are critical for
suppressing noise and improving prediction. We devise an efficient variational
Bayesian method to infer the latent variables and the model parameters. To
further improve the reconstruction accuracy, the latent representations of
testing instances are enforced to be close to that of their neighbours from the
training set via posterior regularization. Experiments on three fMRI recording
datasets demonstrate that our approach can more accurately reconstruct visual
stimuli.



We propose a simple, yet effective, approach towards inducing multilingual
taxonomies from Wikipedia. Given an English taxonomy, our approach leverages
the interlanguage links of Wikipedia followed by character-level classifiers to
induce high-precision, high-coverage taxonomies in other languages. Through
experiments, we demonstrate that our approach significantly outperforms the
state-of-the-art, heuristics-heavy approaches for six languages. As a
consequence of our work, we release presumably the largest and the most
accurate multilingual taxonomic resource spanning over 280 languages.



We propose a novel, semi-supervised approach towards domain taxonomy
induction from an input vocabulary of seed terms. Unlike all previous
approaches, which typically extract direct hypernym edges for terms, our
approach utilizes a novel probabilistic framework to extract hypernym
subsequences. Taxonomy induction from extracted subsequences is cast as an
instance of the minimumcost flow problem on a carefully designed directed
graph. Through experiments, we demonstrate that our approach outperforms
stateof- the-art taxonomy induction approaches across four languages.
Importantly, we also show that our approach is robust to the presence of noise
in the input vocabulary. To the best of our knowledge, no previous approaches
have been empirically proven to manifest noise-robustness in the input
vocabulary.



Our goal is to learn a semantic parser that maps natural language utterances
into executable programs when only indirect supervision is available: examples
are labeled with the correct execution result, but not the program itself.
Consequently, we must search the space of programs for those that output the
correct result, while not being misled by spurious programs: incorrect programs
that coincidentally output the correct result. We connect two common learning
paradigms, reinforcement learning (RL) and maximum marginal likelihood (MML),
and then present a new learning algorithm that combines the strengths of both.
The new algorithm guards against spurious programs by combining the systematic
search traditionally employed in MML with the randomized exploration of RL, and
by updating parameters such that probability is spread more evenly across
consistent programs. We apply our learning algorithm to a new neural semantic
parser and show significant gains over existing state-of-the-art results on a
recent context-dependent semantic parsing task.



We introduce an attention-based Bi-LSTM for Chinese implicit discourse
relations and demonstrate that modeling argument pairs as a joint sequence can
outperform word order-agnostic approaches. Our model benefits from a partial
sampling scheme and is conceptually simple, yet achieves state-of-the-art
performance on the Chinese Discourse Treebank. We also visualize its attention
activity to illustrate the model's ability to selectively focus on the relevant
parts of an input sequence.



The aim of process discovery, originating from the area of process mining, is
to discover a process model based on business process execution data. A
majority of process discovery techniques relies on an event log as an input. An
event log is a static source of historical data capturing the execution of a
business process. In this paper we focus on process discovery relying on online
streams of business process execution events. Learning process models from
event streams poses both challenges and opportunities, i.e. we need to handle
unlimited amounts of data using finite memory and, preferably, constant time.
We propose a generic architecture that allows for adopting several classes of
existing process discovery techniques in context of event streams. Moreover, we
provide several instantiations of the architecture, accompanied by
implementations in the process mining tool-kit ProM (http://promtools.org).
Using these instantiations, we evaluate several dimensions of stream-based
process discovery. The evaluation shows that the proposed architecture allows
us to lift process discovery to the streaming domain.



This paper introduces a generalization of Convolutional Neural Networks
(CNNs) from low-dimensional grid data, such as images, to graph-structured
data. We propose a novel spatial convolution utilizing a random walk to uncover
the relations within the input, analogous to the way the standard convolution
uses the spatial neighborhood of a pixel on the grid. The convolution has an
intuitive interpretation, is efficient and scalable and can also be used on
data with varying graph structure. Furthermore, this generalization can be
applied to many standard regression or classification problems, by learning the
the underlying graph. We empirically demonstrate the performance of the
proposed CNN on MNIST, and challenge the state-of-the-art on Merck molecular
activity data set.



Wit is a quintessential form of rich inter-human interaction, and is often
grounded in a specific situation (e.g., a comment in response to an event). In
this work, we attempt to build computational models that can produce witty
descriptions for a given image. Inspired by a cognitive account of humor
appreciation, we employ linguistic wordplay, specifically puns. We compare our
approach against meaningful baseline approaches via human studies. In a Turing
test style evaluation, people find our model's description for an image to be
wittier than a human's witty description 55% of the time!



Visual Question Answering (VQA) has received a lot of attention over the past
couple of years. A number of deep learning models have been proposed for this
task. However, it has been shown that these models are heavily driven by
superficial correlations in the training data and lack compositionality -- the
ability to answer questions about unseen compositions of seen concepts. This
compositionality is desirable and central to intelligence. In this paper, we
propose a new setting for Visual Question Answering where the test
question-answer pairs are compositionally novel compared to training
question-answer pairs. To facilitate developing models under this setting, we
present a new compositional split of the VQA v1.0 dataset, which we call
Compositional VQA (C-VQA). We analyze the distribution of questions and answers
in the C-VQA splits. Finally, we evaluate several existing VQA models under
this new setting and show that the performances of these models degrade by a
significant amount compared to the original VQA setting.



With the recent advancements in Artificial Intelligence (AI), various
organizations and individuals started debating about the progress of AI as a
blessing or a curse for the future of the society. This paper conducts an
investigation on how the public perceives the progress of AI by utilizing the
data shared on Twitter. Specifically, this paper performs a comparative
analysis on the understanding of users from two categories -- general
AI-Tweeters (AIT) and the expert AI-Tweeters (EAIT) who share posts about AI on
Twitter. Our analysis revealed that users from both the categories express
distinct emotions and interests towards AI. Users from both the categories
regard AI as positive and are optimistic about the progress of AI but the
experts are more negative than the general AI-Tweeters. Characterization of
users manifested that `London' is the popular location of users from where they
tweet about AI. Tweets posted by AIT are highly retweeted than posts made by
EAIT that reveals greater diffusion of information from AIT.



Word embeddings provide point representations of words containing useful
semantic information. We introduce multimodal word distributions formed from
Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty
information. To learn these distributions, we propose an energy-based
max-margin objective. We show that the resulting approach captures uniquely
expressive semantic information, and outperforms alternatives, such as word2vec
skip-grams, and Gaussian embeddings, on benchmark datasets such as word
similarity and entailment.



A central goal in cancer genomics is to identify the somatic alterations that
underpin tumor initiation and progression. This task is challenging as the
mutational profiles of cancer genomes exhibit vast heterogeneity, with many
alterations observed within each individual, few shared somatically mutated
genes across individuals, and important roles in cancer for both frequently and
infrequently mutated genes. While commonly mutated cancer genes are readily
identifiable, those that are rarely mutated across samples are difficult to
distinguish from the large numbers of other infrequently mutated genes. Here,
we introduce a method that considers per-individual mutational profiles within
the context of protein-protein interaction networks in order to identify small
connected subnetworks of genes that, while not individually frequently mutated,
comprise pathways that are perturbed across (i.e., "cover") a large fraction of
the individuals. We devise a simple yet intuitive objective function that
balances identifying a small subset of genes with covering a large fraction of
individuals. We show how to solve this problem optimally using integer linear
programming and also give a fast heuristic algorithm that works well in
practice. We perform a large-scale evaluation of our resulting method, nCOP, on
6,038 TCGA tumor samples across 24 different cancer types. We demonstrate that
our approach nCOP is more effective in identifying cancer genes than both
methods that do not utilize any network information as well as state-of-the-art
network-based methods that aggregate mutational information across individuals.
Overall, our work demonstrates the power of combining per-individual mutational
information with interaction networks in order to uncover genes functionally
relevant in cancers, and in particular those genes that are less frequently
mutated.



One of the most challenging tasks when adopting Bayesian Networks (BNs) is
the one of learning their structure from data. This task is complicated by the
huge search space of possible solutions and turned out to be a well-known
NP-hard problem and, hence, approximations are required. However, to the best
of our knowledge, a quantitative analysis of the performance and
characteristics of the different heuristics to solve this problem has never
been done before.
  For this reason, in this work, we provide a detailed study of the different
state-of-the-arts methods for structural learning on simulated data considering
both BNs with discrete and continuous variables, and with different rates of
noise in the data. In particular, we investigate the characteristics of
different widespread scores proposed for the inference and the statistical
pitfalls within them.



Blind deblurring consists a long studied task, however the outcomes of
generic methods are not effective in real world blurred images. Domain-specific
methods for deblurring targeted object categories, e.g. text or faces,
frequently outperform their generic counterparts, hence they are attracting an
increasing amount of attention. In this work, we develop such a domain-specific
method to tackle deblurring of human faces, henceforth referred to as face
deblurring. Studying faces is of tremendous significance in computer vision,
however face deblurring has yet to demonstrate some convincing results. This
can be partly attributed to the combination of i) poor texture and ii) highly
structure shape that yield the contour/gradient priors (that are typically
used) sub-optimal. In our work instead of making assumptions over the prior, we
adopt a learning approach by inserting weak supervision that exploits the
well-documented structure of the face. Namely, we utilise a deep network to
perform the deblurring and employ a face alignment technique to pre-process
each face. We additionally surpass the requirement of the deep network for
thousands training samples, by introducing an efficient framework that allows
the generation of a large dataset. We utilised this framework to create 2MF2, a
dataset of over two million frames. We conducted experiments with real world
blurred facial images and report that our method returns a result close to the
sharp natural latent image.



We introduce Parseval networks, a form of deep neural networks in which the
Lipschitz constant of linear, convolutional and aggregation layers is
constrained to be smaller than 1. Parseval networks are empirically and
theoretically motivated by an analysis of the robustness of the predictions
made by deep neural networks when their input is subject to an adversarial
perturbation. The most important feature of Parseval networks is to maintain
weight matrices of linear and convolutional layers to be (approximately)
Parseval tight frames, which are extensions of orthogonal matrices to
non-square matrices. We describe how these constraints can be maintained
efficiently during SGD. We show that Parseval networks match the
state-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House
Numbers (SVHN) while being more robust than their vanilla counterpart against
adversarial examples. Incidentally, Parseval networks also tend to train faster
and make a better usage of the full capacity of the networks.



We present SuperPivot, an analysis method for low-resource languages that
occur in a superparallel corpus, i.e., in a corpus that contains an order of
magnitude more languages than parallel corpora currently in use. We show that
SuperPivot performs well for the crosslingual analysis of the linguistic
phenomenon of tense. We produce analysis results for more than 1000 languages,
conducting - to the best of our knowledge - the largest crosslingual
computational study performed to date. We extend existing methodology for
leveraging parallel corpora for typological analysis by overcoming a limiting
assumption of earlier work: We only require that a linguistic feature is
overtly marked in a few of thousands of languages as opposed to requiring that
it be marked in all languages under investigation.



Different from other sequential data, sentences in natural language are
structured by linguistic grammars. Previous generative conversational models
with chain-structured decoder ignore this structure in human language and might
generate plausible responses with less satisfactory relevance and fluency. In
this study, we aim to incorporate the results from linguistic analysis into the
process of sentence generation for high-quality conversation generation.
Specifically, we use a dependency parser to transform each response sentence
into a dependency tree and construct a training corpus of sentence-tree pairs.
A tree-structured decoder is developed to learn the mapping from a sentence to
its tree, where different types of hidden states are used to depict the local
dependencies from an internal tree node to its children. For training
acceleration, we propose a tree canonicalization method, which transforms trees
into equivalent ternary trees. Then, with a proposed tree-structured search
method, the model is able to generate the most probable responses in the form
of dependency trees, which are finally flattened into sequences as the system
output. Experimental results demonstrate that the proposed X2Tree framework
outperforms baseline methods over 11.15% increase of acceptance ratio.



Mental illnesses adversely affect a significant proportion of the population
worldwide. However, the methods traditionally used for estimating and
characterizing the prevalence of mental health conditions are time-consuming
and expensive. Consequently, best-available estimates concerning the prevalence
of mental health conditions are often years out of date. Automated approaches
to supplement these survey methods with broad, aggregated information derived
from social media content provides a potential means for near real-time
estimates at scale. These may, in turn, provide grist for supporting,
evaluating and iteratively improving upon public health programs and
interventions.
  We propose a novel model for automated mental health status quantification
that incorporates user embeddings. This builds upon recent work exploring
representation learning methods that induce embeddings by leveraging social
media post histories. Such embeddings capture latent characteristics of
individuals (e.g., political leanings) and encode a soft notion of homophily.
In this paper, we investigate whether user embeddings learned from twitter post
histories encode information that correlates with mental health statuses. To
this end, we estimated user embeddings for a set of users known to be affected
by depression and post-traumatic stress disorder (PTSD), and for a set of
demographically matched `control' users. We then evaluated these embeddings
with respect to: (i) their ability to capture homophilic relations with respect
to mental health status; and (ii) the performance of downstream mental health
prediction models based on these features. Our experimental results demonstrate
that the user embeddings capture similarities between users with respect to
mental conditions, and are predictive of mental health.



Application programming interfaces (APIs) offer a plethora of functionalities
for developers to reuse without reinventing the wheel. Identifying the
appropriate APIs given a project requirement is critical for the success of a
project, as many functionalities can be reused to achieve faster development.
However, the massive number of APIs would often hinder the developers' ability
to quickly find the right APIs. In this light, we propose a new, automated
approach called WebAPIRec that takes as input a project profile and outputs a
ranked list of {web} APIs that can be used to implement the project. At its
heart, WebAPIRec employs a personalized ranking model that ranks web APIs
specific (personalized) to a project. Based on the historical data of {web} API
usages, WebAPIRec learns a model that minimizes the incorrect ordering of web
APIs, i.e., when a used {web} API is ranked lower than an unused (or a
not-yet-used) web API. We have evaluated our approach on a dataset comprising
9,883 web APIs and 4,315 web application projects from ProgrammableWeb with
promising results. For 84.0% of the projects, WebAPIRec is able to successfully
return correct APIs that are used to implement the projects in the top-5
positions. This is substantially better than the recommendations provided by
ProgrammableWeb's native search functionality. WebAPIRec also outperforms
McMillan et al.'s application search engine and popularity-based
recommendation.



While artificial intelligence (AI) has become widespread, many commercial AI
systems are not yet accessible to individual researchers nor the general public
due to the deep knowledge of the systems required to use them. We believe that
AI has matured to the point where it should be an accessible technology for
everyone. We present an ongoing project whose ultimate goal is to deliver an
open source, user-friendly AI system that is specialized for machine learning
analysis of complex data in the biomedical and health care domains. We discuss
how genetic programming can aid in this endeavor, and highlight specific
examples where genetic programming has automated machine learning analyses in
previous projects.



In recent studies [1][13][12] Recurrent Neural Networks were used for
generative processes and their surprising performance can be explained by their
ability to create good predictions. In addition, data compression is also based
on predictions. What the problem comes down to is whether a data compressor
could be used to perform as well as recurrent neural networks in natural
language processing tasks. If this is possible,then the problem comes down to
determining if a compression algorithm is even more intelligent than a neural
network in specific tasks related to human language. In our journey we
discovered what we think is the fundamental difference between a Data
Compression Algorithm and a Recurrent Neural Network.



Impressive image captioning results are achieved in domains with plenty of
training image and sentence pairs (e.g., MSCOCO). However, transferring to a
target domain with significant domain shifts but no paired training data
(referred to as cross-domain image captioning) remains largely unexplored. We
propose a novel adversarial training procedure to leverage unpaired data in the
target domain. Two critic networks are introduced to guide the captioner,
namely domain critic and multi-modal critic. The domain critic assesses whether
the generated sentences are indistinguishable from sentences in the target
domain. The multi-modal critic assesses whether an image and its generated
sentence are a valid pair. During training, the critics and captioner act as
adversaries -- captioner aims to generate indistinguishable sentences, whereas
critics aim at distinguishing them. The assessment improves the captioner
through policy gradient updates. During inference, we further propose a novel
critic-based planning method to select high-quality sentences without
additional supervision (e.g., tags). To evaluate, we use MSCOCO as the source
domain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k)
as the target domains. Our method consistently performs well on all datasets.
In particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after
adaptation. Utilizing critics during inference further gives another 4.5%
boost.



Dempster-Shafer evidence theory is wildly applied in multi-sensor data
fusion. However, lots of uncertainty and interference exist in practical
situation, especially in the battle field. It is still an open issue to model
the reliability of sensor reports. Many methods are proposed based on the
relationship among collected data. In this letter, we proposed a quantum
mechanical approach to evaluate the reliability of sensor reports, which is
based on the properties of a sensor itself. The proposed method is used to
modify the combining of evidences.



The deployment of Artificial Neural Networks (ANNs) in safety-critical
applications poses a number of new verification and certification challenges.
In particular, for ANN-enabled self-driving vehicles it is important to
establish properties about the resilience of ANNs to noisy or even maliciously
manipulated sensory input. We are addressing these challenges by defining
resilience properties of ANN-based classifiers as the maximal amount of input
or sensor perturbation which is still tolerated. This problem of computing
maximal perturbation bounds for ANNs is then reduced to solving mixed integer
optimization problems (MIP). A number of MIP encoding heuristics are developed
for drastically reducing MIP-solver runtimes, and using parallelization of
MIP-solvers results in an almost linear speed-up in the number (up to a certain
limit) of computing cores in our experiments. We demonstrate the effectiveness
and scalability of our approach by means of computing maximal resilience bounds
for a number of ANN benchmark sets ranging from typical image recognition
scenarios to the autonomous maneuvering of robots.



We present an approach for the verification of feed-forward neural networks
in which all nodes have a piece-wise linear activation function. Such networks
are often used in deep learning and have been shown to be hard to verify for
modern satisfiability modulo theory (SMT) and integer linear programming (ILP)
solvers.
  The starting point of our approach is the addition of a global linear
approximation of the overall network behavior to the verification problem that
helps with SMT-like reasoning over the network behavior. We present a
specialized verification algorithm that employs this approximation in a search
process in which it infers additional node phases for the non-linear nodes in
the network from partial node phase assignments, similar to unit propagation in
classical SAT solving. We also show how to infer additional conflict clauses
and safe node fixtures from the results of the analysis steps performed during
the search. The resulting approach is evaluated on collision avoidance and
handwritten digit recognition case studies.



In the era of big data, k-means clustering has been widely adopted as a basic
processing tool in various contexts. However, its computational cost could be
prohibitively high as the data size and the cluster number are large. It is
well known that the processing bottleneck of k-means lies in the operation of
seeking closest centroid in each iteration. In this paper, a novel solution
towards the scalability issue of k-means is presented. In the proposal, k-means
is supported by an approximate k-nearest neighbors graph. In the k-means
iteration, each data sample is only compared to clusters that its nearest
neighbors reside. Since the number of nearest neighbors we consider is much
less than k, the processing cost in this step becomes minor and irrelevant to
k. The processing bottleneck is therefore overcome. The most interesting thing
is that k-nearest neighbor graph is constructed by iteratively calling the fast
$k$-means itself. Comparing with existing fast k-means variants, the proposed
algorithm achieves hundreds to thousands times speed-up while maintaining high
clustering quality. As it is tested on 10 million 512-dimensional data, it
takes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the
same scale of clustering, it would take 3 years for traditional k-means.



This paper introduces an SLD-resolution technique based on deep learning.
This technique enables neural networks to learn from old and successful
resolution processes and to use learnt experiences to guide new resolution
processes. An implementation of this technique is named SLDR-DL. It includes a
Prolog library of deep feedforward neural networks and some essential functions
of resolution. In the SLDR-DL framework, users can define logical rules in the
form of definite clauses and teach neural networks to use the rules in
reasoning processes.



Application of models to data is fraught. Data-generating collaborators often
only have a very basic understanding of the complications of collating,
processing and curating data. Challenges include: poor data collection
practices, missing values, inconvenient storage mechanisms, intellectual
property, security and privacy. All these aspects obstruct the sharing and
interconnection of data, and the eventual interpretation of data through
machine learning or other approaches. In project reporting, a major challenge
is in encapsulating these problems and enabling goals to be built around the
processing of data. Project overruns can occur due to failure to account for
the amount of time required to curate and collate. But to understand these
failures we need to have a common language for assessing the readiness of a
particular data set. This position paper proposes the use of data readiness
levels: it gives a rough outline of three stages of data preparedness and
speculates on how formalisation of these levels into a common language for data
readiness could facilitate project management.



Large-scale multi-relational embedding refers to the task of learning the
latent representations for entities and relations in large knowledge graphs. An
effective and scalable solution for this problem is crucial for the true
success of knowledge-based inference in a broad range of applications. This
paper proposes a novel framework for optimizing the latent representations with
respect to the \textit{analogical} properties of the embedded entities and
relations. By formulating the learning objective in a differentiable fashion,
our model enjoys both theoretical power and computational scalability, and
significantly outperformed a large number of representative baseline methods on
benchmark datasets. Furthermore, the model offers an elegant unification of
several well-known methods in multi-relational embedding, which can be proven
to be special instantiations of our framework.



We propose a new reinforcement learning algorithm for partially observable
Markov decision processes (POMDP) based on spectral decomposition methods.
While spectral methods have been previously employed for consistent learning of
(passive) latent variable models such as hidden Markov models, POMDPs are more
challenging since the learner interacts with the environment and possibly
changes the future observations in the process. We devise a learning algorithm
running through epochs, in each epoch we employ spectral techniques to learn
the POMDP parameters from a trajectory generated by a fixed policy. At the end
of the epoch, an optimization oracle returns the optimal memoryless planning
policy which maximizes the expected reward based on the estimated POMDP model.
We prove an order-optimal regret bound with respect to the optimal memoryless
policy and efficient scaling with respect to the dimensionality of observation
and action spaces.



Understanding and discovering knowledge from GPS (Global Positioning System)
traces of human activities is an essential topic in mobility-based urban
computing. We propose TrajectoryNet-a neural network architecture for
point-based trajectory classification to infer real world human transportation
modes from GPS traces. To overcome the challenge of capturing the underlying
latent factors in the low-dimensional and heterogeneous feature space imposed
by GPS data, we develop a novel representation that embeds the original feature
space into another space that can be understood as a form of basis expansion.
We also enrich the feature space via segment-based information and use Maxout
activations to improve the predictive power of Recurrent Neural Networks
(RNNs). We achieve over 98% classification accuracy when detecting four types
of transportation modes, outperforming existing models without additional
sensory data or location-based prior knowledge.



Wearable computing is one of the fastest growing technologies today. Smart
watches are poised to take over at least of half the wearable devices market in
the near future. Smart watch screen size, however, is a limiting factor for
growth, as it restricts practical text input. On the other hand, wearable
devices have some features, such as consistent user interaction and hands-free,
heads-up operations, which pave the way for gesture recognition methods of text
entry. This paper proposes a new text input method for smart watches, which
utilizes motion sensor data and machine learning approaches to detect letters
written in the air by a user. This method is less computationally intensive and
less expensive when compared to computer vision approaches. It is also not
affected by lighting factors, which limit computer vision solutions. The
AirDraw system prototype developed to test this approach is presented.
Additionally, experimental results close to 71% accuracy are presented.



Consumers often react expressively to products such as food samples, perfume,
jewelry, sunglasses, and clothing accessories. This research discusses a
multimodal affect recognition system developed to classify whether a consumer
likes or dislikes a product tested at a counter or kiosk, by analyzing the
consumer's facial expression, body posture, hand gestures, and voice after
testing the product. A depth-capable camera and microphone system - Kinect for
Windows - is utilized. An emotion identification engine has been developed to
analyze the images and voice to determine affective state of the customer. The
image is segmented using skin color and adaptive threshold. Face, body and
hands are detected using the Haar cascade classifier. Canny edges are
identified and the lip, body and hand contours are extracted using spatial
filtering. Edge count and orientation around the mouth, cheeks, eyes,
shoulders, fingers and the location of the edges are used as features.
Classification is done by an emotion template mapping algorithm and training a
classifier using support vector machines. The real-time performance, accuracy
and feasibility for multimodal affect recognition in feedback assessment are
evaluated.



Machine learning has become pervasive in multiple domains, impacting a wide
variety of applications, such as knowledge discovery and data mining, natural
language processing, information retrieval, computer vision, social and health
informatics, ubiquitous computing, etc. Two essential problems of machine
learning are how to generate features and how to acquire labels for machines to
learn. Particularly, labeling large amount of data for each domain-specific
problem can be very time consuming and costly. It has become a key obstacle in
making learning protocols realistic in applications. In this paper, we will
discuss how to use the existing general-purpose world knowledge to enhance
machine learning processes, by enriching the features or reducing the labeling
work. We start from the comparison of world knowledge with domain-specific
knowledge, and then introduce three key problems in using world knowledge in
learning processes, i.e., explicit and implicit feature representation,
inference for knowledge linking and disambiguation, and learning with direct or
indirect supervision. Finally we discuss the future directions of this research
topic.



Predicting the efficacy of a drug for a given individual, using
high-dimensional genomic measurements, is at the core of precision medicine.
However, identifying features on which to base the predictions remains a
challenge, especially when the sample size is small. Incorporating expert
knowledge offers a promising alternative to improve a prediction model, but
collecting such knowledge is laborious to the expert if the number of candidate
features is very large. We introduce a probabilistic model that can incorporate
expert feedback about the impact of genomic measurements on the sensitivity of
a cancer cell for a given drug. We also present two methods to intelligently
collect this feedback from the expert, using experimental design and
multi-armed bandit models. In a multiple myeloma blood cancer data set (n=51),
expert knowledge decreased the prediction error by 8%. Furthermore, the
intelligent approaches can be used to reduce the workload of feedback
collection to less than 30% on average compared to a naive approach.



In process mining, precision measures are used to quantify how much a process
model overapproximates the behavior seen in an event log. Although several
measures have been proposed throughout the years, no research has been done to
validate whether these measures achieve the intended aim of quantifying
over-approximation in a consistent way for all models and logs. This paper
fills this gap by postulating a number of axioms for quantifying precision
consistently for any log and any model. Further, we show through
counter-examples that none of the existing measures consistently quantifies
precision.



We propose a logic of asynchronous announcements, where truthful
announcements are publicly sent but individually received by agents. Additional
to epistemic modalities, the logic therefore contains two types of dynamic
modalities, for sending messages and for receiving messages. The semantics
defines truth relative to the current state of reception of messages for all
agents. This means that knowledge need not be truthful, because some messages
may not have been received by the knowing agent. Messages that are
announcements may also result in partial synchronization, namely when an agent
learns from receiving an announcement that other announcements must already
have been received by other agents. We give detailed examples of the semantics,
and prove several semantic results, including that: after an announcement an
agent knows that a proposition is true, if and only if on condition of the
truth of that announcement, the agent knows that after that announcement and
after any number of other agents also receiving it, the proposition is true. We
show that on multi-agent epistemic models, each formula in asynchronous
announcement logic is equivalent to a formula in epistemic logic.



Spoken Language Understanding (SLU) is a key component of goal oriented
dialogue systems that would parse user utterances into semantic frame
representations. Traditionally SLU does not utilize the dialogue history beyond
the previous system turn and contextual ambiguities are resolved by the
downstream components. In this paper, we explore novel approaches for modeling
dialogue context in a recurrent neural network (RNN) based language
understanding system. We propose the Sequential Dialogue Encoder Network, that
allows encoding context from the dialogue history in chronological order. We
compare the performance of our proposed architecture with two context models,
one that uses just the previous turn context and another that encodes dialogue
context in a memory network, but loses the order of utterances in the dialogue
history. Experiments with a multi-domain dialogue dataset demonstrate that the
proposed architecture results in reduced semantic frame error rates.



Continuous/Lifelong learning of high-dimensional data streams is a
challenging research problem. In fact, fully retraining models each time new
data become available is infeasible, due to computational and storage issues,
while na\"ive incremental strategies have been shown to suffer from
catastrophic forgetting. In the context of real-world object recognition
applications (e.g., robotic vision), where continuous learning is crucial, very
few datasets and benchmarks are available to evaluate and compare emerging
techniques. In this work we propose a new dataset and benchmark CORe50,
specifically designed for continuous object recognition, and introduce baseline
approaches for different continuous learning scenarios.



We present a new deep meta reinforcement learner, which we call Deep Episodic
Value Iteration (DEVI). DEVI uses a deep neural network to learn a similarity
metric for a non-parametric model-based reinforcement learning algorithm. Our
model is trained end-to-end via back-propagation. Despite being trained using
the model-free Q-learning objective, we show that DEVI's model-based internal
structure provides `one-shot' transfer to changes in reward and transition
structure, even for tasks with very high-dimensional state spaces.



We consider a novel formulation of the multi-armed bandit model, which we
call the contextual bandit with restricted context, where only a limited number
of features can be accessed by the learner at every iteration. This novel
formulation is motivated by different online problems arising in clinical
trials, recommender systems and attention modeling. Herein, we adapt the
standard multi-armed bandit algorithm known as Thompson Sampling to take
advantage of our restricted context setting, and propose two novel algorithms,
called the Thompson Sampling with Restricted Context(TSRC) and the Windows
Thompson Sampling with Restricted Context(WTSRC), for handling stationary and
nonstationary environments, respectively. Our empirical results demonstrate
advantages of the proposed approaches on several real-life datasets



Visual question answering (or VQA) is a new and exciting problem that
combines natural language processing and computer vision techniques. We present
a survey of the various datasets and models that have been used to tackle this
task. The first part of the survey details the various datasets for VQA and
compares them along some common factors. The second part of this survey details
the different approaches for VQA, classified into four types: non-deep learning
models, deep learning models without attention, deep learning models with
attention, and other models which do not fit into the first three. Finally, we
compare the performances of these approaches and provide some directions for
future work.



Solving algebraic word problems requires executing a series of arithmetic
operations---a program---to obtain a final answer. However, since programs can
be arbitrarily complicated, inducing them directly from question-answer pairs
is a formidable challenge. To make this task more feasible, we solve these
problems by generating answer rationales, sequences of natural language and
human-readable mathematical expressions that derive the final answer through a
series of small steps. Although rationales do not explicitly specify programs,
they provide a scaffolding for their structure via intermediate milestones. To
evaluate our approach, we have created a new 100,000-sample dataset of
questions, answers and rationales. Experimental results show that indirect
supervision of program learning via answer rationales is a promising strategy
for inducing arithmetic programs.



Humans make complex inferences on faces, ranging from objective properties
(gender, ethnicity, expression, age, identity, etc) to subjective judgments
(facial attractiveness, trustworthiness, sociability, friendliness, etc). While
the objective aspects of face perception have been extensively studied,
relatively fewer computational models have been developed for the social
impressions of faces. Bridging this gap, we develop a method to predict human
impressions of faces in 40 subjective social dimensions, using deep
representations from state-of-the-art neural networks. We find that model
performance grows as the human consensus on a face trait increases, and that
model predictions outperform human groups in correlation with human averages.
This illustrates the learnability of subjective social perception of faces,
especially when there is high human consensus. Our system can be used to decide
which photographs from a personal collection will make the best impression. The
results are significant for the field of social robotics, demonstrating that
robots can learn the subjective judgments defining the underlying fabric of
human interaction.



Existing methods for arterial blood pressure (BP) estimation directly map the
input physiological signals to output BP values without explicitly modeling the
underlying temporal dependencies in BP dynamics. As a result, these models
suffer from accuracy decay over a long time and thus require frequent
calibration. In this work, we address this issue by formulating BP estimation
as a sequence prediction problem in which both the input and target are
temporal sequences. We propose a novel deep recurrent neural network (RNN)
consisting of multilayered Long Short-Term Memory (LSTM) networks, which are
incorporated with (1) a bidirectional structure to access larger-scale context
information of input sequence, and (2) residual connections to allow gradients
in deep RNN to propagate more effectively. The proposed deep RNN model was
tested on a static BP dataset, and it achieved root mean square error (RMSE) of
3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction
respectively, surpassing the accuracy of traditional BP prediction models. On a
multi-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81
mmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP
prediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction,
respectively, which outperforms all previous models with notable improvement.
The experimental results suggest that modeling the temporal dependencies in BP
dynamics significantly improves the long-term BP prediction accuracy.



We propose an algorithm to separate simultaneously speaking persons from each
other, the "cocktail party problem", using a single microphone. Our approach
involves a deep recurrent neural networks regression to a vector space that is
descriptive of independent speakers. Such a vector space can embed empirically
determined speaker characteristics and is optimized by distinguishing between
speaker masks. We call this technique source-contrastive estimation. The
methodology is inspired by negative sampling, which has seen success in natural
language processing, where an embedding is learned by correlating and
de-correlating a given input vector with output weights. Although the matrix
determined by the output weights is dependent on a set of known speakers, we
only use the input vectors during inference. Doing so will ensure that source
separation is explicitly speaker-independent. Our approach is similar to recent
deep neural network clustering and permutation-invariant training research; we
use weighted spectral features and masks to augment individual speaker
frequencies while filtering out other speakers. We avoid, however, the severe
computational burden of other approaches with our technique. Furthermore, by
training a vector space rather than combinations of different speakers or
differences thereof, we avoid the so-called permutation problem during
training. Our algorithm offers an intuitive, computationally efficient response
to the cocktail party problem, and most importantly boasts better empirical
performance than other current techniques.



We consider an extension of the set covering problem (SCP) introducing
(i)~multicover and (ii)~generalized upper bound (GUB)~constraints. For the
conventional SCP, the pricing method has been introduced to reduce the size of
instances, and several efficient heuristic algorithms based on such reduction
techniques have been developed to solve large-scale instances. However, GUB
constraints often make the pricing method less effective, because they often
prevent solutions from containing highly evaluated variables together. To
overcome this problem, we develop heuristic algorithms to reduce the size of
instances, in which new evaluation schemes of variables are introduced taking
account of GUB constraints. We also develop an efficient implementation of a
2-flip neighborhood local search algorithm that reduces the number of
candidates in the neighborhood without sacrificing the solution quality. In
order to guide the search to visit a wide variety of good solutions, we also
introduce a path relinking method that generates new solutions by combining two
or more solutions obtained so far. According to computational comparison on
benchmark instances, the proposed method succeeds in selecting a small number
of promising variables properly and performs quite effectively even for
large-scale instances having hard GUB constraints.



It has long been assumed that high dimensional continuous control problems
cannot be solved effectively by discretizing individual dimensions of the
action space due to the exponentially large number of bins over which policies
would have to be learned. In this paper, we draw inspiration from the recent
success of sequence-to-sequence models for structured prediction problems to
develop policies over discretized spaces. Central to this method is the
realization that complex functions over high dimensional spaces can be modeled
by neural networks that use next step prediction. Specifically, we show how
Q-values and policies over continuous spaces can be modeled using a next step
prediction model over discretized dimensions. With this parameterization, it is
possible to both leverage the compositional structure of action spaces during
learning, as well as compute maxima over action spaces (approximately). On a
simple example task we demonstrate empirically that our method can perform
global search, which effectively gets around the local optimization issues that
plague DDPG and NAF. We apply the technique to off-policy (Q-learning) methods
and show that our method can achieve the state-of-the-art for off-policy
methods on several continuous control tasks.



Developing and testing algorithms for autonomous vehicles in real world is an
expensive and time consuming process. Also, in order to utilize recent advances
in machine intelligence and deep learning we need to collect a large amount of
annotated training data in a variety of conditions and environments. We present
a new simulator built on Unreal Engine that offers physically and visually
realistic simulations for both of these goals. Our simulator includes a physics
engine that can operate at a high frequency for real-time hardware-in-the-loop
(HITL) simulations with support for popular protocols (e.g. MavLink). The
simulator is designed from the ground up to be extensible to accommodate new
types of vehicles, hardware platforms and software protocols. In addition, the
modular design enables various components to be easily usable independently in
other projects. We demonstrate the simulator by first implementing a quadrotor
as an autonomous vehicle and then experimentally comparing the software
components with real-world flights.



This work demonstrates how to accelerate dense linear algebra computations
using CLBlast, an open-source OpenCL BLAS library providing optimized routines
for a wide variety of devices. It is targeted at machine learning and HPC
applications and thus provides a fast matrix-multiplication routine (GEMM) to
accelerate the core of many applications (e.g. deep learning, iterative
solvers, astrophysics, computational fluid dynamics, quantum chemistry).
CLBlast has four main advantages over other BLAS libraries: 1) it is optimized
for and tested on a large variety of OpenCL devices including less commonly
used devices such as embedded and low-power GPUs, 2) it can be explicitly tuned
for specific problem-sizes on specific hardware platforms, 3) it can perform
operations in half-precision floating-point FP16 saving precious bandwidth,
time and energy, 4) and it can combine multiple operations in a single batched
routine, accelerating smaller problems significantly. This paper describes the
library and demonstrates the advantages of CLBlast experimentally for different
use-cases on a wide variety of OpenCL hardware.



Although learning-based methods have great potential for robotics, one
concern is that a robot that updates its parameters might cause large amounts
of damage before it learns the optimal policy. We formalize the idea of safe
learning in a probabilistic sense by defining an optimization problem: we
desire to maximize the expected return while keeping the expected damage below
a given safety limit. We study this optimization for the case of a robot
manipulator with safety-based torque limits. We would like to ensure that the
damage constraint is maintained at every step of the optimization and not just
at convergence. To achieve this aim, we introduce a novel method which predicts
how modifying the torque limit, as well as how updating the policy parameters,
might affect the robot's safety. We show through a number of experiments that
our approach allows the robot to improve its performance while ensuring that
the expected damage constraint is not violated during the learning process.



Probabilistic modeling enables combining domain knowledge with learning from
data, thereby supporting learning from fewer training instances than purely
data-driven methods. However, learning probabilistic models is difficult and
has not achieved the level of performance of methods such as deep neural
networks on many tasks. In this paper, we attempt to address this issue by
presenting a method for learning the parameters of a probabilistic program
using backpropagation. Our approach opens the possibility to building deep
probabilistic programming models that are trained in a similar way to neural
networks.



There has recently been significant interest in hard attention models for
tasks such as object recognition, visual captioning and speech recognition.
Hard attention can offer benefits over soft attention such as decreased
computational cost, but training hard attention models can be difficult because
of the discrete latent variables they introduce. Previous work used REINFORCE
and Q-learning to approach these issues, but those methods can provide
high-variance gradient estimates and be slow to train. In this paper, we tackle
the problem of learning hard attention for a sequential task using variational
inference methods, specifically the recently introduced VIMCO and NVIL.
Furthermore, we propose a novel baseline that adapts VIMCO to this setting. We
demonstrate our method on a phoneme recognition task in clean and noisy
environments and show that our method outperforms REINFORCE, with the
difference being greater for a more complicated task.



An optimal warping path between two time series is generally not unique. The
size and form of the set of pairs of time series with non-unique optimal
warping path is unknown. This article shows that optimal warping paths are
unique for almost every pair of time series in a measure-theoretic sense. All
pairs of time series with non-unique optimal warping path form a negligible set
and are geometrically the union of zero sets of quadratic forms. The result is
useful for analyzing and understanding adaptive learning methods in dynamic
time warping spaces.



Knowledge bases (KBs) have attracted increasing attention due to its great
success in various areas, such as Web and mobile search.Existing KBs are
restricted to objective factual knowledge, such as city population or fruit
shape, whereas,subjective knowledge, such as big city, which is commonly
mentioned in Web and mobile queries, has been neglected. Subjective knowledge
differs from objective knowledge in that it has no documented or observed
ground truth. Instead, the truth relies on people's dominant opinion. Thus, we
can use the crowdsourcing technique to get opinion from the crowd. In our work,
we propose a system, called crowdsourced subjective knowledge acquisition
(CoSKA),for subjective knowledge acquisition powered by crowdsourcing and
existing KBs. The acquired knowledge can be used to enrich existing KBs in the
subjective dimension which bridges the gap between existing objective knowledge
and subjective queries.The main challenge of CoSKA is the conflict between
large scale knowledge facts and limited crowdsourcing resource. To address this
challenge, in this work, we define knowledge inference rules and then select
the seed knowledge judiciously for crowdsourcing to maximize the inference
power under the resource constraint. Our experimental results on real knowledge
base and crowdsourcing platform verify the effectiveness of CoSKA system.



The availability of large scale event data with time stamps has given rise to
dynamically evolving knowledge graphs that contain temporal information for
each edge. Reasoning over time in such dynamic knowledge graphs is not yet well
understood. To this end, we present Know-Evolve, a novel deep evolutionary
knowledge network that learns non-linearly evolving entity representations over
time. The occurrence of a fact (edge) is modeled as a multivariate point
process whose intensity function is modulated by the score for that fact
computed based on the learned entity embeddings. We demonstrate significantly
improved performance over various relational learning approaches on two large
scale real-world datasets. Further, our method effectively predicts occurrence
or recurrence time of a fact which is novel compared to prior reasoning
approaches in multi-relational setting.



Latent features learned by deep learning approaches have proven to be a
powerful tool for machine learning. They serve as a data abstraction that makes
learning easier by capturing regularities in data explicitly. Their benefits
motivated their adaptation to relational learning context. In our previous
work, we introduce an approach that learns relational latent features by means
of clustering instances and their relations. The major drawback of latent
representations is that they are often black-box and difficult to interpret.
This work addresses these issues and shows that (1) latent features created by
clustering are interpretable and capture interesting properties of data; (2)
they identify local regions of instances that match well with the label, which
partially explains their benefit; and (3) although the number of latent
features generated by this approach is large, often many of them are highly
redundant and can be removed without hurting performance much.



In this report, an automated bartender system was developed for making orders
in a bar using hand gestures. The gesture recognition of the system was
developed using Machine Learning techniques, where the model was trained to
classify gestures using collected data. The final model used in the system
reached an average accuracy of 95%. The system raised ethical concerns both in
terms of user interaction and having such a system in a real world scenario,
but it could initially work as a complement to a real bartender.



The sense of touch, being the earliest sensory system to develop in a human
body [1], plays a critical part of our daily interaction with the environment.
In order to successfully complete a task, many manipulation interactions
require incorporating haptic feedback. However, manually designing a feedback
mechanism can be extremely challenging. In this work, we consider manipulation
tasks that need to incorporate tactile sensor feedback in order to modify a
provided nominal plan. To incorporate partial observation, we present a new
framework that models the task as a partially observable Markov decision
process (POMDP) and learns an appropriate representation of haptic feedback
which can serve as the state for a POMDP model. The model, that is parametrized
by deep recurrent neural networks, utilizes variational Bayes methods to
optimize the approximate posterior. Finally, we build on deep Q-learning to be
able to select the optimal action in each state without access to a simulator.
We test our model on a PR2 robot for multiple tasks of turning a knob until it
clicks.



Recent approaches based on artificial neural networks (ANNs) have shown
promising results for named-entity recognition (NER). In order to achieve high
performances, ANNs need to be trained on a large labeled dataset. However,
labels might be difficult to obtain for the dataset on which the user wants to
perform NER: label scarcity is particularly pronounced for patient note
de-identification, which is an instance of NER. In this work, we analyze to
what extent transfer learning may address this issue. In particular, we
demonstrate that transferring an ANN model trained on a large labeled dataset
to another dataset with a limited number of labels improves upon the
state-of-the-art results on two different datasets for patient note
de-identification.



Reinforcement learning is a powerful technique to train an agent to perform a
task. However, an agent that is trained using reinforcement learning is only
capable of achieving the single task that is specified via its reward function.
Such an approach does not scale well to settings in which an agent needs to
perform a diverse set of tasks, such as navigating to varying positions in a
room or moving objects to varying locations. Instead, we propose a method that
allows an agent to automatically discover the range of tasks that it is capable
of performing. We use a generator network to propose tasks for the agent to try
to achieve, specified as goal states. The generator network is optimized using
adversarial training to produce tasks that are always at the appropriate level
of difficulty for the agent. Our method thus automatically produces a
curriculum of tasks for the agent to learn. We show that, by using this
framework, an agent can efficiently and automatically learn to perform a wide
set of tasks without requiring any prior knowledge of its environment. Our
method can also learn to achieve tasks with sparse rewards, which traditionally
pose significant challenges.



We introduce a package service model where trucks as well as drones can
deliver packages. Drones can travel on trucks or fly; but while flying, drones
can only carry one package at a time and have to return to a truck to charge
after each delivery. We present a heuristic algorithm to solve the problem of
finding a good schedule for all drones and trucks. The algorithm is based on
two nested local searches, thus the definition of suitable neighbourhoods of
solutions is crucial for the algorithm. Empirical tests show that our algorithm
performs significantly better than a natural Greedy algorithm. Moreover, the
savings compared to solutions without drones turn out to be substantial,
suggesting that delivery systems might considerably benefit from using drones
in addition to trucks.



We introduce a stepping methodology for answer-set programming (ASP) that
allows for debugging answer-set programs and is based on the stepwise
application of rules. Similar to debugging in imperative languages, where the
behaviour of a program is observed during a step-by-step execution, stepping
for ASP allows for observing the effects that rule applications have in the
computation of an answer set. While the approach is inspired from debugging in
imperative programming, it is conceptually different to stepping in other
paradigms due to non-determinism and declarativity that are inherent to ASP. In
particular, unlike statements in an imperative program that are executed
following a strict control flow, there is no predetermined order in which to
consider rules in ASP during a computation. In our approach, the user is free
to decide which rule to consider active in the next step following his or her
intuition. This way, one can focus on interesting parts of the debugging search
space. Bugs are detected during stepping by revealing differences between the
actual semantics of the program and the expectations of the user. As a solid
formal basis for stepping, we develop a framework of computations for
answer-set programs. For fully supporting different solver languages, we build
our framework on an abstract ASP language that is sufficiently general to
capture different solver languages. To this end, we make use of abstract
constraints as an established abstraction for popular language constructs such
as aggregates. Stepping has been implemented in SeaLion, an integrated
development environment for ASP. We illustrate stepping using an example
scenario and discuss the stepping plugin of SeaLion. Moreover, we elaborate on
methodological aspects and the embedding of stepping in the ASP development
process.



The sure thing principle and the law of total probability are basic laws in
classic probability theory. A disjunction fallacy leads to the violation of
these two classical laws. In this paper, an Evidential Markov (EM) decision
making model based on Dempster-Shafer (D-S) evidence theory and Markov
modelling is proposed to address this issue and model the real human
decision-making process. In an evidential framework, the states are extended by
introducing an uncertain state which represents the hesitance of a decision
maker. The classical Markov model can not produce the disjunction effect, which
assumes that a decision has to be certain at one time. However, the state is
allowed to be uncertain in the EM model before the final decision is made. An
extra uncertainty degree parameter is defined by a belief entropy, named Deng
entropy, to assignment the basic probability assignment of the uncertain state,
which is the key to predict the disjunction effect. A classical categorization
decision-making experiment is used to illustrate the effectiveness and validity
of EM model. The disjunction effect can be well predicted and the free
parameters are less compared with the existing models.



By utilizing different communication channels, such as verbal language,
gestures or facial expressions, virtually embodied interactive humans hold a
unique potential to bridge the gap between human-computer interaction and
actual interhuman communication. The use of virtual humans is consequently
becoming increasingly popular in a wide range of areas where such a natural
communication might be beneficial, including entertainment, education, mental
health research and beyond. Behind this development lies a series of
technological advances in a multitude of disciplines, most notably natural
language processing, computer vision, and speech synthesis. In this paper we
discuss a Virtual Human Journalist, a project employing a number of novel
solutions from these disciplines with the goal to demonstrate their viability
by producing a humanoid conversational agent capable of naturally eliciting and
reacting to information from a human user. A set of qualitative and
quantitative evaluation sessions demonstrated the technical feasibility of the
system whilst uncovering a number of deficits in its capacity to engage users
in a way that would be perceived as natural and emotionally engaging. We argue
that naturalness should not always be seen as a desirable goal and suggest that
deliberately suppressing the naturalness of virtual human interactions, such as
by altering its personality cues, might in some cases yield more desirable
results.



Infrared (IR) imaging has the potential to enable more robust action
recognition systems compared to visible spectrum cameras due to lower
sensitivity to lighting conditions and appearance variability. While the action
recognition task on videos collected from visible spectrum imaging has received
much attention, action recognition in IR videos is significantly less explored.
Our objective is to exploit imaging data in this modality for the action
recognition task. In this work, we propose a novel two-stream 3D convolutional
neural network (CNN) architecture by introducing the discriminative code layer
and the corresponding discriminative code loss function. The proposed network
processes IR image and the IR-based optical flow field sequences. We pretrain
the 3D CNN model on the visible spectrum Sports-1M action dataset and finetune
it on the Infrared Action Recognition (InfAR) dataset. To our best knowledge,
this is the first application of the 3D CNN to action recognition in the IR
domain. We conduct an elaborate analysis of different fusion schemes (weighted
average, single and double-layer neural nets) applied to different 3D CNN
outputs. Experimental results demonstrate that our approach can achieve
state-of-the-art average precision (AP) performances on the InfAR dataset: (1)
the proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our
3D CNN model applied to the optical flow fields achieves the best reported
single stream 75.42% AP.



Motivated by the common academic problem of allocating papers to referees for
conference reviewing we propose a novel mechanism for solving the assignment
problem when we have a two sided matching problem with preferences from one
side (the agents/reviewers) over the other side (the objects/papers) and both
sides have capacity constraints. The assignment problem is a fundamental
problem in both computer science and economics with application in many areas
including task and resource allocation. We draw inspiration from multi-criteria
decision making and voting and use order weighted averages (OWAs) to propose a
novel and flexible class of algorithms for the assignment problem. We show an
algorithm for finding a $\Sigma$-OWA assignment in polynomial time, in contrast
to the NP-hardness of finding an egalitarian assignment. Inspired by this
setting we observe an interesting connection between our model and the classic
proportional multi-winner election problem in social choice.



The asynchronous nature of the state-of-the-art reinforcement learning
algorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes
them exceptionally suitable for CPU computations. However, given the fact that
deep reinforcement learning often deals with interpreting visual information, a
large part of the train and inference time is spent performing convolutions. In
this work we present our results on learning strategies in Atari games using a
Convolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0
machine learning framework. We also analyze effects of asynchronous
computations on the convergence of reinforcement learning algorithms.



We propose an efficient method to estimate the accuracy of classifiers using
only unlabeled data. We consider a setting with multiple classification
problems where the target classes may be tied together through logical
constraints. For example, a set of classes may be mutually exclusive, meaning
that a data instance can belong to at most one of them. The proposed method is
based on the intuition that: (i) when classifiers agree, they are more likely
to be correct, and (ii) when the classifiers make a prediction that violates
the constraints, at least one classifier must be making an error. Experiments
on four real-world data sets produce accuracy estimates within a few percent of
the true accuracy, using solely unlabeled data. Our models also outperform
existing state-of-the-art solutions in both estimating accuracies, and
combining multiple classifier outputs. The results emphasize the utility of
logical constraints in estimating accuracy, thus validating our intuition.



Many different methods to train deep generative models have been introduced
in the past. In this paper, we propose to extend the variational auto-encoder
(VAE) framework with a new type of prior which we call "Variational Mixture of
Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture
distribution (e.g., a mixture of Gaussians) with components given by
variational posteriors conditioned on learnable pseudo-inputs. We further
extend this prior to a two layer hierarchical model and show that this
architecture with a coupled prior and posterior, learns significantly better
models. The model also avoids the usual local optima issues related to useless
latent dimensions that plague VAEs. We provide empirical studies on six
datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,
Frey Faces and Histopathology patches, and show that applying the hierarchical
VampPrior delivers state-of-the-art results on all datasets in the unsupervised
permutation invariant setting and the best results or comparable to SOTA
methods for the approach with convolutional networks.



Approximate probabilistic inference algorithms are central to many fields.
Examples include sequential Monte Carlo inference in robotics, variational
inference in machine learning, and Markov chain Monte Carlo inference in
statistics. A key problem faced by practitioners is measuring the accuracy of
an approximate inference algorithm on a specific data set. This paper
introduces the auxiliary inference divergence estimator (AIDE), an algorithm
for measuring the accuracy of approximate inference algorithms. AIDE is based
on the observation that inference algorithms can be treated as probabilistic
models and the random variables used within the inference algorithm can be
viewed as auxiliary variables. This view leads to a new estimator for the
symmetric KL divergence between the approximating distributions of two
inference algorithms. The paper illustrates application of AIDE to algorithms
for inference in regression, hidden Markov, and Dirichlet process mixture
models. The experiments show that AIDE captures the qualitative behavior of a
broad class of inference algorithms and can detect failure modes of inference
algorithms that are missed by standard heuristics.



The Particle Swarm Optimization Policy (PSO-P) has been recently introduced
and proven to produce remarkable results on interacting with academic
reinforcement learning benchmarks in an off-policy, batch-based setting. To
further investigate the properties and feasibility on real-world applications,
this paper investigates PSO-P on the so-called Industrial Benchmark (IB), a
novel reinforcement learning (RL) benchmark that aims at being realistic by
including a variety of aspects found in industrial applications, like
continuous state and action spaces, a high dimensional, partially observable
state space, delayed effects, and complex stochasticity. The experimental
results of PSO-P on IB are compared to results of closed-form control policies
derived from the model-based Recurrent Control Neural Network (RCNN) and the
model-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not
only of interest for academic benchmarks, but also for real-world industrial
applications, since it also yielded the best performing policy in our IB
setting. Compared to other well established RL techniques, PSO-P produced
outstanding results in performance and robustness, requiring only a relatively
low amount of effort in finding adequate parameters or making complex design
decisions.



In this paper, we extend an attention-based neural machine translation (NMT)
model by allowing it to access an entire training set of parallel sentence
pairs even after training. The proposed approach consists of two stages. In the
first stage--retrieval stage--, an off-the-shelf, black-box search engine is
used to retrieve a small subset of sentence pairs from a training set given a
source sentence. These pairs are further filtered based on a fuzzy matching
score based on edit distance. In the second stage--translation stage--, a novel
translation model, called translation memory enhanced NMT (TM-NMT), seamlessly
uses both the source sentence and a set of retrieved sentence pairs to perform
the translation. Empirical evaluation on three language pairs (En-Fr, En-De,
and En-Es) shows that the proposed approach significantly outperforms the
baseline approach and the improvement is more significant when more relevant
sentence pairs were retrieved.



We consider goods that can be shared with k-hop neighbors (i.e., the set of
nodes within k hops from an owner) on a social network. We examine incentives
to buy such a good by devising game-theoretic models where each node decides
whether to buy the good or free ride. First, we find that social inefficiency,
specifically excessive purchase of the good, occurs in Nash equilibria. Second,
the social inefficiency decreases as k increases and thus a good can be shared
with more nodes. Third, and most importantly, the social inefficiency can also
be significantly reduced by charging free riders an access cost and paying it
to owners, leading to the conclusion that organizations and system designers
should impose such a cost. These findings are supported by our theoretical
analysis in terms of the price of anarchy and the price of stability; and by
simulations based on synthetic and real social networks.



Thompson sampling has emerged as an effective heuristic for a broad range of
online decision problems. In its basic form, the algorithm requires computing
and sampling from a posterior distribution over models, which is tractable only
for simple special cases. This paper develops ensemble sampling, which aims to
approximate Thompson sampling while maintaining tractability even in the face
of complex models such as neural networks. Ensemble sampling dramatically
expands on the range of applications for which Thompson sampling is viable. We
establish a theoretical basis that supports the approach and present
computational results that offer further insight.



Word embeddings improve the performance of NLP systems by revealing the
hidden structural relationships between words. These models have recently risen
in popularity due to the performance of scalable algorithms trained in the big
data setting. Despite their success, word embeddings have seen very little use
in computational social science NLP tasks, presumably due to their reliance on
big data, and to a lack of interpretability. I propose a probabilistic
model-based word embedding method which can recover interpretable embeddings,
without big data. The key insight is to leverage the notion of mixed membership
modeling, in which global representations are shared, but individual entities
(i.e. dictionary words) are free to use these representations to uniquely
differing degrees. Leveraging connections to topic models, I show how to train
these models in high dimensions using a combination of state-of-the-art
techniques for word embeddings and topic modeling. Experimental results show an
improvement in predictive performance of up to 63% in MRR over the skip-gram on
small datasets. The models are interpretable, as embeddings of topics are used
to encode embeddings for words (and hence, documents) in a model-based way. I
illustrate this with two computational social science case studies, on NIPS
articles and State of the Union addresses.



Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN)
have achieved state-of-the-art results in a variety of challenging,
high-dimensional domains. This success is mainly attributed to the power of
deep neural networks to learn rich domain representations for approximating the
value function or policy. Batch reinforcement learning methods with linear
representations, on the other hand, are more stable and require less hyper
parameter tuning. Yet, substantial feature engineering is necessary to achieve
good results. In this work we propose a hybrid approach -- the Least Squares
Deep Q-Network (LS-DQN), which combines rich feature representations learned by
a DRL algorithm with the stability of a linear least squares method. We do this
by periodically re-training the last hidden layer of a DRL network with a batch
least squares update. Key to our approach is a Bayesian regularization term for
the least squares update, which prevents over-fitting to the more recent data.
We tested LS-DQN on five Atari games and demonstrate significant improvement
over vanilla DQN and Double-DQN. We also investigated the reasons for the
superior performance of our method. Interestingly, we found that the
performance improvement can be attributed to the large batch size used by the
LS method when optimizing the last layer.



We propose a general framework for entropy-regularized average-reward
reinforcement learning in Markov decision processes (MDPs). Our approach is
based on extending the linear-programming formulation of policy optimization in
MDPs to accommodate convex regularization functions. Our key result is showing
that using the conditional entropy of the joint state-action distributions as
regularization yields a dual optimization problem closely resembling the
Bellman optimality equations. This result enables us to formalize a number of
state-of-the-art entropy-regularized reinforcement learning algorithms as
approximate variants of Mirror Descent or Dual Averaging, and thus to argue
about the convergence properties of these methods. In particular, we show that
the exact version of the TRPO algorithm of Schulman et al. (2015) actually
converges to the optimal policy, while the entropy-regularized policy gradient
methods of Mnih et al. (2016) may fail to converge to a fixed point. Finally,
we illustrate empirically the effects of using various regularization
techniques on learning performance in a simple reinforcement learning setup.



Understanding why a model makes a certain prediction can be as crucial as the
prediction's accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches.



Representation learning has become an invaluable approach for learning from
symbolic data such as text and graphs. However, while complex symbolic datasets
often exhibit a latent hierarchical structure, state-of-the-art methods
typically learn embeddings in Euclidean vector spaces, which do not account for
this property. For this purpose, we introduce a new approach for learning
hierarchical representations of symbolic data by embedding them into hyperbolic
space -- or more precisely into an n-dimensional Poincar\'e ball. Due to the
underlying hyperbolic geometry, this allows us to learn parsimonious
representations of symbolic data by simultaneously capturing hierarchy and
similarity. We introduce an efficient algorithm to learn the embeddings based
on Riemannian optimization and show experimentally that Poincar\'e embeddings
outperform Euclidean embeddings significantly on data with latent hierarchies,
both in terms of representation capacity and in terms of generalization
ability.



The design and analysis of communication systems typically rely on the
development of mathematical models that describe the underlying communication
channel, which dictates the relationship between the transmitted and the
received signals. However, in some systems, such as molecular communication
systems where chemical signals are used for transfer of information, it is not
possible to accurately model this relationship. In these scenarios, because of
the lack of mathematical channel models, a completely new approach to design
and analysis is required. In this work, we focus on one important aspect of
communication systems, the detection algorithms, and demonstrate that by
borrowing tools from deep learning, it is possible to train detectors that
perform well, without any knowledge of the underlying channel models. We
evaluate these algorithms using experimental data that is collected by a
chemical communication platform, where the channel model is unknown and
difficult to model analytically. We show that deep learning algorithms perform
significantly better than a simple detector that was used in previous works,
which also did not assume any knowledge of the channel.



Evaluating expression of the Human epidermal growth factor receptor 2 (Her2)
by visual examination of immunohistochemistry (IHC) on invasive breast cancer
(BCa) is a key part of the diagnostic assessment of BCa due to its recognised
importance as a predictive and prognostic marker in clinical practice. However,
visual scoring of Her2 is subjective and consequently prone to inter-observer
variability. Given the prognostic and therapeutic implications of Her2 scoring,
a more objective method is required. In this paper, we report on a recent
automated Her2 scoring contest, held in conjunction with the annual PathSoc
meeting held in Nottingham in June 2016, aimed at systematically comparing and
advancing the state-of-the-art Artificial Intelligence (AI) based automated
methods for Her2 scoring. The contest dataset comprised of digitised whole
slide images (WSI) of sections from 86 cases of invasive breast carcinoma
stained with both Haematoxylin & Eosin (H&E) and IHC for Her2. The contesting
algorithms automatically predicted scores of the IHC slides for an unseen
subset of the dataset and the predicted scores were compared with the 'ground
truth' (a consensus score from at least two experts). We also report on a
simple Man vs Machine contest for the scoring of Her2 and show that the
automated methods could beat the pathology experts on this contest dataset.
This paper presents a benchmark for comparing the performance of automated
algorithms for scoring of Her2. It also demonstrates the enormous potential of
automated algorithms in assisting the pathologist with objective IHC scoring.



Developments in deep generative models have allowed for tractable learning of
high-dimensional data distributions. While the employed learning procedures
typically assume that training data is drawn i.i.d. from the distribution of
interest, it may be desirable to model distinct distributions which are
observed sequentially, such as when different classes are encountered over
time. Although conditional variations of deep generative models permit multiple
distributions to be modeled by a single network in a disentangled fashion, they
are susceptible to catastrophic forgetting when the distributions are
encountered sequentially. In this paper, we adapt recent work in reducing
catastrophic forgetting to the task of training generative adversarial networks
on a sequence of distinct distributions, enabling continual generative
modeling.



No real-world reward function is perfect. Sensory errors and software bugs
may result in RL agents observing higher (or lower) rewards than they should.
For example, a reinforcement learning agent may prefer states where a sensory
error gives it the maximum reward, but where the true reward is actually small.
We formalise this problem as a generalised Markov Decision Problem called
Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under
strong simplifying assumptions and when trying to compensate for the possibly
corrupt rewards. Two ways around the problem are investigated. First, by giving
the agent richer data, such as in inverse reinforcement learning and
semi-supervised reinforcement learning, reward corruption stemming from
systematic sensory errors may sometimes be completely managed. Second, by using
randomisation to blunt the agent's optimisation, reward corruption can be
partially managed under some assumptions.



Recent work has shown that state-of-the-art classifiers are quite brittle, in
the sense that a small adversarial change of an originally with high confidence
correctly classified input leads to a wrong classification again with high
confidence. This raises concerns that such classifiers are vulnerable to
attacks and calls into question their usage in safety-critical systems. We show
in this paper for the first time formal guarantees on the robustness of a
classifier by giving instance-specific lower bounds on the norm of the input
manipulation required to change the classifier decision. Based on this analysis
we propose the Cross-Lipschitz regularization functional. We show that using
this form of regularization in kernel methods resp. neural networks improves
the robustness of the classifier without any loss in prediction performance.



Security surveillance is one of the most important issues in smart cities,
especially in an era of terrorism. Deploying a number of (video) cameras is a
common surveillance approach. Given the never-ending power offered by vehicles
to metropolises, exploiting vehicle traffic to design camera placement
strategies could potentially facilitate security surveillance. This article
constitutes the first effort toward building the linkage between vehicle
traffic and security surveillance, which is a critical problem for smart
cities. We expect our study could influence the decision making of surveillance
camera placement, and foster more research of principled ways of security
surveillance beneficial to our physical-world life.



A major challenge in designing neural network (NN) systems is to determine
the best structure and parameters for the network given the data for the
machine learning problem at hand. Examples of parameters are the number of
layers and nodes, the learning rates, and the dropout rates. Typically, these
parameters are chosen based on heuristic rules and manually fine-tuned, which
may be very time-consuming, because evaluating the performance of a single
parametrization of the NN may require several hours. This paper addresses the
problem of choosing appropriate parameters for the NN by formulating it as a
box-constrained mathematical optimization problem, and applying a
derivative-free optimization tool that automatically and effectively searches
the parameter space. The optimization tool employs a radial basis function
model of the objective function (the prediction accuracy of the NN) to
accelerate the discovery of configurations yielding high accuracy. Candidate
configurations explored by the algorithm are trained to a small number of
epochs, and only the most promising candidates receive full training. The
performance of the proposed methodology is assessed on benchmark sets and in
the context of predicting drug-drug interactions, showing promising results.
The optimization tool used in this paper is open-source.



Reinforcement learning is a powerful paradigm for learning optimal policies
from experimental data. However, to find optimal policies, most reinforcement
learning algorithms explore all possible actions, which may be harmful for
real-world systems. As a consequence, learning algorithms are rarely applied on
safety-critical systems in the real world. In this paper, we present a learning
algorithm that explicitly considers safety, defined in terms of stability
guarantees. Specifically, we extend control-theoretic results on Lyapunov
stability verification and show how to use statistical models of the dynamics
to obtain high-performance control policies with provable stability
certificates. Moreover, under additional regularity assumptions in terms of a
Gaussian process prior, we prove that one can effectively and safely collect
data in order to learn about the dynamics and thus both improve control
performance and expand the safe region of the state space. In our experiments,
we show how the resulting algorithm can safely optimize a neural network policy
on a simulated inverted pendulum, without the pendulum ever falling down.



Generative moment matching network (GMMN) is a deep generative model that
differs from Generative Adversarial Network (GAN) by replacing the
discriminator in GAN with a two-sample test based on kernel maximum mean
discrepancy (MMD). Although some theoretical guarantees of MMD have been
studied, the empirical performance of GMMN is still not as competitive as that
of GAN on challenging and large benchmark datasets. The computational
efficiency of GMMN is also less desirable in comparison with GAN, partially due
to its requirement for a rather large batch size during the training. In this
paper, we propose to improve both the model expressiveness of GMMN and its
computational efficiency by introducing adversarial kernel learning techniques,
as the replacement of a fixed Gaussian kernel in the original GMMN. The new
approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN.
The new distance measure in MMD GAN is a meaningful loss that enjoys the
advantage of weak topology and can be optimized via gradient descent with
relatively small batch sizes. In our evaluation on multiple benchmark datasets,
including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN
significantly outperforms GMMN, and is competitive with other representative
GAN works.



Attempts to train a comprehensive artificial intelligence capable of solving
multiple tasks have been impeded by a chronic problem called catastrophic
forgetting. Although simply replaying all previous data alleviates the problem,
it requires large memory and even worse, often infeasible in real world
applications where the access to past data is limited. Inspired by the
generative nature of hippocampus as a short-term memory system in primate
brain, we propose the Deep Generative Replay, a novel framework with a
cooperative dual model architecture consisting of a deep generative model
("generator") and a task solving model ("solver"). With only these two models,
training data for previous tasks can easily be sampled and interleaved with
those for a new task. We test our methods in several sequential learning
settings involving image classification tasks.



We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative-filtering methods to make unfair predictions for users from
minority groups. We identify the insufficiency of existing fairness metrics and
propose four new metrics that address different forms of unfairness. These
fairness metrics can be optimized by adding fairness terms to the learning
objective. Experiments on synthetic and real data show that our new metrics can
better measure fairness than the baseline, and that the fairness objectives
effectively help reduce unfairness.



The knowledge representation community has built general-purpose ontologies
which contain large amounts of commonsense knowledge over relevant aspects of
the world, including useful visual information, e.g.: "a ball is used by a
football player", "a tennis player is located at a tennis court". Current
state-of-the-art approaches for visual recognition do not exploit these
rule-based knowledge sources. Instead, they learn recognition models directly
from training examples. In this paper, we study how general-purpose
ontologies---specifically, MIT's ConceptNet ontology---can improve the
performance of state-of-the-art vision systems. As a testbed, we tackle the
problem of sentence-based image retrieval. Our retrieval approach incorporates
knowledge from ConceptNet on top of a large pool of object detectors derived
from a deep learning technique. In our experiments, we show that ConceptNet can
improve performance on a common benchmark dataset. Key to our performance is
the use of the ESPGAME dataset to select visually relevant relations from
ConceptNet. Consequently, a main conclusion of this work is that
general-purpose commonsense ontologies improve performance on visual reasoning
tasks when properly filtered to select meaningful visual relations.



Semi-supervised learning methods using Generative Adversarial Networks (GANs)
have shown promising empirical success recently. Most of these methods use a
shared discriminator/classifier which discriminates real examples from fake
while also predicting the class label. Motivated by the ability of the GANs
generator to capture the data manifold well, we propose to estimate the tangent
space to the data manifold using GANs and employ it to inject invariances into
the classifier. In the process, we propose enhancements over existing methods
for learning the inverse mapping (i.e., the encoder) which greatly improves in
terms of semantic similarity of the reconstructed sample with the input sample.
We observe considerable empirical gains in semi-supervised learning over
baselines, particularly in the cases when the number of labeled examples is
low. We also provide insights into how fake examples influence the
semi-supervised learning procedure.



Adversarial learning of probabilistic models has recently emerged as a
promising alternative to maximum likelihood. Implicit models such as generative
adversarial networks (GAN) often generate better samples compared to explicit
models trained by maximum likelihood. Yet, GANs sidestep the characterization
of an explicit density which makes quantitative evaluations challenging. To
bridge this gap, we propose Flow-GANs, a generative adversarial network for
which we can perform exact likelihood evaluation, thus supporting both
adversarial and maximum likelihood training. When trained adversarially,
Flow-GANs generate high-quality samples but attain extremely poor
log-likelihood scores, inferior even to a mixture model memorizing the training
data; the opposite is true when trained by maximum likelihood. Results on MNIST
and CIFAR-10 demonstrate that hybrid training can attain high held-out
likelihoods while retaining visual fidelity in the generated samples.



To run quantum algorithms on emerging gate-model quantum hardware, quantum
circuits must be compiled to take into account constraints on the hardware. For
near-term hardware, with only limited means to mitigate decoherence, it is
critical to minimize the duration of the circuit. We investigate the
application of temporal planners to the problem of compiling quantum circuits
to newly emerging quantum hardware. While our approach is general, we focus on
compiling to superconducting hardware architectures with nearest neighbor
constraints. Our initial experiments focus on compiling Quantum Alternating
Operator Ansatz (QAOA) circuits whose high number of commuting gates allow
great flexibility in the order in which the gates can be applied. That freedom
makes it more challenging to find optimal compilations but also means there is
a greater potential win from more optimized compilation than for less flexible
circuits. We map this quantum circuit compilation problem to a temporal
planning problem, and generated a test suite of compilation problems for QAOA
circuits of various sizes to a realistic hardware architecture. We report
compilation results from several state-of-the-art temporal planners on this
test set. This early empirical evaluation demonstrates that temporal planning
is a viable approach to quantum circuit compilation.



Event sequence, asynchronously generated with random timestamp, is ubiquitous
among applications. The precise and arbitrary timestamp can carry important
clues about the underlying dynamics, and has lent the event data fundamentally
different from the time-series whereby series is indexed with fixed and equal
time interval. One expressive mathematical tool for modeling event is point
process. The intensity functions of many point processes involve two
components: the background and the effect by the history. Due to its inherent
spontaneousness, the background can be treated as a time series while the other
need to handle the history events. In this paper, we model the background by a
Recurrent Neural Network (RNN) with its units aligned with time series indexes
while the history effect is modeled by another RNN whose units are aligned with
asynchronous events to capture the long-range dynamics. The whole model with
event type and timestamp prediction output layers can be trained end-to-end.
Our approach takes an RNN perspective to point process, and models its
background and history effect. For utility, our method allows a black-box
treatment for modeling the intensity which is often a pre-defined parametric
form in point processes. Meanwhile end-to-end training opens the venue for
reusing existing rich techniques in deep network for point process modeling. We
apply our model to the predictive maintenance problem using a log dataset by
more than 1000 ATMs from a global bank headquartered in North America.



Typical reinforcement learning (RL) agents learn to complete tasks specified
by reward functions tailored to their domain. As such, the policies they learn
do not generalize even to similar domains. To address this issue, we develop a
framework through which a deep RL agent learns to generalize policies from
smaller, simpler domains to more complex ones using a recurrent attention
mechanism. The task is presented to the agent as an image and an instruction
specifying the goal. This meta-controller guides the agent towards its goal by
designing a sequence of smaller subtasks on the part of the state space within
the attention, effectively decomposing it. As a baseline, we consider a setup
without attention as well. Our experiments show that the meta-controller learns
to create subgoals within the attention.



Incremental methods for structure learning of pairwise Markov random fields
(MRFs), such as grafting, improve scalability to large systems by avoiding
inference over the entire feature space in each optimization step. Instead,
inference is performed over an incrementally grown active set of features. In
this paper, we address the computational bottlenecks that current techniques
still suffer by introducing online edge grafting, an incremental, structured
method that activates edges as groups of features in a streaming setting. The
framework is based on reservoir sampling of edges that satisfy a necessary
activation condition, approximating the search for the optimal edge to
activate. Online edge grafting performs an informed edge search set
reorganization using search history and structure heuristics. Experiments show
a significant computational speedup for structure learning and a controllable
trade-off between the speed and the quality of learning.



When used as a surrogate objective for maximum likelihood estimation in
latent variable models, the evidence lower bound (ELBO) produces
state-of-the-art results. Inspired by this, we consider the extension of the
ELBO to a family of lower bounds defined by a particle filter's estimator of
the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs
take the same arguments as the ELBO, but can exploit a model's sequential
structure to form tighter bounds. We present results that relate the tightness
of FIVO's bound to the variance of the particle filter's estimator by
considering the generic case of bounds defined as log-transformed likelihood
estimators. Experimentally, we show that training with FIVO results in
substantial improvements over training the same model architecture with the
ELBO on sequential data.



The existence of a coalition strategy to achieve a goal does not necessarily
mean that the coalition has enough information to know how to follow the
strategy. Neither does it mean that the coalition knows that such a strategy
exists. The article studies an interplay between the distributed knowledge,
coalition strategies, and coalition "know-how" strategies. The main technical
result is a sound and complete trimodal logical system that describes the
properties of this interplay.



Process mining is a research field focused on the analysis of event data with
the aim of extracting insights related to dynamic behavior. Applying process
mining techniques on data from smart home environments has the potential to
provide valuable insights in (un)healthy habits and to contribute to ambient
assisted living solutions. Finding the right event labels to enable the
application of process mining techniques is however far from trivial, as simply
using the triggering sensor as the label for sensor events results in
uninformative models that allow for too much behavior (overgeneralizing).
Refinements of sensor level event labels suggested by domain experts have been
shown to enable discovery of more precise and insightful process models.
However, there exists no automated approach to generate refinements of event
labels in the context of process mining. In this paper we propose a framework
for the automated generation of label refinements based on the time attribute
of events, allowing us to distinguish behaviourally different instances of the
same event type based on their time attribute. We show on a case study with
real life smart home event data that using automatically generated refined
labels in process discovery, we can find more specific, and therefore more
insightful, process models. We observe that one label refinement could have an
effect on the usefulness of other label refinements when used together.
Therefore, we explore four strategies to generate useful combinations of
multiple label refinements and evaluate those on three real life smart home
event logs.



We study Robust Subspace Recovery (RSR) in distributed settings. We consider
a huge data set in an ad hoc network without a central processor, where each
node has access only to one chunk of the data set. We assume that part of the
whole data set lies around a low-dimensional subspace and the other part is
composed of outliers that lie away from that subspace. The goal is to recover
the underlying subspace for the whole data set, without transferring the data
itself between the nodes. We apply the Consensus Based Gradient method for the
Geometric Median Subspace algorithm for RSR. We propose an iterative solution
for the local dual minimization problem and establish its $r$-linear
convergence. We show that this mathematical framework also extends to two
simpler problems: Principal Component Analysis and the geometric median. We
also explain how to distributedly implement the Reaper and Fast Median Subspace
algorithms for RSR. We demonstrate the competitive performance of our
algorithms for both synthetic and real data.



Given a database and a target attribute of interest, how can we tell whether
there exists a functional, or approximately functional dependence of the target
on any set of other attributes in the data? How can we reliably, without bias
to sample size or dimensionality, measure the strength of such a dependence?
And, how can we efficiently discover the optimal or $\alpha$-approximate
top-$k$ dependencies? These are exactly the questions we answer in this paper.
  As we want to be agnostic on the form of the dependence, we adopt an
information-theoretic approach, and construct a reliable, bias correcting score
that can be efficiently computed. Moreover, we give an effective optimistic
estimator of this score, by which for the first time we can mine the
approximate functional dependencies from data with guarantees of optimality.
Empirical evaluation shows that the derived score achieves a good bias for
variance trade-off, can be used within an efficient discovery algorithm, and
indeed discovers meaningful dependencies. Most important, it remains reliable
in the face of data sparsity.



This paper addresses the problem of automatic speech recognition (ASR) error
detection and their use for improving spoken language understanding (SLU)
systems. In this study, the SLU task consists in automatically extracting, from
ASR transcriptions , semantic concepts and concept/values pairs in a e.g
touristic information system. An approach is proposed for enriching the set of
semantic labels with error specific labels and by using a recently proposed
neural approach based on word embeddings to compute well calibrated ASR
confidence measures. Experimental results are reported showing that it is
possible to decrease significantly the Concept/Value Error Rate with a state of
the art system, outperforming previously published results performance on the
same experimental data. It also shown that combining an SLU approach based on
conditional random fields with a neural encoder/decoder attention based
architecture , it is possible to effectively identifying confidence islands and
uncertain semantic output segments useful for deciding appropriate error
handling actions by the dialogue manager strategy .



The goal of this paper is to analyze the geometric properties of deep neural
network classifiers in the input space. We specifically study the topology of
classification regions created by deep networks, as well as their associated
decision boundary. Through a systematic empirical investigation, we show that
state-of-the-art deep nets learn connected classification regions, and that the
decision boundary in the vicinity of datapoints is flat along most directions.
We further draw an essential connection between two seemingly unrelated
properties of deep networks: their sensitivity to additive perturbations in the
inputs, and the curvature of their decision boundary. The directions where the
decision boundary is curved in fact remarkably characterize the directions to
which the classifier is the most vulnerable. We finally leverage a fundamental
asymmetry in the curvature of the decision boundary of deep nets, and propose a
method to discriminate between original images, and images perturbed with small
adversarial examples. We show the effectiveness of this purely geometric
approach for detecting small adversarial perturbations in images, and for
recovering the labels of perturbed images.



Deep networks have recently been shown to be vulnerable to universal
perturbations: there exist very small image-agnostic perturbations that cause
most natural images to be misclassified by such classifiers. In this paper, we
propose the first quantitative analysis of the robustness of classifiers to
universal perturbations, and draw a formal link between the robustness to
universal perturbations, and the geometry of the decision boundary.
Specifically, we establish theoretical bounds on the robustness of classifiers
under two decision boundary models (flat and curved models). We show in
particular that the robustness of deep networks to universal perturbations is
driven by a key property of their curvature: there exists shared directions
along which the decision boundary of deep networks is systematically positively
curved. Under such conditions, we prove the existence of small universal
perturbations. Our analysis further provides a novel geometric method for
computing universal perturbations, in addition to explaining their properties.



Generative adversarial networks (GANs) can implicitly learn rich
distributions over images, audio, and data which are hard to model with an
explicit likelihood. We present a practical Bayesian formulation for
unsupervised and semi-supervised learning with GANs. Within this framework, we
use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of
the generator and discriminator networks. The resulting approach is
straightforward and obtains good performance without any standard interventions
such as feature matching, or mini-batch discrimination. By exploring an
expressive posterior over the parameters of the generator, the Bayesian GAN
avoids mode-collapse, produces interpretable and diverse candidate samples, and
provides state-of-the-art quantitative results for semi-supervised learning on
benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,
Wasserstein GANs, and DCGAN ensembles.



We study causal inference in a multi-environment setting, in which the
functional relations for producing the variables from their direct causes
remain the same across environments, while the distribution of exogenous noises
may vary. We introduce the idea of using the invariance of the functional
relations of the variables to their causes across a set of environments. We
define a notion of completeness for a causal inference algorithm in this
setting and prove the existence of such algorithm by proposing the baseline
algorithm. Additionally, we present an alternate algorithm that has
significantly improved computational and sample complexity compared to the
baseline algorithm. The experiment results show that the proposed algorithm
outperforms the other existing algorithms.



This paper focuses on detecting anomalies in a digital video broadcasting
(DVB) system from providers' perspective. We learn a probabilistic
deterministic real timed automaton profiling benign behavior of encryption
control in the DVB control access system. This profile is used as a one-class
classifier. Anomalous items in a testing sequence are detected when the
sequence is not accepted by the learned model.



While domain adaptation has been actively researched in recent years, most
theoretical results and algorithms focus on the single-source-single-target
adaptation setting. Naive application of such algorithms on multiple source
domain adaptation problem may lead to suboptimal solutions. As a step toward
bridging the gap, we propose a new generalization bound for domain adaptation
when there are multiple source domains with labeled instances and one target
domain with unlabeled instances. Compared with existing bounds, the new bound
does not require expert knowledge about the target distribution, nor the
optimal combination rule for multisource domains. Interestingly, our theory
also leads to an efficient learning strategy using adversarial neural networks:
we show how to interpret it as learning feature representations that are
invariant to the multiple domain shifts while still being discriminative for
the learning task. To this end, we propose two models, both of which we call
multisource domain adversarial networks (MDANs): the first model optimizes
directly our bound, while the second model is a smoothed approximation of the
first one, leading to a more data-efficient and task-adaptive model. The
optimization tasks of both models are minimax saddle point problems that can be
optimized by adversarial training. To demonstrate the effectiveness of MDANs,
we conduct extensive experiments showing superior adaptation performance on
three real-world datasets: sentiment analysis, digit classification, and
vehicle counting.



New types of machine learning hardware in development and entering the market
hold the promise of revolutionizing deep learning in a manner as profound as
GPUs. However, existing software frameworks and training algorithms for deep
learning have yet to evolve to fully leverage the capability of the new wave of
silicon. We already see the limitations of existing algorithms for models that
exploit structured input via complex and instance-dependent control flow, which
prohibits minibatching. We present an asynchronous model-parallel (AMP)
training algorithm that is specifically motivated by training on networks of
interconnected devices. Through an implementation on multi-core CPUs, we show
that AMP training converges to the same accuracy as conventional synchronous
training algorithms in a similar number of epochs, but utilizes the available
hardware more efficiently even for small minibatch sizes, resulting in
significantly shorter overall training times. Our framework opens the door for
scaling up a new class of deep learning models that cannot be efficiently
trained today.



Recent progress in variational inference has paid much attention to the
flexibility of variational posteriors. Work has been done to use implicit
distributions, i.e., distributions without tractable likelihoods as the
variational posterior. However, existing methods on implicit posteriors still
face challenges of noisy estimation and can hardly scale to high-dimensional
latent variable models. In this paper, we present an implicit variational
inference approach with kernel density ratio fitting that addresses these
challenges. As far as we know, for the first time implicit variational
inference is successfully applied to Bayesian neural networks, which shows
promising results on both regression and classification tasks.



Process mining techniques focus on extracting insight in processes from event
logs. Process mining has the potential to provide valuable insights in
(un)healthy habits and to contribute to ambient assisted living solutions when
applied on data from smart home environments. However, events recorded in smart
home environments are on the level of sensor triggers, at which process
discovery algorithms produce overgeneralizing process models that allow for too
much behavior and that are difficult to interpret for human experts. We show
that abstracting the events to a higher-level interpretation can enable
discovery of more precise and more comprehensible models. We present a
framework for the extraction of features that can be used for abstraction with
supervised learning methods that is based on the XES IEEE standard for event
logs. This framework can automatically abstract sensor-level events to their
interpretation at the human activity level, after training it on training data
for which both the sensor and human activity events are known. We demonstrate
our abstraction framework on three real-life smart home event logs and show
that the process models that can be discovered after abstraction are more
precise indeed.



Many model-based Visual Odometry (VO) algorithms have been proposed in the
past decade, often restricted to the type of camera optics, or the underlying
motion manifold observed. We envision robots to be able to learn and perform
these tasks, in a minimally supervised setting, as they gain more experience.
To this end, we propose a fully trainable solution to visual ego-motion
estimation for varied camera optics. We propose a visual ego-motion learning
architecture that maps observed optical flow vectors to an ego-motion density
estimate via a Mixture Density Network (MDN). By modeling the architecture as a
Conditional Variational Autoencoder (C-VAE), our model is able to provide
introspective reasoning and prediction for ego-motion induced scene-flow.
Additionally, our proposed model is especially amenable to bootstrapped
ego-motion learning in robots where the supervision in ego-motion estimation
for a particular camera sensor can be obtained from standard navigation-based
sensor fusion strategies (GPS/INS and wheel-odometry fusion). Through
experiments, we show the utility of our proposed approach in enabling the
concept of self-supervised learning for visual ego-motion estimation in
autonomous robots.



We introduce contextual explanation networks (CENs)---a class of models that
learn to predict by generating and leveraging intermediate explanations. CENs
are deep networks that generate parameters for context-specific probabilistic
graphical models which are further used for prediction and play the role of
explanations. Contrary to the existing post-hoc model-explanation tools, CENs
learn to predict and to explain jointly. Our approach offers two major
advantages: (i) for each prediction, valid instance-specific explanations are
generated with no computational overhead and (ii) prediction via explanation
acts as a regularization and boosts performance in low-resource settings. We
prove that local approximations to the decision boundary of our networks are
consistent with the generated explanations. Our results on image and text
classification and survival analysis tasks demonstrate that CENs are
competitive with the state-of-the-art while offering additional insights behind
each prediction, valuable for decision support.



Multisensory polices are known to enhance both state estimation and target
tracking. However, in the space of end-to-end sensorimotor control, this
multi-sensor outlook has received limited attention. Moreover, systematic ways
to make policies robust to partial sensor failure are not well explored. In
this work, we propose a specific customization of Dropout, called
\textit{Sensor Dropout}, to improve multisensory policy robustness and handle
partial failure in the sensor-set. We also introduce an additional auxiliary
loss on the policy network in order to reduce variance in the band of potential
multi- and uni-sensory policies to reduce jerks during policy switching
triggered by an abrupt sensor failure or deactivation/activation. Finally,
through the visualization of gradients, we show that the learned policies are
conditioned on the same latent states representation despite having diverse
observations spaces - a hallmark of true sensor-fusion. Simulation results of
the multisensory policy, as visualized in TORCS racing game, can be seen here:
https://youtu.be/QAK2lcXjNZc.



Recent advances in combining deep learning and Reinforcement Learning have
shown a promising path for designing new control agents that can learn optimal
policies for challenging control tasks. These new methods address the main
limitations of conventional Reinforcement Learning methods such as customized
feature engineering and small action/state space dimension requirements. In
this paper, we leverage one of the state-of-the-art Reinforcement Learning
methods, known as Trust Region Policy Optimization, to tackle intersection
management for autonomous vehicles. We show that using this method, we can
perform fine-grained acceleration control of autonomous vehicles in a grid
street plan to achieve a global design objective.



Deep neural networks trained on large supervised datasets have led to
impressive results in recent years. However, since well-annotated datasets can
be prohibitively expensive and time-consuming to collect, recent work has
explored the use of larger but noisy datasets that can be more easily obtained.
In this paper, we investigate the behavior of deep neural networks on training
sets with massively noisy labels. We show that successful learning is possible
even with an essentially arbitrary amount of noise. For example, on MNIST we
find that accuracy of above 90 percent is still attainable even when the
dataset has been diluted with 100 noisy examples for each clean example. Such
behavior holds across multiple patterns of label noise, even when noisy labels
are biased towards confusing classes. Further, we show how the required dataset
size for successful training increases with higher label noise. Finally, we
present simple actionable techniques for improving learning in the regime of
high label noise.



Motivated by concerns for user privacy, we design a steganographic system
("stegosystem") that enables two users to exchange encrypted messages without
an adversary detecting that such an exchange is taking place. We propose a new
linguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network.
We demonstrate our approach on the Twitter and Enron email datasets and show
that it yields high-quality steganographic text while significantly improving
capacity (encrypted bits per word) relative to the state-of-the-art.



The multi-agent path-finding (MAPF) problem has recently received a lot of
attention. However, it does not capture important characteristics of many
real-world domains, such as automated warehouses, where agents are constantly
engaged with new tasks. In this paper, we therefore study a lifelong version of
the MAPF problem, called the multi-agent pickup and delivery (MAPD) problem. In
the MAPD problem, agents have to attend to a stream of delivery tasks in an
online setting. One agent has to be assigned to each delivery task. This agent
has to first move to a given pickup location and then to a given delivery
location while avoiding collisions with other agents. We present two decoupled
MAPD algorithms, Token Passing (TP) and Token Passing with Task Swaps (TPTS).
Theoretically, we show that they solve all well-formed MAPD instances, a
realistic subclass of MAPD instances. Experimentally, we compare them against a
centralized strawman MAPD algorithm without this guarantee in a simulated
warehouse system. TP can easily be extended to a fully distributed MAPD
algorithm and is the best choice when real-time computation is of primary
concern since it remains efficient for MAPD instances with hundreds of agents
and tasks. TPTS requires limited communication among agents and balances well
between TP and the centralized MAPD algorithm.



Deep learning algorithms for connectomics rely upon localized classification,
rather than overall morphology. This leads to a high incidence of erroneously
merged objects. Humans, by contrast, can easily detect such errors by acquiring
intuition for the correct morphology of objects. Biological neurons have
complicated and variable shapes, which are challenging to learn, and merge
errors take a multitude of different forms. We present an algorithm, MergeNet,
that shows 3D ConvNets can, in fact, detect merge errors from high-level
neuronal morphology. MergeNet follows unsupervised training and operates across
datasets. We demonstrate the performance of MergeNet both on a variety of
connectomics data and on a dataset created from merged MNIST images.



We present a new model DrNET that learns disentangled image representations
from video. Our approach leverages the temporal coherence of video and a novel
adversarial loss to learn a representation that factorizes each frame into a
stationary part and a temporally varying component. The disentangled
representation can be used for a range of tasks. For example, applying a
standard LSTM to the time-vary components enables prediction of future frames.
We evaluate our approach on a range of synthetic and real videos, demonstrating
the ability to coherently generate hundreds of steps into the future.



Generative Adversarial Networks (GANs) have gathered a lot of attention from
the computer vision community, yielding impressive results for image
generation. Advances in the adversarial generation of natural language from
noise however are not commensurate with the progress made in generating images,
and still lag far behind likelihood based methods. In this paper, we take a
step towards generating natural language with a GAN objective alone. We
introduce a simple baseline that addresses the discrete output space problem
without relying on gradient estimators and show that it is able to achieve
state-of-the-art results on a Chinese poem generation dataset. We present
quantitative results on generating sentences from context-free and
probabilistic context-free grammars, and qualitative language modeling results.
A conditional version is also described that can generate sequences conditioned
on sentence characteristics.



Partially observable environments present an important open challenge in the
domain of sequential control learning with delayed rewards. Despite numerous
attempts during the two last decades, the majority of reinforcement learning
algorithms and associated approximate models, applied to this context, still
assume Markovian state transitions. In this paper, we explore the use of a
recently proposed attention-based model, the Gated End-to-End Memory Network,
for sequential control. We call the resulting model the Gated End-to-End Memory
Policy Network. More precisely, we use a model-free value-based algorithm to
learn policies for partially observed domains using this memory-enhanced neural
network. This model is end-to-end learnable and it features unbounded memory.
Indeed, because of its attention mechanism and associated non-parametric
memory, the proposed model allows us to define an attention mechanism over the
observation stream unlike recurrent models. We show encouraging results that
illustrate the capability of our attention-based model in the context of the
continuous-state non-stationary control problem of stock trading. We also
present an OpenAI Gym environment for simulated stock exchange and explain its
relevance as a benchmark for the field of non-Markovian decision process
learning.



We introduce neural networks for end-to-end differentiable proving of queries
to knowledge bases by operating on dense vector representations of symbols.
These neural networks are constructed recursively by taking inspiration from
the backward chaining algorithm as used in Prolog. Specifically, we replace
symbolic unification with a differentiable computation on vector
representations of symbols using a radial basis function kernel, thereby
combining symbolic reasoning with learning subsymbolic vector representations.
By using gradient descent, the resulting neural network can be trained to infer
facts from a given incomplete knowledge base. It learns to (i) place
representations of similar symbols in close proximity in a vector space, (ii)
make use of such similarities to prove queries, (iii) induce logical rules, and
(iv) use provided and induced logical rules for multi-hop reasoning. We
demonstrate that this architecture outperforms ComplEx, a state-of-the-art
neural link prediction model, on three out of four benchmark knowledge bases
while at the same time inducing interpretable function-free first-order logic
rules.



Learning meaningful representations that maintain the content necessary for a
particular task while filtering away detrimental variations is a problem of
great interest in machine learning. In this paper, we tackle the problem of
learning representations invariant to a specific factor or trait of data. The
representation learning process is formulated as an adversarial minimax game.
We analyze the optimal equilibrium of such a game and find that it amounts to
maximizing the uncertainty of inferring the detrimental factor given the
representation while maximizing the certainty of making task-specific
predictions. On three benchmark tasks, namely fair and bias-free
classification, language-independent generation, and lighting-independent image
classification, we show that the proposed framework induces an invariant
representation, and leads to better generalization evidenced by the improved
performance.



Given recent proposals to synthesize consciousness, how many forms of
conscious machines can one distinguish and on what grounds? Based on current
clinical scales of consciousness, that measure cognitive awareness and
wakefulness, we take a perspective on how contemporary artificially intelligent
machines and synthetically engineered life forms would measure on these scales.
To do so, we argue that awareness and wakefulness can be associated to
computational and autonomous complexity respectively. Then, building on
insights from cognitive robotics, we ask what function consciousness serves,
and interpret it as an evolutionary game-theoretic strategy. We make the case
for a third type of complexity necessary for describing consciousness, namely,
social complexity. Having identified these complexity types, allows us to
represent both, biological and synthetic systems in a common morphospace. This
suggests an embodiment-based taxonomy of consciousness. In particular, we
distinguish four forms of consciousness, based on embodiment: biological,
synthetic, group (resulting from group interactions) and simulated
consciousness (embodied by virtual agents within a simulated reality). Such a
taxonomy is useful for studying comparative signatures of consciousness across
domains, in order to highlight design principles necessary to engineer
conscious machines. This is particularly relevant in the light of recent
developments at the crossroads of neuroscience, biomedical engineering,
artificial intelligence and biomimetics.



This paper considers an optimal task allocation problem for human robot
collaboration in human robot systems with persistent tasks. Such human robot
systems consist of human operators and intelligent robots collaborating with
each other to accomplish complex tasks that cannot be done by either part
alone. The system objective is to maximize the probability of successfully
executing persistent tasks that are formulated as linear temporal logic
specifications and minimize the average cost between consecutive visits of a
particular proposition. This paper proposes to model the human robot
collaboration under a framework with the composition of multiple Markov
Decision Process (MDP) with possibly unknown transition probabilities, which
characterizes how human cognitive states, such as human trust and fatigue,
stochastically change with the robot performance. Under the unknown MDP models,
an algorithm is developed to learn the model and obtain an optimal task
allocation policy that minimizes the expected average cost for each task cycle
and maximizes the probability of satisfying linear temporal logic constraints.
Moreover, this paper shows that the difference between the optimal policy based
on the learned model and that based on the underlying ground truth model can be
bounded by arbitrarily small constant and large confidence level with
sufficient samples. The case study of an assembly process demonstrates the
effectiveness and benefits of our proposed learning based human robot
collaboration.



Robots will eventually be part of every household. It is thus critical to
enable algorithms to learn from and be guided by non-expert users. In this
paper, we bring a human in the loop, and enable a human teacher to give
feedback to a learning agent in the form of natural language. We argue that a
descriptive sentence can provide a much stronger learning signal than a numeric
reward in that it can easily point to where the mistakes are and how to correct
them. We focus on the problem of image captioning in which the quality of the
output can easily be judged by non-experts. We propose a hierarchical
phrase-based captioning model trained with policy gradients, and design a
feedback network that provides reward to the learner by conditioning on the
human-provided feedback. We show that by exploiting descriptive feedback our
model learns to perform better than when given independently written human
captions.



Topic models have been widely explored as probabilistic generative models of
documents. Traditional inference methods have sought closed-form derivations
for updating the models, however as the expressiveness of these models grows,
so does the difficulty of performing fast and accurate inference over their
parameters. This paper presents alternative neural approaches to topic
modelling by providing parameterisable distributions over topics which permit
training by backpropagation in the framework of neural variational inference.
In addition, with the help of a stick-breaking construction, we propose a
recurrent network that is able to discover a notionally unbounded number of
topics, analogous to Bayesian non-parametric topic models. Experimental results
on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the
effectiveness and efficiency of these neural topic models.



We present Attract-Repel, an algorithm for improving the semantic quality of
word vectors by injecting constraints extracted from lexical resources.
Attract-Repel facilitates the use of constraints from mono- and cross-lingual
resources, yielding semantically specialised cross-lingual vector spaces. Our
evaluation shows that the method can make use of existing cross-lingual
lexicons to construct high-quality vector spaces for a plethora of different
languages, facilitating semantic transfer from high- to lower-resource ones.
The effectiveness of our approach is demonstrated with state-of-the-art results
on semantic similarity datasets in six languages. We next show that
Attract-Repel-specialised vectors boost performance in the downstream task of
dialogue state tracking (DST) across multiple languages. Finally, we show that
cross-lingual vector spaces produced by our algorithm facilitate the training
of multilingual DST models, which brings further performance improvements.



Off-policy model-free deep reinforcement learning methods using previously
collected data can improve sample efficiency over on-policy policy gradient
techniques. On the other hand, on-policy algorithms are often more stable and
easier to use. This paper examines, both theoretically and empirically,
approaches to merging on- and off-policy updates for deep reinforcement
learning. Theoretical results show that off-policy updates with a value
function estimator can be interpolated with on-policy policy gradient updates
whilst still satisfying performance bounds. Our analysis uses control variate
methods to produce a family of policy gradient algorithms, with several
recently proposed algorithms being special cases of this family. We then
provide an empirical comparison of these techniques with the remaining
algorithmic details fixed, and show how different mixing of off-policy gradient
estimates with on-policy samples contribute to improvements in empirical
performance. The final algorithm provides a generalization and unification of
existing deep policy gradient techniques, has theoretical guarantees on the
bias introduced by off-policy updates, and improves on the state-of-the-art
model-free deep RL methods on a number of OpenAI Gym continuous control
benchmarks.



Variational autoencoders (VAEs) learn representations of data by jointly
training a probabilistic encoder and decoder network. Typically these models
encode all features of the data into a single variable. Here we are interested
in learning disentangled representations that encode distinct aspects of the
data into separate variables. We propose to learn such representations using
model architectures that generalise from standard VAEs, employing a general
graphical model structure in the encoder and decoder. This allows us to train
partially-specified models that make relatively strong assumptions about a
subset of interpretable variables and rely on the flexibility of neural
networks to learn representations for the remaining variables. We further
define a general objective for semi-supervised learning in this model class,
which can be approximated using an importance sampling procedure. We evaluate
our framework's ability to learn disentangled representations, both by
qualitative exploration of its generative capacity, and quantitative evaluation
of its discriminative ability on a variety of models and datasets.



We introduce the relational ontology log, or relational olog, a knowledge
representation system based on the category of sets and relations. It is
inspired by Spivak and Kent's olog, a recent categorical framework for
knowledge representation. Relational ologs interpolate between ologs and
description logic, the dominant formalism for knowledge representation today.
In this paper, we investigate relational ologs both for their own sake and to
gain insight into the relationship between the algebraic and logical approaches
to knowledge representation. On a practical level, we show by example that
relational ologs have a friendly and intuitive--yet fully precise--graphical
syntax, derived from the string diagrams of monoidal categories. We explain
several other useful features of relational ologs not possessed by most
description logics, such as a type system and a rich, flexible notion of
instance data. In a more theoretical vein, we draw on categorical logic to show
how relational ologs can be translated to and from logical theories in a
fragment of first-order logic. Although we make extensive use of categorical
language, this paper is designed to be self-contained and has considerable
expository content. The only prerequisites are knowledge of first-order logic
and the rudiments of category theory.



We give a simple, fast algorithm for hyperparameter optimization inspired by
techniques from the analysis of Boolean functions. We focus on the
high-dimensional regime where the canonical example is training a neural
network with a large number of hyperparameters. The algorithm --- an iterative
application of compressed sensing techniques for orthogonal polynomials ---
requires only uniform sampling of the hyperparameters and is thus easily
parallelizable.
  Experiments for training deep neural networks on Cifar-10 show that compared
to state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds
significantly improved solutions, in some cases better than what is attainable
by hand-tuning. In terms of overall running time (i.e., time required to sample
various settings of hyperparameters plus additional computation time), we are
at least an order of magnitude faster than Hyperband and Bayesian Optimization.
We also outperform Random Search 8x.
  Additionally, our method comes with provable guarantees and yields the first
improvements on the sample complexity of learning decision trees in over two
decades. In particular, we obtain the first quasi-polynomial time algorithm for
learning noisy decision trees with polynomial sample complexity.



How useful can machine learning be in a quantum laboratory? Here we raise the
question of the potential of intelligent machines in the context of scientific
research. We investigate this question by using the projective simulation
model, a physics-oriented approach to artificial intelligence. In our approach,
the projective simulation system is challenged to design complex photonic
quantum experiments that produce high-dimensional entangled multiphoton states,
which are of high interest in modern quantum experiments. The artificial
intelligence system learns to create a variety of entangled states, in number
surpassing the best previously studied automated approaches, and improves the
efficiency of their realization. In the process, the system autonomously
(re)discovers experimental techniques which are only becoming standard in
modern quantum optical experiments - a trait which was not explicitly demanded
from the system but emerged through the process of learning. Such features
highlight the possibility that machines could have a significantly more
creative role in future research.



In recent years, deep learning techniques have been developed to improve the
performance of program synthesis from input-output examples. Albeit its
significant progress, the programs that can be synthesized by state-of-the-art
approaches are still simple in terms of their complexity. In this work, we move
a significant step forward along this direction by proposing a new class of
challenging tasks in the domain of program synthesis from input-output
examples: learning a context-free parser from pairs of input programs and their
parse trees. We show that this class of tasks are much more challenging than
previously studied tasks, and the test accuracy of existing approaches is
almost 0%.
  We tackle the challenges by developing three novel techniques inspired by
three novel observations, which reveal the key ingredients of using deep
learning to synthesize a complex program. First, the use of a
non-differentiable machine is the key to effectively restrict the search space.
Thus our proposed approach learns a neural program operating a domain-specific
non-differentiable machine. Second, recursion is the key to achieve
generalizability. Thus, we bake-in the notion of recursion in the design of our
non-differentiable machine. Third, reinforcement learning is the key to learn
how to operate the non-differentiable machine, but it is also hard to train the
model effectively with existing reinforcement learning algorithms from a cold
boot. We develop a novel two-phase reinforcement learning-based search
algorithm to overcome this issue. In our evaluation, we show that using our
novel approach, neural parsing programs can be learned to achieve 100% test
accuracy on test inputs that are 500x longer than the training samples.



We discuss problems with the standard approaches to evaluation for tasks like
visual question answering, and argue that artificial data can be used to
address these as a complement to current practice. We demonstrate that with the
help of existing 'deep' linguistic processing technology we are able to create
challenging abstract datasets, which enable us to investigate the language
understanding abilities of multimodal deep learning models in detail.



Automated story generation is the problem of automatically selecting a
sequence of events, actions, or words that can be told as a story. We seek to
develop a system that can generate stories by learning everything it needs to
know from textual story corpora. To date, recurrent neural networks that learn
language models at character, word, or sentence levels have had little success
generating coherent stories. We explore the question of event representations
that provide a mid-level of abstraction between words and sentences in order to
retain the semantic information of the original data while minimizing event
sparsity. We present a technique for preprocessing textual story data into
event sequences. We then present a technique for automated story generation
whereby we decompose the problem into the generation of successive events
(event2event) and the generation of natural language sentences from events
(event2sentence). We give empirical results comparing different event
representations and their effects on event successor generation and the
translation of events to natural language.



Using established principles from Information Theory and Statistics, we show
that in a deep neural network invariance to nuisance factors is equivalent to
information minimality of the learned representation, and that stacking layers
and injecting noise during training naturally bias the network towards learning
invariant representations. We then show that, in order to avoid memorization,
we need to limit the quantity of information stored in the weights, which leads
to a novel usage of the Information Bottleneck Lagrangian on the weights as a
learning criterion. This also has an alternative interpretation as minimizing a
PAC-Bayesian bound on the test error. Finally, we exploit a duality between
weights and activations induced by the architecture, to show that the
information in the weights bounds the minimality and Total Correlation of the
layers, therefore showing that regularizing the weights explicitly or
implicitly, using SGD, not only helps avoid overfitting, but also fosters
invariance and disentangling of the learned representation. The theory also
enables predicting sharp phase transitions between underfitting and overfitting
random labels at precise information values, and sheds light on the relation
between the geometry of the loss function, in particular so-called "flat
minima," and generalization.



We propose a generative machine comprehension model that learns jointly to
ask and answer questions based on documents. The proposed model uses a
sequence-to-sequence framework that encodes the document and generates a
question (answer) given an answer (question). Significant improvement in model
performance is observed empirically on the SQuAD corpus, confirming our
hypothesis that the model benefits from jointly learning to perform both tasks.
We believe the joint model's novelty offers a new perspective on machine
comprehension beyond architectural engineering, and serves as a first step
towards autonomous information seeking.



We present a novel training framework for neural sequence models,
particularly for grounded dialog generation. The standard training paradigm for
these models is maximum likelihood estimation (MLE), or minimizing the
cross-entropy of the human responses. Across a variety of domains, a recurring
problem with MLE trained generative neural dialog models (G) is that they tend
to produce 'safe' and generic responses ("I don't know", "I can't tell"). In
contrast, discriminative dialog models (D) that are trained to rank a list of
candidate human responses outperform their generative counterparts; in terms of
automatic metrics, diversity, and informativeness of the responses. However, D
is not useful in practice since it cannot be deployed to have real
conversations with users.
  Our work aims to achieve the best of both worlds -- the practical usefulness
of G and the strong performance of D -- via knowledge transfer from D to G. Our
primary contribution is an end-to-end trainable generative visual dialog model,
where G receives gradients from D as a perceptual (not adversarial) loss of the
sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS)
approximation to the discrete distribution -- specifically, an RNN augmented
with a sequence of GS samplers, coupled with the straight-through gradient
estimator to enable end-to-end differentiability. We also introduce a stronger
encoder for visual dialog, and employ a self-attention mechanism for answer
encoding along with a metric learning loss to aid D in better capturing
semantic similarities in answer responses. Overall, our proposed model
outperforms state-of-the-art on the VisDial dataset by a significant margin
(2.67% on recall@10). The source code can be downloaded from
https://github.com/jiasenlu/visDial.pytorch.



With growing interest in adversarial machine learning, it is important for
machine learning practitioners and users to understand how their models may be
attacked. We propose a web-based visualization tool, Adversarial-Playground, to
demonstrate the efficacy of common adversarial methods against a deep neural
network (DNN) model, built on top of the TensorFlow library.
Adversarial-Playground provides users an efficient and effective experience in
exploring techniques generating adversarial examples, which are inputs crafted
by an adversary to fool a machine learning system. To enable
Adversarial-Playground to generate quick and accurate responses for users, we
use two primary tactics: (1) We propose a faster variant of the
state-of-the-art Jacobian saliency map approach that maintains a comparable
evasion rate. (2) Our visualization does not transmit the generated adversarial
images to the client, but rather only the matrix describing the sample and the
vector representing classification likelihoods.
  The source code along with the data from all of our experiments are available
at \url{https://github.com/QData/AdversarialDNN-Playground}.



Making inferences from partial information constitutes a critical aspect of
cognition. During visual perception, pattern completion enables recognition of
poorly visible or occluded objects. We combined psychophysics, physiology and
computational models to test the hypothesis that pattern completion is
implemented by recurrent computations and present three pieces of evidence that
are consistent with this hypothesis. First, subjects robustly recognized
objects even when rendered <10% visible, but recognition was largely impaired
when processing was interrupted by backward masking. Second, invasive
physiological responses along the human ventral cortex exhibited visually
selective responses to partially visible objects that were delayed compared to
whole objects, suggesting the need for additional computations. These
physiological delays were correlated with the effects of backward masking.
Third, state-of-the-art feed-forward computational architectures were not
robust to partial visibility. However, recognition performance was recovered
when the model was augmented with attractor-based recurrent connectivity. These
results provide a strong argument of plausibility for the role of recurrent
computations in making visual inferences from partial information.



It has been previously observed that variational autoencoders tend to ignore
the latent code when combined with a decoding distribution that is too
flexible. This undermines the purpose of unsupervised representation learning.
In this paper, we additionally show that existing training criteria can lead to
extremely poor amortized inference distributions and overestimation of the
posterior variance, even when trained to optimality. We identify the reason for
both short-comings in the regularization term used in the ELBO criterion to
match the variational posterior to the latent prior distribution. We propose a
class of training criteria termed InfoVAE that solves the two problems. We show
that these models maximize the mutual information between input and latent
features, make effective use of the latent features regardless of the
flexibility of the decoding distribution, and avoid the variance
over-estimation problem. Through extensive qualitative and quantitative
analyses, we demonstrate that our models do not suffer from these problems, and
outperform models trained with ELBO on multiple metrics of performance.



We explore deep reinforcement learning methods for multi-agent domains. We
begin by analyzing the difficulty of traditional algorithms in the multi-agent
case: Q-learning is challenged by an inherent non-stationarity of the
environment, while policy gradient suffers from a variance that increases as
the number of agents grows. We then present an adaptation of actor-critic
methods that considers action policies of other agents and is able to
successfully learn policies that require complex multi-agent coordination.
Additionally, we introduce a training regimen utilizing an ensemble of policies
for each agent that leads to more robust multi-agent policies. We show the
strength of our approach compared to existing methods in cooperative as well as
competitive scenarios, where agent populations are able to discover various
physical and informational coordination strategies.



This study investigates how adequate coordination among the different
cognitive processes of a humanoid robot can be developed through end-to-end
learning of direct perception of visuomotor stream. We propose a deep dynamic
neural network model built on a dynamic vision network, a motor generation
network, and a higher-level network. The proposed model was designed to process
and to integrate direct perception of dynamic visuomotor patterns in a
hierarchical model characterized by different spatial and temporal constraints
imposed on each level. We conducted synthetic robotic experiments in which a
robot learned to read human's intention through observing the gestures and then
to generate the corresponding goal-directed actions. Results verify that the
proposed model is able to learn the tutored skills and to generalize them to
novel situations. The model showed synergic coordination of perception, action
and decision making, and it integrated and coordinated a set of cognitive
skills including visual perception, intention reading, attention switching,
working memory, action preparation and execution in a seamless manner. Analysis
reveals that coherent internal representations emerged at each level of the
hierarchy. Higher-level representation reflecting actional intention developed
by means of continuous integration of the lower-level visuo-proprioceptive
stream.



This study presents a dynamic neural network model based on the predictive
coding framework for perceiving and predicting the dynamic visuo-proprioceptive
patterns. In our previous study [1], we have shown that the deep dynamic neural
network model was able to coordinate visual perception and action generation in
a seamless manner. In the current study, we extended the previous model under
the predictive coding framework to endow the model with a capability of
perceiving and predicting dynamic visuo-proprioceptive patterns as well as a
capability of inferring intention behind the perceived visuomotor information
through minimizing prediction error. A set of synthetic experiments were
conducted in which a robot learned to imitate the gestures of another robot in
a simulation environment. The experimental results showed that with given
intention states, the model was able to mentally simulate the possible incoming
dynamic visuo-proprioceptive patterns in a top-down process without the inputs
from the external environment. Moreover, the results highlighted the role of
minimizing prediction error in inferring underlying intention of the perceived
visuo-proprioceptive patterns, supporting the predictive coding account of the
mirror neuron systems. The results also revealed that minimizing prediction
error in one modality induced the recall of the corresponding representation of
another modality acquired during the consolidative learning of raw-level
visuo-proprioceptive patterns.



Common-sense or background knowledge is required to understand natural
language, but in most neural natural language understanding (NLU) systems, the
requisite background knowledge is indirectly acquired from static corpora. We
develop a new reading architecture for the dynamic integration of explicit
background knowledge in NLU models. A new task-agnostic reading module provides
refined word representations to a task-specific NLU architecture by processing
background knowledge in the form of free-text statements, together with the
task-specific inputs. Strong performance on the tasks of document question
answering (DQA) and recognizing textual entailment (RTE) demonstrate the
effectiveness and flexibility of our approach. Analysis shows that our models
learn to exploit knowledge selectively and in a semantically appropriate way.



We provide a novel notion of what it means to be interpretable, looking past
the usual association with human understanding. Our key insight is that
interpretability is not an absolute concept and so we define it relative to a
target model, which may or may not be a human. We define a framework that
allows for comparing interpretable procedures by linking it to important
practical aspects such as accuracy and robustness. We characterize many of the
current state-of-the-art interpretable methods in our framework portraying its
general applicability. Finally, principled interpretable strategies are
proposed and empirically evaluated on synthetic data, as well as on the largest
public olfaction dataset that was made recently available \cite{olfs}. We also
experiment on MNIST with a simple target model and different oracle models of
varying complexity. This leads to the insight that the improvement in the
target model is not only a function of the oracle models performance, but also
its relative complexity with respect to the target model.



On a daily investment decision in a security market, the price earnings (PE)
ratio is one of the most widely applied methods being used as a firm valuation
tool by investment experts. Unfortunately, recent academic developments in
financial econometrics and machine learning rarely look at this tool. In
practice, fundamental PE ratios are often estimated only by subjective expert
opinions. The purpose of this research is to formalize a process of fundamental
PE estimation by employing advanced dynamic Bayesian network (DBN) methodology.
The estimated PE ratio from our model can be used either as a information
support for an expert to make investment decisions, or as an automatic trading
system illustrated in experiments. Forward-backward inference and EM parameter
estimation algorithms are derived with respect to the proposed DBN structure.
Unlike existing works in literatures, the economic interpretation of our DBN
model is well-justified by behavioral finance evidences of volatility. A simple
but practical trading strategy is invented based on the result of Bayesian
inference. Extensive experiments show that our trading strategy equipped with
the inferenced PE ratios consistently outperforms standard investment
benchmarks.



In this paper we explore methods to exploit symmetries for ensuring sample
efficiency in reinforcement learning (RL), this problem deserves ever
increasing attention with the recent advances in the use of deep networks for
complex RL tasks which require large amount of training data. We introduce a
novel method to detect symmetries using reward trails observed during episodic
experience and prove its completeness. We also provide a framework to
incorporate the discovered symmetries for functional approximation. Finally we
show that the use of potential based reward shaping is especially effective for
our symmetry exploitation mechanism. Experiments on various classical problems
show that our method improves the learning performance significantly by
utilizing symmetry information.



In the artificial intelligence field, learning often corresponds to changing
the parameters of a parameterized function. A learning rule is an algorithm or
mathematical expression that specifies precisely how the parameters should be
changed. When creating an artificial intelligence system, we must make two
decisions: what representation should be used (i.e., what parameterized
function should be used) and what learning rule should be used to search
through the resulting set of representable functions. Using most learning
rules, these two decisions are coupled in a subtle (and often unintentional)
way. That is, using the same learning rule with two different representations
that can represent the same sets of functions can result in two different
outcomes. After arguing that this coupling is undesirable, particularly when
using artificial neural networks, we present a method for partially decoupling
these two decisions for a broad class of learning rules that span unsupervised
learning, reinforcement learning, and supervised learning.



We study the skip-thought model with neighborhood information as weak
supervision. More specifically, we propose a skip-thought neighbor model to
consider the adjacent sentences as a neighborhood. We train our skip-thought
neighbor model on a large corpus with continuous sentences, and then evaluate
the trained model on 7 tasks, which include semantic relatedness, paraphrase
detection, and classification benchmarks. Both quantitative comparison and
qualitative investigation are conducted. We empirically show that, our
skip-thought neighbor model performs as well as the skip-thought model on
evaluation tasks. In addition, we found that, incorporating an autoencoder path
in our model didn't aid our model to perform better, while it hurts the
performance of the skip-thought model.



Online platforms can be divided into information-oriented and social-oriented
domains. The former refers to forums or E-commerce sites that emphasize
user-item interactions, like Trip.com and Amazon; whereas the latter refers to
social networking services (SNSs) that have rich user-user connections, such as
Facebook and Twitter. Despite their heterogeneity, these two domains can be
bridged by a few overlapping users, dubbed as bridge users. In this work, we
address the problem of cross-domain social recommendation, i.e., recommending
relevant items of information domains to potential users of social networks. To
our knowledge, this is a new problem that has rarely been studied before.
  Existing cross-domain recommender systems are unsuitable for this task since
they have either focused on homogeneous information domains or assumed that
users are fully overlapped. Towards this end, we present a novel Neural Social
Collaborative Ranking (NSCR) approach, which seamlessly sews up the user-item
interactions in information domains and user-user connections in SNSs. In the
information domain part, the attributes of users and items are leveraged to
strengthen the embedding learning of users and items. In the SNS part, the
embeddings of bridge users are propagated to learn the embeddings of other
non-bridge users. Extensive experiments on two real-world datasets demonstrate
the effectiveness and rationality of our NSCR method.



Designing an auction that maximizes expected revenue is an intricate task.
Indeed, as of today--despite major efforts and impressive progress over the
past few years--only the single-item case is fully understood. In this work, we
initiate the exploration of the use of tools from deep learning on this topic.
The design objective is revenue optimal, dominant-strategy incentive compatible
auctions. We show that multi-layer neural networks can learn almost-optimal
auctions for settings for which there are analytical solutions, such as
Myerson's auction for a single item, Manelli and Vincent's mechanism for a
single bidder with additive preferences over two items, or Yao's auction for
two additive bidders with binary support distributions and multiple items, even
if no prior knowledge about the form of optimal auctions is encoded in the
network and the only feedback during training is revenue and regret. We further
show how characterization results, even rather implicit ones such as Rochet's
characterization through induced utilities and their gradients, can be
leveraged to obtain more precise fits to the optimal design. We conclude by
demonstrating the potential of deep learning for deriving optimal auctions with
high revenue for poorly understood problems.



Factoid question answering (QA) has recently benefited from the development
of deep learning (DL) systems. Neural network models outperform traditional
approaches in domains where large datasets exist, such as SQuAD (ca. 100,000
questions) for Wikipedia articles. However, these systems have not yet been
applied to QA in more specific domains, such as biomedicine, because datasets
are generally too small to train a DL system from scratch. For example, the
BioASQ dataset for biomedical QA comprises less then 900 factoid (single
answer) and list (multiple answers) QA instances. In this work, we adapt a
neural QA system trained on a large open-domain dataset (SQuAD, source) to a
biomedical dataset (BioASQ, target) by employing various transfer learning
techniques. Our network architecture is based on a state-of-the-art QA system,
extended with biomedical word embeddings and a novel mechanism to answer list
questions. In contrast to existing biomedical QA systems, our system does not
rely on domain-specific ontologies, parsers or entity taggers, which are
expensive to create. Despite this fact, our systems achieve state-of-the-art
results on factoid questions and competitive results on list questions.



For sophisticated reinforcement learning (RL) systems to interact usefully
with real-world environments, we need to communicate complex goals to these
systems. In this work, we explore goals defined in terms of (non-expert) human
preferences between pairs of trajectory segments. We show that this approach
can effectively solve complex RL tasks without access to the reward function,
including Atari games and simulated robot locomotion, while providing feedback
on less than one percent of our agent's interactions with the environment. This
reduces the cost of human oversight far enough that it can be practically
applied to state-of-the-art RL systems. To demonstrate the flexibility of our
approach, we show that we can successfully train complex novel behaviors with
about an hour of human time. These behaviors and environments are considerably
more complex than any that have been previously learned from human feedback.



Unsupervised learning of low-dimensional, semantic representations of words
and entities has recently gained attention. In this paper we describe the
Semantic Entity Retrieval Toolkit (SERT) that provides implementations of our
previously published entity representation models. The toolkit provides a
unified interface to different representation learning algorithms, fine-grained
parsing configuration and can be used transparently with GPUs. In addition,
users can easily modify existing models or implement their own models in the
framework. After model training, SERT can be used to rank entities according to
a textual query and extract the learned entity/word representation for use in
downstream algorithms, such as clustering or recommendation.



Measurement error in the observed values of the variables can greatly change
the output of various causal discovery methods. This problem has received much
attention in multiple fields, but it is not clear to what extent the causal
model for the measurement-error-free variables can be identified in the
presence of measurement error with unknown variance. In this paper, we study
precise sufficient identifiability conditions for the measurement-error-free
causal model and show what information of the causal model can be recovered
from observed data. In particular, we present two different sets of
identifiability conditions, based on the second-order statistics and
higher-order statistics of the data, respectively. The former was inspired by
the relationship between the generating model of the
measurement-error-contaminated data and the factor analysis model, and the
latter makes use of the identifiability result of the over-complete independent
component analysis problem.



In this paper, we propose generative probabilistic models for label
aggregation. We use Gibbs sampling and a novel variational inference algorithm
to perform the posterior inference. Empirical results show that our methods
consistently outperform state-of-the-art methods.



Recommendation algorithms that incorporate techniques from deep learning are
becoming increasingly popular. Due to the structure of the data coming from
recommendation domains (i.e., one-hot-encoded vectors of item preferences),
these algorithms tend to have large input and output dimensionalities that
dominate their overall size. This makes them difficult to train, due to the
limited memory of graphical processing units, and difficult to deploy on mobile
devices with limited hardware. To address these difficulties, we propose Bloom
embeddings, a compression technique that can be applied to the input and output
of neural network models dealing with sparse high-dimensional binary-coded
instances. Bloom embeddings are computationally efficient, and do not seriously
compromise the accuracy of the model up to 1/5 compression ratios. In some
cases, they even improve over the original accuracy, with relative increases up
to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4
alternative methods, obtaining favorable results. We also discuss a number of
further advantages of Bloom embeddings, such as 'on-the-fly' constant-time
operation, zero or marginal space requirements, training time speedups, or the
fact that they do not require any change to the core model architecture or
training configuration.



Monte Carlo tree search (MCTS) is extremely popular in computer Go which
determines each action by enormous simulations in a broad and deep search tree.
However, human experts select most actions by pattern analysis and careful
evaluation rather than brute search of millions of future nteractions. In this
paper, we propose a computer Go system that follows experts way of thinking and
playing. Our system consists of two parts. The first part is a novel deep
alternative neural network (DANN) used to generate candidates of next move.
Compared with existing deep convolutional neural network (DCNN), DANN inserts
recurrent layer after each convolutional layer and stacks them in an
alternative manner. We show such setting can preserve more contexts of local
features and its evolutions which are beneficial for move prediction. The
second part is a long-term evaluation (LTE) module used to provide a reliable
evaluation of candidates rather than a single probability from move predictor.
This is consistent with human experts nature of playing since they can foresee
tens of steps to give an accurate estimation of candidates. In our system, for
each candidate, LTE calculates a cumulative reward after several future
interactions when local variations are settled. Combining criteria from the two
parts, our system determines the optimal choice of next move. For more
comprehensive experiments, we introduce a new professional Go dataset (PGD),
consisting of 253233 professional records. Experiments on GoGoD and PGD
datasets show the DANN can substantially improve performance of move prediction
over pure DCNN. When combining LTE, our system outperforms most relevant
approaches and open engines based on MCTS.



We show that relation extraction can be reduced to answering simple reading
comprehension questions, by associating one or more natural-language questions
with each relation slot. This reduction has several advantages: we can (1)
learn relation-extraction models by extending recent neural
reading-comprehension techniques, (2) build very large training sets for those
models by combining relation-specific crowd-sourced questions with distant
supervision, and even (3) do zero-shot learning by extracting new relation
types that are only specified at test-time, for which we have no labeled
training examples. Experiments on a Wikipedia slot-filling task demonstrate
that the approach can generalize to new questions for known relation types with
high accuracy, and that zero-shot generalization to unseen relation types is
possible, at lower accuracy levels, setting the bar for future work on this
task.



Despite the growing prominence of generative adversarial networks (GANs),
optimization in GANs is still a poorly understood topic. In this paper, we
analyze the "gradient descent" form of GAN optimization i.e., the natural
setting where we simultaneously take small gradient steps in both generator and
discriminator parameters. We show that even though GAN optimization does not
correspond to a convex-concave game (even for simple parameterizations), under
proper conditions, equilibrium points of this optimization procedure are still
\emph{locally asymptotically stable} for the traditional GAN formulation. On
the other hand, we show that the recently proposed Wasserstein GAN can have
non-convergent limit cycles near equilibrium. Motivated by this stability
analysis, we propose an additional regularization term for gradient descent GAN
updates, which \emph{is} able to guarantee local stability for both the WGAN
and the traditional GAN, and also shows practical promise in speeding up
convergence and addressing mode collapse.



Traditional approaches to building a large scale knowledge graph have usually
relied on extracting information (entities, their properties, and relations
between them) from unstructured text (e.g. Dbpedia). Recent advances in
Convolutional Neural Networks (CNN) allow us to shift our focus to learning
entities and relations from images, as they build robust models that require
little or no pre-processing of the images. In this paper, we present an
approach to identify and extract spatial relations (e.g., The girl is standing
behind the table) from images using CNNs. Our research addresses two specific
challenges: providing insight into how spatial relations are learned by the
network and which parts of the image are used to predict these relations. We
use the pre-trained network VGGNet to extract features from an image and train
a Multi-layer Perceptron (MLP) on a set of synthetic images and the sun09
dataset to extract spatial relations. The MLP predicts spatial relations
without a bounding box around the objects or the space in the image depicting
the relation. To understand how the spatial relations are represented in the
network, a heatmap is overlayed on the image to show the regions that are
deemed important by the network. Also, we analyze the MLP to show the
relationship between the activation of consistent groups of nodes and the
prediction of a spatial relation. We show how the loss of these groups affects
the networks ability to identify relations.



In built infrastructure monitoring, an efficient path planning algorithm is
essential for robotic inspection of large surfaces using computer vision. In
this work, we first formulate the inspection path planning problem as an
extended travelling salesman problem (TSP) in which both the coverage and
obstacle avoidance were taken into account. An enhanced discrete particle swarm
optimization (DPSO) algorithm is then proposed to solve the TSP, with
performance improvement by using deterministic initialization, random mutation,
and edge exchange. Finally, we take advantage of parallel computing to
implement the DPSO in a GPU-based framework so that the computation time can be
significantly reduced while keeping the hardware requirement unchanged. To show
the effectiveness of the proposed algorithm, experimental results are included
for datasets obtained from UAV inspection of an office building and a bridge.



We propose a two-stage neural model to tackle question generation from
documents. Our model first estimates the probability that word sequences in a
document compose "interesting" answers using a neural model trained on a
question-answering corpus. We thus take a data-driven approach to
interestingness. Predicted key phrases then act as target answers that
condition a sequence-to-sequence question generation model with a copy
mechanism. Empirically, our neural key phrase detection model significantly
outperforms an entity-tagging baseline system and existing rule-based
approaches. We demonstrate that the question generator formulates good quality
natural language questions from extracted key phrases, and a human study
indicates that our system's generated question-answer pairs are competitive
with those of an earlier approach. We foresee our system being used in an
educational setting to assess reading comprehension and also as a data
augmentation technique for semi-supervised learning.



Backdoors and backbones of Boolean formulas are hidden structural properties.
A natural goal, already in part realized, is that solver algorithms seek to
obtain substantially better performance by exploiting these structures.
  However, the present paper is not intended to improve the performance of SAT
solvers, but rather is a cautionary paper. In particular, the theme of this
paper is that there is a potential chasm between the existence of such
structures in the Boolean formula and being able to effectively exploit them.
This does not mean that these structures are not useful to solvers. It does
mean that one must be very careful not to assume that it is computationally
easy to go from the existence of a structure to being able to get one's hands
on it and/or being able to exploit the structure.
  For example, in this paper we show that, under the assumption that P $\neq$
NP, there are easily recognizable sets of Boolean formulas for which it is hard
to determine whether they have a large backbone. We also show that, also under
the assumption P $\neq$ NP, there are easily recognizable families of Boolean
formulas with strong backdoors that are easy to find, yet for which it is hard
to determine whether they are satisfiable.



Joint extraction of entities and relations is an important task in
information extraction. To tackle this problem, we firstly propose a novel
tagging scheme that can convert the joint extraction task to a tagging problem.
Then, based on our tagging scheme, we study different end-to-end models to
extract entities and their relations directly, without identifying entities and
relations separately. We conduct experiments on a public dataset produced by
distant supervision method and the experimental results show that the tagging
based methods are better than most of the existing pipelined and joint learning
methods. What's more, the end-to-end model proposed in this paper, achieves the
best results on the public dataset.



Multi-task learning (MTL) has led to successes in many applications of
machine learning, from natural language processing and speech recognition to
computer vision and drug discovery. This article aims to give a general
overview of MTL, particularly in deep neural networks. It introduces the two
most common methods for MTL in Deep Learning, gives an overview of the
literature, and discusses recent advances. In particular, it seeks to help ML
practitioners apply MTL by shedding light on how MTL works and providing
guidelines for choosing appropriate auxiliary tasks.



We propose a novel embedding model that represents relationships among
several elements in bibliographic information with high representation ability
and flexibility. Based on this model, we present a novel search system that
shows the relationships among the elements in the ACL Anthology Reference
Corpus. The evaluation results show that our model can achieve a high
prediction ability and produce reasonable search results.



This paper gives an overview of impersonation bots that generate output in
one, or possibly, multiple modalities. We also discuss rapidly advancing areas
of machine learning and artificial intelligence that could lead to
frighteningly powerful new multi-modal social bots. Our main conclusion is that
most commonly known bots are one dimensional (i.e., chatterbot), and far from
deceiving serious interrogators. However, using recent advances in machine
learning, it is possible to unleash incredibly powerful, human-like armies of
social bots, in potentially well coordinated campaigns of deception and
influence.



Traditional GANs use a deterministic generator function (typically a neural
network) to transform a random noise input $z$ to a sample $\mathbf{x}$ that
the discriminator seeks to distinguish. We propose a new GAN called Bayesian
Conditional Generative Adversarial Networks (BC-GANs) that use a random
generator function to transform a deterministic input $y'$ to a sample
$\mathbf{x}$. Our BC-GANs extend traditional GANs to a Bayesian framework, and
naturally handle unsupervised learning, supervised learning, and
semi-supervised learning problems. Experiments show that the proposed BC-GANs
outperforms the state-of-the-arts.



The availability of large idea repositories (e.g., the U.S. patent database)
could significantly accelerate innovation and discovery by providing people
with inspiration from solutions to analogous problems. However, finding useful
analogies in these large, messy, real-world repositories remains a persistent
challenge for either human or automated methods. Previous approaches include
costly hand-created databases that have high relational structure (e.g.,
predicate calculus representations) but are very sparse. Simpler
machine-learning/information-retrieval similarity metrics can scale to large,
natural-language datasets, but struggle to account for structural similarity,
which is central to analogy. In this paper we explore the viability and value
of learning simpler structural representations, specifically, "problem
schemas", which specify the purpose of a product and the mechanisms by which it
achieves that purpose. Our approach combines crowdsourcing and recurrent neural
networks to extract purpose and mechanism vector representations from product
descriptions. We demonstrate that these learned vectors allow us to find
analogies with higher precision and recall than traditional
information-retrieval methods. In an ideation experiment, analogies retrieved
by our models significantly increased people's likelihood of generating
creative ideas compared to analogies retrieved by traditional methods. Our
results suggest a promising approach to enabling computational analogy at scale
is to learn and leverage weaker structural representations.



This approach presents a multi-valued representation of the neutrosophic
information. It highlights the link between the bifuzzy information and
neutrosophic one. The constructed deca-valued structure shows the neutrosophic
information complexity. This deca-valued structure led to construction of two
new concepts for the neutrosophic information: neutro-entropy and anti-entropy.
These two concepts are added to the two existing: entropy and non-entropy.
Thus, we obtained the following triad: entropy, neutro-entropy and
anti-entropy.



This paper introduces Dex, a reinforcement learning environment toolkit
specialized for training and evaluation of continual learning methods as well
as general reinforcement learning problems. We also present the novel continual
learning method of incremental learning, where a challenging environment is
solved using optimal weight initialization learned from first solving a similar
easier environment. We show that incremental learning can produce vastly
superior results than standard methods by providing a strong baseline method
across ten Dex environments. We finally develop a saliency method for
qualitative analysis of reinforcement learning, which shows the impact
incremental learning has on network attention.



Diffusions and related random walk procedures are of central importance in
many areas of machine learning, data analysis, and applied mathematics. Because
they spread mass agnostically at each step in an iterative manner, they can
sometimes spread mass "too aggressively," thereby failing to find the "right"
clusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process,
which is both faster and stays more local than the classical spectral diffusion
process. As an application, we use our CRD Process to develop an improved local
algorithm for graph clustering. Our local graph clustering method can find
local clusters in a model of clustering where one begins the CRD Process in a
cluster whose vertices are connected better internally than externally by an
$O(\log^2 n)$ factor, where $n$ is the number of nodes in the cluster. Thus,
our CRD Process is the first local graph clustering algorithm that is not
subject to the well-known quadratic Cheeger barrier. Our result requires a
certain smoothness condition, which we expect to be an artifact of our
analysis. Our empirical evaluation demonstrates improved results, in particular
for realistic social graphs where there are moderately good---but not very
good---clusters.



This work proposes a new algorithm for training a re-weighted L2 Support
Vector Machine (SVM), inspired on the re-weighted Lasso algorithm of Cand\`es
et al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In
particular, the margin required for each training vector is set independently,
defining a new weighted SVM model. These weights are selected to be binary, and
they are automatically adapted during the training of the model, resulting in a
variation of the Frank-Wolfe optimization algorithm with essentially the same
computational complexity as the original algorithm.
  As shown experimentally, this algorithm is computationally cheaper to apply
since it requires less iterations to converge, and it produces models with a
sparser representation in terms of support vectors and which are more stable
with respect to the selection of the regularization hyper-parameter.



It is critical in many applications to understand what features are important
for a model, and why individual predictions were made. For tree ensemble
methods these questions are usually answered by attributing importance values
to input features, either globally or for a single prediction. Here we show
that current feature attribution methods are inconsistent, which means changing
the model to rely more on a given feature can actually decrease the importance
assigned to that feature. To address this problem we develop fast exact
solutions for SHAP (SHapley Additive exPlanation) values, which were recently
shown to be the unique additive feature attribution method based on conditional
expectations that is both consistent and locally accurate. We integrate these
improvements into the latest version of XGBoost, demonstrate the
inconsistencies of current methods, and show how using SHAP values results in
significantly improved supervised clustering performance. Feature importance
values are a key part of understanding widely used models such as gradient
boosting trees and random forests, so improvements to them have broad practical
implications.



As a means of human-based computation, crowdsourcing has been widely used to
annotate large-scale unlabeled datasets. One of the obvious challenges is how
to aggregate these possibly noisy labels provided by a set of heterogeneous
annotators. Another challenge stems from the difficulty in evaluating the
annotator reliability without even knowing the ground truth, which can be used
to build incentive mechanisms in crowdsourcing platforms. When each instance is
associated with many possible labels simultaneously, the problem becomes even
harder because of its combinatorial nature. In this paper, we present new
flexible Bayesian models and efficient inference algorithms for multi-label
annotation aggregation by taking both annotator reliability and label
dependency into account. Extensive experiments on real-world datasets confirm
that the proposed methods outperform other competitive alternatives, and the
model can recover the type of the annotators with high accuracy. Besides, we
empirically find that the mixture of multiple independent Bernoulli
distribution is able to accurately capture label dependency in this
unsupervised multi-label annotation aggregation scenario.



We propose a simple yet effective technique for neural network learning. The
forward propagation is computed as usual. In back propagation, only a small
subset of the full gradient is computed to update the model parameters. The
gradient vectors are sparsified in such a way that only the top-$k$ elements
(in terms of magnitude) are kept. As a result, only $k$ rows or columns
(depending on the layout) of the weight matrix are modified, leading to a
linear reduction ($k$ divided by the vector dimension) in the computational
cost. Surprisingly, experimental results demonstrate that we can update only
1--4\% of the weights at each back propagation pass. This does not result in a
larger number of training iterations. More interestingly, the accuracy of the
resulting models is actually improved rather than degraded, and a detailed
analysis is given. The code is available at https://github.com/jklj077/meProp



Generative adversarial nets (GANs) are a promising technique for modeling a
distribution from samples. It is however well known that GAN training suffers
from instability due to the nature of its maximin formulation. In this paper,
we explore ways to tackle the instability problem by dualizing the
discriminator. We start from linear discriminators in which case conjugate
duality provides a mechanism to reformulate the saddle point objective into a
maximization problem, such that both the generator and the discriminator of
this 'dualing GAN' act in concert. We then demonstrate how to extend this
intuition to non-linear formulations. For GANs with linear discriminators our
approach is able to remove the instability in training, while for GANs with
nonlinear discriminators our approach provides an alternative to the commonly
used GAN training algorithm.



We build deep RL agents that execute declarative programs expressed in formal
language. The agents learn to ground the terms in this language in their
environment, and can generalize their behavior at test time to execute new
programs that refer to objects that were not referenced during training. The
agents develop disentangled interpretable representations that allow them to
generalize to a wide variety of zero-shot semantic tasks.



Inspired by the recent evolution of deep neural networks (DNNs) in machine
learning, we explore their application to PL-related topics. This paper is the
first step towards this goal; we propose a proof-synthesis method for the
negation-free propositional logic in which we use a DNN to obtain a guide of
proof search. The idea is to view the proof-synthesis problem as a translation
from a proposition to its proof. We train seq2seq, which is a popular network
in neural machine translation, so that it generates a proof encoded as a
$\lambda$-term of a given proposition. We implement the whole framework and
empirically observe that a generated proof term is close to a correct proof in
terms of the tree edit distance of AST. This observation justifies using the
output from a trained seq2seq model as a guide for proof search.



The neural network is a powerful computing framework that has been exploited
by biological evolution and by humans for solving diverse problems. Although
the computational capabilities of neural networks are determined by their
structure, the current understanding of the relationships between a neural
network's architecture and function is still primitive. Here we reveal that
neural network's modular architecture plays a vital role in determining the
neural dynamics and memory performance of the network. In particular, we
demonstrate that there exists an optimal modularity for memory performance,
where a balance between local cohesion and global connectivity is established,
allowing optimally modular networks to remember longer. Our results suggest
that insights from dynamical analysis of neural networks and information
spreading processes can be leveraged to better design neural networks and may
shed light on the brain's modular organization.



In Acoustic Scene Classification (ASC) two major approaches have been
followed . While one utilizes engineered features such as
mel-frequency-cepstral-coefficients (MFCCs), the other uses learned features
that are the outcome of an optimization algorithm. I-vectors are the result of
a modeling technique that usually takes engineered features as input. It has
been shown that standard MFCCs extracted from monaural audio signals lead to
i-vectors that exhibit poor performance, especially on indoor acoustic scenes.
At the same time, Convolutional Neural Networks (CNNs) are well known for their
ability to learn features by optimizing their filters. They have been applied
on ASC and have shown promising results. In this paper, we first propose a
novel multi-channel i-vector extraction and scoring scheme for ASC, improving
their performance on indoor and outdoor scenes. Second, we propose a CNN
architecture that achieves promising ASC results. Further, we show that
i-vectors and CNNs capture complementary information from acoustic scenes.
Finally, we propose a hybrid system for ASC using multi-channel i-vectors and
CNNs by utilizing a score fusion technique. Using our method, we participated
in the ASC task of the DCASE-2016 challenge. Our hybrid approach achieved 1 st
rank among 49 submissions, substantially improving the previous state of the
art.



We introduce a new formulation of the Hidden Parameter Markov Decision
Process (HiP-MDP), a framework for modeling families of related tasks using
low-dimensional latent embeddings. Our new framework correctly models the joint
uncertainty in the latent parameters and the state space. We also replace the
original Gaussian Process-based model with a Bayesian Neural Network, enabling
more scalable inference. Thus, we expand the scope of the HiP-MDP to
applications with higher dimensions and more complex dynamics.



Observational learning is a type of learning that occurs as a function of
observing, retaining and possibly replicating or imitating the behaviour of
another agent. It is a core mechanism appearing in various instances of social
learning and has been found to be employed in several intelligent species,
including humans. In this paper, we investigate to what extent the explicit
modelling of other agents is necessary to achieve observational learning
through machine learning. Especially, we argue that observational learning can
emerge from pure Reinforcement Learning (RL), potentially coupled with memory.
Through simple scenarios, we demonstrate that an RL agent can leverage the
information provided by the observations of an other agent performing a task in
a shared environment. The other agent is only observed through the effect of
its actions on the environment and never explicitly modeled. Two key aspects
are borrowed from observational learning: i) the observer behaviour needs to
change as a result of viewing a 'teacher' (another agent) and ii) the observer
needs to be motivated somehow to engage in making use of the other agent's
behaviour. The later is naturally modeled by RL, by correlating the learning
agent's reward with the teacher agent's behaviour.



In this paper, we try to solve the problem of temporal link prediction in
information networks. This implies predicting the time it takes for a link to
appear in the future, given its features that have been extracted at the
current network snapshot. To this end, we introduce a probabilistic
non-parametric approach, called "Non-Parametric Generalized Linear Model"
(NP-GLM), which infers the hidden underlying probability distribution of the
link advent time given its features. We then present a learning algorithm for
NP-GLM and an inference method to answer time-related queries. Extensive
experiments conducted on both synthetic data and real-world Sina Weibo social
network demonstrate the effectiveness of NP-GLM in solving temporal link
prediction problem vis-a-vis competitive baselines.



Animals (especially humans) have an amazing ability to learn new tasks
quickly, and switch between them flexibly. How brains support this ability is
largely unknown, both neuroscientifically and algorithmically. One reasonable
supposition is that modules drawing on an underlying general-purpose sensory
representation are dynamically allocated on a per-task basis. Recent results
from neuroscience and artificial intelligence suggest the role of the general
purpose visual representation may be played by a deep convolutional neural
network, and give some clues how task modules based on such a representation
might be discovered and constructed. In this work, we investigate module
architectures in an embodied two-dimensional touchscreen environment, in which
an agent's learning must occur via interactions with an environment that emits
images and rewards, and accepts touches as input. This environment is designed
to capture the physical structure of the task environments that are commonly
deployed in visual neuroscience and psychophysics. We show that in this
context, very simple changes in the nonlinear activations used by such a module
can significantly influence how fast it is at learning visual tasks and how
suitable it is for switching to new tasks.



Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown
to deliver insightful explanations in the form of input space relevances for
understanding feed-forward neural network classification decisions. In the
present work, we extend the usage of LRP to recurrent neural networks. We
propose a specific propagation rule applicable to multiplicative connections as
they arise in recurrent network architectures such as LSTMs and GRUs. We apply
our technique to a word-based bi-directional LSTM model on a five-class
sentiment prediction task, and evaluate the resulting LRP relevances both
qualitatively and quantitatively, obtaining better results than a
gradient-based related method which was used in previous work.



To perform tasks specified by natural language instructions, autonomous
agents need to extract semantically meaningful representations of language and
map it to visual elements and actions in the environment. This problem is
called task-oriented language grounding. We propose an end-to-end trainable
neural architecture for task-oriented language grounding in 3D environments
which assumes no prior linguistic or perceptual knowledge and requires only raw
pixels from the environment and the natural language instruction as input. The
proposed model combines the image and text representations using a
Gated-Attention mechanism and learns a policy to execute the natural language
instruction using standard reinforcement and imitation learning methods. We
show the effectiveness of the proposed model on unseen instructions as well as
unseen maps, both quantitatively and qualitatively. We also introduce a novel
environment based on a 3D game engine to simulate the challenges of
task-oriented language grounding over a rich set of instructions and
environment states.



We study the reachability problem for systems implemented as feed-forward
neural networks whose activation function is implemented via ReLU functions. We
draw a correspondence between establishing whether some arbitrary output can
ever be outputed by a neural system and linear problems characterising a neural
system of interest. We present a methodology to solve cases of practical
interest by means of a state-of-the-art linear programs solver. We evaluate the
technique presented by discussing the experimental results obtained by
analysing reachability properties for a number of benchmarks in the literature.



We study pure coordination games where in every outcome, all players have
identical payoffs, 'win' or 'lose'. We identify and discuss a range of 'purely
rational principles' guiding the reasoning of rational players in such games
and analyze which classes of coordination games can be solved by such players
with no preplay communication or conventions. We observe that it is highly
nontrivial to delineate a boundary between purely rational principles and other
decision methods, such as conventions, for solving such coordination games.



Advances in image processing and computer vision in the latest years have
brought about the use of visual features in artwork recommendation. Recent
works have shown that visual features obtained from pre-trained deep neural
networks (DNNs) perform very well for recommending digital art. Other recent
works have shown that explicit visual features (EVF) based on attractiveness
can perform well in preference prediction tasks, but no previous work has
compared DNN features versus specific attractiveness-based visual features
(e.g. brightness, texture) in terms of recommendation performance. In this
work, we study and compare the performance of DNN and EVF features for the
purpose of physical artwork recommendation using transactional data from
UGallery, an online store of physical paintings. In addition, we perform an
exploratory analysis to understand if DNN embedded features have some relation
with certain EVF. Our results show that DNN features outperform EVF, that
certain EVF features are more suited for physical artwork recommendation and,
finally, we show evidence that certain neurons in the DNN might be partially
encoding visual features such as brightness, providing an opportunity for
explaining recommendations based on visual neural models.



Existing Markov Chain Monte Carlo (MCMC) methods are either based on
general-purpose and domain-agnostic schemes which can lead to slow convergence,
or hand-crafting of problem-specific proposals by an expert. We propose
A-NICE-MC, a novel method to train flexible parametric Markov chain kernels to
produce samples with desired properties. First, we propose an efficient
likelihood-free adversarial training method to train a Markov chain and mimic a
given data distribution. Then, we leverage flexible volume preserving flows to
obtain parametric kernels for MCMC. Using a bootstrap approach, we show how to
train efficient Markov chains to sample from a prescribed posterior
distribution by iteratively improving the quality of both the model and the
samples. A-NICE-MC provides the first framework to automatically design
efficient domain-specific MCMC proposals. Empirical results demonstrate that
A-NICE-MC combines the strong guarantees of MCMC with the expressiveness of
deep neural networks, and is able to significantly outperform competing methods
such as Hamiltonian Monte Carlo.



We present a straightforward source-to-source transformation that introduces
justifications for user-defined constraints into the CHR programming language.
Then a scheme of two rules suffices to allow for logical retraction (deletion,
removal) of constraints during computation. Without the need to recompute from
scratch, these rules remove not only the constraint but also undo all
consequences of the rule applications that involved the constraint. We prove a
confluence result concerning the rule scheme and show its correctness. When
algorithms are written in CHR, constraints represent both data and operations.
CHR is already incremental by nature, i.e. constraints can be added at runtime.
Logical retraction adds decrementality. Hence any algorithm written in CHR with
justifications will become fully dynamic. Operations can be undone and data can
be removed at any point in the computation without compromising the correctness
of the result. We present two classical examples of dynamic algorithms, written
in our prototype implementation of CHR with justifications that is available
online: maintaining the minimum of a changing set of numbers and shortest paths
in a graph whose edges change.



In this article, we extend the conventional framework of
convolutional-Restricted-Boltzmann-Machine to learn highly abstract features
among abitrary number of time related input maps by constructing a layer of
multiplicative units, which capture the relations among inputs. In many cases,
more than two maps are strongly related, so it is wise to make multiplicative
unit learn relations among more input maps, in other words, to find the optimal
relational-order of each unit. In order to enable our machine to learn
relational order, we developed a reinforcement-learning method whose optimality
is proven to train the network.



We propose a simple and efficient approach to learning sparse models. Our
approach consists of (1) projecting the data into a lower dimensional space,
(2) learning a dense model in the lower dimensional space, and then (3)
recovering the sparse model in the original space via compressive sensing. We
apply this approach to Non-negative Matrix Factorization (NMF), tensor
decomposition and linear classification---showing that it obtains $10\times$
compression with negligible loss in accuracy on real data, and obtains up to
$5\times$ speedups. Our main theoretical contribution is to show the following
result for NMF: if the original factors are sparse, then their projections are
the sparsest solutions to the projected NMF problem. This explains why our
method works for NMF and shows an interesting new property of random
projections: they can preserve the solutions of non-convex optimization
problems such as NMF.



This paper presents a technique for symmetry reduction that adaptively
assigns a prefix of variables in a system of constraints so that the generated
prefix-assignments are pairwise nonisomorphic under the action of the symmetry
group of the system. The technique is based on McKay's canonical extension
framework [J. Algorithms 26 (1998), no. 2, 306-324]. Among key features of the
technique are (i) adaptability - the prefix sequence can be user-prescribed and
truncated for compatibility with the group of symmetries; (ii)
parallelisability - prefix-assignments can be processed in parallel
independently of each other; (iii) versatility - the method is applicable
whenever the group of symmetries can be concisely represented as the
automorphism group of a vertex-colored graph; and (iv) implementability - the
method can be implemented relying on a canonical labeling map for
vertex-colored graphs as the only nontrivial subroutine. To demonstrate the
tentative practical applicability of our technique we have prepared a
preliminary implementation and report on a limited set of experiments that
demonstrate ability to reduce symmetry on hard instances.



A number of recent works have proposed techniques for end-to-end learning of
communication protocols among cooperative multi-agent populations, and have
simultaneously found the emergence of grounded human-interpretable language in
the protocols developed by the agents, all learned without any human
supervision!
  In this paper, using a Task and Tell reference game between two agents as a
testbed, we present a sequence of 'negative' results culminating in a
'positive' one -- showing that while most agent-invented languages are
effective (i.e. achieve near-perfect task rewards), they are decidedly not
interpretable or compositional.
  In essence, we find that natural language does not emerge 'naturally',
despite the semblance of ease of natural-language-emergence that one may gather
from recent literature. We discuss how it is possible to coax the invented
languages to become more and more human-like and compositional by increasing
restrictions on how two agents may communicate.



The number of complete chloroplastic genomes increases day after day, making
it possible to rethink plants phylogeny at the biomolecular era. Given a set of
close plants sharing in the order of one hundred of core chloroplastic genes,
this article focuses on how to extract the largest subset of sequences in order
to obtain the most supported species tree. Due to computational complexity, a
discrete and distributed Particle Swarm Optimization (DPSO) is proposed. It is
finally applied to the core genes of Rosales order.



This paper describes our submission to the 2017 BioASQ challenge. We
participated in Task B, Phase B which is concerned with biomedical question
answering (QA). We focus on factoid and list question, using an extractive QA
model, that is, we restrict our system to output substrings of the provided
text snippets. At the core of our system, we use FastQA, a state-of-the-art
neural QA system. We extended it with biomedical word embeddings and changed
its answer layer to be able to answer list questions in addition to factoid
questions. We pre-trained the model on a large-scale open-domain QA dataset,
SQuAD, and then fine-tuned the parameters on the BioASQ training set. With our
approach, we achieve state-of-the-art results on factoid questions and
competitive results on list questions.



We present a deep, fully convolutional neural network that learns to route a
circuit layout net with appropriate choice of metal tracks and wire class
combinations. Inputs to the network are the encoded layouts containing spatial
location of pins to be routed. After 15 fully convolutional stages followed by
a score comparator, the network outputs 8 layout layers (corresponding to 4
route layers, 3 via layers and an identity-mapped pin layer) which are then
decoded to obtain the routed layouts. We formulate this as a binary
segmentation problem on a per-pixel per-layer basis, where the network is
trained to correctly classify pixels in each layout layer to be 'on' or 'off'.
To demonstrate learnability of layout design rules, we train the network on a
dataset of 50,000 train and 10,000 validation samples that we generate based on
certain pre-defined layout constraints. Precision, recall and $F_1$ score
metrics are used to track the training progress. Our network achieves
$F_1\approx97\%$ on the train set and $F_1\approx92\%$ on the validation set.
We use PyTorch for implementing our model. Code is made publicly available at
https://github.com/sjain-stanford/deep-route .



Maximum Likelihood Estimation (MLE) suffers from data sparsity problem in
sequence prediction tasks where training resource is rare. In order to
alleviate this problem, in this paper, we propose a novel generative bridging
network (GBN) to train sequence prediction models, which contains a generator
and a bridge. Unlike MLE directly maximizing the likelihood of the ground
truth, the bridge extends the point-wise ground truth to a bridge distribution
(containing inexhaustible examples), and the generator is trained to minimize
their KL-divergence. In order to guide the training of generator with
additional signals, the bridge distribution can be set or trained to possess
specific properties, by using different constraints. More specifically, to
increase output diversity, enhance language smoothness and relieve learning
burden, three different regularization constraints are introduced to construct
bridge distributions. By combining these bridges with a sequence generator,
three independent GBNs are proposed, namely uniform GBN, language-model GBN and
coaching GBN. Experiment conducted on two recognized sequence prediction tasks
(machine translation and abstractive text summarization) shows that our
proposed GBNs can yield significant improvements over strong baseline systems.
Furthermore, by analyzing samples drawn from bridge distributions, expected
influences on the sequence model training are verified.



Class-agnostic object tracking is particularly difficult in cluttered
environments as target specific discriminative models cannot be learned a
priori. Inspired by how the human visual cortex employs spatial attention and
separate "where" and "what" processing pathways to actively suppress irrelevant
visual features, this work develops a hierarchical attentive recurrent model
for single object tracking in videos. The first layer of attention discards the
majority of background by selecting a region containing the object of interest,
while the subsequent layers tune in on visual features particular to the
tracked object. This framework is fully differentiable and can be trained in a
purely data driven fashion by gradient methods. To improve training
convergence, we augment the loss function with terms for a number of auxiliary
tasks relevant for tracking. Evaluation of the proposed model is performed on
two datasets: pedestrian tracking on the KTH activity recognition dataset and
the more difficult KITTI object tracking dataset.



A vibrant theoretical research area are efficient exact parameterized
algorithms. Very recent solving competitions such as the PACE challenge show
that there is also increasing practical interest in the parameterized
algorithms community. An important research question is whether dedicated
parameterized exact algorithms exhibit certain practical relevance and one can
even beat well-established problem solvers. We consider the logic-based
declarative modeling language and problem solving framework Answer Set
Programming (ASP). State-of-the-art ASP solvers rely considerably on Sat-based
algorithms. An ASP solver (DynASP2), which is based on a classical dynamic
programming on tree decompositions, has been published very recently.
Unfortunately, DynASP2 can outperform modern ASP solvers on programs of small
treewidth only if the question of interest is to count the number of solutions.
In this paper, we describe underlying concepts of our new implementation
(DynASP2.5) that shows competitive behavior to state-of-the-art ASP solvers
even for finding just one solution when solving problems as the Steiner tree
problem that have been modeled in ASP on graphs with low treewidth. Our
implementation is based on a novel approach that we call multi-pass dynamic
programming (M-DPSINC).



In this paper, we study Reiter's propositional default logic when the
treewidth of a certain graph representation (semi-primal graph) of the input
theory is bounded. We establish a dynamic programming algorithm on tree
decompositions that decides whether a theory has a consistent stable extension
(Ext). Our algorithm can even be used to enumerate all generating defaults
(ExtEnum) that lead to stable extensions.
  We show that our algorithm decides Ext in linear time in the input theory and
triple exponential time in the treewidth (so-called fixed-parameter linear
algorithm).
  Further, our algorithm solves ExtEnum with a pre-computation step that is
linear in the input theory and triple exponential in the treewidth followed by
a linear delay to output solutions.



We present an approach for agents to learn representations of a global map
from sensor data, to aid their exploration in new environments. To achieve
this, we embed procedures mimicking that of traditional Simultaneous
Localization and Mapping (SLAM) into the soft attention based addressing of
external memory architectures, in which the external memory acts as an internal
representation of the environment. This structure encourages the evolution of
SLAM-like behaviors inside a completely differentiable deep neural network. We
show that this approach can help reinforcement learning agents to successfully
explore new environments where long-term memory is essential. We validate our
approach in both challenging grid-world environments and preliminary Gazebo
experiments. A video of our experiments can be found at: https://goo.gl/G2Vu5y.



Financial portfolio management is the process of constant redistribution of a
fund into different financial products. This paper presents a
financial-model-free Reinforcement Learning framework to provide a deep machine
learning solution to the portfolio management problem. The framework consists
of the Ensemble of Identical Independent Evaluators (EIIE) topology, a
Portfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL)
scheme, and a fully exploiting and explicit reward function. This framework is
realized in three instants in this work with a Convolutional Neural Network
(CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory
(LSTM). They are, along with a number of recently reviewed or published
portfolio-selection strategies, examined in three back-test experiments with a
trading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are
electronic and decentralized alternatives to government-issued money, with
Bitcoin as the best-known example of a cryptocurrency. All three instances of
the framework monopolize the top three positions in all experiments,
outdistancing other compared trading algorithms. Although with a high
commission rate of 0.25% in the backtests, the framework is able to achieve at
least 4-fold returns in 50 days.



The challenge of sharing and communicating information is crucial in complex
human-robot interaction (HRI) scenarios. Ontologies and symbolic reasoning are
the state-of-the-art approaches for a natural representation of knowledge,
especially within the Semantic Web domain. In such a context, scripted
paradigms have been adopted to achieve high expressiveness. Nevertheless, since
symbolic reasoning is a high complexity problem, optimizing its performance
requires a careful design of the knowledge. Specifically, a robot architecture
requires the integration of several components implementing different behaviors
and generating a series of beliefs. Most of the components are expected to
access, manipulate, and reason upon a run-time generated semantic
representation of knowledge grounding robot behaviors and perceptions through
formal axioms, with soft real-time requirements.



In this paper the elements of the CAPTCHA usability are analyzed. CAPTCHA, as
a time progressive element in computer science, has been under constant
interest of ordinary, professional as well as the scientific users of the
Internet. The analysis is given based on the usability elements of CAPTCHA
which are abbreviated as user-centric approach to the CAPTCHA. To demonstrate
it, the specific type of Dice CAPTCHA is used in the experiment. The experiment
is conducted on 190 Internet users with different demographic characteristics
on laptop and tablet computers. The obtained results are statistically
processed. At the end, the results are compared and conclusion of their use is
drawn.



We consider the problem of learning the functions computing children from
parents in a Structural Causal Model once the underlying causal graph has been
identified. This is in some sense the second step after causal discovery.
Taking a probabilistic approach to estimating these functions, we derive a
natural myopic active learning scheme that identifies the intervention which is
optimally informative about all of the unknown functions jointly, given
previously observed data. We test the derived algorithms on simple examples, to
demonstrate that they produce a structured exploration policy that
significantly improves on unstructured base-lines.



It is widely observed that deep learning models with learned parameters
generalize well, even with much more model parameters than the number of
training samples. We systematically investigate the underlying reasons why deep
neural networks often generalize well, and reveal the difference between the
minima (with the same training error) that generalize well and those they
don't. We show that it is the characteristics the landscape of the loss
function that explains the good generalization capability. For the landscape of
loss function for deep networks, the volume of basin of attraction of good
minima dominates over that of poor minima, which guarantees optimization
methods with random initialization to converge to good minima. We theoretically
justify our findings through analyzing 2-layer neural networks; and show that
the low-complexity solutions have a small norm of Hessian matrix with respect
to model parameters. For deeper networks, extensive numerical evidence helps to
support our arguments.



In this work, we perform an exploratory study on synthesizing deep neural
networks using biological synaptic strength distributions, and the potential
influence of different distributions on modelling performance particularly for
the scenario associated with small data sets. Surprisingly, a CNN with
convolutional layer synaptic strengths drawn from biologically-inspired
distributions such as log-normal or correlated center-surround distributions
performed relatively well suggesting a possibility for designing deep neural
network architectures that do not require many data samples to learn, and can
sidestep current training procedures while maintaining or boosting modelling
performance.



Deep reinforcement learning (RL) methods have significant potential for
dialogue policy optimisation. However, they suffer from a poor performance in
the early stages of learning. This is especially problematic for on-line
learning with real users. Two approaches are introduced to tackle this problem.
Firstly, to speed up the learning process, two sample-efficient neural networks
algorithms: trust region actor-critic with experience replay (TRACER) and
episodic natural actor-critic with experience replay (eNACER) are presented.
For TRACER, the trust region helps to control the learning step size and avoid
catastrophic model changes. For eNACER, the natural gradient identifies the
steepest ascent direction in policy space to speed up the convergence. Both
models employ off-policy learning with experience replay to improve
sample-efficiency. Secondly, to mitigate the cold start issue, a corpus of
demonstration data is utilised to pre-train the models prior to on-line
reinforcement learning. Combining these two approaches, we demonstrate a
practical approach to learn deep RL-based dialogue policies and demonstrate
their effectiveness in a task-oriented information seeking domain.



In reinforcement learning (RL) tasks, an efficient exploration mechanism
should be able to encourage an agent to take actions that lead to less frequent
states which may yield higher accumulative future return. However, both knowing
about the future and evaluating the frequentness of states are non-trivial
tasks, especially for deep RL domains, where a state is represented by
high-dimensional image frames. In this paper, we propose a novel informed
exploration framework for deep RL tasks, where we build the capability for a RL
agent to predict over the future transitions and evaluate the frequentness for
the predicted future frames in a meaningful manner. To this end, we train a
deep prediction model to generate future frames given a state-action pair, and
a convolutional autoencoder model to generate deep features for conducting
hashing over the seen frames. In addition, to utilize the counts derived from
the seen frames to evaluate the frequentness for the predicted frames, we
tackle the challenge of making the hash codes for the predicted future frames
to match with their corresponding seen frames. In this way, we could derive a
reliable metric for evaluating the novelty of the future direction pointed by
each action, and hence inform the agent to explore the least frequent one. We
use Atari 2600 games as the testing environment and demonstrate that the
proposed framework achieves significant performance gain over a
state-of-the-art informed exploration approach in most of the domains.



Regression or classification? This is perhaps the most basic question faced
when tackling a new supervised learning problem. We present an Evolutionary
Deep Learning (EDL) algorithm that automatically solves this by identifying the
question type with high accuracy, along with a proposed deep architecture.
Typically, a significant amount of human insight and preparation is required
prior to executing machine learning algorithms. For example, when creating deep
neural networks, the number of parameters must be selected in advance and
furthermore, a lot of these choices are made based upon pre-existing knowledge
of the data such as the use of a categorical cross entropy loss function.
Humans are able to study a dataset and decide whether it represents a
classification or a regression problem, and consequently make decisions which
will be applied to the execution of the neural network. We propose the
Automated Problem Identification (API) algorithm, which uses an evolutionary
algorithm interface to TensorFlow to manipulate a deep neural network to decide
if a dataset represents a classification or a regression problem. We test API
on 16 different classification, regression and sentiment analysis datasets with
up to 10,000 features and up to 17,000 unique target values. API achieves an
average accuracy of $96.3\%$ in identifying the problem type without hardcoding
any insights about the general characteristics of regression or classification
problems. For example, API successfully identifies classification problems even
with 1000 target values. Furthermore, the algorithm recommends which loss
function to use and also recommends a neural network architecture. Our work is
therefore a step towards fully automated machine learning.



In the field of reinforcement learning there has been recent progress towards
safety and high-confidence bounds on policy performance. However, to our
knowledge, no practical methods exist for determining high-confidence policy
performance bounds in the inverse reinforcement learning setting---where the
true reward function is unknown and only samples of expert behavior are given.
We propose a sampling method based on Bayesian inverse reinforcement learning
that uses demonstrations to determine practical high-confidence upper bounds on
the $\alpha$-worst-case difference in expected return between any evaluation
policy and the optimal policy under the expert's unknown reward function. We
evaluate our proposed bound on both a standard grid navigation task and a
simulated driving task and achieve tighter and more accurate bounds than a
feature count-based baseline. We also give examples of how our proposed bound
can be utilized to perform risk-aware policy selection and risk-aware policy
improvement. Because our proposed bound requires several orders of magnitude
fewer demonstrations than existing high-confidence bounds, it is the first
practical method that allows agents that learn from demonstration to express
confidence in the quality of their learned policy.



Complex systems can be modelled at various levels of detail. Ideally, causal
models of the same system should be consistent with one another in the sense
that they agree in their predictions of the effects of interventions. We
formalise this notion of consistency in the case of Structural Equation Models
(SEMs) by introducing exact transformations between SEMs. This provides a
general language to consider, for instance, the different levels of description
in the following three scenarios: (a) models with large numbers of variables
versus models in which the `irrelevant' or unobservable variables have been
marginalised out; (b) micro-level models versus macro-level models in which the
macro-variables are aggregate features of the micro-variables; (c) dynamical
time series models versus models of their stationary behaviour. Our analysis
stresses the importance of well specified interventions in the causal modelling
process and sheds light on the interpretation of cyclic SEMs.



Question-answering (QA) on video contents is a significant challenge for
achieving human-level intelligence as it involves both vision and language in
real-world settings. Here we demonstrate the possibility of an AI agent
performing video story QA by learning from a large amount of cartoon videos. We
develop a video-story learning model, i.e. Deep Embedded Memory Networks
(DEMN), to reconstruct stories from a joint scene-dialogue video stream using a
latent embedding space of observed data. The video stories are stored in a
long-term memory component. For a given question, an LSTM-based attention model
uses the long-term memory to recall the best question-story-answer triplet by
focusing on specific words containing key information. We trained the DEMN on a
novel QA dataset of children's cartoon video series, Pororo. The dataset
contains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained
sentences for scene description, and 8,913 story-related QA pairs. Our
experimental results show that the DEMN outperforms other QA models. This is
mainly due to 1) the reconstruction of video stories in a scene-dialogue
combined form that utilize the latent embedding and 2) attention. DEMN also
achieved state-of-the-art results on the MovieQA benchmark.



We present a conditional generative model that maps low-dimensional
embeddings of multiple modalities of data to a common latent space hence
extracting semantic relationships between them. The embedding specific to a
modality is first extracted and subsequently a constrained optimization
procedure is performed to project the two embedding spaces to a common
manifold. The individual embeddings are generated back from this common latent
space. However, in order to enable independent conditional inference for
separately extracting the corresponding embeddings from the common latent space
representation, we deploy a proxy variable trick - wherein, the single shared
latent space is replaced by the respective separate latent spaces of each
modality. We design an objective function, such that, during training we can
force these separate spaces to lie close to each other, by minimizing the
distance between their probability distribution functions. Experimental results
demonstrate that the learned joint model can generalize to learning concepts of
double MNIST digits with additional attributes of colors,from both textual and
speech input.



Social dilemmas are situations where individuals face a temptation to
increase their payoffs at a cost to total welfare. Building artificially
intelligent agents that achieve good outcomes in these situations is important
because many real world interactions include a tension between selfish
interests and the welfare of others. We show how to modify modern reinforcement
learning methods to construct agents that act in ways that are simple to
understand, nice (begin by cooperating), provokable (try to avoid being
exploited), and forgiving (try to return to mutual cooperation). We show both
theoretically and experimentally that such agents can maintain cooperation in
Markov social dilemmas. Our construction does not require training methods
beyond a modification of self-play, thus if an environment is such that good
strategies can be constructed in the zero-sum case (eg. Atari) then we can
construct agents that solve social dilemmas in this environment.



We propose a framework for feature selection that employs kernel-based
measures of independence to find a subset of covariates that is maximally
predictive of the response. Building on past work in kernel dimension
reduction, we formulate our approach as a constrained optimization problem
involving the trace of the conditional covariance operator, and additionally
provide some consistency results. We then demonstrate on a variety of synthetic
and real data sets that our method compares favorably with other
state-of-the-art algorithms.



Sentiment analysis is the Natural Language Processing (NLP) task dealing with
the detection and classification of sentiments in texts. While some tasks deal
with identifying the presence of sentiment in the text (Subjectivity analysis),
other tasks aim at determining the polarity of the text categorizing them as
positive, negative and neutral. Whenever there is a presence of sentiment in
the text, it has a source (people, group of people or any entity) and the
sentiment is directed towards some entity, object, event or person. Sentiment
analysis tasks aim to determine the subject, the target and the polarity or
valence of the sentiment. In our work, we try to automatically extract
sentiment (positive or negative) from Facebook posts using a machine learning
approach.While some works have been done in code-mixed social media data and in
sentiment analysis separately, our work is the first attempt (as of now) which
aims at performing sentiment analysis of code-mixed social media text. We have
used extensive pre-processing to remove noise from raw text. Multilayer
Perceptron model has been used to determine the polarity of the sentiment. We
have also developed the corpus for this task by manually labeling Facebook
posts with their associated sentiments.



Various measures can be used to estimate bias or unfairness in a predictor.
Previous work has already established that some of these measures are
incompatible with each other. Here we show that, when groups differ in
prevalence of the predicted event, several intuitive, reasonable measures of
fairness (probability of positive prediction given occurrence or
non-occurrence; probability of occurrence given prediction or non-prediction;
and ratio of predictions over occurrences for each group) are all mutually
exclusive: if one of them is equal among groups, the other two must differ. The
only exceptions are for perfect, or trivial (always-positive or
always-negative) predictors. As a consequence, any non-perfect, non-trivial
predictor must necessarily be "unfair" under two out of three reasonable sets
of criteria. This result readily generalizes to a wide range of well-known
statistical quantities (sensitivity, specificity, false positive rate,
precision, etc.), all of which can be divided into three mutually exclusive
groups. Importantly, The results applies to all predictors, whether algorithmic
or human. We conclude with possible ways to handle this effect when assessing
and designing prediction methods.



Modeling users for the purpose of identifying their preferences and then
personalizing services on the basis of these models is a complex task,
primarily due to the need to take into consideration various explicit and
implicit signals, missing or uncertain information, contextual aspects, and
more. In this study, a novel generic approach for uncovering latent preference
patterns from user data is proposed and evaluated. The approach relies on
representing the data using graphs, and then systematically extracting
graph-based features and using them to enrich the original user models. The
extracted features encapsulate complex relationships between users, items, and
metadata. The enhanced user models can then serve as an input to any
recommendation algorithm. The proposed approach is domain-independent
(demonstrated on data from movies, music, and business recommender systems),
and is evaluated using several state-of-the-art machine learning methods, on
different recommendation tasks, and using different evaluation metrics. The
results show a unanimous improvement in the recommendation accuracy across
tasks and domains. In addition, the evaluation provides a deeper analysis
regarding the performance of the approach in special scenarios, including high
sparsity and variability of ratings.



Content-invariance in mapping codes learned by GAEs is a useful feature for
various relation learning tasks. In this paper we show that the
content-invariance of mapping codes for images of 2D and 3D rotated objects can
be substantially improved by extending the standard GAE loss (symmetric
reconstruction error) with a regularization term that penalizes the symmetric
cross-reconstruction error. This error term involves reconstruction of pairs
with mapping codes obtained from other pairs exhibiting similar
transformations. Although this would principally require knowledge of the
transformations exhibited by training pairs, our experiments show that a
bootstrapping approach can sidestep this issue, and that the regularization
term can effectively be used in an unsupervised setting.



Dealing with sparse rewards is one of the biggest challenges in Reinforcement
Learning (RL). We present a novel technique called Hindsight Experience Replay
which allows sample-efficient learning from rewards which are sparse and binary
and therefore avoid the need for complicated reward engineering. It can be
combined with an arbitrary off-policy RL algorithm and may be seen as a form of
implicit curriculum.
  We demonstrate our approach on the task of manipulating objects with a
robotic arm. In particular, we run experiments on three different tasks:
pushing, sliding, and pick-and-place, in each case using only binary rewards
indicating whether or not the task is completed. Our ablation studies show that
Hindsight Experience Replay is a crucial ingredient which makes training
possible in these challenging environments. We show that our policies trained
on a physics simulation can be deployed on a physical robot and successfully
complete the task.



Despite large incentives, ecorrectness in software remains an elusive goal.
Declarative programming techniques, where algorithms are derived from a
specification of the desired behavior, offer hope to address this problem,
since there is a combinatorial reduction in complexity in programming in terms
of specifications instead of algorithms, and arbitrary desired properties can
be expressed and enforced in specifications directly. However, limitations on
performance have prevented programming with declarative specifications from
becoming a mainstream technique for general-purpose programming. To address the
performance bottleneck in deriving an algorithm from a specification, I propose
information-gain computation, a framework where an adaptive evaluation strategy
is used to efficiently perform a search which derives algorithms that provide
information about a query most directly. Within this framework, opportunities
to compress the search space present themselves, which suggest that
information-theoretic bounds on the performance of such a system might be
articulated and a system designed to achieve them. In a preliminary empirical
study of adaptive evaluation for a simple test program, the evaluation strategy
adapts successfully to evaluate a query efficiently.



While natural languages are compositional, how state-of-the-art neural models
achieve compositionality is still unclear. We propose a deep network, which not
only achieves competitive accuracy for text classification, but also exhibits
compositional behavior. That is, while creating hierarchical representations of
a piece of text, such as a sentence, the lower layers of the network distribute
their layer-specific attention weights to individual words. In contrast, the
higher layers compose meaningful phrases and clauses, whose lengths increase as
the networks get deeper until fully composing the sentence.



Real world multimedia data is often composed of multiple modalities such as
an image or a video with associated text (e.g. captions, user comments, etc.)
and metadata. Such multimodal data packages are prone to manipulations, where a
subset of these modalities can be altered to misrepresent or repurpose data
packages, with possible malicious intent. It is, therefore, important to
develop methods to assess or verify the integrity of these multimedia packages.
Using computer vision and natural language processing methods to directly
compare the image (or video) and the associated caption to verify the integrity
of a media package is only possible for a limited set of objects and scenes. In
this paper, we present a novel deep learning-based approach for assessing the
semantic integrity of multimedia packages containing images and captions, using
a reference set of multimedia packages. We construct a joint embedding of
images and captions with deep multimodal representation learning on the
reference dataset in a framework that also provides image-caption consistency
scores (ICCSs). The integrity of query media packages is assessed as the
inlierness of the query ICCSs with respect to the reference dataset. We present
the MultimodAl Information Manipulation dataset (MAIM), a new dataset of media
packages from Flickr, which we make available to the research community. We use
both the newly created dataset as well as Flickr30K and MS COCO datasets to
quantitatively evaluate our proposed approach. The reference dataset does not
contain unmanipulated versions of tampered query packages. Our method is able
to achieve F1 scores of 0.75, 0.89 and 0.94 on MAIM, Flickr30K and MS COCO,
respectively, for detecting semantically incoherent media packages.



Recently, shared mobility has been proven to be an effective way to relieve
urban traffic congestion and reduce energy consumption. Despite the emergence
of several nationwide platforms, the pricing schemes and the vehicle
dispatching problem of such platforms are optimized in an ad hoc manner. In
this paper, we introduce a general framework that incorporates geographic
information and time-sensitive dynamic environment parameters (such as the
dynamically changing demand) and models the pricing and dispatching problem as
a Markov Decision Process with continuous state and action spaces. Despite of
the PSPACE-hardness of general MDPs, we provide efficient algorithms finding
the exact revenue (or welfare) optimal (potentially randomized) pricing
schemes. We also characterize the optimal solution via primal-dual analysis of
a convex program. Finally, we also discuss generalizing our model by showing
how to reduce a wide range of general settings in practice to our model.



The regret bound of an optimization algorithms is one of the basic criteria
for evaluating the performance of the given algorithm. By inspecting the
differences between the regret bounds of traditional algorithms and adaptive
one, we provide a guide for choosing an optimizer with respect to the given
data set and the loss function. For analysis, we assume that the loss function
is convex and its gradient is Lipschitz continuous.



This paper aims at providing insight on the transferability of deep CNN
features to unsupervised problems. We study the impact of different pretrained
CNN feature extractors on the problem of image set clustering for object
classification as well as fine-grained classification. We propose a rather
straightforward pipeline combining deep-feature extraction using a CNN
pretrained on ImageNet and a classic clustering algorithm to classify sets of
images. This approach is compared to state-of-the-art algorithms in
image-clustering and provides better results. These results strengthen the
belief that supervised training of deep CNN on large datasets, with a large
variability of classes, extracts better features than most carefully designed
engineering approaches, even for unsupervised tasks. We also validate our
approach on a robotic application, consisting in sorting and storing objects
smartly based on clustering.



Automatic image description systems are commonly trained and evaluated on
large image description datasets. Recently, researchers have started to collect
such datasets for languages other than English. An unexplored question is how
different these datasets are from English and, if there are any differences,
what causes them to differ. This paper provides a cross-linguistic comparison
of Dutch, English, and German image descriptions. We find that these
descriptions are similar in many respects, but the familiarity of crowd workers
with the subjects of the images has a noticeable influence on description
specificity.



Recent progress in logic programming (e.g., the development of the Answer Set
Programming paradigm) has made it possible to teach it to general undergraduate
and even high school students. Given the limited exposure of these students to
computer science, the complexity of downloading, installing and using tools for
writing logic programs could be a major barrier for logic programming to reach
a much wider audience. We developed an online answer set programming
environment with a self contained file system and a simple interface, allowing
users to write logic programs and perform several tasks over the programs.



We introduce a graphical framework for fair division in cake cutting, where
comparisons between agents are limited by an underlying network structure. We
generalize the classical fairness notions of envy-freeness and proportionality
to this graphical setting. Given a simple undirected graph G, an allocation is
envy-free on G if no agent envies any of her neighbor's share, and is
proportional on G if every agent values her own share no less than the average
among her neighbors, with respect to her own measure. These generalizations
open new research directions in developing simple and efficient algorithms that
can produce fair allocations under specific graph structures.
  On the algorithmic frontier, we first propose a moving-knife algorithm that
outputs an envy-free allocation on trees. The algorithm is significantly
simpler than the discrete and bounded envy-free algorithm recently designed by
Aziz and Mackenzie for complete graphs. Next, we give a discrete and bounded
algorithm for computing a proportional allocation on descendant graphs, a class
of graphs by taking a rooted tree and connecting all its ancestor-descendant
pairs.



Diversity is one of the fundamental properties for the survival of species,
populations, and organizations. Recent advances in deep learning allow for the
rapid and automatic assessment of organizational diversity and possible
discrimination by race, sex, age and other parameters. Automating the process
of assessing the organizational diversity using the deep neural networks and
eliminating the human factor may provide a set of real-time unbiased reports to
all stakeholders. In this pilot study we applied the deep-learned predictors of
race and sex to the executive management and board member profiles of the 500
largest companies from the 2016 Forbes Global 2000 list and compared the
predicted ratios to the ratios within each company's country of origin and
ranked them by the sex-, age- and race- diversity index (DI). While the study
has many limitations and no claims are being made concerning the individual
companies, it demonstrates a method for the rapid and impartial assessment of
organizational diversity using deep neural networks.



For safe and efficient planning and control in autonomous driving, we need a
driving policy which can achieve desirable driving quality in long-term horizon
with guaranteed safety and feasibility. Optimization-based approaches, such as
Model Predictive Control (MPC), can provide such optimal policies, but their
computational complexity is generally unacceptable for real-time
implementation. To address this problem, we propose a fast integrated planning
and control framework that combines learning- and optimization-based approaches
in a two-layer hierarchical structure. The first layer, defined as the "policy
layer", is established by a neural network which learns the long-term optimal
driving policy generated by MPC. The second layer, called the "execution
layer", is a short-term optimization-based controller that tracks the reference
trajecotries given by the "policy layer" with guaranteed short-term safety and
feasibility. Moreover, with efficient and highly-representative features, a
small-size neural network is sufficient in the "policy layer" to handle many
complicated driving scenarios. This renders online imitation learning with
Dataset Aggregation (DAgger) so that the performance of the "policy layer" can
be improved rapidly and continuously online. Several exampled driving scenarios
are demonstrated to verify the effectiveness and efficiency of the proposed
framework.



The enormous amount of texts published daily by Internet users has fostered
the development of methods to analyze this content in several natural language
processing areas, such as sentiment analysis. The main goal of this task is to
classify the polarity of a message. Even though many approaches have been
proposed for sentiment analysis, some of the most successful ones rely on the
availability of large annotated corpus, which is an expensive and
time-consuming process. In recent years, distant supervision has been used to
obtain larger datasets. So, inspired by these techniques, in this paper we
extend such approaches to incorporate popular graphic symbols used in
electronic messages, the emojis, in order to create a large sentiment corpus
for Portuguese. Trained on almost one million tweets, several models were
tested in both same domain and cross-domain corpora. Our methods obtained very
competitive results in five annotated corpora from mixed domains (Twitter and
product reviews), which proves the domain-independent property of such
approach. In addition, our results suggest that the combination of emoticons
and emojis is able to properly capture the sentiment of a message.



We describe the Inspire system which participated in the first competition on
Inductive Logic Programming (ILP). Inspire is based on Answer Set Programming
(ASP), its most important feature is an ASP encoding for hypothesis space
generation: given a set of facts representing the mode bias, and a set of cost
configuration parameters, each answer set of this encoding represents a single
rule that is considered for finding a hypothesis that entails the given
examples. Compared with state-of-the-art methods that use the length of the
rule body as a metric for rule complexity, our approach permits a much more
fine-grained specification of the shape of hypothesis candidate rules. Similar
to the ILASP system, our system iteratively increases the rule cost limit until
it finds a suitable hypothesis. Different from ILASP, our approach generates a
new search space for each cost limit. The Inspire system searches for a
hypothesis that entails a single example at a time, utilizing a simplification
of the ASP encoding used in the XHAIL system. To evaluate ASP we use Clingo. We
perform experiments with the development and test set of the ILP competition.
For comparison we also adapted the ILASP system to process competition
instances. Experimental results show, that Inspire performs better than ILASP,
and that cost parameters for the hypothesis search space are an important
factor for finding suitable hypotheses efficiently.



Every year at the United Nations, member states deliver statements during the
General Debate discussing major issues in world politics. These speeches
provide invaluable information on governments' perspectives and preferences on
a wide range of issues, but have largely been overlooked in the study of
international politics. This paper introduces a new dataset consisting of over
7,701 English-language country statements from 1970-2016. We demonstrate how
the UN General Debate Corpus (UNGDC) can be used to derive country positions on
different policy dimensions using text analytic methods. The paper provides
applications of these estimates, demonstrating the contribution the UNGDC can
make to the study of international politics.



Adversarial samples are strategically modified samples, which are crafted
with the purpose of fooling a classifier at hand. An attacker introduces
specially crafted adversarial samples to a deployed classifier, which are being
mis-classified by the classifier. However, the samples are perceived to be
drawn from entirely different classes and thus it becomes hard to detect the
adversarial samples. Most of the prior works have been focused on synthesizing
adversarial samples in the image domain. In this paper, we propose a new method
of crafting adversarial text samples by modification of the original samples.
Modifications of the original text samples are done by deleting or replacing
the important or salient words in the text or by introducing new words in the
text sample. Our algorithm works best for the datasets which have
sub-categories within each of the classes of examples. While crafting
adversarial samples, one of the key constraint is to generate meaningful
sentences which can at pass off as legitimate from language (English)
viewpoint. Experimental results on IMDB movie review dataset for sentiment
analysis and Twitter dataset for gender detection show the efficiency of our
proposed method.



The amount of text that is generated every day is increasing dramatically.
This tremendous volume of mostly unstructured text cannot be simply processed
and perceived by computers. Therefore, efficient and effective techniques and
algorithms are required to discover useful patterns. Text mining is the task of
extracting meaningful information from text, which has gained significant
attentions in recent years. In this paper, we describe several of the most
fundamental text mining tasks and techniques including text pre-processing,
classification and clustering. Additionally, we briefly explain text mining in
biomedical and health care domains.



In this paper, we propose a multi-task learning from demonstration method
that works using raw images as input to autonomously accomplish a wide variety
of tasks in the real world using a low-cost robotic arm. The controller is a
single recurrent neural network that can generate robot arm trajectories to
perform different manipulation tasks. In order to learn complex skills from
relatively few demonstrations, we share parameters across different tasks. Our
network also combines VAE-GAN-based reconstruction with autoregressive
multimodal action prediction for improved data efficiency. Our results show
that weight sharing and reconstruction substantially improve generalization and
robustness, and that training on multiple tasks simultaneously greatly improves
the success rate on all of the tasks. Our experiments, performed on a
real-world low-cost Lynxmotion arm, illustrate a variety of picking and placing
tasks, as well as non-prehensile manipulation.



Achieving artificial visual reasoning - the ability to answer image-related
questions which require a multi-step, high-level process - is an important step
towards artificial general intelligence. This multi-modal task requires
learning a question-dependent, structured reasoning process over images from
language. Standard deep learning approaches tend to exploit biases in the data
rather than learn this underlying structure, while leading methods learn to
visually reason successfully but are hand-crafted for reasoning. We show that a
general-purpose, Conditional Batch Normalization approach achieves
state-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%
error rate. We outperform the next best end-to-end method (4.5%) and even
methods that use extra supervision (3.1%). We probe our model to shed light on
how it reasons, showing it has learned a question-dependent, multi-step
process. Previous work has operated under the assumption that visual reasoning
calls for a specialized architecture, but we show that a general architecture
with proper conditioning can learn to visually reason effectively.



Robotic motion planning problems are typically solved by constructing a
search tree of valid maneuvers from a start to a goal configuration. Limited
onboard computation and real-time planning constraints impose a limit on how
large this search tree can grow. Heuristics play a crucial role in such
situations by guiding the search towards potentially good directions and
consequently minimizing search effort. Moreover, it must infer such directions
in an efficient manner using only the information uncovered by the search up
until that time. However, state of the art methods do not address the problem
of computing a heuristic that explicitly minimizes search effort. In this
paper, we do so by training a heuristic policy that maps the partial
information from the search to decide which node of the search tree to expand.
Unfortunately, naively training such policies leads to slow convergence and
poor local minima. We present SaIL, an efficient algorithm that trains
heuristic policies by imitating "clairvoyant oracles" - oracles that have full
information about the world and demonstrate decisions that minimize search
effort. We leverage the fact that such oracles can be efficiently computed
using dynamic programming and derive performance guarantees for the learnt
heuristic. We validate the approach on a spectrum of environments which show
that SaIL consistently outperforms state of the art algorithms. Our approach
paves the way forward for learning heuristics that demonstrate an anytime
nature - finding feasible solutions quickly and incrementally refining it over
time.



Deep neural networks excel in regimes with large amounts of data, but tend to
struggle when data is scarce or when they need to adapt quickly to changes in
the task. Recent work in meta-learning seeks to overcome this shortcoming by
training a meta-learner on a distribution of similar tasks; the goal is for the
meta-learner to generalize to novel but related tasks by learning a high-level
strategy that captures the essence of the problem it is asked to solve.
However, most recent approaches to meta-learning are extensively hand-designed,
either using architectures that are specialized to a particular application, or
hard-coding algorithmic components that tell the meta-learner how to solve the
task. We propose a class of simple and generic meta-learner architectures,
based on temporal convolutions, that is domain- agnostic and has no particular
strategy or algorithm encoded into it. We validate our
temporal-convolution-based meta-learner (TCML) through experiments pertaining
to both supervised and reinforcement learning, and demonstrate that it
outperforms state-of-the-art methods that are less general and more complex.



In this paper, we present RegNet, the first deep convolutional neural network
(CNN) to infer a 6 degrees of freedom (DOF) extrinsic calibration between
multimodal sensors, exemplified using a scanning LiDAR and a monocular camera.
Compared to existing approaches, RegNet casts all three conventional
calibration steps (feature extraction, feature matching and global regression)
into a single real-time capable CNN. Our method does not require any human
interaction and bridges the gap between classical offline and target-less
online calibration approaches as it provides both a stable initial estimation
as well as a continuous online correction of the extrinsic parameters. During
training we randomly decalibrate our system in order to train RegNet to infer
the correspondence between projected depth measurements and RGB image and
finally regress the extrinsic calibration. Additionally, with an iterative
execution of multiple CNNs, that are trained on different magnitudes of
decalibration, our approach compares favorably to state-of-the-art methods in
terms of a mean calibration error of 0.28 degrees for the rotational and 6 cm
for the translation components even for large decalibrations up to 1.5 m and 20
degrees.



Machine learning based system are increasingly being used for sensitive tasks
such as security surveillance, guiding autonomous vehicle, taking investment
decisions, detecting and blocking network intrusion and malware etc. However,
recent research has shown that machine learning models are venerable to attacks
by adversaries at all phases of machine learning (eg, training data collection,
training, operation). All model classes of machine learning systems can be
misled by providing carefully crafted inputs making them wrongly classify
inputs. Maliciously created input samples can affect the learning process of a
ML system by either slowing down the learning process, or affecting the
performance of the learned mode, or causing the system make error(s) only in
attacker's planned scenario. Because of these developments, understanding
security of machine learning algorithms and systems is emerging as an important
research area among computer security and machine learning researchers and
practitioners. We present a survey of this emerging area in machine learning.



We introduce the Deep Symbolic Network (DSN) model, which aims at becoming
the white-box version of Deep Neural Networks (DNN). The DSN model provides a
simple, universal yet powerful structure, similar to DNN, to represent any
knowledge of the world, which is transparent to humans. The conjecture behind
the DSN model is that any type of real world objects sharing enough common
features are mapped into human brains as a symbol. Those symbols are connected
by links, representing the composition, correlation, causality, or other
relationships between them, forming a deep, hierarchical symbolic network
structure. Powered by such a structure, the DSN model is expected to learn like
humans, because of its unique characteristics. First, it is universal, using
the same structure to store any knowledge. Second, it can learn symbols from
the world and construct the deep symbolic networks automatically, by utilizing
the fact that real world objects have been naturally separated by
singularities. Third, it is symbolic, with the capacity of performing causal
deduction and generalization. Fourth, the symbols and the links between them
are transparent to us, and thus we will know what it has learned or not - which
is the key for the security of an AI system. Fifth, its transparency enables it
to learn with relatively small data. Sixth, its knowledge can be accumulated.
Last but not least, it is more friendly to unsupervised learning than DNN. We
present the details of the model, the algorithm powering its automatic learning
ability, and describe its usefulness in different use cases. The purpose of
this paper is to generate broad interest to develop it within an open source
project centered on the Deep Symbolic Network (DSN) model towards the
development of general AI.



Foreign policy analysis has been struggling to find ways to measure policy
preferences and paradigm shifts in international political systems. This paper
presents a novel, potential solution to this challenge, through the application
of a neural word embedding (Word2vec) model on a dataset featuring speeches by
heads of state or government in the United Nations General Debate. The paper
provides three key contributions based on the output of the Word2vec model.
First, it presents a set of policy attention indices, synthesizing the semantic
proximity of political speeches to specific policy themes. Second, it
introduces country-specific semantic centrality indices, based on topological
analyses of countries' semantic positions with respect to each other. Third, it
tests the hypothesis that there exists a statistical relation between the
semantic content of political speeches and UN voting behavior, falsifying it
and suggesting that political speeches contain information of different nature
then the one behind voting outcomes. The paper concludes with a discussion of
the practical use of its results and consequences for foreign policy analysis,
public accountability, and transparency.



It has been shown that most machine learning algorithms are susceptible to
adversarial perturbations. Slightly perturbing an image in a carefully chosen
direction in the image space may cause a trained neural network model to
misclassify it. Recently, it was shown that physical adversarial examples
exist: printing perturbed images then taking pictures of them would still
result in misclassification. This raises security and safety concerns.
  However, these experiments ignore a crucial property of physical objects: the
camera can view objects from different distances and at different angles. In
this paper, we show experiments that suggest that current constructions of
physical adversarial examples do not disrupt object detection from a moving
platform. Instead, a trained neural network classifies most of the pictures
taken from different distances and angles of a perturbed image correctly. We
believe this is because the adversarial property of the perturbation is
sensitive to the scale at which the perturbed picture is viewed, so (for
example) an autonomous car will misclassify a stop sign only from a small range
of distances.
  Our work raises an important question: can one construct examples that are
adversarial for many or most viewing conditions? If so, the construction should
offer very significant insights into the internal representation of patterns by
deep networks. If not, there is a good prospect that adversarial examples can
be reduced to a curiosity with little practical impact.



Sensor-based activity recognition seeks the profound high-level knowledge
about human activities from multitudes of low-level sensor readings.
Conventional pattern recognition approaches have made tremendous progress in
the past years. However, those methods often heavily rely on heuristic
hand-crafted feature extraction, which could hinder their generalization
performance. Additionally, existing methods are undermined for unsupervised and
incremental learning tasks. Recently, the recent advancement of deep learning
makes it possible to perform automatic high-level feature extraction thus
achieves promising performance in many areas. Since then, deep learning based
methods have been widely adopted for the sensor-based activity recognition
tasks. This paper surveys the recent advance of deep learning based
sensor-based activity recognition. We summarize existing literature from three
aspects: sensor modality, deep model, and application. We also present detailed
insights on existing work and propose grand challenges for future research.



The Semantic Web began to emerge as its standards and technologies developed
rapidly in the recent years. The continuing development of Semantic Web
technologies has facilitated publishing explicit semantics with data on the Web
in RDF data model. This study proposes a semantic search framework to support
efficient keyword-based semantic search on RDF data utilizing near neighbor
explorations. The framework augments the search results with the resources in
close proximity by utilizing the entity type semantics. Along with the search
results, the system generates a relevance confidence score measuring the
inferred semantic relatedness of returned entities based on the degree of
similarity. Furthermore, the evaluations assessing the effectiveness of the
framework and the accuracy of the results are presented.



Pythagorean fuzzy sets provide stronger ability than intuitionistic fuzzy
sets to model uncertainty information and knowledge, but little effort has been
paid to conflict analysis of Pythagorean fuzzy information systems. In this
paper, we present three types of positive, central, and negative alliances with
different thresholds, and employ examples to illustrate how to construct the
positive, central, and negative alliances. Then we study conflict analysis of
Pythagorean fuzzy information systems based on Bayesian minimum risk theory.
Finally, we investigate group conflict analysis of Pythagorean fuzzy
information systems based on Bayesian minimum risk theory.



Models that can execute natural language instructions for situated robotic
tasks such as assembly and navigation have several useful applications in
homes, offices, and remote scenarios. We study the semantics of
spatially-referred configuration and arrangement instructions, based on the
challenging Bisk-2016 blank-labeled block dataset. This task involves finding a
source block and moving it to the target position (mentioned via a reference
block and offset), where the blocks have no names or colors and are just
referred to via spatial location features. We present novel models for the
subtasks of source block classification and target position regression, based
on joint-loss language and spatial-world representation learning, as well as
CNN-based and dual attention models to compute the alignment between the world
blocks and the instruction phrases. For target position prediction, we compare
two inference approaches: annealed sampling via policy gradient versus
expectation inference via supervised regression. Our models achieve the new
state-of-the-art on this task, with an improvement of 47% on source block
accuracy and 22% on target position distance.



The interpretation of spatial references is highly contextual, requiring
joint inference over both language and the environment. We consider the task of
spatial reasoning in a simulated environment, where an agent can act and
receive rewards. The proposed model learns a representation of the world
steered by instruction text. This design allows for precise alignment of local
neighborhoods with corresponding verbalizations, while also handling global
references in the instructions. We train our model with reinforcement learning
using a variant of generalized value iteration. The model outperforms
state-of-the-art approaches on several metrics, yielding a 45% reduction in
goal localization error.



Automatic photo aesthetic assessment is a challenging artificial intelligence
task. Existing computational approaches have focused on modeling a single
aesthetic score or a class (good or bad), however these do not provide any
details on why the photograph is good or bad, or which attributes contribute to
the quality of the photograph. To obtain both accuracy and human interpretation
of the score, we advocate learning the aesthetic attributes along with the
prediction of the overall score. For this purpose, we propose a novel multitask
deep convolution neural network, which jointly learns eight aesthetic
attributes along with the overall aesthetic score. We report near human
performance in the prediction of the overall aesthetic score. To understand the
internal representation of these attributes in the learned model, we also
develop the visualization technique using back propagation of gradients. These
visualizations highlight the important image regions for the corresponding
attributes, thus providing insights about model's representation of these
attributes. We showcase the diversity and complexity associated with different
attributes through a qualitative analysis of the activation maps.



Neural networks are generally built by interleaving (adaptable) linear layers
with (fixed) nonlinear activation functions. To increase their flexibility,
several authors have proposed methods for adapting the activation functions
themselves, endowing them with varying degrees of flexibility. None of these
approaches, however, have gained wide acceptance in practice, and research in
this topic remains open. In this paper, we introduce a novel family of flexible
activation functions that are based on an inexpensive kernel expansion at every
neuron. Leveraging over several properties of kernel-based models, we propose
multiple variations for designing and initializing these kernel activation
functions (KAFs), including a multidimensional scheme allowing to nonlinearly
combine information from different paths in the network. The resulting KAFs can
approximate any mapping defined over a subset of the real line, either convex
or nonconvex. Furthermore, they are smooth over their entire domain, linear in
their parameters, and they can be regularized using any known scheme, including
the use of $\ell_1$ penalties to enforce sparseness. To the best of our
knowledge, no other known model satisfies all these properties simultaneously.
In addition, we provide a relatively complete overview on alternative
techniques for adapting the activation functions, which is currently lacking in
the literature. A large set of experiments validates our proposal.



Methods that align distributions by minimizing an adversarial distance
between them have recently achieved impressive results. However, these
approaches are difficult to optimize with gradient descent and they often do
not converge well without careful hyperparameter tuning and proper
initialization. We investigate whether turning the adversarial min-max problem
into an optimization problem by replacing the maximization part with its dual
improves the quality of the resulting alignment and explore its connections to
Maximum Mean Discrepancy. Our empirical results suggest that using the dual
formulation for the restricted family of linear discriminators results in a
more stable convergence to a desirable solution when compared with the
performance of a primal min-max GAN-like objective and an MMD objective under
the same restrictions. We test our hypothesis on the problem of aligning two
synthetic point clouds on a plane and on a real-image domain adaptation problem
on digits. In both cases, the dual formulation yields an iterative procedure
that gives more stable and monotonic improvement over time.



This paper introduces an unsupervised framework to extract semantically rich
features for video representation. Inspired by how the human visual system
groups objects based on motion cues, we propose a deep convolutional neural
network that disentangles motion, foreground and background information. The
proposed architecture consists of a 3D convolutional feature encoder for blocks
of 16 frames, which is trained for reconstruction tasks over the first and last
frames of the sequence. A preliminary supervised experiment was conducted to
verify the feasibility of proposed method by training the model with a fraction
of videos from the UCF-101 dataset taking as ground truth the bounding boxes
around the activity regions. Qualitative results indicate that the network can
successfully segment foreground and background in videos as well as update the
foreground appearance based on disentangled motion features. The benefits of
these learned features are shown in a discriminative classification task, where
initializing the network with the proposed pretraining method outperforms both
random initialization and autoencoder pretraining. Our model and source code
are publicly available at https://imatge-upc.github.io/unsupervised-2017-cvprw/ .



Machine learning plays a role in many aspects of modern IR systems, and deep
learning is applied in all of them. The fast pace of modern-day research has
given rise to many different approaches for many different IR problems. The
amount of information available can be overwhelming both for junior students
and for experienced researchers looking for new research topics and directions.
Additionally, it is interesting to see what key insights into IR problems the
new technologies are able to give us. The aim of this full-day tutorial is to
give a clear overview of current tried-and-trusted neural methods in IR and how
they benefit IR research. It covers key architectures, as well as the most
promising future directions.



In this paper, we describe the Lithium Natural Language Processing (NLP)
system - a resource-constrained, high- throughput and language-agnostic system
for information extraction from noisy user generated text on social media.
Lithium NLP extracts a rich set of information including entities, topics,
hashtags and sentiment from text. We discuss several real world applications of
the system currently incorporated in Lithium products. We also compare our
system with existing commercial and academic NLP systems in terms of
performance, information extracted and languages supported. We show that
Lithium NLP is at par with and in some cases, outperforms state- of-the-art
commercial NLP systems.



Modern software systems in many application areas offer to the user a
multitude of parameters, switches and other customisation hooks. Humans tend to
have difficulties determining the best configurations for particular
applications. Modern optimising compilers are an example of such software
systems; their many parameters need to be tuned for optimal performance, but
are often left at the default values for convenience. In this work, we
automatically determine compiler parameter settings that result in optimised
performance for particular applications. Specifically, we apply a
state-of-the-art automated parameter configuration procedure based on
cutting-edge machine learning and optimisation techniques to two prominent
JavaScript compilers and demonstrate that significant performance improvements,
more than 35% in some cases, can be achieved over the default parameter
settings on a diverse set of benchmarks.



Learners regularly abandon online coding tutorials when they get bored or
frustrated, but there are few techniques for anticipating this abandonment to
intervene. In this paper, we examine the feasibility of predicting abandonment
with machine-learned classifiers. Using interaction logs from an online
programming game, we extracted a collection of features that are potentially
related to learner abandonment and engagement, then developed classifiers for
each level. Across the first five levels of the game, our classifiers
successfully predicted 61% to 76% of learners who did not complete the next
level, achieving an average AUC of 0.68. In these classifiers, features
negatively associated with abandonment included account activation and
help-seeking behaviors, whereas features positively associated with abandonment
included features indicating difficulty and disengagement. These findings
highlight the feasibility of providing timely intervention to learners likely
to quit.



We present the first general purpose framework for marginal maximum a
posteriori estimation of probabilistic program variables. By using a series of
code transformations, the evidence of any probabilistic program, and therefore
of any graphical model, can be optimized with respect to an arbitrary subset of
its sampled variables. To carry out this optimization, we develop the first
Bayesian optimization package to directly exploit the source code of its
target, leading to innovations in problem-independent hyperpriors, unbounded
optimization, and implicit constraint satisfaction; delivering significant
performance improvements over prominent existing packages. We present
applications of our method to a number of tasks including engineering design
and parameter optimization.



The vision systems of the eagle and the snake outperform everything that we
can make in the laboratory, but snakes and eagles cannot build an eyeglass or a
telescope or a microscope. (Judea Pearl)



A significant amount of research in recent years has been dedicated towards
single agent deep reinforcement learning. Much of the success of deep
reinforcement learning can be attributed towards the use of experience replay
memories within which state transitions are stored. Function approximation
methods such as convolutional neural networks (referred to as deep Q-Networks,
or DQNs, in this context) can subsequently be trained through sampling the
stored transitions. However, considerations are required when using experience
replay memories within multi-agent systems, as stored transitions can become
outdated due to agents updating their respective policies in parallel [1]. In
this work we apply leniency [2] to multi-agent deep reinforcement learning
(MA-DRL), acting as a control mechanism to determine which state-transitions
sampled are allowed to update the DQN. Our resulting Lenient-DQN (LDQN) is
evaluated using variations of the Coordinated Multi-Agent Object Transportation
Problem (CMOTP) outlined by Busoniu et al. [3]. The LDQN significantly
outperforms the existing hysteretic DQN (HDQN) [4] within environments that
yield stochastic rewards. Based on results from experiments conducted using
vanilla and double Q-learning versions of the lenient and hysteretic
algorithms, we advocate a hybrid approach where learners initially use vanilla
Q-learning before transitioning to double Q-learners upon converging on a
cooperative joint policy.



VAEs (Variational AutoEncoders) have proved to be powerful in the context of
density modeling and have been used in a variety of contexts for creative
purposes. In many settings, the data we model possesses continuous attributes
that we would like to take into account at generation time. We propose in this
paper GLSR-VAE, a Geodesic Latent Space Regularization for the Variational
AutoEncoder architecture and its generalizations which allows a fine control on
the embedding of the data into the latent space. When augmenting the VAE loss
with this regularization, changes in the learned latent space reflects changes
of the attributes of the data. This deeper understanding of the VAE latent
space structure offers the possibility to modulate the attributes of the
generated data in a continuous way. We demonstrate its efficiency on a
monophonic music generation task where we manage to generate variations of
discrete sequences in an intended and playful way.



Sub-8-bit representation of DNNs incur some discernible loss of accuracy
despite rigorous (re)training at low-precision. Such loss of accuracy
essentially makes them equivalent to a much shallower counterpart, diminishing
the power of being deep networks. To address this problem of accuracy drop we
introduce the notion of \textit{residual networks} where we add more
low-precision edges to sensitive branches of the sub-8-bit network to
compensate for the lost accuracy. Further, we present a perturbation theory to
identify such sensitive edges. Aided by such an elegant trade-off between
accuracy and compute, the 8-2 model (8-bit activations, ternary weights),
enhanced by ternary residual edges, turns out to be sophisticated enough to
achieve very high accuracy ($\sim 1\%$ drop from our FP-32 baseline), despite
$\sim 1.6\times$ reduction in model size, $\sim 26\times$ reduction in number
of multiplications, and potentially $\sim 2\times$ power-performance gain
comparing to 8-8 representation, on the state-of-the-art deep network
ResNet-101 pre-trained on ImageNet dataset. Moreover, depending on the varying
accuracy requirements in a dynamic environment, the deployed low-precision
model can be upgraded/downgraded on-the-fly by partially enabling/disabling
residual connections. For example, disabling the least important residual
connections in the above enhanced network, the accuracy drop is $\sim 2\%$
(from FP32), despite $\sim 1.9\times$ reduction in model size, $\sim 32\times$
reduction in number of multiplications, and potentially $\sim 2.3\times$
power-performance gain comparing to 8-8 representation. Finally, all the
ternary connections are sparse in nature, and the ternary residual conversion
can be done in a resource-constraint setting with no low-precision
(re)training.



Through the success of deep learning, Artificial Neural Networks (ANNs) are
among the most used artificial intelligence methods nowadays. ANNs have led to
major breakthroughs in various domains, such as particle physics, reinforcement
learning, speech recognition, computer vision, and so on. Taking inspiration
from the network properties of biological neural networks (e.g. sparsity,
scale-freeness), we argue that (contrary to general practice) Artificial Neural
Networks (ANN), too, should not have fully-connected layers. We show how ANNs
perform perfectly well with sparsely-connected layers. Following a Darwinian
evolutionary approach, we propose a novel algorithm which evolves an initial
random sparse topology (i.e. an Erd\H{o}s-R\'enyi random graph) of two
consecutive layers of neurons into a scale-free topology, during the ANN
training process. The resulting sparse layers can safely replace the
corresponding fully-connected layers. Our method allows to quadratically reduce
the number of parameters in the fully conencted layers of ANNs, yielding
quadratically faster computational times in both phases (i.e. training and
inference), at no decrease in accuracy. We demonstrate our claims on two
popular ANN types (restricted Boltzmann machine and multi-layer perceptron), on
two types of tasks (supervised and unsupervised learning), and on 14 benchmark
datasets. We anticipate that our approach will enable ANNs having billions of
neurons and evolved topologies to be capable of handling complex real-world
tasks that are intractable using state-of-the-art methods.



We study revenue optimization learning algorithms for repeated posted-price
auctions where a seller interacts with a single strategic buyer that holds a
fixed private valuation for a good and seeks to maximize his cumulative
discounted surplus. For this setting, first, we propose a novel algorithm that
never decreases offered prices and has a tight strategic regret bound in
$\Theta(\log\log T)$ under some mild assumptions on the buyer surplus
discounting. This result closes the open research question on the existence of
a no-regret horizon-independent weakly consistent pricing. The proposed
algorithm is inspired by our observation that a double decrease of offered
prices in a weakly consistent algorithm is enough to cause a linear regret.
This motivates us to construct a novel transformation that maps a
right-consistent algorithm to a weakly consistent one that never decreases
offered prices.
  Second, we outperform the previously known strategic regret upper bound of
the algorithm PRRFES, where the improvement is achieved by means of a finer
constant factor $C$ of the principal term $C\log\log T$ in this upper bound.
Finally, we generalize results on strategic regret previously known for
geometric discounting of the buyer's surplus to discounting of other types,
namely: the optimality of the pricing PRRFES to the case of geometrically
concave decreasing discounting; and linear lower bound on the strategic regret
of a wide range of horizon-independent weakly consistent algorithms to the case
of arbitrary discounts.



Sequential Constraint Grammar (SCG) (Karlsson, 1990) and its extensions have
lacked clear connections to formal language theory. The purpose of this article
is to lay a foundation for these connections by simplifying the definition of
strings processed by the grammar and by showing that Nonmonotonic SCG is
undecidable and that derivations similar to the Generative Phonology exist. The
current investigations propose resource bounds that restrict the generative
power of SCG to a subset of context sensitive languages and present a strong
finite-state condition for grammars as wholes. We show that a grammar is
equivalent to a finite-state transducer if it is implemented with a Turing
machine that runs in o(n log n) time. This condition opens new finite-state
hypotheses and avenues for deeper analysis of SCG instances in the way inspired
by Finite-State Phonology.



AI systems are increasingly applied to complex tasks that involve interaction
with humans. During training, such systems are potentially dangerous, as they
haven't yet learned to avoid actions that could cause serious harm. How can an
AI system explore and learn without making a single mistake that harms humans
or otherwise causes serious damage? For model-free reinforcement learning,
having a human "in the loop" and ready to intervene is currently the only way
to prevent all catastrophes. We formalize human intervention for RL and show
how to reduce the human labor required by training a supervised learner to
imitate the human's intervention decisions. We evaluate this scheme on Atari
games, with a Deep RL agent being overseen by a human for four hours. When the
class of catastrophes is simple, we are able to prevent all catastrophes
without affecting the agent's learning (whereas an RL baseline fails due to
catastrophic forgetting). However, this scheme is less successful when
catastrophes are more complex: it reduces but does not eliminate catastrophes
and the supervised learner fails on adversarial examples found by the agent.
Extrapolating to more challenging environments, we show that our implementation
would not scale (due to the infeasible amount of human labor required). We
outline extensions of the scheme that are necessary if we are to train
model-free agents without a single catastrophe.



Many relevant tasks require an agent to reach a certain state, or to
manipulate objects into a desired configuration. For example, we might want a
robot to align and assemble a gear onto an axle or insert and turn a key in a
lock. These goal-oriented tasks present a considerable challenge for
reinforcement learning, since their natural reward function is sparse and
prohibitive amounts of exploration are required to reach the goal and receive
some learning signal. Past approaches tackle these problems by exploiting
expert demonstrations or by manually designing a task-specific reward shaping
function to guide the learning agent. Instead, we propose a method to learn
these tasks without requiring any prior knowledge other than obtaining a single
state in which the task is achieved. The robot is trained in reverse, gradually
learning to reach the goal from a set of start states increasingly far from the
goal. Our method automatically generates a curriculum of start states that
adapts to the agent's performance, leading to efficient training on
goal-oriented tasks. We demonstrate our approach on difficult simulated
navigation and fine-grained manipulation problems, not solvable by
state-of-the-art reinforcement learning methods.



Electronic medical records contain multi-format electronic medical data that
consist of an abundance of medical knowledge. Facing with patient's symptoms,
experienced caregivers make right medical decisions based on their professional
knowledge that accurately grasps relationships between symptoms, diagnosis and
corresponding treatments. In this paper, we aim to capture these relationships
by constructing a large and high-quality heterogenous graph linking patients,
diseases, and drugs (PDD) in EMRs. Specifically, we propose a novel framework
to extract important medical entities from MIMIC-III (Medical Information Mart
for Intensive Care III) and automatically link them with the existing
biomedical knowledge graphs, including ICD-9 ontology and DrugBank. The PDD
graph presented in this paper is accessible on the Web via the SPARQL endpoint,
and provides a pathway for medical discovery and applications, such as
effective treatment recommendations.



We present Autonomous Rssi based RElative poSitioning and Tracking (ARREST),
a new robotic sensing system for tracking and following a moving, RF-emitting
object, which we refer to as the Leader, solely based on signal strength
information. This kind of system can expand the horizon of autonomous mobile
tracking and distributed robotics into many scenarios with limited visibility
such as nighttime, dense forests, and cluttered environments. Our proposed
tracking agent, which we refer to as the TrackBot, uses a single rotating,
off-the-shelf, directional antenna, novel angle and relative speed estimation
algorithms, and Kalman filtering to continually estimate the relative position
of the Leader with decimeter level accuracy (which is comparable to a
state-of-the-art multiple access point based RF-localization system) and the
relative speed of the Leader with accuracy on the order of 1 m/s. The TrackBot
feeds the relative position and speed estimates into a Linear Quadratic
Gaussian (LQG) controller to generate a set of control outputs to control the
orientation and the movement of the TrackBot. We perform an extensive set of
real world experiments with a full-fledged prototype to demonstrate that the
TrackBot is able to stay within 5m of the Leader with: (1) more than $99\%$
probability in line of sight scenarios, and (2) more than $70\%$ probability in
no line of sight scenarios, when it moves 1.8X faster than the Leader. For
ground truth estimation in real world experiments, we also developed an
integrated TDoA based distance and angle estimation system with centimeter
level localization accuracy in line of sight scenarios. While providing a first
proof of concept, our work opens the door to future research aimed at further
improvements of autonomous RF-based tracking.



Much work has been done in understanding human creativity and defining
measures to evaluate creativity. This is necessary mainly for the reason of
having an objective and automatic way of quantifying creative artifacts. In
this work, we propose a regression-based learning framework which takes into
account quantitatively the essential criteria for creativity like novelty,
influence, value and unexpectedness. As it is often the case with most creative
domains, there is no clear ground truth available for creativity. Our proposed
learning framework is applicable to all creative domains; yet we evaluate it on
a dataset of movies created from IMDb and Rotten Tomatoes due to availability
of audience and critic scores, which can be used as proxy ground truth labels
for creativity. We report promising results and observations from our
experiments in the following ways : 1) Correlation of creative criteria with
critic scores, 2) Improvement in movie rating prediction with inclusion of
various creative criteria, and 3) Identification of creative movies.



The human language is one of the most natural interfaces for humans to
interact with robots. This paper presents a robot system that retrieves
everyday objects with unconstrained natural language descriptions. A core issue
for the system is semantic and spatial grounding, which is to infer objects and
their spatial relationships from images and natural language expressions. We
introduce a two-stage neural-network grounding pipeline that maps natural
language referring expressions directly to objects in the images. The first
stage uses visual descriptions in the referring expressions to generate a
candidate set of relevant objects. The second stage examines all pairwise
relationships between the candidates and predicts the most likely referred
object according to the spatial descriptions in the referring expressions. A
key feature of our system is that by leveraging a large dataset of images
labeled with text descriptions, it allows unrestricted object types and natural
language referring expressions. Preliminary results indicate that our system
outperforms a near state-of-the-art object comprehension system on standard
benchmark datasets. We also present a robot system that follows voice commands
to pick and place previously unseen objects.



Bayesian optimization has recently attracted the attention of the automatic
machine learning community for its excellent results in hyperparameter tuning.
BO is characterized by the sample efficiency with which it can optimize
expensive black-box functions. The efficiency is achieved in a similar fashion
to the learning to learn methods: surrogate models (typically in the form of
Gaussian processes) learn the target function and perform intelligent sampling.
This surrogate model can be applied even in the presence of noise; however, as
with most regression methods, it is very sensitive to outlier data. This can
result in erroneous predictions and, in the case of BO, biased and inefficient
exploration. In this work, we present a GP model that is robust to outliers
which uses a Student-t likelihood to segregate outliers and robustly conduct
Bayesian optimization. We present numerical results evaluating the proposed
method in both artificial functions and real problems.



Object detection is an essential task for autonomous robots operating in
dynamic and changing environments. A robot should be able to detect objects in
the presence of sensor noise that can be induced by changing lighting
conditions for cameras and false depth readings for range sensors, especially
RGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion
approach for object detection that learns weighting the predictions of
different sensor modalities in an online manner. Our approach is based on a
mixture of convolutional neural network (CNN) experts and incorporates multiple
modalities including appearance, depth and motion. We test our method in
extensive robot experiments, in which we detect people in a combined indoor and
outdoor scenario from RGB-D data, and we demonstrate that our method can adapt
to harsh lighting changes and severe camera motion blur. Furthermore, we
present a new RGB-D dataset for people detection in mixed in- and outdoor
environments, recorded with a mobile robot.



Unprecedented high volumes of data are becoming available with the growth of
the advanced metering infrastructure. These are expected to benefit planning
and operation of the future power system, and to help the customers transition
from a passive to an active role. In this paper, we explore for the first time
in the smart grid context the benefits of using Deep Reinforcement Learning, a
hybrid type of methods that combines Reinforcement Learning with Deep Learning,
to perform on-line optimization of schedules for building energy management
systems. The learning procedure was explored using two methods, Deep Q-learning
and Deep Policy Gradient, both of them being extended to perform multiple
actions simultaneously. The proposed approach was validated on the large-scale
Pecan Street Inc. database. This highly-dimensional database includes
information about photovoltaic power generation, electric vehicles as well as
buildings appliances. Moreover, these on-line energy scheduling strategies
could be used to provide real-time feedback to consumers to encourage more
efficient use of electricity.



We introduce a parallel offline algorithm for computing hybrid conditional
plans, called HCP-ASP, oriented towards robotics applications. HCP-ASP relies
on modeling actuation actions and sensing actions in an expressive nonmonotonic
language of answer set programming (ASP), and computation of the branches of a
conditional plan in parallel using an ASP solver. In particular, thanks to
external atoms, continuous feasibility checks (like collision checks) are
embedded into formal representations of actuation actions and sensing actions
in ASP; and thus each branch of a hybrid conditional plan describes a feasible
execution of actions to reach their goals. Utilizing nonmonotonic constructs
and nondeterministic choices, partial knowledge about states and
nondeterministic effects of sensing actions can be explicitly formalized in
ASP; and thus each branch of a conditional plan can be computed by an ASP
solver without necessitating a conformant planner and an ordering of sensing
actions in advance. We apply our method in a service robotics domain and report
experimental evaluations. Furthermore, we present performance comparisons with
other compilation based conditional planners on standardized benchmark domains.
This paper is under consideration for acceptance in TPLP.



Conventional wisdom holds that model-based planning is a powerful approach to
sequential decision-making. It is often very challenging in practice, however,
because while a model can be used to evaluate a plan, it does not prescribe how
to construct a plan. Here we introduce the "Imagination-based Planner", the
first model-based, sequential decision-making agent that can learn to
construct, evaluate, and execute plans. Before any action, it can perform a
variable number of imagination steps, which involve proposing an imagined
action and evaluating it with its model-based imagination. All imagined actions
and outcomes are aggregated, iteratively, into a "plan context" which
conditions future real and imagined actions. The agent can even decide how to
imagine: testing out alternative imagined actions, chaining sequences of
actions together, or building a more complex "imagination tree" by navigating
flexibly among the previously imagined states using a learned policy. And our
agent can learn to plan economically, jointly optimizing for external rewards
and computational costs associated with using its imagination. We show that our
architecture can learn to solve a challenging continuous control problem, and
also learn elaborate planning strategies in a discrete maze-solving task. Our
work opens a new direction toward learning the components of a model-based
planning system and how to use them.



Existing region-based object detectors are limited to regions with fixed box
geometry to represent objects, even if those are highly non-rectangular. In
this paper we introduce DP-FCN, a deep model for object detection which
explicitly adapts to shapes of objects with deformable parts. Without
additional annotations, it learns to focus on discriminative elements and to
align them, and simultaneously brings more invariance for classification and
geometric information to refine localization. DP-FCN is composed of three main
modules: a Fully Convolutional Network to efficiently maintain spatial
resolution, a deformable part-based RoI pooling layer to optimize positions of
parts and build invariance, and a deformation-aware localization module
explicitly exploiting displacements of parts to improve accuracy of bounding
box regression. We experimentally validate our model and show significant
gains. DP-FCN achieves state-of-the-art performances of 83.1% and 80.9% on
PASCAL VOC 2007 and 2012 with VOC data only.



We introduce Imagination-Augmented Agents (I2As), a novel architecture for
deep reinforcement learning combining model-free and model-based aspects. In
contrast to most existing model-based reinforcement learning and planning
methods, which prescribe how a model should be used to arrive at a policy, I2As
learn to interpret predictions from a learned environment model to construct
implicit plans in arbitrary ways, by using the predictions as additional
context in deep policy networks. I2As show improved data efficiency,
performance, and robustness to model misspecification compared to several
baselines.



We present a novel method for obtaining high-quality, domain-targeted
multiple choice questions from crowd workers. Generating these questions can be
difficult without trading away originality, relevance or diversity in the
answer options. Our method addresses these problems by leveraging a large
corpus of domain-specific text and a small set of existing questions. It
produces model suggestions for document selection and answer distractor choice
which aid the human question generation process. With this method we have
assembled SciQ, a dataset of 13.7K multiple choice science exam questions
(Dataset available at http://allenai.org/data.html). We demonstrate that the
method produces in-domain questions by providing an analysis of this new
dataset and by showing that humans cannot distinguish the crowdsourced
questions from original questions. When using SciQ as additional training data
to existing questions, we observe accuracy improvements on real science exams.



Computational models for sarcasm detection have often relied on the content
of utterances in isolation. However, speaker's sarcastic intent is not always
obvious without additional context. Focusing on social media discussions, we
investigate two issues: (1) does modeling of conversation context help in
sarcasm detection and (2) can we understand what part of conversation context
triggered the sarcastic reply. To address the first issue, we investigate
several types of Long Short-Term Memory (LSTM) networks that can model both the
conversation context and the sarcastic response. We show that the conditional
LSTM network (Rocktaschel et al., 2015) and LSTM networks with sentence level
attention on context and response outperform the LSTM model that reads only the
response. To address the second issue, we present a qualitative analysis of
attention weights produced by the LSTM models with attention and discuss the
results compared with human performance on the task.



As intelligent systems gain autonomy and capability, it becomes vital to
ensure that their objectives match those of their human users; this is known as
the value-alignment problem. In robotics, value alignment is key to the design
of collaborative robots that can integrate into human workflows, successfully
inferring and adapting to their users' objectives as they go. We argue that a
meaningful solution to value alignment must combine multi-agent decision theory
with rich mathematical models of human cognition, enabling robots to tap into
people's natural collaborative capabilities. We present a solution to the
cooperative inverse reinforcement learning (CIRL) dynamic game based on
well-established cognitive models of decision making and theory of mind. The
solution captures a key reciprocity relation: the human will not plan her
actions in isolation, but rather reason pedagogically about how the robot might
learn from them; the robot, in turn, can anticipate this and interpret the
human's actions pragmatically. To our knowledge, this work constitutes the
first formal analysis of value alignment grounded in empirically validated
cognitive models.



Video Question Answering is a challenging problem in visual information
retrieval, which provides the answer to the referenced video content according
to the question. However, the existing visual question answering approaches
mainly tackle the problem of static image question, which may be ineffectively
for video question answering due to the insufficiency of modeling the temporal
dynamics of video contents. In this paper, we study the problem of video
question answering by modeling its temporal dynamics with frame-level attention
mechanism. We propose the attribute-augmented attention network learning
framework that enables the joint frame-level attribute detection and unified
video representation learning for video question answering. We then incorporate
the multi-step reasoning process for our proposed attention network to further
improve the performance. We construct a large-scale video question answering
dataset. We conduct the experiments on both multiple-choice and open-ended
video question answering tasks to show the effectiveness of the proposed
method.



Both hybrid automata and action languages are formalisms for describing the
evolution of dynamic systems. This paper establishes a formal relationship
between them. We show how to succinctly represent hybrid automata in an action
language which in turn is defined as a high-level notation for answer set
programming modulo theories (ASPMT) --- an extension of answer set programs to
the first-order level similar to the way satisfiability modulo theories (SMT)
extends propositional satisfiability (SAT). We first show how to represent
linear hybrid automata with convex invariants by an action language modulo
theories. A further translation into SMT allows for computing them using SMT
solvers that support arithmetic over reals. Next, we extend the representation
to the general class of non-linear hybrid automata allowing even non-convex
invariants. We represent them by an action language modulo ODE (Ordinary
Differential Equations), which can be compiled into satisfiability modulo ODE.
We developed a prototype system cplus2aspmt based on these translations, which
allows for a succinct representation of hybrid transition systems that can be
computed effectively by the state-of-the-art SMT solver dReal.



We describe a generalization of the Hierarchical Dirichlet Process Hidden
Markov Model (HDP-HMM) which is able to encode prior information that state
transitions are more likely between "nearby" states. This is accomplished by
defining a similarity function on the state space and scaling transition
probabilities by pair-wise similarities, thereby inducing correlations among
the transition distributions. We present an augmented data representation of
the model as a Markov Jump Process in which: (1) some jump attempts fail, and
(2) the probability of success is proportional to the similarity between the
source and destination states. This augmentation restores conditional conjugacy
and admits a simple Gibbs sampler. We evaluate the model and inference method
on a speaker diarization task and a "harmonic parsing" task using four-part
chorale data, as well as on several synthetic datasets, achieving favorable
comparisons to existing models.



In this paper we argue for the fundamental importance of the value
distribution: the distribution of the random return received by a reinforcement
learning agent. This is in contrast to the common approach to reinforcement
learning which models the expectation of this return, or value. Although there
is an established body of literature studying the value distribution, thus far
it has always been used for a specific purpose such as implementing risk-aware
behaviour. We begin with theoretical results in both the policy evaluation and
control settings, exposing a significant distributional instability in the
latter. We then use the distributional perspective to design a new algorithm
which applies Bellman's equation to the learning of approximate value
distributions. We evaluate our algorithm using the suite of games from the
Arcade Learning Environment. We obtain both state-of-the-art results and
anecdotal evidence demonstrating the importance of the value distribution in
approximate reinforcement learning. Finally, we combine theoretical and
empirical evidence to highlight the ways in which the value distribution
impacts learning in the approximate setting.



A population-based optimization algorithm was designed, inspired by two main
thinking modes in philosophy, both based on dialectic concept and
thesis-antithesis paradigm. They impose two different kinds of dialectics.
Idealistic and materialistic antitheses are formulated as optimization models.
Based on the models, the population is coordinated for dialectical
interactions. At the population-based context, the formulated optimization
models are reduced to a simple detection problem for each thinker (particle).
According to the assigned thinking mode to each thinker and her/his
measurements of corresponding dialectic with other candidate particles, they
deterministically decide to interact with a thinker in maximum dialectic with
their theses. The position of a thinker at maximum dialectic is known as an
available antithesis among the existing solutions. The dialectical interactions
at each ideological community are distinguished by meaningful distributions of
step-sizes for each thinking mode. In fact, the thinking modes are regarded as
exploration and exploitation elements of the proposed algorithm. The result is
a delicate balance without any requirement for adjustment of step-size
coefficients. Main parameter of the proposed algorithm is the number of
particles appointed to each thinking modes, or equivalently for each kind of
motions. An additional integer parameter is defined to boost the stability of
the final algorithm in some particular problems. The proposed algorithm is
evaluated by a testbed of 12 single-objective continuous benchmark functions.
Moreover, its performance and speed were highlighted in sparse reconstruction
and antenna selection problems, at the context of compressed sensing and
massive MIMO, respectively. The results indicate fast and efficient performance
in comparison with well-known evolutionary algorithms and dedicated
state-of-the-art algorithms.



Supervisory signals have the potential to make low-dimensional data
representations, like those learned by mixture and topic models, more
interpretable and useful. We propose a framework for training latent variable
models that explicitly balances two goals: recovery of faithful generative
explanations of high-dimensional data, and accurate prediction of associated
semantic labels. Existing approaches fail to achieve these goals due to an
incomplete treatment of a fundamental asymmetry: the intended application is
always predicting labels from data, not data from labels. Our
prediction-constrained objective for training generative models coherently
integrates loss-based supervisory signals while enabling effective
semi-supervised learning from partially labeled data. We derive learning
algorithms for semi-supervised mixture and topic models using stochastic
gradient descent with automatic differentiation. We demonstrate improved
prediction quality compared to several previous supervised topic models,
achieving predictions competitive with high-dimensional logistic regression on
text sentiment analysis and electronic health records tasks while
simultaneously learning interpretable topics.



Machine translation is a natural candidate problem for reinforcement learning
from human feedback: users provide quick, dirty ratings on candidate
translations to guide a system to improve. Yet, current neural machine
translation training focuses on expensive human-generated reference
translations. We describe a reinforcement learning algorithm that improves
neural machine translation systems from simulated human feedback. Our algorithm
combines the advantage actor-critic algorithm (Mnih et al., 2016) with the
attention-based neural encoder-decoder architecture (Luong et al., 2015). This
algorithm (a) is well-designed for problems with a large action space and
delayed rewards, (b) effectively optimizes traditional corpus-level machine
translation metrics, and (c) is robust to skewed, high-variance, granular
feedback modeled after actual human behaviors.



Deep neural networks have become a primary tool for solving problems in many
fields. They are also used for addressing information retrieval problems and
show strong performance in several tasks. Training these models requires large,
representative datasets and for most IR tasks, such data contains sensitive
information from users. Privacy and confidentiality concerns prevent many data
owners from sharing the data, thus today the research community can only
benefit from research on large-scale datasets in a limited manner. In this
paper, we discuss privacy preserving mimic learning, i.e., using predictions
from a privacy preserving trained model instead of labels from the original
sensitive training data as a supervision signal. We present the results of
preliminary experiments in which we apply the idea of mimic learning and
privacy preserving mimic learning for the task of document re-ranking as one of
the core IR tasks. This research is a step toward laying the ground for
enabling researchers from data-rich environments to share knowledge learned
from actual users' data, which should facilitate research collaborations.



Entity retrieval is the task of finding entities such as people or products
in response to a query, based solely on the textual documents they are
associated with. Recent semantic entity retrieval algorithms represent queries
and experts in finite-dimensional vector spaces, where both are constructed
from text sequences.
  We investigate entity vector spaces and the degree to which they capture
structural regularities. Such vector spaces are constructed in an unsupervised
manner without explicit information about structural aspects. For concreteness,
we address these questions for a specific type of entity: experts in the
context of expert finding. We discover how clusterings of experts correspond to
committees in organizations, the ability of expert representations to encode
the co-author graph, and the degree to which they encode academic rank. We
compare latent, continuous representations created using methods based on
distributional semantics (LSI), topic models (LDA) and neural networks
(word2vec, doc2vec, SERT). Vector spaces created using neural methods, such as
doc2vec and SERT, systematically perform better at clustering than LSI, LDA and
word2vec. When it comes to encoding entity relations, SERT performs best.



In this paper, we present a new task that investigates how people interact
with and make judgments about towers of blocks. In Experiment~1, participants
in the lab solved a series of problems in which they had to re-configure three
blocks from an initial to a final configuration. We recorded whether they used
one hand or two hands to do so. In Experiment~2, we asked participants online
to judge whether they think the person in the lab used one or two hands. The
results revealed a close correspondence between participants' actions in the
lab, and the mental simulations of participants online. To explain
participants' actions and mental simulations, we develop a model that plans
over a symbolic representation of the situation, executes the plan using a
geometric solver, and checks the plan's feasibility by taking into account the
physical constraints of the scene. Our model explains participants' actions and
judgments to a high degree of quantitative accuracy.



This volume consists of papers presented at the Sixteenth Conference on
Theoretical Aspects of Rationality and Knowledge (TARK) held at the University
of Liverpool, UK, from July 24 to 26, 2017.
  TARK conferences bring together researchers from a wide variety of fields,
including Computer Science (especially, Artificial Intelligence, Cryptography,
Distributed Computing), Economics (especially, Decision Theory, Game Theory,
Social Choice Theory), Linguistics, Philosophy (especially, Philosophical
Logic), and Cognitive Psychology, in order to further understand the issues
involving reasoning about rationality and knowledge.



A variety of representation learning approaches have been investigated for
reinforcement learning; much less attention, however, has been given to
investigating the utility of sparse coding. Outside of reinforcement learning,
sparse coding representations have been widely used, with non-convex objectives
that result in discriminative representations. In this work, we develop a
supervised sparse coding objective for policy evaluation. Despite the
non-convexity of this objective, we prove that all local minima are global
minima, making the approach amenable to simple optimization strategies. We
empirically show that it is key to use a supervised objective, rather than the
more straightforward unsupervised sparse coding approach. We compare the
learned representations to a canonical fixed sparse representation, called
tile-coding, demonstrating that the sparse coding representation outperforms a
wide variety of tilecoding representations.



Domain adaptation is an important open problem in deep reinforcement learning
(RL). In many scenarios of interest data is hard to obtain, so agents may learn
a source policy in a setting where data is readily available, with the hope
that it generalises well to the target domain. We propose a new multi-stage RL
agent, DARLA (DisentAngled Representation Learning Agent), which learns to see
before learning to act. DARLA's vision is based on learning a disentangled
representation of the observed environment. Once DARLA can see, it is able to
acquire source policies that are robust to many domain shifts - even with no
access to the target domain. DARLA significantly outperforms conventional
baselines in zero-shot domain adaptation scenarios, an effect that holds across
a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms
(DQN, A3C and EC).



In this work we present a technique to use natural language to help
reinforcement learning generalize to unseen environments. This technique uses
neural machine translation, specifically the use of encoder-decoder networks,
to learn associations between natural language behavior descriptions and
state-action information. We then use this learned model to guide agent
exploration using a modified version of policy shaping to make it more
effective at learning in unseen environments. We evaluate this technique using
the popular arcade game, Frogger, under ideal and non-ideal conditions. This
evaluation shows that our modified policy shaping algorithm improves over a
Q-learning agent as well as a baseline version of policy shaping.



Matching 3D rigid point clouds in complex environments robustly and
accurately is still a core technique used in many applications. This paper
proposes a new architecture combining error estimation from sample covariances
and dual global probability alignment based on the convolution of adaptive
Gaussian Mixture Models (GMM) from point clouds. Firstly, a novel adaptive GMM
is defined using probability distributions from the corresponding points. Then
rigid point cloud alignment is performed by maximizing the global probability
from the convolution of dual adaptive GMMs in the whole 2D or 3D space, which
can be efficiently optimized and has a large zone of accurate convergence.
Thousands of trials have been conducted on 200 models from public 2D and 3D
datasets to demonstrate superior robustness and accuracy in complex
environments with unpredictable noise, outliers, occlusion, initial rotation,
shape and missing points.



The paper provides an analysis of the voting method known as delegable proxy
voting, or liquid democracy. The analysis first positions liquid democracy
within the theory of binary aggregation. It then focuses on two issues of the
system: the occurrence of delegation cycles; and the effect of delegations on
individual rationality when voting on logically interdependent propositions. It
finally points to proposals on how the system may be modified in order to
address the above issues.



While there have been many attempts, going back to BAN logic, to base
reasoning about security protocols on epistemic notions, they have not been all
that successful. Arguably, this has been due to the particular logics chosen.
We present a simple logic based on the well-understood modal operators of
knowledge, time, and probability, and show that it is able to handle issues
that have often been swept under the rug by other approaches, while being
flexible enough to capture all the higher- level security notions that appear
in BAN logic. Moreover, while still assuming that the knowledge operator allows
for unbounded computation, it can handle the fact that a computationally
bounded agent cannot decrypt messages in a natural way, by distinguishing
strings and message terms. We demonstrate that our logic can capture BAN logic
notions by providing a translation of the BAN operators into our logic,
capturing belief by a form of probabilistic knowledge.



We introduce an axiomatic approach to group recommendations, in line of
previous work on the axiomatic treatment of trust-based recommendation systems,
ranking systems, and other foundational work on the axiomatic approach to
internet mechanisms in social choice settings. In group recommendations we wish
to recommend to a group of agents, consisting of both opinionated and undecided
members, a joint choice that would be acceptable to them. Such a system has
many applications, such as choosing a movie or a restaurant to go to with a
group of friends, recommending games for online game players, & other communal
activities.
  Our method utilizes a given social graph to extract information on the
undecided, relying on the agents influencing them. We first show that a set of
fairly natural desired requirements (a.k.a axioms) leads to an impossibility,
rendering mutual satisfaction of them unreachable. However, we also show a
modified set of axioms that fully axiomatize a group variant of the random-walk
recommendation system, expanding a previous result from the individual
recommendation case.



We give the motivation for scoring clustering algorithms and a metric $M : A
\rightarrow \mathbb{N}$ from the set of clustering algorithms to the natural
numbers which we realize as \begin{equation} M(A) = \sum_i \alpha_i |f_i -
\beta_i|^{w_i} \end{equation} where $\alpha_i,\beta_i,w_i$ are parameters used
for scoring the feature $f_i$, which is computed empirically.. We give a method
by which one can score features such as stability, noise sensitivity, etc and
derive the necessary parameters. We conclude by giving a sample set of scores.



A novel data-driven stochastic robust optimization (DDSRO) framework is
proposed for optimization under uncertainty leveraging labeled multi-class
uncertainty data. Uncertainty data in large datasets are often collected from
various conditions, which are encoded by class labels. Machine learning methods
including Dirichlet process mixture model and maximum likelihood estimation are
employed for uncertainty modeling. A DDSRO framework is further proposed based
on the data-driven uncertainty model through a bi-level optimization structure.
The outer optimization problem follows a two-stage stochastic programming
approach to optimize the expected objective across different data classes;
adaptive robust optimization is nested as the inner problem to ensure the
robustness of the solution while maintaining computational tractability. A
decomposition-based algorithm is further developed to solve the resulting
multi-level optimization problem efficiently. Case studies on process network
design and planning are presented to demonstrate the applicability of the
proposed framework and algorithm.



We propose a recurrent extension of the Ladder networks whose structure is
motivated by the inference required in hierarchical latent variable models. We
demonstrate that the recurrent Ladder is able to handle a wide variety of
complex learning tasks that benefit from iterative inference and temporal
modeling. The architecture shows close-to-optimal results on temporal modeling
of video data, competitive results on music modeling, and improved perceptual
grouping based on higher order abstractions, such as stochastic textures and
motion cues. We present results for fully supervised, semi-supervised, and
unsupervised tasks. The results suggest that the proposed architecture and
principles are powerful tools for learning a hierarchy of abstractions,
learning iterative inference and handling temporal information.



Face deidentification is an active topic amongst privacy and security
researchers. Early deidentification methods relying on image blurring or
pixelization were replaced in recent years with techniques based on formal
anonymity models that provide privacy guaranties and at the same time aim at
retaining certain characteristics of the data even after deidentification. The
latter aspect is particularly important, as it allows to exploit the
deidentified data in applications for which identity information is irrelevant.
In this work we present a novel face deidentification pipeline, which ensures
anonymity by synthesizing artificial surrogate faces using generative neural
networks (GNNs). The generated faces are used to deidentify subjects in images
or video, while preserving non-identity-related aspects of the data and
consequently enabling data utilization. Since generative networks are very
adaptive and can utilize a diverse set of parameters (pertaining to the
appearance of the generated output in terms of facial expressions, gender,
race, etc.), they represent a natural choice for the problem of face
deidentification. To demonstrate the feasibility of our approach, we perform
experiments using automated recognition tools and human annotators. Our results
show that the recognition performance on deidentified images is close to
chance, suggesting that the deidentification process based on GNNs is highly
effective.



Topological models of empirical and formal inquiry are increasingly
prevalent. They have emerged in such diverse fields as domain theory [1, 16],
formal learning theory [18], epistemology and philosophy of science [10, 15, 8,
9, 2], statistics [6, 7] and modal logic [17, 4]. In those applications, open
sets are typically interpreted as hypotheses deductively verifiable by true
propositional information that rules out relevant possibilities. However, in
statistical data analysis, one routinely receives random samples logically
compatible with every statistical hypothesis. We bridge the gap between
propositional and statistical data by solving for the unique topology on
probability measures in which the open sets are exactly the statistically
verifiable hypotheses. Furthermore, we extend that result to a topological
characterization of learnability in the limit from statistical data.



We present an approach to synthesizing photographic images conditioned on
semantic layouts. Given a semantic label map, our approach produces an image
with photographic appearance that conforms to the input layout. The approach
thus functions as a rendering engine that takes a two-dimensional semantic
specification of the scene and produces a corresponding photographic image.
Unlike recent and contemporaneous work, our approach does not rely on
adversarial training. We show that photographic images can be synthesized from
semantic layouts by a single feedforward network with appropriate structure,
trained end-to-end with a direct regression objective. The presented approach
scales seamlessly to high resolutions; we demonstrate this by synthesizing
photographic images at 2-megapixel resolution, the full resolution of our
training data. Extensive perceptual experiments on datasets of outdoor and
indoor scenes demonstrate that images synthesized by the presented approach are
considerably more realistic than alternative approaches. The results are shown
in the supplementary video at https://youtu.be/0fhUJT21-bs



Language is increasingly being used to define rich visual recognition
problems with supporting image collections sourced from the web. Structured
prediction models are used in these tasks to take advantage of correlations
between co-occurring labels and visual input but risk inadvertently encoding
social biases found in web corpora. In this work, we study data and models
associated with multilabel object classification and visual semantic role
labeling. We find that (a) datasets for these tasks contain significant gender
bias and (b) models trained on these datasets further amplify existing bias.
For example, the activity cooking is over 33% more likely to involve females
than males in a training set, and a trained model further amplifies the
disparity to 68% at test time. We propose to inject corpus-level constraints
for calibrating existing structured prediction models and design an algorithm
based on Lagrangian relaxation for collective inference. Our method results in
almost no performance loss for the underlying recognition task but decreases
the magnitude of bias amplification by 47.5% and 40.5% for multilabel
classification and visual semantic role labeling, respectively.



Recommendation to groups of users is a challenging and currently only
passingly studied task. Especially the evaluation aspect often appears ad-hoc
and instead of truly evaluating on groups of users, synthesizes groups by
merging individual preferences.
  In this paper, we present a user study, recording the individual and shared
preferences of actual groups of participants, resulting in a robust,
standardized evaluation benchmark. Using this benchmarking dataset, that we
share with the research community, we compare the respective performance of a
wide range of music group recommendation techniques proposed in the



Convolutional Neural Networks have been highly successful in performing a
host of computer vision tasks such as object recognition, object detection,
image segmentation and texture synthesis. In 2015, Gatys et. al [7] show how
the style of a painter can be extracted from an image of the painting and
applied to another normal photograph, thus recreating the photo in the style of
the painter. The method has been successfully applied to a wide range of images
and has since spawned multiple applications and mobile apps. In this paper, the
neural style transfer algorithm is applied to fashion so as to synthesize new
custom clothes. We construct an approach to personalize and generate new custom
clothes based on a users preference and by learning the users fashion choices
from a limited set of clothes from their closet. The approach is evaluated by
analyzing the generated images of clothes and how well they align with the
users fashion style.



Model based iterative reconstruction (MBIR) algorithms for low-dose X-ray CT
are computationally expensive. To address this problem, we recently proposed
the world-first deep convolutional neural network (CNN) for low-dose X-ray CT
and won the second place in 2016 AAPM Low-Dose CT Grand Challenge. However,
some of the texture were not fully recovered. To cope with this problem, here
we propose a deep residual learning approach in directional wavelet domain. The
proposed method is motivated by an observation that a deep convolutional neural
network can be interpreted as a multilayer convolutional framelets expansion
using non-local basis convolved with data-driven local basis. We further extend
the idea to derive a deep convolutional framelet expansion by combining global
redundant transforms and signal boosting from multiple signal representations.
Extensive experimental results confirm that the proposed network has
significantly improved performance and preserves the detail texture of the
original images



One question central to Reinforcement Learning is how to learn a feature
representation that supports algorithm scaling and re-use of learned
information from different tasks. Successor Features approach this problem by
learning a feature representation that satisfies a temporal constraint. We
present an implementation of an approach that decouples the feature
representation from the reward function, making it suitable for transferring
knowledge between domains. We then assess the advantages and limitations of
using Successor Features for transfer.



Computer vision has benefited from initializing multiple deep layers with
weights pretrained on large supervised training sets like ImageNet. Natural
language processing (NLP) typically sees initialization of only the lowest
layer of deep models with pretrained word vectors. In this paper, we use a deep
LSTM encoder from an attentional sequence-to-sequence model trained for machine
translation (MT) to contextualize word vectors. We show that adding these
context vectors (CoVe) improves performance over using only unsupervised word
and character vectors on a wide variety of common NLP tasks: sentiment analysis
(SST, IMDb), question classification (TREC), entailment (SNLI), and question
answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe
improves performance of our baseline models to the state of the art.



In this paper, we explore the utilization of natural language to drive
transfer for reinforcement learning (RL). Despite the wide-spread application
of deep RL techniques, learning generalized policy representations that work
across domains remains a challenging problem. We demonstrate that textual
descriptions of environments provide a compact intermediate channel to
facilitate effective policy transfer. We employ a model-based RL approach
consisting of a differentiable planning module, a model-free component and a
factorized representation to effectively utilize entity descriptions. Our model
outperforms prior work on both transfer and multi-task scenarios in a variety
of different environments.



Low-rank modeling has many important applications in computer vision and
machine learning. While the matrix rank is often approximated by the convex
nuclear norm, the use of nonconvex low-rank regularizers has demonstrated
better empirical performance. However, the resulting optimization problem is
much more challenging. Recent state-of-the-art requires an expensive full SVD
in each iteration. In this paper, we show that for many commonly-used nonconvex
low-rank regularizers, a cutoff can be derived to automatically threshold the
singular values obtained from the proximal operator. This allows such operator
being efficiently approximated by power method. Based on it, we develop a
proximal gradient algorithm (and its accelerated variant) with inexact proximal
splitting and prove that a convergence rate of O(1/T) where T is the number of
iterations is guaranteed. Furthermore, we show the proposed algorithm can be
well parallelized, which achieves nearly linear speedup w.r.t the number of
threads. Extensive experiments are performed on matrix completion and robust
principal component analysis, which shows a significant speedup over the
state-of-the-art. Moreover, the matrix solution obtained is more accurate and
has a lower rank than that of the nuclear norm regularizer.



Recently, some E-commerce sites launch a new interaction box called Tips on
their mobile apps. Users can express their experience and feelings or provide
suggestions using short texts typically several words or one sentence. In
essence, writing some tips and giving a numerical rating are two facets of a
user's product assessment action, expressing the user experience and feelings.
Jointly modeling these two facets is helpful for designing a better
recommendation system. While some existing models integrate text information
such as item specifications or user reviews into user and item latent factors
for improving the rating prediction, no existing works consider tips for
improving recommendation quality. We propose a deep learning based framework
named NRT which can simultaneously predict precise ratings and generate
abstractive tips with good linguistic quality simulating user experience and
feelings. For abstractive tips generation, gated recurrent neural networks are
employed to "translate" user and item latent representations into a concise
sentence. Extensive experiments on benchmark datasets from different domains
show that NRT achieves significant improvements over the state-of-the-art
methods. Moreover, the generated tips can vividly predict the user experience
and feelings.



Exemplar-based face sketch synthesis methods usually meet the challenging
problem that input photos are captured in different lighting conditions from
training photos. The critical step causing the failure is the search of similar
patch candidates for an input photo patch. Conventional illumination invariant
patch distances are adopted rather than directly relying on pixel intensity
difference, but they will fail when local contrast within a patch changes. In
this paper, we propose a fast preprocessing method named Bidirectional
Luminance Remapping (BLR), which interactively adjust the lighting of training
and input photos. Our method can be directly integrated into state-of-the-art
exemplar-based methods to improve their robustness with ignorable computational
cost.



Discriminative correlation filters (DCFs) have been shown to perform
superiorly in visual tracking. They only need a small set of training samples
from the initial frame to generate an appearance model. However, existing DCFs
learn the filters separately from feature extraction, and update these filters
using a moving average operation with an empirical weight. These DCF trackers
hardly benefit from the end-to-end training. In this paper, we propose the
CREST algorithm to reformulate DCFs as a one-layer convolutional neural
network. Our method integrates feature extraction, response map generation as
well as model update into the neural networks for an end-to-end training. To
reduce model degradation during online update, we apply residual learning to
take appearance changes into account. Extensive experiments on the benchmark
datasets demonstrate that our CREST tracker performs favorably against
state-of-the-art trackers.



The past decade has seen a revolution in genomic technologies that enable a
flood of genome-wide profiling of chromatin marks. Recent literature tried to
understand gene regulation by predicting gene expression from large-scale
chromatin measurements. Two fundamental challenges exist for such learning
tasks: (1) genome-wide chromatin signals are spatially structured,
high-dimensional and highly modular; and (2) the core aim is to understand what
are the relevant factors and how they work together? Previous studies either
failed to model complex dependencies among input signals or relied on separate
feature analysis to explain the decisions. This paper presents an
attention-based deep learning approach; we call AttentiveChrome, that uses a
unified architecture to model and to interpret dependencies among chromatin
factors for controlling gene regulation. AttentiveChrome uses a hierarchy of
multiple Long short-term memory (LSTM) modules to encode the input signals and
to model how various chromatin marks cooperate automatically. AttentiveChrome
trains two levels of attention jointly with the target prediction, enabling it
to attend differentially to relevant marks and to locate important positions
per mark. We evaluate the model across 56 different cell types (tasks) in
human. Not only is the proposed architecture more accurate, but its attention
scores also provide a better interpretation than state-of-the-art feature
visualization methods such as saliency map.
  Code and data are shared at www.deepchrome.org



As technology become more advanced, those who design, use and are otherwise
affected by it want to know that it will perform correctly, and understand why
it does what it does, and how to use it appropriately. In essence they want to
be able to trust the systems that are being designed. In this survey we present
assurances that are the method by which users can understand how to trust this
technology. Trust between humans and autonomy is reviewed, and the implications
for the design of assurances are highlighted. A survey of research that has
been performed with respect to assurances is presented, and several key ideas
are extracted in order to refine the definition of assurances. Several
directions for future research are identified and discussed.



While there is currently a lot of enthusiasm about "big data", useful data is
usually "small" and expensive to acquire. In this paper, we present a new
paradigm of learning partial differential equations from {\em small} data. In
particular, we introduce \emph{hidden physics models}, which are essentially
data-efficient learning machines capable of leveraging the underlying laws of
physics, expressed by time dependent and nonlinear partial differential
equations, to extract patterns from high-dimensional data generated from
experiments. The proposed methodology may be applied to the problem of
learning, system identification, or data-driven discovery of partial
differential equations. Our framework relies on Gaussian processes, a powerful
tool for probabilistic inference over functions, that enables us to strike a
balance between model complexity and data fitting. The effectiveness of the
proposed approach is demonstrated through a variety of canonical problems,
spanning a number of scientific domains, including the Navier-Stokes,
Schr\"odinger, Kuramoto-Sivashinsky, and time dependent linear fractional
equations. The methodology provides a promising new direction for harnessing
the long-standing developments of classical methods in applied mathematics and
mathematical physics to design learning machines with the ability to operate in
complex domains without requiring large quantities of data.



Deep neural networks have become ubiquitous for applications related to
visual recognition and language understanding tasks. However, it is often
prohibitive to use typical neural networks on devices like mobile phones or
smart watches since the model sizes are huge and cannot fit in the limited
memory available on such devices. While these devices could make use of machine
learning models running on high-performance data centers with CPUs or GPUs,
this is not feasible for many applications because data can be privacy
sensitive and inference needs to be performed directly "on" device.
  We introduce a new architecture for training compact neural networks using a
joint optimization framework. At its core lies a novel objective that jointly
trains using two different types of networks--a full trainer neural network
(using existing architectures like Feed-forward NNs or LSTM RNNs) combined with
a simpler "projection" network that leverages random projections to transform
inputs or intermediate representations into bits. The simpler network encodes
lightweight and efficient-to-compute operations in bit space with a low memory
footprint. The two networks are trained jointly using backpropagation, where
the projection network learns from the full network similar to apprenticeship
learning. Once trained, the smaller network can be used directly for inference
at low memory and computation cost. We demonstrate the effectiveness of the new
approach at significantly shrinking the memory requirements of different types
of neural networks while preserving good accuracy on visual recognition and
text classification tasks. We also study the question "how many neural bits are
required to solve a given task?" using the new framework and show empirical
results contrasting model predictive capacity (in bits) versus accuracy on
several datasets.



We explain that the difficulties of training deep neural networks come from a
syndrome of three consistency issues. This paper describes our efforts in their
analysis and treatment. The first issue is the training speed inconsistency in
different layers. We propose to address it with an intuitive,
simple-to-implement, low footprint second-order method. The second issue is the
scale inconsistency between the layer inputs and the layer residuals. We
explain how second-order information provides favorable convenience in removing
this roadblock. The third and most challenging issue is the inconsistency in
residual propagation. Based on the fundamental theorem of linear algebra, we
provide a mathematical characterization of the famous vanishing gradient
problem. Thus, an important design principle for future optimization and neural
network design is derived. We conclude this paper with the construction of a
novel contractive neural network.



Algorithms learned from data are increasingly used for deciding many aspects
in our life: from movies we see, to prices we pay, or medicine we get. Yet
there is growing evidence that decision making by inappropriately trained
algorithms may unintentionally discriminate people. For example, in automated
matching of candidate CVs with job descriptions, algorithms may capture and
propagate ethnicity related biases. Several repairs for selected algorithms
have already been proposed, but the underlying mechanisms how such
discrimination happens from the computational perspective are not yet
scientifically understood. We need to develop theoretical understanding how
algorithms may become discriminatory, and establish fundamental machine
learning principles for prevention. We need to analyze machine learning process
as a whole to systematically explain the roots of discrimination occurrence,
which will allow to devise global machine learning optimization criteria for
guaranteed prevention, as opposed to pushing empirical constraints into
existing algorithms case-by-case. As a result, the state-of-the-art will
advance from heuristic repairing, to proactive and theoretically supported
prevention. This is needed not only because law requires to protect vulnerable
people. Penetration of big data initiatives will only increase, and computer
science needs to provide solid explanations and accountability to the public,
before public concerns lead to unnecessarily restrictive regulations against
machine learning.



Recent studies have shown that attackers can force deep learning models to
misclassify so-called "adversarial examples": maliciously generated images
formed by making imperceptible modifications to pixel values. With growing
interest in deep learning for security applications, it is important for
security experts and users of machine learning to recognize how learning
systems may be attacked. Due to the complex nature of deep learning, it is
challenging to understand how deep models can be fooled by adversarial
examples. Thus, we present a web-based visualization tool,
Adversarial-Playground, to demonstrate the efficacy of common adversarial
methods against a convolutional neural network (CNN) system.
Adversarial-Playground is educational, modular and interactive. (1) It enables
non-experts to compare examples visually and to understand why an adversarial
example can fool a CNN-based image classifier. (2) It can help security experts
explore more vulnerability of deep learning as a software module. (3) Building
an interactive visualization is challenging in this domain due to the large
feature space of image classification (generating adversarial examples is slow
in general and visualizing images are costly). Through multiple novel design
choices, our tool can provide fast and accurate responses to user requests.
Empirically, we find that our client-server division strategy reduced the
response time by an average of 1.5 seconds per sample. Our other innovation, a
faster variant of JSMA evasion algorithm, empirically performed twice as fast
as JSMA and yet maintains a comparable evasion rate.
  Project source code and data from our experiments available at:
https://github.com/QData/AdversarialDNN-Playground



It has been postulated that a good representation is one that disentangles
the underlying explanatory factors of variation. However, it remains an open
question what kind of training framework could potentially achieve that.
Whereas most previous work focuses on the static setting (e.g., with images),
we postulate that some of the causal factors could be discovered if the learner
is allowed to interact with its environment. The agent can experiment with
different actions and observe their effects. More specifically, we hypothesize
that some of these factors correspond to aspects of the environment which are
independently controllable, i.e., that there exists a policy and a learnable
feature for each such aspect of the environment, such that this policy can
yield changes in that feature with minimal changes to other features that
explain the statistical variations in the observed data. We propose a specific
objective function to find such factors and verify experimentally that it can
indeed disentangle independently controllable aspects of the environment
without any extrinsic reward signal.



We describe the University of Maryland machine translation systems submitted
to the WMT17 German-English Bandit Learning Task. The task is to adapt a
translation system to a new domain, using only bandit feedback: the system
receives a German sentence to translate, produces an English sentence, and only
gets a scalar score as feedback. Targeting these two challenges (adaptation and
bandit learning), we built a standard neural machine translation system and
extended it in two ways: (1) robust reinforcement learning techniques to learn
effectively from the bandit feedback, and (2) domain adaptation using data
selection from a large corpus of parallel data.



Agent-based modeling and simulation tools provide a mature platform for
development of complex simulations. They however, have not been applied much in
the domain of mainstream modeling and simulation of computer networks. In this
article, we evaluate how and if these tools can offer any value-addition in the
modeling & simulation of complex networks such as pervasive computing,
large-scale peer-to-peer systems, and networks involving considerable
environment and human/animal/habitat interaction. Specifically, we demonstrate
the effectiveness of NetLogo - a tool that has been widely used in the area of
agent-based social simulation.



Making roads safer by avoiding road collisions is one of the main reasons for
inventing Autonomous vehicles (AVs). In this context, designing agent-based
collision avoidance components of AVs which truly represent human cognition and
emotions look is a more feasible approach as agents can replace human drivers.
However, to the best of our knowledge, very few human emotion and
cognition-inspired agent-based studies have previously been conducted in this
domain. Furthermore, these agent-based solutions have not been validated using
any key validation technique. Keeping in view this lack of validation
practices, we have selected state-of-the-art Emotion Enabled Cognitive Agent
(EEC_Agent), which was proposed to avoid lateral collisions between semi-AVs.
The architecture of EEC_Agent has been revised using Exploratory Agent Based
Modeling (EABM) level of the Cognitive Agent Based Computing (CABC) framework
and real-time fear emotion generation mechanism using the Ortony, Clore &
Collins (OCC) model has also been introduced. Then the proposed fear generation
mechanism has been validated using the Validated Agent Based Modeling level of
CABC framework using a Virtual Overlay MultiAgent System (VOMAS). Extensive
simulation and practical experiments demonstrate that the Enhanced EEC_Agent
exhibits the capability to feel different levels of fear, according to
different traffic situations and also needs a smaller Stopping Sight Distance
(SSD) and Overtaking Sight Distance (OSD) as compared to human drivers.



In the real world, agents or entities are in a continuous state of
interactions. These inter- actions lead to various types of complexity
dynamics. One key difficulty in the study of complex agent interactions is the
difficulty of modeling agent communication on the basis of rewards. Game theory
offers a perspective of analysis and modeling these interactions. Previously,
while a large amount of literature is available on game theory, most of it is
from specific domains and does not cater for the concepts from an agent- based
perspective. Here in this paper, we present a comprehensive multidisciplinary
state-of-the-art review and taxonomy of game theory models of complex
interactions between agents.



The success of various applications including robotics, digital content
creation, and visualization demand a structured and abstract representation of
the 3D world from limited sensor data. Inspired by the nature of human
perception of 3D shapes as a collection of simple parts, we explore such an
abstract shape representation based on primitives. Given a single depth image
of an object, we present 3D-PRNN, a generative recurrent neural network that
synthesizes multiple plausible shapes composed of a set of primitives. Our
generative model encodes symmetry characteristics of common man-made objects,
preserves long-range structural coherence, and describes objects of varying
complexity with a compact representation. We also propose a method based on
Gaussian Fields to generate a large scale dataset of primitive-based shape
representations to train our network. We evaluate our approach on a wide range
of examples and show that it outperforms nearest-neighbor based shape retrieval
methods and is on-par with voxel-based generative models while using a
significantly reduced parameter space.



In this paper, we propose the nonlinearity generation method to speed up and
stabilize the training of deep convolutional neural networks. The proposed
method modifies a family of activation functions as nonlinearity generators
(NGs). NGs make the activation functions linear symmetric for their inputs to
lower model capacity, and automatically introduce nonlinearity to enhance the
capacity of the model during training. The proposed method can be considered an
unusual form of regularization: the model parameters are obtained by training a
relatively low-capacity model, that is relatively easy to optimize at the
beginning, with only a few iterations, and these parameters are reused for the
initialization of a higher-capacity model. We derive the upper and lower bounds
of variance of the weight variation, and show that the initial symmetric
structure of NGs helps stabilize training. We evaluate the proposed method on
different frameworks of convolutional neural networks over two object
recognition benchmark tasks (CIFAR-10 and CIFAR-100). Experimental results
showed that the proposed method allows us to (1) speed up the convergence of
training, (2) allow for less careful weight initialization, (3) improve or at
least maintain the performance of the model at negligible extra computational
cost, and (4) easily train a very deep model.



In this paper, we study several GAN related topics mathematically, including
Inception score, label smoothing, gradient vanishing and the -log(D(x))
alternative. We show that Inception score is actually equivalent to Mode score,
both consisting of two entropy terms, which has the drawback of ignoring the
prior distribution of the labels. We thus propose AM score as an alternative
that leverages cross-entropy and takes the reference distribution into account.
Empirical results indicate that AM score outperforms Inception score. We study
label smoothing, gradient vanishing and -log(D(x)) alternative from the
perspective of class-aware gradient, with which we show the exact problems when
applying label smoothing to fake samples along with the log(1-D(x)) generator
loss, which is previously unclear, and more importantly show that the problem
does not exist when using the -log(D(x)) generator loss.



Variational Inference is a popular technique to approximate a possibly
intractable Bayesian posterior with a more tractable one. Recently, Boosting
Variational Inference has been proposed as a new paradigm to approximate the
posterior by a mixture of densities by greedily adding components to the
mixture. In the present work, we study the convergence properties of this
approach from a modern optimization viewpoint by establishing connections to
the classic Frank-Wolfe algorithm. Our analyses yields novel theoretical
insights on the Boosting of Variational Inference regarding the sufficient
conditions for convergence, explicit sublinear/linear rates, and algorithmic
simplifications.



In this paper we present a new dataset and user simulator e-QRAQ (explainable
Query, Reason, and Answer Question) which tests an Agent's ability to read an
ambiguous text; ask questions until it can answer a challenge question; and
explain the reasoning behind its questions and answer. The User simulator
provides the Agent with a short, ambiguous story and a challenge question about
the story. The story is ambiguous because some of the entities have been
replaced by variables. At each turn the Agent may ask for the value of a
variable or try to answer the challenge question. In response the User
simulator provides a natural language explanation of why the Agent's query or
answer was useful in narrowing down the set of possible answers, or not. To
demonstrate one potential application of the e-QRAQ dataset, we train a new
neural architecture based on End-to-End Memory Networks to successfully
generate both predictions and partial explanations of its current understanding
of the problem. We observe a strong correlation between the quality of the
prediction and explanation.



In this paper, we methodologically address the problem of cumulative reward
overestimation in deep reinforcement learning. We generalise notions from
information-theoretic bounded rationality to handle high-dimensional state
spaces efficiently. The resultant algorithm encompasses a wide range of
learning outcomes that can be demonstrated by tuning a Lagrange multiplier that
intrinsically penalises rewards. We show that deep Q-networks arise as a
special case of our proposed approach. We introduce a novel scheduling scheme
for bounded-rational behaviour that ensures sample efficiency and robustness.
In experiments on Atari games, we show that our algorithm outperforms other
deep reinforcement learning algorithms (e.g., deep and double deep Q-networks)
in terms of both game-play performance and sample complexity.



The vanishing gradient problem was a major obstacle for the success of deep
learning. In recent years it was gradually alleviated through multiple
different techniques. However the problem was not really overcome in a
fundamental way, since it is inherent to neural networks with activation
functions based on dot products. In a series of papers, we are going to analyze
alternative neural network structures which are not based on dot products. In
this first paper, we revisit neural networks built up of layers based on
distance measures and Gaussian activation functions. These kinds of networks
were only sparsely used in the past since they are hard to train when using
plain stochastic gradient descent methods. We show that by using Root Mean
Square Propagation (RMSProp) it is possible to efficiently learn multi-layer
neural networks. Furthermore we show that when appropriately initialized these
kinds of neural networks suffer much less from the vanishing and exploding
gradient problem than traditional neural networks even for deep networks.



Blind spots are one of the causes of road accidents in the hilly and flat
areas. These blind spot accidents can be decreased by establishing an Internet
of Vehicles (IoV) using Vehicle-2-Vehicle (V2V) and Vehicle-2-Infrastrtructure
(V2I) communication systems. But the problem with these IoV is that most of
them are using DSRC or single Radio Access Technology (RAT) as a wireless
technology, which has been proven to be failed for efficient communication
between vehicles. Recently, Cognitive Radio (CR) based IoV have to be proven
best wireless communication systems for vehicular networks. However, the
spectrum mobility is a challenging task to keep CR based vehicular networks
interoperable and has not been addressed sufficiently in existing research. In
our previous research work, the Cognitive Radio Site (CR-Site) has been
proposed as in-vehicle CR-device, which can be utilized to establish efficient
IoV systems. H In this paper, we have introduced the Emotions Inspired
Cognitive Agent (EIC_Agent) based spectrum mobility mechanism in CR-Site and
proposed a novel emotions controlled spectrum mobility scheme for efficient
syntactic interoperability between vehicles. For this purpose, a probabilistic
deterministic finite automaton using fear factor is proposed to perform
efficient spectrum mobility using fuzzy logic. In addition, the quantitative
computation of different fear intensity levels has been performed with the help
of fuzzy logic. The system has been tested using active data from different GSM
service providers on Mangla-Mirpur road. This is supplemented by extensive
simulation experiments which validate the proposed scheme for CR based
high-speed vehicular networks. The qualitative comparison with the
existing-state-of the-art has proven the superiority of the proposed emotions
controlled syntactic interoperable spectrum mobility scheme within cognitive
radio based IoV systems.



Rear end collisions are deadliest in nature and cause most of traffic
casualties and injuries. In the existing research, many rear end collision
avoidance solutions have been proposed. However, the problem with these
proposed solutions is that they are highly dependent on precise mathematical
models. Whereas, the real road driving is influenced by non-linear factors such
as road surface situations, driver reaction time, pedestrian flow and vehicle
dynamics, hence obtaining the accurate mathematical model of the vehicle
control system is challenging. This problem with precise control based rear end
collision avoidance schemes has been addressed using fuzzy logic, but the
excessive number of fuzzy rules straightforwardly prejudice their efficiency.
Furthermore, these fuzzy logic based controllers have been proposed without
using proper agent based modeling that helps in mimicking the functions of an
artificial human driver executing these fuzzy rules. Keeping in view these
limitations, we have proposed an Enhanced Emotion Enabled Cognitive Agent
(EEEC_Agent) based controller that helps the Autonomous Vehicles (AVs) to
perform rear end collision avoidance with less number of rules, designed after
fear emotion, and high efficiency. To introduce a fear emotion generation
mechanism in EEEC_Agent, Orton, Clore & Collins (OCC) model has been employed.
The fear generation mechanism of EEEC_Agent has been verified using NetLogo
simulation. Furthermore, practical validation of EEEC_Agent functions has been
performed using specially built prototype AV platform. Eventually, the
qualitative comparative study with existing state of the art research works
reflect that proposed model outperforms recent research.



Background Road collisions and casualties pose a serious threat to commuters
around the globe. Autonomous Vehicles (AVs) aim to make the use of technology
to reduce the road accidents. However, the most of research work in the context
of collision avoidance has been performed to address, separately, the rear end,
front end and lateral collisions in less congested and with high
inter-vehicular distances. Purpose The goal of this paper is to introduce the
concept of a social agent, which interact with other AVs in social manners like
humans are social having the capability of predicting intentions, i.e.
mentalizing and copying the actions of each other, i.e. mirroring. The proposed
social agent is based on a human-brain inspired mentalizing and mirroring
capabilities and has been modelled for collision detection and avoidance under
congested urban road traffic.
  Method We designed our social agent having the capabilities of mentalizing
and mirroring and for this purpose we utilized Exploratory Agent Based Modeling
(EABM) level of Cognitive Agent Based Computing (CABC) framework proposed by
Niazi and Hussain.
  Results Our simulation and practical experiments reveal that by embedding
Richardson's arms race model within AVs, collisions can be avoided while
travelling on congested urban roads in a flock like topologies. The performance
of the proposed social agent has been compared at two different levels.



Deep neural networks are used in many state-of-the-art systems for machine
perception. Once a network is trained to do a specific task, e.g., bird
classification, it cannot easily be trained to do new tasks, e.g.,
incrementally learning to recognize additional bird species or learning an
entirely different task such as flower recognition. When new tasks are added,
typical deep neural networks are prone to catastrophically forgetting previous
tasks. Networks that are capable of assimilating new information incrementally,
much like how humans form new memories over time, will be more efficient than
re-training the model from scratch each time a new task needs to be learned.
There have been multiple attempts to develop schemes that mitigate catastrophic
forgetting, but these methods have not been directly compared, the tests used
to evaluate them vary considerably, and these methods have only been evaluated
on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics
and benchmarks for directly comparing five different mechanisms designed to
mitigate catastrophic forgetting in neural networks: regularization,
ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on
real-world images and sounds show that the mechanism(s) that are critical for
optimal performance vary based on the incremental training paradigm and type of
data being used, but they all demonstrate that the catastrophic forgetting
problem has yet to be solved.



Highly automated robot ecologies (HARE), or societies of independent
autonomous robots or agents, are rapidly becoming an important part of much of
the world's critical infrastructure. As with human societies, regulation,
wherein a governing body designs rules and processes for the society, plays an
important role in ensuring that HARE meet societal objectives. However, to
date, a careful study of interactions between a regulator and HARE is lacking.
In this paper, we report on three user studies which give insights into how to
design systems that allow people, acting as the regulatory authority, to
effectively interact with HARE. As in the study of political systems in which
governments regulate human societies, our studies analyze how interactions
between HARE and regulators are impacted by regulatory power and individual
(robot or agent) autonomy. Our results show that regulator power, decision
support, and adaptive autonomy can each diminish the social welfare of HARE,
and hint at how these seemingly desirable mechanisms can be designed so that
they become part of successful HARE.



As deep neural networks become more complex and input datasets grow larger,
it can take days or even weeks to train a deep neural network to the desired
accuracy. Therefore, distributed Deep Learning at a massive scale is a critical
capability, since it offers the potential to reduce the training time from
weeks to hours. In this paper, we present a software-hardware co-optimized
distributed Deep Learning system that can achieve near-linear scaling up to
hundreds of GPUs. The core algorithm is a multi-ring communication pattern that
provides a good tradeoff between latency and bandwidth and adapts to a variety
of system configurations. The communication algorithm is implemented as a
library for easy use. This library has been integrated into Tensorflow, Caffe,
and Torch. We train Resnet-101 on Imagenet 22K with 64 IBM Power8 S822LC
servers (256 GPUs) in about 7 hours to an accuracy of 33.8 % validation
accuracy. Microsoft's ADAM and Google's DistBelief results did not reach 30 %
validation accuracy for Imagenet 22K. Compared to Facebook AI Research's recent
paper on 256 GPU training, we use a different communication algorithm, and our
combined software and hardware system offers better communication overhead for
Resnet-50. A PowerAI DDL enabled version of Torch completed 90 epochs of
training on Resnet 50 for 1K classes in 50 minutes using 64 IBM Power8 S822LC
servers (256 GPUs).



Generative statistical models of chord sequences play crucial roles in music
processing. To capture syntactic similarities among certain chords (e.g. in C
major key, between G and G7 and between F and Dm), we study hidden Markov
models and probabilistic context-free grammar models with latent variables
describing syntactic categories of chord symbols and their unsupervised
learning techniques for inducing the latent grammar from data. Surprisingly, we
find that these models often outperform conventional Markov models in
predictive power, and the self-emergent categories often correspond to
traditional harmonic functions. This implies the need for chord categories in
harmony models from the informatics perspective.



Sequence-to-sequence models have shown promising improvements on the temporal
task of video captioning, but they optimize word-level cross-entropy loss
during training. First, using policy gradient and mixed-loss methods for
reinforcement learning, we directly optimize sentence-level task-based metrics
(as rewards), achieving significant improvements over the baseline, based on
both automatic metrics and human evaluation on multiple datasets. Next, we
propose a novel entailment-enhanced reward (CIDEnt) that corrects
phrase-matching based metrics (such as CIDEr) to only allow for
logically-implied partial matches and avoid contradictions, achieving further
significant improvements over the CIDEr-reward model. Overall, our
CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.



We present a simple sequential sentence encoder for multi-domain natural
language inference. Our encoder is based on stacked bidirectional LSTM-RNNs
with shortcut connections and fine-tuning of word embeddings. The overall
supervised model uses the above encoder to encode two input sentences into two
vectors, and then uses a classifier over the vector combination to label the
relationship between these two sentences as that of entailment, contradiction,
or neural. Our Shortcut-Stacked sentence encoders achieve strong improvements
over existing encoders on matched and mismatched multi-domain natural language
inference (top non-ensemble single-model result in the EMNLP RepEval 2017
Shared Task (Nangia et al., 2017)). Moreover, they achieve the new
state-of-the-art encoding result on the original SNLI dataset (Bowman et al.,
2015).



In this paper, we propose a secure multibiometric system that uses deep
neural networks and error-correction coding. We present a feature-level fusion
framework to generate a secure multibiometric template from each user's
multiple biometrics. Two fusion architectures, fully connected architecture and
bilinear architecture, are implemented to develop a robust multibiometric
shared representation. The shared representation is used to generate a
cancelable biometric template that involves the selection of a different set of
reliable and discriminative features for each user. This cancelable template is
a binary vector and is passed through an appropriate error-correcting decoder
to find a closest codeword and this codeword is hashed to generate the final
secure template. The efficacy of the proposed approach is shown using a
multimodal database where we achieve state-of-the-art matching performance,
along with cancelability and security.



Recently software development companies started to embrace Machine Learning
(ML) techniques for introducing a series of advanced functionality in their
products such as personalisation of the user experience, improved search,
content recommendation and automation. The technical challenges for tackling
these problems are heavily researched in literature. A less studied area is a
pragmatic approach to the role of humans in a complex modern industrial
environment where ML based systems are developed. Key stakeholders affect the
system from inception and up to operation and maintenance. Product managers
want to embed "smart" experiences for their users and drive the decisions on
what should be built next; software engineers are challenged to build or
utilise ML software tools that require skills that are well outside of their
comfort zone; legal and risk departments may influence design choices and data
access; operations teams are requested to maintain ML systems which are
non-stationary in their nature and change behaviour over time; and finally ML
practitioners should communicate with all these stakeholders to successfully
build a reliable system. This paper discusses some of the challenges we faced
in Atlassian as we started investing more in the ML space.



Active learning aims to select a small subset of data for annotation such
that a classifier learned on the data is highly accurate. This is usually done
using heuristic selection methods, however the effectiveness of such methods is
limited and moreover, the performance of heuristics varies between datasets. To
address these shortcomings, we introduce a novel formulation by reframing the
active learning as a reinforcement learning problem and explicitly learning a
data selection policy, where the policy takes the role of the active learning
heuristic. Importantly, our method allows the selection policy learned using
simulation on one language to be transferred to other languages. We demonstrate
our method using cross-lingual named entity recognition, observing uniform
improvements over traditional active learning.



Many stochastic optimization algorithms work by estimating the gradient of
the cost function on the fly by sampling datapoints uniformly at random from a
training set. However, the estimator might have a large variance, which
inadvertently slows down the convergence rate of the algorithms. One way to
reduce this variance is to sample the datapoints from a carefully selected
non-uniform distribution. In this work, we propose a novel non-uniform sampling
approach that uses the multi-armed bandit framework. Theoretically, we show
that our algorithm asymptotically approximates the optimal variance within a
factor of 3. Empirically, we show that using this datapoint-selection technique
results in a significant reduction in the convergence time and variance of
several stochastic optimization algorithms such as SGD, SVRG and SAGA. This
approach for sampling datapoints is general, and can be used in conjunction
with any algorithm that uses an unbiased gradient estimation -- we expect it to
have broad applicability beyond the specific examples explored in this work.



We propose a new approach to train the Generative Adversarial Nets (GANs)
with a mixture of generators to overcome the mode collapsing problem. The main
intuition is to employ multiple generators, instead of using a single one as in
the original GAN. The idea is simple, yet proven to be extremely effective at
covering diverse data modes, easily overcoming the mode collapse and delivering
state-of-the-art results. A minimax formulation is able to establish among a
classifier, a discriminator, and a set of generators in a similar spirit with
GAN. Generators create samples that are intended to come from the same
distribution as the training data, whilst the discriminator determines whether
samples are true data or generated by generators, and the classifier specifies
which generator a sample comes from. The distinguishing feature is that
internal samples are created from multiple generators, and then one of them
will be randomly selected as final output similar to the mechanism of a
probabilistic mixture model. We term our method Mixture GAN (MGAN). We develop
theoretical analysis to prove that, at the equilibrium, the Jensen-Shannon
divergence (JSD) between the mixture of generators' distributions and the
empirical data distribution is minimal, whilst the JSD among generators'
distributions is maximal, hence effectively avoiding the mode collapse. By
utilizing parameter sharing, our proposed model adds minimal computational cost
to the standard GAN, and thus can also efficiently scale to large-scale
datasets. We conduct extensive experiments on synthetic 2D data and natural
image databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior
performance of our MGAN in achieving state-of-the-art Inception scores over
latest baselines, generating diverse and appealing recognizable objects at
different resolutions, and specializing in capturing different types of objects
by generators.



Model-free deep reinforcement learning algorithms have been shown to be
capable of learning a wide range of robotic skills, but typically require a
very large number of samples to achieve good performance. Model-based
algorithms, in principle, can provide for much more efficient learning, but
have proven difficult to extend to expressive, high-capacity models such as
deep neural networks. In this work, we demonstrate that medium-sized neural
network models can in fact be combined with model predictive control (MPC) to
achieve excellent sample complexity in a model-based reinforcement learning
algorithm, producing stable and plausible gaits to accomplish various complex
locomotion tasks. We also propose using deep neural network dynamics models to
initialize a model-free learner, in order to combine the sample efficiency of
model-based approaches with the high task-specific performance of model-free
methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure
model-based approach trained on just random action data can follow arbitrary
trajectories with excellent sample efficiency, and that our hybrid algorithm
can accelerate model-free learning on high-speed benchmark tasks, achieving
sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents.
Videos can be found at https://sites.google.com/view/mbmf



We present four logic puzzles and after that their solutions. Joseph Yeo
designed 'Cheryl's Birthday'. Mike Hartley came up with a novel solution for
'One Hundred Prisoners and a Light Bulb'. Jonathan Welton designed 'A Blind
Guess' and 'Abby's Birthday'. Hans van Ditmarsch and Barteld Kooi authored the
puzzlebook 'One Hundred Prisoners and a Light Bulb' that contains other
knowledge puzzles, and that can also be found on the webpage
http://personal.us.es/hvd/lightbulb.html dedicated to the book.



We discuss memory models which are based on tensor decompositions using
latent representations of entities and events. We show how episodic memory and
semantic memory can be realized and discuss how new memory traces can be
generated from sensory input: Existing memories are the basis for perception
and new memories are generated via perception. We relate our mathematical
approach to the hippocampal memory indexing theory. We describe the first
detailed mathematical models for the complete processing pipeline from sensory
input and its semantic decoding, i.e., perception, to the formation of episodic
and semantic memories and their declarative semantic decodings. Our main
hypothesis is that perception includes an active semantic decoding process,
which relies on latent representations of entities and predicates, and that
episodic and semantic memories depend on the same decoding process. We
contribute to the debate between the leading memory consolidation theories,
i.e., the standard consolidation theory (SCT) and the multiple trace theory
(MTT). The latter is closely related to the complementary learning systems
(CLS) framework. In particular, we show explicitly how episodic memory can
teach the neocortex to form a semantic memory, which is a core issue in MTT and
CLS.



We address the problem of end-to-end visual storytelling. Given a photo
album, our model first selects the most representative (summary) photos, and
then composes a natural language story for the album. For this task, we make
use of the Visual Storytelling dataset and a model composed of three
hierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album
photos, select representative (summary) photos, and compose the story.
Automatic and human evaluations show our model achieves better performance on
selection, generation, and retrieval than baselines.



Intelligent conversational assistants, such as Apple's Siri, Microsoft's
Cortana, and Amazon's Echo, have quickly become a part of our digital life.
However, these assistants have major limitations, which prevents users from
conversing with them as they would with human dialog partners. This limits our
ability to observe how users really want to interact with the underlying
system. To address this problem, we developed a crowd-powered conversational
assistant, Chorus, and deployed it to see how users and workers would interact
together when mediated by the system. Chorus sophisticatedly converses with end
users over time by recruiting workers on demand, which in turn decide what
might be the best response for each user sentence. Up to the first month of our
deployment, 59 users have held conversations with Chorus during 320
conversational sessions. In this paper, we present an account of Chorus'
deployment, with a focus on four challenges: (i) identifying when conversations
are over, (ii) malicious users and workers, (iii) on-demand recruiting, and
(iv) settings in which consensus is not enough. Our observations could assist
the deployment of crowd-powered conversation systems and crowd-powered systems
in general.



The simulation of pedestrian crowd that reflects reality is a major challenge
for researches. Several crowd simulation models have been proposed such as
cellular automata model, agent-based model, fluid dynamic model, etc. It is
important to note that agent-based model is able, over others approaches, to
provide a natural description of the system and then to capture complex human
behaviors. In this paper, we propose a multi-agent simulation model in which
pedestrian positions are updated at discrete time intervals. It takes into
account the major normal conditions of a simple pedestrian situated in a crowd
such as preferences, realistic perception of environment, etc. Our objective is
to simulate the pedestrian crowd realistically towards a simulation of
believable pedestrian behaviors. Typical pedestrian phenomena, including the
unidirectional and bidirectional movement in a corridor as well as the flow
through bottleneck, are simulated. The conducted simulations show that our
model is able to produce realistic pedestrian behaviors. The obtained
fundamental diagram and flow rate at bottleneck agree very well with classic
conclusions and empirical study results. It is hoped that the idea of this
study may be helpful in promoting the modeling and simulation of pedestrian
crowd in a simple way.



t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely
used dimensionality reduction methods for data visualization, but it has a
perplexity hyperparameter that requires manual selection. In practice, proper
tuning of t-SNE perplexity requires users to understand the inner working of
the method as well as to have hands-on experience. We propose a model selection
objective for t-SNE perplexity that requires negligible extra computation
beyond that of the t-SNE itself. We empirically validate that the perplexity
settings found by our approach are consistent with preferences elicited from
human experts across a number of datasets. The similarities of our approach to
Bayesian information criteria (BIC) and minimum description length (MDL) are
also analyzed.



Data-driven techniques are used in cyber-physical systems (CPS) for
controlling autonomous vehicles, handling demand responses for energy
management, and modeling human physiology for medical devices. These
data-driven techniques extract models from training data, where their
performance is often analyzed with respect to random errors in the training
data. However, if the training data is maliciously altered by attackers, the
effect of these attacks on the learning algorithms underpinning data-driven CPS
have yet to be considered. In this paper, we analyze the resilience of
classification algorithms to training data attacks. Specifically, a generic
metric is proposed that is tailored to measure resilience of classification
algorithms with respect to worst-case tampering of the training data. Using the
metric, we show that traditional linear classification algorithms are resilient
under restricted conditions. To overcome these limitations, we propose a linear
classification algorithm with a majority constraint and prove that it is
strictly more resilient than the traditional algorithms. Evaluations on both
synthetic data and a real-world retrospective arrhythmia medical case-study
show that the traditional algorithms are vulnerable to tampered training data,
whereas the proposed algorithm is more resilient (as measured by worst-case
tampering).



Advances in remote sensing technologies have made it possible to use
high-resolution visual data for weather observation and forecasting tasks. We
propose the use of multi-layer neural networks for understanding complex
atmospheric dynamics based on multichannel satellite images. The capability of
our model was evaluated by using a linear regression task for single typhoon
coordinates prediction. A specific combination of models and different
activation policies enabled us to obtain an interesting prediction result in
the northeastern hemisphere (ENH).



Users try to articulate their complex information needs during search
sessions by reformulating their queries. To make this process more effective,
search engines provide related queries to help users in specifying the
information need in their search process. In this paper, we propose a
customized sequence-to-sequence model for session-based query suggestion. In
our model, we employ a query-aware attention mechanism to capture the structure
of the session context. is enables us to control the scope of the session from
which we infer the suggested next query, which helps not only handle the noisy
data but also automatically detect session boundaries. Furthermore, we observe
that, based on the user query reformulation behavior, within a single session a
large portion of query terms is retained from the previously submitted queries
and consists of mostly infrequent or unseen terms that are usually not included
in the vocabulary. We therefore empower the decoder of our model to access the
source words from the session context during decoding by incorporating a copy
mechanism. Moreover, we propose evaluation metrics to assess the quality of the
generative models for query suggestion. We conduct an extensive set of
experiments and analysis. e results suggest that our model outperforms the
baselines both in terms of the generating queries and scoring candidate queries
for the task of query suggestion.



The model-based control of building heating systems for energy saving
encounters severe physical, mathematical and calibration difficulties in the
numerous attempts that has been published until now. This topic is addressed
here via a new model-free control setting, where the need of any mathematical
description disappears. Several convincing computer simulations are presented.
Comparisons with classic PI controllers and flatness-based predictive control
are provided.



There exist two main approaches to automatically extract affective
orientation: lexicon-based and corpus-based. In this work, we argue that these
two methods are compatible and show that combining them can improve the
accuracy of emotion classifiers. In particular, we introduce a novel variant of
the Label Propagation algorithm that is tailored to distributed word
representations, we apply batch gradient descent to accelerate the optimization
of label propagation and to make the optimization feasible for large graphs,
and we propose a reproducible method for emotion lexicon expansion. We conclude
that label propagation can expand an emotion lexicon in a meaningful way and
that the expanded emotion lexicon can be leveraged to improve the accuracy of
an emotion classifier.



Over 150,000 new people in the United States are diagnosed with colorectal
cancer each year. Nearly a third die from it (American Cancer Society). The
only approved noninvasive diagnosis tools currently involve fecal blood count
tests (FOBTs) or stool DNA tests. Fecal blood count tests take only five
minutes and are available over the counter for as low as \$15. They are highly
specific, yet not nearly as sensitive, yielding a high percentage (25%) of
false negatives (Colon Cancer Alliance). Moreover, FOBT results are far too
generalized, meaning that a positive result could mean much more than just
colorectal cancer, and could just as easily mean hemorrhoids, anal fissure,
proctitis, Crohn's disease, diverticulosis, ulcerative colitis, rectal ulcer,
rectal prolapse, ischemic colitis, angiodysplasia, rectal trauma, proctitis
from radiation therapy, and others. Stool DNA tests, the modern benchmark for
CRC screening, have a much higher sensitivity and specificity, but also cost
\$600, take two weeks to process, and are not for high-risk individuals or
people with a history of polyps. To yield a cheap and effective CRC screening
alternative, a unique ensemble-based classification algorithm is put in place
that considers the FIT result, BMI, smoking history, and diabetic status of
patients. This method is tested under ten-fold cross validation to have a .95
AUC, 92% specificity, 89% sensitivity, .88 F1, and 90% precision. Once
clinically validated, this test promises to be cheaper, faster, and potentially
more accurate when compared to a stool DNA test.



From medicines to materials, small organic molecules are indispensable for
human well-being. To plan their syntheses, chemists employ a problem solving
technique called retrosynthesis. In retrosynthesis, target molecules are
recursively transformed into increasingly simpler precursor compounds until a
set of readily available starting materials is obtained. Computer-aided
retrosynthesis would be a highly valuable tool, however, past approaches were
slow and provided results of unsatisfactory quality. Here, we employ Monte
Carlo Tree Search (MCTS) to efficiently discover retrosynthetic routes. MCTS
was combined with an expansion policy network that guides the search, and an
"in-scope" filter network to pre-select the most promising retrosynthetic
steps. These deep neural networks were trained on 12 million reactions, which
represents essentially all reactions ever published in organic chemistry. Our
system solves almost twice as many molecules and is 30 times faster in
comparison to the traditional search method based on extracted rules and
hand-coded heuristics. Finally after a 60 year history of computer-aided
synthesis planning, chemists can no longer distinguish between routes generated
by a computer system and real routes taken from the scientific literature. We
anticipate that our method will accelerate drug and materials discovery by
assisting chemists to plan better syntheses faster, and by enabling fully
automated robot synthesis.



Robotic manipulation in complex open-world scenarios requires both reliable
physical manipulation skills and effective and generalizable perception. In
this paper, we propose a method where general purpose pretrained visual models
serve as an object-centric prior for the perception system of a learned policy.
We devise an object-level attentional mechanism that can be used to determine
relevant objects from a few trajectories or demonstrations, and then
immediately incorporate those objects into a learned policy. A task-independent
meta-attention locates possible objects in the scene, and a task-specific
attention identifies which objects are predictive of the trajectories. The
scope of the task-specific attention is easily adjusted by showing
demonstrations with distractor objects or with diverse relevant objects. Our
results indicate that this approach exhibits good generalization across object
instances using very few samples, and can be used to learn a variety of
manipulation tasks using reinforcement learning.



Learning representation for graph classification turns a variable-size graph
into a fixed-size vector (or matrix). Such a representation works nicely with
algebraic manipulations. Here we introduce a simple method to augment an
attributed graph with a virtual node that is bidirectionally connected to all
existing nodes. The virtual node represents the latent aspects of the graph,
which are not immediately available from the attributes and local connectivity
structures. The expanded graph is then put through any node representation
method. The representation of the virtual node is then the representation of
the entire graph. In this paper, we use the recently introduced Column Network
for the expanded graph, resulting in a new end-to-end graph classification
model dubbed Virtual Column Network (VCN). The model is validated on two tasks:
(i) predicting bio-activity of chemical compounds, and (ii) finding software
vulnerability from source code. Results demonstrate that VCN is competitive
against well-established rivals.



Disagreement-based approaches generate multiple classifiers and exploit the
disagreement among them with unlabeled data to improve learning performance.
Co-training is a representative paradigm of them, which trains two classifiers
separately on two sufficient and redundant views; while for the applications
where there is only one view, several successful variants of co-training with
two different classifiers on single-view data instead of two views have been
proposed. For these disagreement-based approaches, there are several important
issues which still are unsolved, in this article we present theoretical
analyses to address these issues, which provides a theoretical foundation of
co-training and disagreement-based approaches.



In this paper, we consider a novel machine learning problem, that is,
learning a classifier from noisy label distributions. In this problem, each
instance with a feature vector belongs to at least one group. Then, instead of
the true label of each instance, we observe the label distribution of the
instances associated with a group, where the label distribution is distorted by
an unknown noise. Our goals are to (1) estimate the true label of each
instance, and (2) learn a classifier that predicts the true label of a new
instance. We propose a probabilistic model that considers true label
distributions of groups and parameters that represent the noise as hidden
variables. The model can be learned based on a variational Bayesian method. In
numerical experiments, we show that the proposed model outperforms existing
methods in terms of the estimation of the true labels of instances.



As an inevitable trend of future 5G networks, Software Defined architecture
has many advantages in providing central- ized control and flexible resource
management. But it is also confronted with various security challenges and
potential threats with emerging services and technologies. As the focus of
network security, Intrusion Detection Systems (IDS) are usually deployed
separately without collaboration. They are also unable to detect novel attacks
with limited intelligent abilities, which are hard to meet the needs of
software defined 5G. In this paper, we propose an intelligent intrusion system
taking the advances of software defined technology and artificial intelligence
based on Software Defined 5G architecture. It flexibly combines security
function mod- ules which are adaptively invoked under centralized management
and control with a globle view. It can also deal with unknown intrusions by
using machine learning algorithms. Evaluation results prove that the
intelligent intrusion detection system achieves a better performance.



Debate summarization is one of the novel and challenging research areas in
automatic text summarization which has been largely unexplored. In this paper,
we develop a debate summarization pipeline to summarize key topics which are
discussed or argued in the two opposing sides of online debates. We view that
the generation of debate summaries can be achieved by clustering, cluster
labeling, and visualization. In our work, we investigate two different
clustering approaches for the generation of the summaries. In the first
approach, we generate the summaries by applying purely term-based clustering
and cluster labeling. The second approach makes use of X-means for clustering
and Mutual Information for labeling the clusters. Both approaches are driven by
ontologies. We visualize the results using bar charts. We think that our
results are a smooth entry for users aiming to receive the first impression
about what is discussed within a debate topic containing waste number of
argumentations.



Usage of online textual media is steadily increasing. Daily, more and more
news stories, blog posts and scientific articles are added to the online
volumes. These are all freely accessible and have been employed extensively in
multiple research areas, e.g. automatic text summarization, information
retrieval, information extraction, etc. Meanwhile, online debate forums have
recently become popular, but have remained largely unexplored. For this reason,
there are no sufficient resources of annotated debate data available for
conducting research in this genre. In this paper, we collected and annotated
debate data for an automatic summarization task. Similar to extractive gold
standard summary generation our data contains sentences worthy to include into
a summary. Five human annotators performed this task. Inter-annotator
agreement, based on semantic similarity, is 36% for Cohen's kappa and 48% for
Krippendorff's alpha. Moreover, we also implement an extractive summarization
system for online debates and discuss prominent features for the task of
summarizing online debate data automatically.



Previous research on automatic pain estimation from facial expressions has
focused primarily on "one-size-fits-all" metrics (such as PSPI). In this work,
we focus on directly estimating each individual's self-reported visual-analog
scale (VAS) pain metric, as this is considered the gold standard for pain
measurement. The VAS pain score is highly subjective and context-dependent, and
its range can vary significantly among different persons. To tackle these
issues, we propose a novel two-stage personalized model, named DeepFaceLIFT,
for automatic estimation of VAS. This model is based on (1) Neural Network and
(2) Gaussian process regression models, and is used to personalize the
estimation of self-reported pain via a set of hand-crafted personal features
and multi-task learning. We show on the benchmark dataset for pain analysis
(The UNBC-McMaster Shoulder Pain Expression Archive) that the proposed
personalized model largely outperforms the traditional, unpersonalized models:
the intra-class correlation improves from a baseline performance of 19\% to a
personalized performance of 35\% while also providing confidence in the
model\textquotesingle s estimates -- in contrast to existing models for the
target task. Additionally, DeepFaceLIFT automatically discovers the
pain-relevant facial regions for each person, allowing for an easy
interpretation of the pain-related facial cues.



Training model to generate data has increasingly attracted research attention
and become important in modern world applications. We propose in this paper a
new geometry-based optimization approach to address this problem. Orthogonal to
current state-of-the-art density-based approaches, most notably VAE and GAN, we
present a fresh new idea that borrows the principle of minimal enclosing ball
to train a generator G\left(\bz\right) in such a way that both training and
generated data, after being mapped to the feature space, are enclosed in the
same sphere. We develop theory to guarantee that the mapping is bijective so
that its inverse from feature space to data space results in expressive
nonlinear contours to describe the data manifold, hence ensuring data generated
are also lying on the data manifold learned from training data. Our model
enjoys a nice geometric interpretation, hence termed Geometric Enclosing
Networks (GEN), and possesses some key advantages over its rivals, namely
simple and easy-to-control optimization formulation, avoidance of mode
collapsing and efficiently learn data manifold representation in a completely
unsupervised manner. We conducted extensive experiments on synthesis and
real-world datasets to illustrate the behaviors, strength and weakness of our
proposed GEN, in particular its ability to handle multi-modal data and quality
of generated data.



Missing data and noisy observations pose significant challenges for reliably
predicting events from irregularly sampled multivariate time series
(longitudinal) data. Imputation methods, which are typically used for
completing the data prior to event prediction, lack a principled mechanism to
account for the uncertainty due to missingness. Alternatively, state-of-the-art
joint modeling techniques can be used for jointly modeling the longitudinal and
event data and compute event probabilities conditioned on the longitudinal
observations. These approaches, however, make strong parametric assumptions and
do not easily scale to multivariate signals with many observations. Our
proposed approach consists of several key innovations. First, we develop a
flexible and scalable joint model based upon sparse multiple-output Gaussian
processes. Unlike state-of-the-art joint models, the proposed model can explain
highly challenging structure including non-Gaussian noise while scaling to
large data. Second, we derive an optimal policy for predicting events using the
distribution of the event occurrence estimated by the joint model. The derived
policy trades-off the cost of a delayed detection versus incorrect assessments
and abstains from making decisions when the estimated event probability does
not satisfy the derived confidence criteria. Experiments on a large dataset
show that the proposed framework significantly outperforms state-of-the-art
techniques in event prediction.



Stochastic gradient descent (SGD) is a popular stochastic optimization method
in machine learning. Traditional parallel SGD algorithms, e.g., SimuParallel
SGD, often require all nodes to have the same performance or to consume equal
quantities of data. However, these requirements are difficult to satisfy when
the parallel SGD algorithms run in a heterogeneous computing environment;
low-performance nodes will exert a negative influence on the final result. In
this paper, we propose an algorithm called weighted parallel SGD (WP-SGD).
WP-SGD combines weighted model parameters from different nodes in the system to
produce the final output. WP-SGD makes use of the reduction in standard
deviation to compensate for the loss from the inconsistency in performance of
nodes in the cluster, which means that WP-SGD does not require that all nodes
consume equal quantities of data. We also analyze the theoretical feasibility
of running two other parallel SGD algorithms combined with WP-SGD in a
heterogeneous environment. The experimental results show that WP-SGD
significantly outperforms the traditional parallel SGD algorithms on
distributed training systems with an unbalanced workload.



Entity alignment is the task of finding entities in two knowledge bases (KBs)
that represent the same real-world object. When facing KBs in different natural
languages, conventional cross-lingual entity alignment methods rely on machine
translation to eliminate the language barriers. These approaches often suffer
from the uneven quality of translations between languages. While recent
embedding-based techniques encode entities and relationships in KBs and do not
need machine translation for cross-lingual entity alignment, a significant
number of attributes remain largely unexplored. In this paper, we propose a
joint attribute-preserving embedding model for cross-lingual entity alignment.
It jointly embeds the structures of two KBs into a unified vector space and
further refines it by leveraging attribute correlations in the KBs. Our
experimental results on real-world datasets show that this approach
significantly outperforms the state-of-the-art embedding approaches for
cross-lingual entity alignment and could be complemented with methods based on
machine translation.



This paper introduces a causation coefficient which is defined in terms of
probabilistic causal models. This coefficient is suggested as the natural
causal analogue of the Pearson correlation coefficient and permits comparing
causation and correlation to each other in a simple, yet rigorous manner.
Together, these coefficients provide a natural way to classify the possible
correlation/causation relationships that can occur in practice and examples of
each relationship are provided. In addition, the typical relationship between
correlation and causation is analyzed to provide insight into why correlation
and causation are often conflated. Finally, example calculations of the
causation coefficient are shown on a real data set.



Support vector data description (SVDD) is a popular technique for detecting
anomalies. The SVDD classifier partitions the whole space into an inlier
region, which consists of the region near the training data, and an outlier
region, which consists of points away from the training data. The computation
of the SVDD classifier requires a kernel function, and the Gaussian kernel is a
common choice for the kernel function. The Gaussian kernel has a bandwidth
parameter, whose value is important for good results. A small bandwidth leads
to overfitting, and the resulting SVDD classifier overestimates the number of
anomalies. A large bandwidth leads to underfitting, and the classifier fails to
detect many anomalies. In this paper we present a new automatic, unsupervised
method for selecting the Gaussian kernel bandwidth. The selected value can be
computed quickly, and it is competitive with existing bandwidth selection
methods.



As AI continues to advance, human-AI teams are inevitable. However, progress
in AI is routinely measured in isolation, without a human in the loop. It is
crucial to benchmark progress in AI, not just in isolation, but also in terms
of how it translates to helping humans perform certain tasks, i.e., the
performance of human-AI teams.
  In this work, we design a cooperative game - GuessWhich - to measure human-AI
team performance in the specific context of the AI being a visual
conversational agent. GuessWhich involves live interaction between the human
and the AI. The AI, which we call ALICE, is provided an image which is unseen
by the human. Following a brief description of the image, the human questions
ALICE about this secret image to identify it from a fixed pool of images.
  We measure performance of the human-ALICE team by the number of guesses it
takes the human to correctly identify the secret image after a fixed number of
dialog rounds with ALICE. We compare performance of the human-ALICE teams for
two versions of ALICE. Our human studies suggest a counterintuitive trend -
that while AI literature shows that one version outperforms the other when
paired with an AI questioner bot, we find that this improvement in AI-AI
performance does not translate to improved human-AI performance. This suggests
a mismatch between benchmarking of AI in isolation and in the context of
human-AI teams.



Music is usually highly structured and it is still an open question how to
design models which can successfully learn to recognize and represent musical
structure. A fundamental problem is that structurally related patterns can have
very distinct appearances, because the structural relationships are often based
on transformations of musical material, like chromatic or diatonic
transposition, inversion, retrograde, or rhythm change. In this preliminary
work, we study the potential of two unsupervised learning techniques -
Restricted Boltzmann Machines (RBMs) and Gated Autoencoders (GAEs) - to capture
pre-defined transformations from constructed data pairs. We evaluate the models
by using the learned representations as inputs in a discriminative task where
for a given type of transformation (e.g. diatonic transposition), the specific
relation between two musical patterns must be recognized (e.g. an upward
transposition of diatonic steps). Furthermore, we measure the reconstruction
error of models when reconstructing musical transformed patterns. Lastly, we
test the models in an analogy-making task. We find that it is difficult to
learn musical transformations with the RBM and that the GAE is much more
adequate for this task, since it is able to learn representations of specific
transformations that are largely content-invariant. We believe these results
show that models such as GAEs may provide the basis for more encompassing music
analysis systems, by endowing them with a better understanding of the
structures underlying music.



We present LADDER, the first deep reinforcement learning agent that can
successfully learn control policies for large-scale real-world problems
directly from raw inputs composed of high-level semantic information. The agent
is based on an asynchronous stochastic variant of DQN (Deep Q Network) named
DASQN. The inputs of the agent are plain-text descriptions of states of a game
of incomplete information, i.e. real-time large scale online auctions, and the
rewards are auction profits of very large scale. We apply the agent to an
essential portion of JD's online RTB (real-time bidding) advertising business
and find that it easily beats the former state-of-the-art bidding policy that
had been carefully engineered and calibrated by human experts: during JD.com's
June 18th anniversary sale, the agent increased the company's ads revenue from
the portion by more than 50%, while the advertisers' ROI (return on investment)
also improved significantly.



Transfer learning borrows knowledge from a source domain to facilitate
learning in a target domain. Two primary issues to be addressed in transfer
learning are what and how to transfer. For a pair of domains, adopting
different transfer learning algorithms results in different knowledge
transferred between them. To discover the optimal transfer learning algorithm
that maximally improves the learning performance in the target domain,
researchers have to exhaustively explore all existing transfer learning
algorithms, which is computationally intractable. As a trade-off, a sub-optimal
algorithm is selected, which requires considerable expertise in an ad-hoc way.
Meanwhile, it is widely accepted in educational psychology that human beings
improve transfer learning skills of deciding what to transfer through
meta-cognitive reflection on inductive transfer learning practices. Motivated
by this, we propose a novel transfer learning framework known as Learning to
Transfer (L2T) to automatically determine what and how to transfer are the best
by leveraging previous transfer learning experiences. We establish the L2T
framework in two stages: 1) we first learn a reflection function encrypting
transfer learning skills from experiences; and 2) we infer what and how to
transfer for a newly arrived pair of domains by optimizing the reflection
function. Extensive experiments demonstrate the L2T's superiority over several
state-of-the-art transfer learning algorithms and its effectiveness on
discovering more transferable knowledge.



Long Short-Term Memory (LSTM) is the primary recurrent neural networks
architecture for acoustic modeling in automatic speech recognition systems.
Residual learning is an efficient method to help neural networks converge
easier and faster. In this paper, we propose several types of residual LSTM
methods for our acoustic modeling. Our experiments indicate that, compared with
classic LSTM, our architecture shows more than 8% relative reduction in Phone
Error Rate (PER) on TIMIT tasks. At the same time, our residual fast LSTM
approach shows 4% relative reduction in PER on the same task. Besides, we find
that all this architecture could have good results on THCHS-30, Librispeech and
Switchboard corpora.



This article constructs a Turing Machine which can solve for $\beta^{'}$
which is RE-complete. Such a machine is only possible if there is something
wrong with the foundations of computer science and mathematics. We therefore
check our work by looking very closely at Cantor's diagonalization and
construct a novel formal language as an Abelian group which allows us, through
equivalence relations, to provide a non-trivial counterexample to Cantor's
argument. As if that wasn't enough, we then discover that the impredicative
nature of G\"odel's diagonalization lemma leads to logical tautology,
invalidating any meaning behind the method, leaving no doubt that
diagonalization is flawed. Our discovery in regards to these foundational
arguments opens the door to solving the P vs NP problem.



Inter-connected objects, either via public or private networks are the near
future of modern societies. Such inter-connected objects are referred to as
Internet-of-Things (IoT) and/or Cyber-Physical Systems (CPS). One example of
such a system is based on Unmanned Aerial Vehicles (UAVs). The fleet of such
vehicles are prophesied to take on multiple roles involving mundane to
high-sensitive, such as, prompt pizza or shopping deliveries to your homes to
battlefield deployment for reconnaissance and combat missions. Drones, as we
refer to UAVs in this paper, either can operate individually (solo missions) or
part of a fleet (group missions), with and without constant connection with the
base station. The base station acts as the command centre to manage the
activities of the drones. However, an independent, localised and effective
fleet control is required, potentially based on swarm intelligence, for the
reasons: 1) increase in the number of drone fleets, 2) number of drones in a
fleet might be multiple of tens, 3) time-criticality in making decisions by
such fleets in the wild, 4) potential communication congestions/lag, and 5) in
some cases working in challenging terrains that hinders or mandates-limited
communication with control centre (i.e., operations spanning long period of
times or military usage of such fleets in enemy territory). This self-ware,
mission-focused and independent fleet of drones that potential utilises swarm
intelligence for a) air-traffic and/or flight control management, b) obstacle
avoidance, c) self-preservation while maintaining the mission criteria, d)
collaboration with other fleets in the wild (autonomously) and e) assuring the
security, privacy and safety of physical (drones itself) and virtual (data,
software) assets. In this paper, we investigate the challenges faced by fleet
of drones and propose a potential course of action on how to overcome them.



Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.



The Recurrent Chinese Restaurant Process (RCRP) is a powerful statistical
method for modeling evolving clusters in large scale social media data. With
the RCRP, one can allow both the number of clusters and the cluster parameters
in a model to change over time. However, application of the RCRP has largely
been limited due to the non-conjugacy between the cluster evolutionary priors
and the Multinomial likelihood. This non-conjugacy makes inference di cult and
restricts the scalability of models which use the RCRP, leading to the RCRP
being applied only in simple problems, such as those that can be approximated
by a single Gaussian emission. In this paper, we provide a novel solution for
the non-conjugacy issues for the RCRP and an example of how to leverage our
solution for one speci c problem - the social event discovery problem. By
utilizing Sequential Monte Carlo methods in inference, our approach can be
massively paralleled and is highly scalable, to the extent it can work on tens
of millions of documents. We are able to generate high quality topical and
location distributions of the clusters that can be directly interpreted as real
social events, and our experimental results suggest that the approaches
proposed achieve much better predictive performance than techniques reported in
prior work. We also demonstrate how the techniques we develop can be used in a
much more general ways toward similar problems.



The increasing availability of affect-rich multimedia resources has bolstered
interest in understanding sentiment and emotions in and from visual content.
Adjective-noun pairs (ANP) are a popular mid-level semantic construct for
capturing affect via visually detectable concepts such as "cute dog" or
"beautiful landscape". Current state-of-the-art methods approach ANP prediction
by considering each of these compound concepts as individual tokens, ignoring
the underlying relationships in ANPs. This work aims at disentangling the
contributions of the `adjectives' and `nouns' in the visual prediction of ANPs.
Two specialised classifiers, one trained for detecting adjectives and another
for nouns, are fused to predict 553 different ANPs. The resulting ANP
prediction model is more interpretable as it allows us to study contributions
of the adjective and noun components. Source code and models are available at
https://imatge-upc.github.io/affective-2017-musa2/ .



Efficient Monte Carlo inference often requires manual construction of
model-specific proposals. We propose an approach to automated proposal
construction by training neural networks to provide fast approximations to
block Gibbs conditionals. The learned proposals generalize to occurrences of
common structural motifs both within a given model and across models, allowing
for the construction of a library of learned inference primitives that can
accelerate inference on unseen models with no model-specific training required.
We explore several applications including open-universe Gaussian mixture
models, in which our learned proposals outperform a hand-tuned sampler, and a
real-world named entity recognition task, in which our sampler's ability to
escape local modes yields higher final F1 scores than single-site Gibbs.



Based on a natural connection between ResNet and transport equation or its
characteristic equation, we propose a continuous flow model for both ResNet and
plain net. Through this continuous model, a ResNet can be explicitly
constructed as a refinement of a plain net. The flow model provides an
alternative perspective to understand phenomena in deep neural networks, such
as why it is necessary and sufficient to use 2-layer blocks in ResNets, why
deeper is better, and why ResNets are even deeper, and so on. It also opens a
gate to bring in more tools from the huge area of differential equations.



The nursing literature shows that cultural competence is an important
requirement for effective healthcare. We claim that personal assistive robots
should likewise be culturally competent, that is, they should be aware of
general cultural characteristics and of the different forms they take in
different individuals, and take these into account while perceiving, reasoning,
and acting. The CARESSES project is an Europe-Japan collaborative effort that
aims at designing, developing and evaluating culturally competent assistive
robots. These robots will be able to adapt the way they behave, speak and
interact to the cultural identity of the person they assist. This paper
describes the approach taken in the CARESSES project, its initial steps, and
its future plans.



In recent years, car makers and tech companies have been racing towards self
driving cars. It seems that the main parameter in this race is who will have
the first car on the road. The goal of this paper is to add to the equation two
additional crucial parameters. The first is standardization of safety assurance
--- what are the minimal requirements that every self-driving car must satisfy,
and how can we verify these requirements. The second parameter is scalability
--- engineering solutions that lead to unleashed costs will not scale to
millions of cars, which will push interest in this field into a niche academic
corner, and drive the entire field into a "winter of autonomous driving". In
the first part of the paper we propose a white-box, interpretable, mathematical
model for safety assurance, which we call Responsibility-Sensitive Safety
(RSS). In the second part we describe a design of a system that adheres to our
safety assurance requirements and is scalable to millions of cars.



SafePredict is a novel meta-algorithm that works with any base prediction
algorithm for online data to guarantee an arbitrarily chosen correctness rate,
$1-\epsilon$, by allowing refusals. Allowing refusals means that the
meta-algorithm may refuse to emit a prediction produced by the base algorithm
on occasion so that the error rate on non-refused predictions does not exceed
$\epsilon$. The SafePredict error bound does not rely on any assumptions on the
data distribution or the base predictor. When the base predictor happens not to
exceed the target error rate $\epsilon$, SafePredict refuses only a finite
number of times. When the error rate of the base predictor changes through time
SafePredict makes use of a weight-shifting heuristic that adapts to these
changes without knowing when the changes occur yet still maintains the
correctness guarantee. Empirical results show that (i) SafePredict compares
favorably with state-of-the art confidence based refusal mechanisms which fail
to offer robust error guarantees; and (ii) combining SafePredict with such
refusal mechanisms can in many cases further reduce the number of refusals. Our
software (currently in Python) is included in the supplementary material.



The deployment of deep convolutional neural networks (CNNs) in many real
world applications is largely hindered by their high computational cost. In
this paper, we propose a novel learning scheme for CNNs to simultaneously 1)
reduce the model size; 2) decrease the run-time memory footprint; and 3) lower
the number of computing operations, without compromising accuracy. This is
achieved by enforcing channel-level sparsity in the network in a simple but
effective way. Different from many existing approaches, the proposed method
directly applies to modern CNN architectures, introduces minimum overhead to
the training process, and requires no special software/hardware accelerators
for the resulting models. We call our approach network slimming, which takes
wide and large networks as input models, but during training insignificant
channels are automatically identified and pruned afterwards, yielding thin and
compact models with comparable accuracy. We empirically demonstrate the
effectiveness of our approach with several state-of-the-art CNN models,
including VGGNet, ResNet and DenseNet, on various image classification
datasets. For VGGNet, a multi-pass version of network slimming gives a 20x
reduction in model size and a 5x reduction in computing operations.



Actual causation is concerned with the question "what caused what?". Consider
a transition between two subsequent observations within a system of elements.
Even under perfect knowledge of the system, a straightforward answer to this
question may not be available. Counterfactual accounts of actual causation
based on graphical models, paired with system interventions, have demonstrated
initial success in addressing specific problem cases. We present a formal
account of actual causation, applicable to discrete dynamical systems of
interacting elements, that considers all counterfactual states of a state
transition from t-1 to t. Within such a transition, causal links are considered
from two complementary points of view: we can ask if any occurrence at time t
has an actual cause at t-1, but also if any occurrence at time t-1 has an
actual effect at t. We address the problem of identifying such actual causes
and actual effects in a principled manner by starting from a set of basic
requirements for causation (existence, composition, information, integration,
and exclusion). We present a formal framework to implement these requirements
based on system manipulations and partitions. This framework is used to provide
a complete causal account of the transition by identifying and quantifying the
strength of all actual causes and effects linking two occurrences. Finally, we
examine several exemplary cases and paradoxes of causation and show that they
can be illuminated by the proposed framework for quantifying actual causation.



Human action recognition involves the characterization of human actions
through the automated analysis of video data and is integral in the development
of smart computer vision systems. However, several challenges like dynamic
backgrounds, camera stabilization, complex actions, occlusions etc. make action
recognition in a real time and robust fashion difficult. Several complex
approaches exist but are computationally intensive. This paper presents a novel
approach of using a combination of good features along with iterative optical
flow algorithm to compute feature vectors which are classified using a
multilayer perceptron (MLP) network. The use of multiple features for motion
descriptors enhances the quality of tracking. Resilient backpropagation
algorithm is used for training the feedforward neural network reducing the
learning time. The overall system accuracy is improved by optimizing the
various parameters of the multilayer perceptron network.



The electronic health record (EHR) contains a large amount of
multi-dimensional and unstructured clinical data of significant operational and
research value. Distinguished from previous studies, our approach embraces a
double-annotated dataset and strays away from obscure "black-box" models to
comprehensive deep learning models. In this paper, we present a novel neural
attention mechanism that not only classifies clinically important findings.
Specifically, convolutional neural networks (CNN) with attention analysis are
used to classify radiology head computed tomography reports based on five
categories that radiologists would account for in assessing acute and
communicable findings in daily practice. The experiments show that our CNN
attention models outperform non-neural models, especially when trained on a
larger dataset. Our attention analysis demonstrates the intuition behind the
classifier's decision by generating a heatmap that highlights attended terms
used by the CNN model; this is valuable when potential downstream medical
decisions are to be performed by human experts or the classifier information is
to be used in cohort construction such as for epidemiological studies.



The Koopman operator has recently garnered much attention for its value in
dynamical systems analysis and data-driven model discovery. However, its
application has been hindered by the computational complexity of extended
dynamic mode decomposition; this requires a combinatorially large basis set to
adequately describe many nonlinear systems of interest, e.g. cyber-physical
infrastructure systems, biological networks, social systems, and fluid
dynamics. Often the dictionaries generated for these problems are manually
curated, requiring domain-specific knowledge and painstaking tuning. In this
paper we introduce a deep learning framework for learning Koopman operators of
nonlinear dynamical systems. We show that this novel method automatically
selects efficient deep dictionaries, outperforming state-of-the-art methods. We
benchmark this method on partially observed nonlinear systems, including the
glycolytic oscillator and show it is able to predict quantitatively 100 steps
into the future, using only a single timepoint, and qualitative oscillatory
behavior 400 steps into the future.



Would it be possible to explain the emergence of new computational ideas
using the computation itself? Would it be feasible to describe the discovery
process of new algorithmic solutions using only mathematics? This study is the
first effort to analyze the nature of such inquiry from the viewpoint of effort
to find a new algorithmic solution to a given problem. We define program
reachability as a probability function whose argument is a form of the
energetic cost (algorithmic entropy) of the problem.



This paper addresses the task of learning an image clas-sifier when some
categories are defined by semantic descriptions only (e.g. visual attributes)
while the others are defined by exemplar images as well. This task is often
referred to as the Zero-Shot classification task (ZSC). Most of the previous
methods rely on learning a common embedding space allowing to compare visual
features of unknown categories with semantic descriptions. This paper argues
that these approaches are limited as i) efficient discrimi-native classifiers
can't be used ii) classification tasks with seen and unseen categories
(Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently.
In contrast , this paper suggests to address ZSC and GZSC by i) learning a
conditional generator using seen classes ii) generate artificial training
examples for the categories without exemplars. ZSC is then turned into a
standard supervised learning problem. Experiments with 4 generative models and
5 datasets experimentally validate the approach, giving state-of-the-art
results on both ZSC and GZSC.



The technology in the area of automated vehicles is gaining speed and
promises many advantages. However, with the recent introduction of
conditionally automated driving, we have also seen accidents. Test protocols
for both, conditionally automated (e.g., on highways) and automated vehicles do
not exist yet and leave researchers and practitioners with different
challenges. For instance, current test procedures do not suffice for fully
automated vehicles, which are supposed to be completely in charge for the
driving task and have no driver as a back up. This paper presents current
challenges of testing the functionality and safety of automated vehicles
derived from conducting focus groups and interviews with 26 participants from
five countries having a background related to testing automotive safety-related
topics.We provide an overview of the state-of-practice of testing active safety
features as well as challenges that needs to be addressed in the future to
ensure safety for automated vehicles. The major challenges identified through
the interviews and focus groups, enriched by literature on this topic are
related to 1) virtual testing and simulation, 2) safety, reliability, and
quality, 3) sensors and sensor models, 4) required scenario complexity and
amount of test cases, and 5) handover of responsibility between the driver and
the vehicle.



Automatically evaluating the quality of dialogue responses for unstructured
domains is a challenging problem. Unfortunately, existing automatic evaluation
metrics are biased and correlate very poorly with human judgements of response
quality. Yet having an accurate automatic evaluation procedure is crucial for
dialogue research, as it allows rapid prototyping and testing of new models
with fewer expensive human evaluations. In response to this challenge, we
formulate automatic dialogue evaluation as a learning problem. We present an
evaluation model (ADEM) that learns to predict human-like scores to input
responses, using a new dataset of human response scores. We show that the ADEM
model's predictions correlate significantly, and at a level much higher than
word-overlap metrics such as BLEU, with human judgements at both the utterance
and system-level. We also show that ADEM can generalize to evaluating dialogue
models unseen during training, an important step for automatic dialogue
evaluation.



This paper provides a theoretical justification of the superior
classification performance of deep rectifier networks over shallow rectifier
networks from the geometrical perspective of piecewise linear (PWL) classifier
boundaries. We show that, for a given threshold on the approximation error, the
required number of boundary facets to approximate a general smooth boundary
grows exponentially with the dimension of the data, and thus the number of
boundary facets, referred to as boundary resolution, of a PWL classifier is an
important quality measure that can be used to estimate a lower bound on the
classification errors. However, learning naively an exponentially large number
of boundary facets requires the determination of an exponentially large number
of parameters and also requires an exponentially large number of training
patterns. To overcome this issue of "curse of dimensionality", compressive
representations of high resolution classifier boundaries are required. To show
the superior compressive power of deep rectifier networks over shallow
rectifier networks, we prove that the maximum boundary resolution of a single
hidden layer rectifier network classifier grows exponentially with the number
of units when this number is smaller than the dimension of the patterns. When
the number of units is larger than the dimension of the patterns, the growth
rate is reduced to a polynomial order. Consequently, the capacity of generating
a high resolution boundary will increase if the same large number of units are
arranged in multiple layers instead of a single hidden layer. Taking high
dimensional spherical boundaries as examples, we show how deep rectifier
networks can utilize geometric symmetries to approximate a boundary with the
same accuracy but with a significantly fewer number of parameters than single
hidden layer nets.



This paper focuses on the problem of learning 6-DOF grasping with a parallel
jaw gripper in simulation. We propose the notion of a geometry-aware
representation in grasping based on the assumption that knowledge of 3D
geometry is at the heart of interaction. Our key idea is constraining and
regularizing grasping interaction learning through 3D geometry prediction.
Specifically, we formulate the learning of deep geometry-aware grasping model
in two steps: First, we learn to build mental geometry-aware representation by
reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via
generative 3D shape modeling. Second, we learn to predict grasping outcome with
its internal geometry-aware representation. The learned outcome prediction
model is used to sequentially propose grasping solutions via
analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best
of our knowledge, we are presenting for the first time a method to learn a
6-DOF grasping net from RGBD input; (2) We build a grasping dataset from
demonstrations in virtual reality with rich sensory and interaction
annotations. This dataset includes 101 everyday objects spread across 7
categories, additionally, we propose a data augmentation strategy for effective
learning; (3) We demonstrate that the learned geometry-aware representation
leads to about 10 percent relative performance improvement over the baseline
CNN on grasping objects from our dataset. (4) We further demonstrate that the
model generalizes to novel viewpoints and object instances.



We look at the unbiased Maker-Breaker Hamiltonicity game played on the edge
set of a complete graph $K_n$, where Maker's goal is to claim a Hamiltonian
cycle. First, we prove that, independent of who starts, Maker can win the game
for $n = 8$ and $n = 9$. Then we use an inductive argument to show that,
independent of who starts, Maker can win the game if and only if $n \geq 8$.
This, in particular, resolves in the affirmative the long-standing conjecture
of Papaioannou.
  We also study two standard positional games related to Hamiltonicity game.
For Hamiltonian Path game, we show that Maker can claim a Hamiltonian path if
and only if $n \geq 5$, independent of who starts. Next, we look at Fixed
Hamiltonian Path game, where the goal of Maker is to claim a Hamiltonian path
between two predetermined vertices. We prove that if Maker starts the game, he
wins if and only if $n \geq 7$, and if Breaker starts, Maker wins if and only
if $n \geq 8$. Using this result, we are able to improve the previously best
upper bound on the smallest number of edges a graph on $n$ vertices can have,
knowing that Maker can win the Maker-Breaker Hamiltonicity game played on its
edges.
  To resolve the outcomes of the mentioned games on small (finite) boards, we
devise algorithms for efficiently searching game trees and then obtain our
results with the help of a computer.



We investigate task clustering for deep-learning based multi-task and
few-shot learning in a many-task setting. We propose a new method to measure
task similarities with cross-task transfer performance matrix for the deep
learning scenario. Although this matrix provides us critical information
regarding similarity between tasks, its asymmetric property and unreliable
performance scores can affect conventional clustering methods adversely.
Additionally, the uncertain task-pairs, i.e., the ones with extremely
asymmetric transfer scores, may collectively mislead clustering algorithms to
output an inaccurate task-partition. To overcome these limitations, we propose
a novel task-clustering algorithm by using the matrix completion technique. The
proposed algorithm constructs a partially-observed similarity matrix based on
the certainty of cluster membership of the task-pairs. We then use a matrix
completion algorithm to complete the similarity matrix. Our theoretical
analysis shows that under mild constraints, the proposed algorithm will
perfectly recover the underlying "true" similarity matrix with a high
probability. Our results show that the new task clustering method can discover
task clusters for training flexible and superior neural network models in a
multi-task learning setup for sentiment classification and dialog intent
classification tasks. Our task clustering approach also extends metric-based
few-shot learning methods to adapt multiple metrics, which demonstrates
empirical advantages when the tasks are diverse.



In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the
complete 3D structure of a given object from a single arbitrary depth view
using generative adversarial networks. Unlike the existing work which typically
requires multiple views of the same object or class labels to recover the full
3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of
a depth view of the object as input, and is able to generate the complete 3D
occupancy grid by filling in the occluded/missing regions. The key idea is to
combine the generative capabilities of autoencoders and the conditional
Generative Adversarial Networks (GAN) framework, to infer accurate and
fine-grained 3D structures of objects in high-dimensional voxel space.
Extensive experiments on large synthetic datasets show that the proposed
3D-RecGAN significantly outperforms the state of the art in single view 3D
object reconstruction, and is able to reconstruct unseen types of objects. Our
code and data are available at: https://github.com/Yang7879/3D-RecGAN.



We propose some algorithms to find local minima in nonconvex optimization and
to obtain global minima in some degree from the Newton Second Law without
friction. With the key observation of the velocity observable and controllable
in the motion, the algorithms simulate the Newton Second Law without friction
based on symplectic Euler scheme. From the intuitive analysis of analytical
solution, we give a theoretical analysis for the high-speed convergence in the
algorithm proposed. Finally, we propose the experiments for strongly convex
function, non-strongly convex function and nonconvex function in
high-dimension.



Traditional tools for configuring cloud services can run much slower than the
workflows they are trying to optimize. For example, in the case studies
reported here, we find cases where (using traditional methods) it takes hours
to find ways to make a workflow terminate in tens of seconds. Such slow
optimizers are a poor choice of tools for reacting to changing operational
environmental conditions. Hence, they are unsuited for cloud services that
support rapidly changing workflows, e.g., scientific workflows or workflows
from the media or telecommunication industries.
  To solve this problem, this paper presents RIOT (Randomized Instance Order
Types), a new configuration tool. RIOT has a very low optimization overhead--
often, less than 10\% of the system runtime, especially for every complex
workflow. Instead of simulating many configurations, RIOT uses a novel
surrogate sampling method to quickly find promising solutions. As shown by this
paper, RIOT achieves comparable results to the other approaches but does so in
a fraction of the time.



We review our current software tools and theoretical methods for applying the
Neural Engineering Framework to state-of-the-art neuromorphic hardware. These
methods can be used to implement linear and nonlinear dynamical systems that
exploit axonal transmission time-delays, and to fully account for nonideal
mixed-analog-digital synapses that exhibit higher-order dynamics with
heterogeneous time-constants. This summarizes earlier versions of these methods
that have been discussed in a more biological context (Voelker & Eliasmith,
2017) or regarding a specific neuromorphic architecture (Voelker et al., 2017).



Generating molecules with desired chemical properties is important for drug
discovery. The use of generative neural networks is promising for this task.
However, from visual inspection, it often appears that generated samples lack
diversity. In this paper, we quantify this internal chemical diversity, and we
raise the following challenge: can a nontrivial AI model reproduce natural
chemical diversity for desired molecules? To illustrate this question, we
consider two generative models: a Reinforcement Learning model and the recently
introduced ORGAN. Both fail at this challenge. We hope this challenge will
stimulate research in this direction.



We address the problem of generating query suggestions to support users in
completing their underlying tasks (which motivated them to search in the first
place). Given an initial query, these query suggestions should provide a
coverage of possible subtasks the user might be looking for. We propose a
probabilistic modeling framework that obtains keyphrases from multiple sources
and generates query suggestions from these keyphrases. Using the test suites of
the TREC Tasks track, we evaluate and analyze each component of our model.



Today, the practice of returning entities from a knowledge base in response
to search queries has become widespread. One of the distinctive characteristics
of entities is that they are typed, i.e., assigned to some hierarchically
organized type system (type taxonomy). The primary objective of this paper is
to gain a better understanding of how entity type information can be utilized
in entity retrieval. We perform this investigation in an idealized "oracle"
setting, assuming that we know the distribution of target types of the relevant
entities for a given query. We perform a thorough analysis of three main
aspects: (i) the choice of type taxonomy, (ii) the representation of
hierarchical type information, and (iii) the combination of type-based and
term-based similarity in the retrieval model. Using a standard entity search
test collection based on DBpedia, we find that type information proves most
useful when using large type taxonomies that provide very specific types. We
provide further insights on the extensional coverage of entities and on the
utility of target types.



With the availability of large databases and recent improvements in deep
learning methodology, the performance of AI systems is reaching or even
exceeding the human level on an increasing number of complex tasks. Impressive
examples of this development can be found in domains such as image
classification, sentiment analysis, speech understanding or strategic game
playing. However, because of their nested non-linear structure, these highly
successful machine learning and artificial intelligence models are usually
applied in a black box manner, i.e., no information is provided about what
exactly makes them arrive at their predictions. Since this lack of transparency
can be a major drawback, e.g., in medical applications, the development of
methods for visualizing, explaining and interpreting deep learning models has
recently attracted increasing attention. This paper summarizes recent
developments in this field and makes a plea for more interpretability in
artificial intelligence. Furthermore, it presents two approaches to explaining
predictions of deep learning models, one method which computes the sensitivity
of the prediction with respect to changes in the input and one approach which
meaningfully decomposes the decision in terms of the input variables. These
methods are evaluated on three classification tasks.



The areas of machine learning and communication technology are converging.
Today's communications systems generate a huge amount of traffic data, which
can help to significantly enhance the design and management of networks and
communication components when combined with advanced machine learning methods.
Furthermore, recently developed end-to-end training procedures offer new ways
to jointly optimize the components of a communication system. Also in many
emerging application fields of communication technology, e.g., smart cities or
internet of things, machine learning methods are of central importance. This
paper gives an overview over the use of machine learning in different areas of
communications and discusses two exemplar applications in wireless networking.
Furthermore, it identifies promising future research topics and discusses their
potential impact.



Active appearance models (AAMs) are a class of generative models that have
seen tremendous success in face analysis. However, model learning depends on
the availability of detailed annotation of canonical landmark points. As a
result, when accurate AAM fitting is required on a different set of variations
(expression, pose, identity), a new dataset is collected and annotated. To
overcome the need for time consuming data collection and annotation, transfer
learning approaches have received recent attention. The goal is to transfer
knowledge from previously available datasets (source) to a new dataset
(target). We propose a subspace transfer learning method, in which we select a
subspace from the source that best describes the target space. We propose a
metric to compute the directional similarity between the source eigenvectors
and the target subspace. We show an equivalence between this metric and the
variance of target data when projected onto source eigenvectors. Using this
equivalence, we select a subset of source principal directions that capture the
variance in target data. To define our model, we augment the selected source
subspace with the target subspace learned from a handful of target examples. In
experiments done on six publicly available datasets, we show that our approach
outperforms the state of the art in terms of the RMS fitting error as well as
the percentage of test examples for which AAM fitting converges to the ground
truth.



Natural disasters can have catastrophic impacts on the functionality of
infrastructure systems and cause severe physical and socio-economic losses.
Given budget constraints, it is crucial to optimize decisions regarding
mitigation, preparedness, response, and recovery practices for these systems.
This requires accurate and efficient means to evaluate the infrastructure
system reliability. While numerous research efforts have addressed and
quantified the impact of natural disasters on infrastructure systems, typically
using the Monte Carlo approach, they still suffer from high computational cost
and, thus, are of limited applicability to large systems. This paper presents a
deep learning framework for accelerating infrastructure system reliability
analysis. In particular, two distinct deep neural network surrogates are
constructed and studied: (1) A classifier surrogate which speeds up the
connectivity determination of networks, and (2) An end-to-end surrogate that
replaces a number of components such as roadway status realization,
connectivity determination, and connectivity averaging. The proposed approach
is applied to a simulation-based study of the two-terminal connectivity of a
California transportation network subject to extreme probabilistic earthquake
events. Numerical results highlight the effectiveness of the proposed approach
in accelerating the transportation system two-terminal reliability analysis
with extremely high prediction accuracy.



Recent advances in Deep Neural Networks (DNNs) have led to the development of
DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can
drive without any human intervention. Most major manufacturers including Tesla,
GM, Ford, BMW, and Waymo/Google are working on building and testing different
types of autonomous vehicles. The lawmakers of several US states including
California, Texas, and New York have passed new legislation to fast-track the
process of testing and deployment of autonomous vehicles on their roads.
  However, despite their spectacular progress, DNNs, just like traditional
software, often demonstrate incorrect or unexpected corner case behaviors that
can lead to potentially fatal collisions. Several such real-world accidents
involving autonomous cars have already happened including one which resulted in
a fatality. Most existing testing techniques for DNN-driven vehicles are
heavily dependent on the manual collection of test data under different driving
conditions which become prohibitively expensive as the number of test
conditions increases.
  In this paper, we design, implement and evaluate DeepTest, a systematic
testing tool for automatically detecting erroneous behaviors of DNN-driven
vehicles that can potentially lead to fatal crashes. First, our tool is
designed to automatically generated test cases leveraging real-world changes in
driving conditions like rain, fog, lighting conditions, etc. DeepTest
systematically explores different parts of the DNN logic by generating test
inputs that maximize the numbers of activated neurons. DeepTest found thousands
of erroneous behaviors under different realistic driving conditions (e.g.,
blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in
three top performing DNNs in the Udacity self-driving car challenge.



Reinforcement learning algorithms discover policies that maximize reward, but
do not necessarily guarantee safety during learning or execution phases. We
introduce a new approach to learn optimal policies while enforcing properties
expressed in temporal logic. To this end, given the temporal logic
specification that is to be obeyed by the learning system, we propose to
synthesize a reactive system called a shield. The shield is introduced in the
traditional learning process in two alternative ways, depending on the location
at which the shield is implemented. In the first one, the shield acts each time
the learning agent is about to make a decision and provides a list of safe
actions. In the second way, the shield is introduced after the learning agent.
The shield monitors the actions from the learner and corrects them only if the
chosen action causes a violation of the specification. We discuss which
requirements a shield must meet to preserve the convergence guarantees of the
learner. Finally, we demonstrate the versatility of our approach on several
challenging reinforcement learning scenarios.



Large-scale deep neural networks (DNNs) are both compute and memory
intensive. As the size of DNNs continues to grow, it is critical to improve the
energy efficiency and performance while maintaining accuracy. For DNNs, the
model size is an important factor affecting performance, scalability and energy
efficiency. Weight pruning achieves good compression ratios but suffers from
three drawbacks: 1) the irregular network structure after pruning; 2) the
increased training complexity; and 3) the lack of rigorous guarantee of
compression ratio and inference accuracy. To overcome these limitations, this
paper proposes CirCNN, a principled approach to represent weights and process
neural networks using block-circulant matrices. CirCNN utilizes the Fast
Fourier Transform (FFT)-based fast multiplication, simultaneously reducing the
computational complexity (both in inference and training) from O(n2) to
O(nlogn) and the storage complexity from O(n2) to O(n), with negligible
accuracy loss. Compared to other approaches, CirCNN is distinct due to its
mathematical rigor: it can converge to the same effectiveness as DNNs without
compression. The CirCNN architecture, a universal DNN inference engine that can
be implemented on various hardware/software platforms with configurable network
architecture. To demonstrate the performance and energy efficiency, we test
CirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN
architecture achieves very high energy efficiency and performance with a small
hardware footprint. Based on the FPGA implementation and ASIC synthesis
results, CirCNN achieves 6-102X energy efficiency improvements compared with
the best state-of-the-art results.



Generative models are widely used for unsupervised learning with various
applications, including data compression and signal restoration. Training
methods for such systems focus on the generality of the network given limited
amount of training data. A less researched type of techniques concerns
generation of only a single type of input. This is useful for applications such
as constraint handling, noise reduction and anomaly detection. In this paper we
present a technique to limit the generative capability of the network using
negative learning. The proposed method searches the solution in the gradient
direction for the desired input and in the opposite direction for the undesired
input. One of the application can be anomaly detection where the undesired
inputs are the anomalous data. In the results section we demonstrate the
features of the algorithm using MNIST handwritten digit dataset and latter
apply the technique to a real-world obstacle detection problem. The results
clearly show that the proposed learning technique can significantly improve the
performance for anomaly detection.



Many genres of natural language text are narratively structured, a testament
to our predilection for organizing our experiences as narratives. There is
broad consensus that understanding a narrative requires identifying and
tracking the goals and desires of the characters and their narrative outcomes.
However, to date, there has been limited work on computational models for this
problem. We introduce a new dataset, DesireDB, which includes gold-standard
labels for identifying statements of desire, textual evidence for desire
fulfillment, and annotations for whether the stated desire is fulfilled given
the evidence in the narrative context. We report experiments on tracking desire
fulfillment using different methods, and show that LSTM Skip-Thought model
achieves F-measure of 0.7 on our corpus.



We develop an end-to-end training algorithm for whole-image breast cancer
diagnosis based on mammograms. It requires lesion annotations only at the first
stage of training. After that, a whole image classifier can be trained using
only image level labels. This greatly reduced the reliance on lesion
annotations. Our approach is implemented using an all convolutional design that
is simple yet provides superior performance in comparison with the previous
methods. On DDSM, our best single-model achieves a per-image AUC score of 0.88
and three-model averaging increases the score to 0.91. On INbreast, our best
single-model achieves a per-image AUC score of 0.96. Using DDSM as benchmark,
our models compare favorably with the current state-of-the-art. We also
demonstrate that a whole image model trained on DDSM can be easily transferred
to INbreast without using its lesion annotations and using only a small amount
of training data.
  Code and model availability: https://github.com/lishen/end2end-all-conv



Anomaly detectors are often used to produce a ranked list of statistical
anomalies, which are examined by human analysts in order to extract the actual
anomalies of interest. Unfortunately, in realworld applications, this process
can be exceedingly difficult for the analyst since a large fraction of
high-ranking anomalies are false positives and not interesting from the
application perspective. In this paper, we aim to make the analyst's job easier
by allowing for analyst feedback during the investigation process. Ideally, the
feedback influences the ranking of the anomaly detector in a way that reduces
the number of false positives that must be examined before discovering the
anomalies of interest. In particular, we introduce a novel technique for
incorporating simple binary feedback into tree-based anomaly detectors. We
focus on the Isolation Forest algorithm as a representative tree-based anomaly
detector, and show that we can significantly improve its performance by
incorporating feedback, when compared with the baseline algorithm that does not
incorporate feedback. Our technique is simple and scales well as the size of
the data increases, which makes it suitable for interactive discovery of
anomalies in large datasets.



Recent efforts in bioinformatics have achieved tremendous progress in the
machine reading of biomedical literature, and the assembly of the extracted
biochemical interactions into large-scale models such as protein signaling
pathways. However, batch machine reading of literature at today's scale (PubMed
alone indexes over 1 million papers per year) is unfeasible due to both cost
and processing overhead. In this work, we introduce a focused reading approach
to guide the machine reading of biomedical literature towards what literature
should be read to answer a biomedical query as efficiently as possible. We
introduce a family of algorithms for focused reading, including an intuitive,
strong baseline, and a second approach which uses a reinforcement learning (RL)
framework that learns when to explore (widen the search) or exploit (narrow
it). We demonstrate that the RL approach is capable of answering more queries
than the baseline, while being more efficient, i.e., reading fewer documents.



Generating texts from structured data (e.g., a table) is important for
various natural language processing tasks such as question answering and dialog
systems. In recent studies, researchers use neural language models and
encoder-decoder frameworks for table-to-text generation. However, these neural
network-based approaches do not model the order of contents during text
generation. When a human writes a summary based on a given table, he or she
would probably consider the content order before wording. In a biography, for
example, the nationality of a person is typically mentioned before occupation
in a biography. In this paper, we propose an order-planning text generation
model to capture the relationship between different fields and use such
relationship to make the generated text more fluent and smooth. We conducted
experiments on the WikiBio dataset and achieve significantly higher performance
than previous methods in terms of BLEU, ROUGE, and NIST scores.



Sand--bubblers are crabs of the genera Dotilla and Scopimera which are known
to produce remarkable patterns and structures at tropical beaches. From these
pattern-making abilities, we may draw inspiration for digital visual art. A
simple mathematical model is proposed and an algorithm is designed that may
create such sand-bubbler patterns artificially. In addition, design parameters
to modify the patterns are identified and analyzed by computational aesthetic
measures. Finally, an extension of the algorithm is discussed that may enable
controlling and guiding generative evolution of the art-making process.



We propose a new algorithm, Mean Actor-Critic (MAC), for discrete-action
continuous-state reinforcement learning. MAC is a policy gradient algorithm
that uses the agent's explicit representation of all action values to estimate
the gradient of the policy, rather than using only the actions that were
actually executed. This significantly reduces variance in the gradient updates
and removes the need for a variance reduction baseline. We show empirical
results on two control domains where MAC performs as well as or better than
other policy gradient approaches, and on five Atari games, where MAC is
competitive with state-of-the-art policy search algorithms.



There is an increasing interest on accelerating neural networks for real-time
applications. We study the student-teacher strategy, in which a small and fast
student network is trained with the auxiliary information provided by a large
and accurate teacher network. We use conditional adversarial networks to learn
the loss function to transfer knowledge from teacher to student. The proposed
method is particularly effective for relatively small student networks.
Moreover, experimental results show the effect of network size when the modern
networks are used as student. We empirically study trade-off between inference
time and classification accuracy, and provide suggestions on choosing a proper
student.



We propose two multimodal deep learning architectures that allow for
cross-modal dataflow (XFlow) between the feature extractors, thereby extracting
more interpretable features and obtaining a better representation than through
unimodal learning, for the same amount of training data. These models can
usefully exploit correlations between audio and visual data, which have a
different dimensionality and are therefore nontrivially exchangeable. Our work
improves on existing multimodal deep learning metholodogies in two essential
ways: (1) it presents a novel method for performing cross-modality (before
features are learned from individual modalities) and (2) extends the previously
proposed cross-connections, which only transfer information between streams
that process compatible data. Both cross-modal architectures outperformed their
baselines (by up to 7.5%) when evaluated on the AVletters dataset.



One key challenge in talent search is to translate complex criteria of a
hiring position into a search query, while it is relatively easy for a searcher
to list examples of suitable candidates for a given position. To improve search
efficiency, we propose the next generation of talent search at LinkedIn, also
referred to as Search By Ideal Candidates. In this system, a searcher provides
one or several ideal candidates as the input to hire for a given position. The
system then generates a query based on the ideal candidates and uses it to
retrieve and rank results. Shifting from the traditional Query-By-Keyword to
this new Query-By-Example system poses a number of challenges: How to generate
a query that best describes the candidates? When moving to a completely
different paradigm, how does one leverage previous product logs to learn
ranking models and/or evaluate the new system with no existing usage logs?
Finally, given the different nature between the two search paradigms, the
ranking features typically used for Query-By-Keyword systems might not be
optimal for Query-By-Example. This paper describes our approach to solving
these challenges. We present experimental results confirming the effectiveness
of the proposed solution, particularly on query building and search ranking
tasks. As of writing this paper, the new system has been available to all
LinkedIn members.



Fine-tuning of a deep convolutional neural network (CNN) is often desired.
This paper provides an overview of our publicly available py-faster-rcnn-ft
software library that can be used to fine-tune the VGG_CNN_M_1024 model on
custom subsets of the Microsoft Common Objects in Context (MS COCO) dataset.
For example, we improved the procedure so that the user does not have to look
for suitable image files in the dataset by hand which can then be used in the
demo program. Our implementation randomly selects images that contain at least
one object of the categories on which the model is fine-tuned.



In this note, we point out a basic link between generative adversarial (GA)
training and binary classification -- any powerful discriminator essentially
computes an (f-)divergence between real and generated samples. The result,
repeatedly re-derived in decision theory, has implications for GA Networks
(GANs), providing an alternative perspective on training f-GANs by designing
the discriminator loss function.



Capturing the temporal dynamics of user preferences over items is important
for recommendation. Existing methods mainly assume that all time steps in
user-item interaction history are equally relevant to recommendation, which
however does not apply in real-world scenarios where user-item interactions can
often happen accidentally. More importantly, they learn user and item dynamics
separately, thus failing to capture their joint effects on user-item
interactions. To better model user and item dynamics, we present the
Interacting Attention-gated Recurrent Network (IARN) which adopts the attention
model to measure the relevance of each time step. In particular, we propose a
novel attention scheme to learn the attention scores of user and item history
in an interacting way, thus to account for the dependencies between user and
item dynamics in shaping user-item interactions. By doing so, IARN can
selectively memorize different time steps of a user's history when predicting
her preferences over different items. Our model can therefore provide
meaningful interpretations for recommendation results, which could be further
enhanced by auxiliary features. Extensive validation on real-world datasets
shows that IARN consistently outperforms state-of-the-art methods.



Deep learning has been shown to outperform traditional machine learning
algorithms across a wide range of problem domains. However, current deep
learning algorithms have been criticized as uninterpretable "black-boxes" which
cannot explain their decision making processes. This is a major shortcoming
that prevents the widespread application of deep learning to domains with
regulatory processes such as finance. As such, industries such as finance have
to rely on traditional models like decision trees that are much more
interpretable but less effective than deep learning for complex problems. In
this paper, we propose CLEAR-Trade, a novel financial AI visualization
framework for deep learning-driven stock market prediction that mitigates the
interpretability issue of deep learning methods. In particular, CLEAR-Trade
provides a effective way to visualize and explain decisions made by deep stock
market prediction models. We show the efficacy of CLEAR-Trade in enhancing the
interpretability of stock market prediction by conducting experiments based on
S&P 500 stock index prediction. The results demonstrate that CLEAR-Trade can
provide significant insight into the decision-making process of deep
learning-driven financial models, particularly for regulatory processes, thus
improving their potential uptake in the financial industry.



We develop a second order primal-dual method for optimization problems in
which the objective function is given by the sum of a strongly convex twice
differentiable term and a possibly nondifferentiable convex regularizer. After
introducing an auxiliary variable, we utilize the proximal operator of the
nonsmooth regularizer to transform the associated augmented Lagrangian into a
function that is once, but not twice, continuously differentiable. The saddle
point of this function corresponds to the solution of the original optimization
problem. We employ a generalization of the Hessian to define second order
updates on this function and prove global exponential stability of the
corresponding differential inclusion. Furthermore, we develop a globally
convergent customized algorithm that utilizes the primal-dual augmented
Lagrangian as a merit function. We show that the search direction can be
computed efficiently and prove quadratic/superlinear asymptotic convergence. We
use the $\ell_1$-regularized least squares problem and the problem of designing
a distributed controller for a spatially-invariant system to demonstrate the
merits and the effectiveness of our method.



This paper presents the EACare project, an ambitious multi-disciplinary
collaboration with the aim to develop an embodied system, capable of carrying
out neuropsychological tests to detect early signs of dementia, e.g., due to
Alzheimer's disease. The system will use methods from Machine Learning and
Social Robotics, and be trained with examples of recorded clinician-patient
interactions. The interaction will be developed using a participatory design
approach. We describe the scope and method of the project, and report on a
first Wizard of Oz prototype.



In outdoor environments, mobile robots are required to navigate through
terrain with varying characteristics, some of which might significantly affect
the integrity of the platform. Ideally, the robot should be able to identify
areas that are safe for navigation based on its own percepts about the
environment while avoiding damage to itself. Bayesian optimisation (BO) has
been successfully applied to the task of learning a model of terrain
traversability while guiding the robot through more traversable areas. An
issue, however, is that localisation uncertainty can end up guiding the robot
to unsafe areas and distort the model being learnt. In this paper, we address
this problem and present a novel method that allows BO to consider localisation
uncertainty by applying a Gaussian process model for uncertain inputs as a
prior. We evaluate the proposed method in simulation and in experiments with a
real robot navigating over rough terrain and compare it to standard BO methods
which assume deterministic inputs.



In this paper, we propose an uncertainty-aware learning from demonstration
method by presenting a novel uncertainty estimation method utilizing a mixture
density network appropriate for modeling complex and noisy human behaviors. The
proposed uncertainty acquisition can be done with a single forward path without
Monte Carlo sampling and is suitable for real-time robotics applications. The
properties of the proposed uncertainty measure are analyzed through three
different synthetic examples, absence of data, heavy measurement noise, and
composition of functions scenarios. We show that each case can be distinguished
using the proposed uncertainty measure and presented an uncertainty-aware
learn- ing from demonstration method of an autonomous driving using this
property. The proposed uncertainty-aware learning from demonstration method
outperforms other compared methods in terms of safety using a complex
real-world driving dataset.



We build a model using Gaussian processes to infer a spatio-temporal vector
field from observed agent trajectories. Significant landmarks or influence
points in agent surroundings are jointly derived through vector calculus
operations that indicate presence of sources and sinks. We evaluate these
influence points by using the Kullback-Leibler divergence between the posterior
and prior Laplacian of the inferred spatio-temporal vector field. Through
locating significant features that influence trajectories, our model aims to
give greater insight into underlying causal utility functions that determine
agent decision-making. A key feature of our model is that it infers a joint
Gaussian process over the observed trajectories, the time-varying vector field
of utility and canonical vector calculus operators. We apply our model to both
synthetic data and lion GPS data collected at the Bubye Valley Conservancy in
southern Zimbabwe.



Machine learning (ML) plays an ever-increasing role in advanced automotive
functionality for driver assistance and autonomous operation; however, its
adequacy from the perspective of safety certification remains controversial. In
this paper, we analyze the impacts that the use of ML as an implementation
approach has on ISO 26262 safety lifecycle and ask what could be done to
address them. We then provide a set of recommendations on how to adapt the
standard to accommodate ML.



Obtaining enough labeled data to robustly train complex discriminative models
is a major bottleneck in the machine learning pipeline. A popular solution is
combining multiple sources of weak supervision using generative models. The
structure of these models affects training label quality, but is difficult to
learn without any ground truth labels. We instead rely on these weak
supervision sources having some structure by virtue of being encoded
programmatically. We present Coral, a paradigm that infers generative model
structure by statically analyzing the code for these heuristics, thus reducing
the data required to learn structure significantly. We prove that Coral's
sample complexity scales quasilinearly with the number of heuristics and number
of relations found, improving over the standard sample complexity, which is
exponential in $n$ for identifying $n^{\textrm{th}}$ degree relations.
Experimentally, Coral matches or outperforms traditional structure learning
approaches by up to 3.81 F1 points. Using Coral to model dependencies instead
of assuming independence results in better performance than a fully supervised
model by 3.07 accuracy points when heuristics are used to label radiology data
without ground truth labels.



Falsification is drawing attention in quality assurance of heterogeneous
systems whose complexities are beyond most verification techniques'
scalability. In this paper we introduce the idea of causality aid in
falsification: by providing a falsification solver -- that relies on stochastic
optimization of a certain cost function -- with suitable causal information
expressed by a Bayesian network, search for a falsifying input value can be
efficient. Our experiment results show the idea's viability.



Quantum information technologies, and intelligent learning systems, are both
emergent technologies that will likely have a transforming impact on our
society. The respective underlying fields of research -- quantum information
(QI) versus machine learning (ML) and artificial intelligence (AI) -- have
their own specific challenges, which have hitherto been investigated largely
independently. However, in a growing body of recent work, researchers have been
probing the question to what extent these fields can learn and benefit from
each other. QML explores the interaction between quantum computing and ML,
investigating how results and techniques from one field can be used to solve
the problems of the other. Recently, we have witnessed breakthroughs in both
directions of influence. For instance, quantum computing is finding a vital
application in providing speed-ups in ML, critical in our "big data" world.
Conversely, ML already permeates cutting-edge technologies, and may become
instrumental in advanced quantum technologies. Aside from quantum speed-up in
data analysis, or classical ML optimization used in quantum experiments,
quantum enhancements have also been demonstrated for interactive learning,
highlighting the potential of quantum-enhanced learning agents. Finally, works
exploring the use of AI for the very design of quantum experiments, and for
performing parts of genuine research autonomously, have reported their first
successes. Beyond the topics of mutual enhancement, researchers have also
broached the fundamental issue of quantum generalizations of ML/AI concepts.
This deals with questions of the very meaning of learning and intelligence in a
world that is described by quantum mechanics. In this review, we describe the
main ideas, recent developments, and progress in a broad spectrum of research
investigating machine learning and artificial intelligence in the quantum
domain.



In this paper, we propose: (a) a restart schedule for an adaptive simulated
annealer, and (b) parallel simulated annealing, with an adaptive and
parameter-free annealing schedule. The foundation of our approach is the
Modified Lam annealing schedule, which adaptively controls the temperature
parameter to track a theoretically ideal rate of acceptance of neighboring
states. A sequential implementation of Modified Lam simulated annealing is
almost parameter-free. However, it requires prior knowledge of the annealing
length. We eliminate this parameter using restarts, with an exponentially
increasing schedule of annealing lengths. We then extend this restart schedule
to parallel implementation, executing several Modified Lam simulated annealers
in parallel, with varying initial annealing lengths, and our proposed parallel
annealing length schedule. To validate our approach, we conduct experiments on
an NP-Hard scheduling problem with sequence-dependent setup constraints. We
compare our approach to fixed length restarts, both sequentially and in
parallel. Our results show that our approach can achieve substantial
performance gains, throughout the course of the run, demonstrating our approach
to be an effective anytime algorithm.



Power grids are critical infrastructure assets that face non-technical losses
(NTL) such as electricity theft or faulty meters. NTL may range up to 40% of
the total electricity distributed in emerging countries. Industrial NTL
detection systems are still largely based on expert knowledge when deciding
whether to carry out costly on-site inspections of customers. Electricity
providers are reluctant to move to large-scale deployments of automated systems
that learn NTL profiles from data due to the latter's propensity to suggest a
large number of unnecessary inspections. In this paper, we propose a novel
system that combines automated statistical decision making with expert
knowledge. First, we propose a machine learning framework that classifies
customers into NTL or non-NTL using a variety of features derived from the
customers' consumption data. The methodology used is specifically tailored to
the level of noise in the data. Second, in order to allow human experts to feed
their knowledge in the decision loop, we propose a method for visualizing
prediction results at various granularity levels in a spatial hologram. Our
approach allows domain experts to put the classification results into the
context of the data and to incorporate their knowledge for making the final
decisions of which customers to inspect. This work has resulted in appreciable
results on a real-world data set of 3.6M customers. Our system is being
deployed in a commercial NTL detection software.



In this short essay, we discuss some basic features of cognitive activity at
several different space-time scales: from neural networks in the brain to
civilizations. One motivation for such comparative study is its heuristic
value. Attempts to better understand the functioning of "wetware" involved in
cognitive activities of central nervous system by comparing it with a computing
device have a long tradition. We suggest that comparison with Internet might be
more adequate. We briefly touch upon such subjects as encoding, compression,
and Saussurean trichotomy langue/langage/parole in various environments.



Emotion recognition from facial expressions is tremendously useful,
especially when coupled with smart devices and wireless multimedia
applications. However, the inadequate network bandwidth often limits the
spatial resolution of the transmitted video, which will heavily degrade the
recognition reliability. We develop a novel framework to achieve robust emotion
recognition from low bit rate video. While video frames are downsampled at the
encoder side, the decoder is embedded with a deep network model for joint
super-resolution (SR) and recognition. Notably, we propose a novel max-mix
training strategy, leading to a single "One-for-All" model that is remarkably
robust to a vast range of downsampling factors. That makes our framework well
adapted for the varied bandwidths in real transmission scenarios, without
hampering scalability or efficiency. The proposed framework is evaluated on the
AVEC 2016 benchmark, and demonstrates significantly improved stand-alone
recognition performance, as well as rate-distortion (R-D) performance, than
either directly recognizing from LR frames, or separating SR and recognition.



Reinforcement Learning is divided in two main paradigms: model-free and
model-based. Each of these two paradigms has strengths and limitations, and has
been successfully applied to real world domains that are appropriate to its
corresponding strengths. In this paper, we present a new approach aimed at
bridging the gap between these two paradigms. We aim to take the best of the
two paradigms and combine them in an approach that is at the same time
data-efficient and cost-savvy. We do so by learning a probabilistic dynamics
model and leveraging it as a prior for the intertwined model-free optimization.
As a result, our approach can exploit the generality and structure of the
dynamics model, but is also capable of ignoring its inevitable inaccuracies, by
directly incorporating the evidence provided by the direct observation of the
cost. Preliminary results demonstrate that our approach outperforms purely
model-based and model-free approaches, as well as the approach of simply
switching from a model-based to a model-free setting.



The infinite restricted Boltzmann machine (iRBM) is an extension of the
classic RBM. It enjoys a good property of automatically deciding the size of
the hidden layer according to specific training data. With sufficient training,
the iRBM can achieve a competitive performance with that of the classic RBM.
However, the convergence of learning the iRBM is slow, due to the fact that the
iRBM is sensitive to the ordering of its hidden units, the learned filters
change slowly from the left-most hidden unit to right. To break this dependency
between neighboring hidden units and speed up the convergence of training, a
novel training strategy is proposed. The key idea of the proposed training
strategy is randomly regrouping the hidden units before each gradient descent
step. Potentially, a mixing of infinite many iRBMs with different permutations
of the hidden units can be achieved by this learning method, which has a
similar effect of preventing the model from over-fitting as the dropout. The
original iRBM is also modified to be capable of carrying out discriminative
training. To evaluate the impact of our method on convergence speed of learning
and the model's generalization ability, several experiments have been performed
on the binarized MNIST and CalTech101 Silhouettes datasets. Experimental
results indicate that the proposed training strategy can greatly accelerate
learning and enhance generalization ability of iRBMs.



For complex segmentation tasks, fully automatic systems are inherently
limited in their achievable accuracy for extracting relevant objects.
Especially in cases where only few data sets need to be processed for a highly
accurate result, semi-automatic segmentation techniques exhibit a clear benefit
for the user. One area of application is medical image processing during an
intervention for a single patient. We propose a learning-based cooperative
segmentation approach which includes the computing entity as well as the user
into the task. Our system builds upon a state-of-the-art fully convolutional
artificial neural network (FCN) as well as an active user model for training.
During the segmentation process, a user of the trained system can iteratively
add additional hints in form of pictorial scribbles as seed points into the FCN
system to achieve an interactive and precise segmentation result. The
segmentation quality of interactive FCNs is evaluated. Iterative FCN approaches
can yield superior results compared to networks without the user input channel
component, due to a consistent improvement in segmentation quality after each
interaction.



Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been
attracting a lot of attention in recent studies. It has been shown that for
many state of the art DNNs performing image classification there exist
universal adversarial perturbations --- image-agnostic perturbations mere
addition of which to natural images with high probability leads to their
misclassification. In this work we propose a new algorithm for constructing
such universal perturbations. Our approach is based on computing the so-called
$(p, q)$-singular vectors of the Jacobian matrices of hidden layers of a
network. Resulting perturbations present interesting visual patterns, and by
using only 64 images we were able to construct universal perturbations with
more than 60 \% fooling rate on the dataset consisting of 50000 images. We also
investigate a correlation between the maximal singular value of the Jacobian
matrix and the fooling rate of the corresponding singular vector, and show that
the constructed perturbations generalize across networks.



We study the problem of causal structure learning when the experimenter is
limited to perform at most $k$ non-adaptive experiments of size $1$. We
formulate the problem of finding the best intervention target set as an
optimization problem, which aims to maximize the average number of edges whose
directions are resolved. We prove that the objective function is submodular and
a greedy algorithm is a $(1-\frac{1}{e})$-approximation algorithm for the
problem. We further present an accelerated variant of the greedy algorithm,
which can lead to orders of magnitude performance speedup. We validate our
proposed approach on synthetic and real graphs. The results show that compared
to the purely observational setting, our algorithm orients majority of the
edges through only a small number of interventions.



Randomized experiments have been critical tools of decision making for
decades. However, subjects can show significant heterogeneity in response to
treatments in many important applications. Therefore it is not enough to simply
know which treatment is optimal for the entire population. What we need is a
model that correctly customize treatment assignment base on subject
characteristics. The problem of constructing such models from randomized
experiments data is known as Uplift Modeling in the literature. Many algorithms
have been proposed for uplift modeling and some have generated promising
results on various data sets. Yet little is known about the theoretical
properties of these algorithms. In this paper, we propose a new tree-based
ensemble algorithm for uplift modeling. Experiments show that our algorithm can
achieve competitive results on both synthetic and industry-provided data. In
addition, by properly tuning the "node size" parameter, our algorithm is proved
to be consistent under mild regularity conditions. This is the first consistent
algorithm for uplift modeling that we are aware of.



The recent development of CNN-based image dehazing has revealed the
effectiveness of end-to-end modeling. However, extending the idea to end-to-end
video dehazing has not been explored yet. In this paper, we propose an
End-to-End Video Dehazing Network (EVD-Net), to exploit the temporal
consistency between consecutive video frames. A thorough study has been
conducted over a number of structure options, to identify the best temporal
fusion strategy. Furthermore, we build an End-to-End United Video Dehazing and
Detection Network(EVDD-Net), which concatenates and jointly trains EVD-Net with
a video object detection model. The resulting augmented end-to-end pipeline has
demonstrated much more stable and accurate detection results in hazy video.



The rapid advances in e-commerce and Web 2.0 technologies have greatly
increased the impact of commercial advertisements on the general public. As a
key enabling technology, a multitude of recommender systems exists which
analyzes user features and browsing patterns to recommend appealing
advertisements to users. In this work, we seek to study the characteristics or
attributes that characterize an effective advertisement and recommend a useful
set of features to aid the designing and production processes of commercial
advertisements. We analyze the temporal patterns from multimedia content of
advertisement videos including auditory, visual and textual components, and
study their individual roles and synergies in the success of an advertisement.
The objective of this work is then to measure the effectiveness of an
advertisement, and to recommend a useful set of features to advertisement
designers to make it more successful and approachable to users. Our proposed
framework employs the signal processing technique of cross modality feature
learning where data streams from different components are employed to train
separate neural network models and are then fused together to learn a shared
representation. Subsequently, a neural network model trained on this joint
feature embedding representation is utilized as a classifier to predict
advertisement effectiveness. We validate our approach using subjective ratings
from a dedicated user study, the sentiment strength of online viewer comments,
and a viewer opinion metric of the ratio of the Likes and Views received by
each advertisement from an online platform.



Although neural machine translation (NMT) with the encoder-decoder framework
has achieved great success in recent times, it still suffers from some
drawbacks: RNNs tend to forget old information which is often useful and the
encoder only operates through words without considering word relationship. To
solve these problems, we introduce a relation networks (RN) into NMT to refine
the encoding representations of the source. In our method, the RN first
augments the representation of each source word with its neighbors and reasons
all the possible pairwise relations between them. Then the source
representations and all the relations are fed to the attention module and the
decoder together, keeping the main encoder-decoder architecture unchanged.
Experiments on two Chinese-to-English data sets in different scales both show
that our method can outperform the competitive baselines significantly.



In crowdfunding, an entrepreneur often has to decide how to disclose the
campaign status in order to collect as many contributions as possible. We
propose information design as a tool to help the entrepreneur to improve
revenue by influencing backers' beliefs. We introduce a heuristic algorithm to
dynamically compute information-disclosure policies for the entrepreneur,
followed by an empirical evaluation to demonstrate its competitiveness over the
widely-adopted immediate-disclosure policy. Our results demonstrate that
despite its ease of implementation, the immediate-disclosure policy is not
optimal when backers follow thresholding policies. With appropriate heuristics,
an entrepreneur can benefit from dynamic information disclosure. Our work sheds
light on information design in a dynamic setting where agents make decisions
using thresholding policies.



Recurrent neural networks (RNNs) are widely used to model sequential data but
their non-linear dependencies between sequence elements prevent parallelizing
training over sequence length. We show the training of RNNs with only linear
sequential dependencies can be parallelized over the sequence length using the
parallel scan algorithm, leading to rapid training on long sequences with small
minibatch size. We abstract prior linear sequence models into a new framework
of linear surrogate RNNs and develop a linear surrogate long short-term memory
(LS-LSTM) powered by a parallel linear recurrence CUDA kernel we implemented.
We evaluate the LS-LSTM on a long sequence noisy autoregressive task and find
the LS-LSTM achieves slightly superior train and test performance to a similar
sized LSTM in 4x less training time. We analyze latency and throughput of the
LS-LSTM and find the LS-LSTM reaches up to 175x the throughput of the LSTM in
the small minibatch long sequence regime.



Knowledge graph (KG) is known to be helpful for the task of question
answering (QA), since it provides well-structured relational information
between entities, and allows one to further infer indirect facts. However, it
is challenging to build QA systems which can learn to reason over knowledge
graphs based on question-answer pairs alone. First, when people ask questions,
their expressions are noisy (for example, typos in texts, or variations in
pronunciations), which is non-trivial for the QA system to match those
mentioned entities to the knowledge graph. Second, many questions require
multi-hop logic reasoning over the knowledge graph to retrieve the answers. To
address these challenges, we propose a novel and unified deep learning
architecture, and an end-to-end variational learning algorithm which can handle
noise in questions, and learn multi-hop reasoning simultaneously. Our method
achieves state-of-the-art performance on a recent benchmark dataset in the
literature. We also derive a series of new benchmark datasets, including
questions for multi-hop reasoning, questions paraphrased by neural translation
model, and questions in human voice. Our method yields very promising results
on all these challenging datasets.



Multi-scanner Antivirus systems provide insightful information on the nature
of a suspect application; however there is often a lack of consensus and
consistency between different Anti-Virus engines. In this article, we analyze
more than 250 thousand malware signatures generated by 61 different Anti-Virus
engines after analyzing 82 thousand different Android malware applications. We
identify 41 different malware classes grouped into three major categories,
namely Adware, Harmful Threats and Unknown or Generic signatures. We further
investigate the relationships between such 41 classes using community detection
algorithms from graph theory to identify similarities between them; and we
finally propose a Structure Equation Model to identify which Anti-Virus engines
are more powerful at detecting each macro-category. As an application, we show
how such models can help in identifying whether Unknown malware applications
are more likely to be of Harmful or Adware type.



As the use of cloud computing continues to rise, controlling cost becomes
increasingly important. Yet there is evidence that 30\% - 45\% of cloud spend
is wasted. Existing tools for cloud provisioning typically rely on highly
trained human experts to specify what to monitor, thresholds for triggering
action, and actions. In this paper we explore the use of reinforcement learning
(RL) to acquire policies to balance performance and spend, allowing humans to
specify what they want as opposed to how to do it, minimizing the need for
cloud expertise. Empirical results with tabular, deep, and dueling double deep
Q-learning with the CloudSim simulator show the utility of RL and the relative
merits of the approaches. We also demonstrate effective policy transfer
learning from an extremely simple simulator to CloudSim, with the next step
being transfer from CloudSim to an Amazon Web Services physical environment.



Weighted finite automata (WFA) can expressively model functions defined over
strings but are inherently linear models. Given the recent successes of
nonlinear models in machine learning, it is natural to wonder whether
ex-tending WFA to the nonlinear setting would be beneficial. In this paper, we
propose a novel model of neural network based nonlinearWFA model (NL-WFA) along
with a learning algorithm. Our learning algorithm is inspired by the spectral
learning algorithm for WFAand relies on a nonlinear decomposition of the
so-called Hankel matrix, by means of an auto-encoder network. The expressive
power of NL-WFA and the proposed learning algorithm are assessed on both
synthetic and real-world data, showing that NL-WFA can lead to smaller model
sizes and infer complex grammatical structures from data.



In this paper, we conduct an empirical study on discovering the ordered
collective dynamics obtained by a population of artificial intelligence (AI)
agents. Our intention is to put AI agents into a simulated natural context, and
then to understand their induced dynamics at the population level. In
particular, we aim to verify if the principles developed in the real world
could also be used in understanding an artificially-created intelligent
population. To achieve this, we simulate a large-scale predator-prey world,
where the laws of the world are designed by only the findings or logical
equivalence that have been discovered in nature. We endow the agents with the
intelligence based on deep reinforcement learning, and scale the population
size up to millions. Our results show that the population dynamics of AI
agents, driven only by each agent's individual self interest, reveals an
ordered pattern that is similar to the Lotka-Volterra model studied in
population biology. We further discover the emergent behaviors of collective
adaptations in studying how the agents' grouping behaviors will change with the
environmental resources. Both of the two findings could be explained by the
self-organization theory in nature.



In this paper, we report work in progress on the development of Mr. Jones --
a proactive orchestrator and decision support agent for a collaborative
decision making setting embodied by a smart room. The duties of such an agent
may range across interactive problem solving with other agents in the
environment, developing automated summaries of meetings, visualization of the
internal decision making process, proactive data and resource management, and
so on. Specifically, we highlight the importance of integrating higher level
symbolic reasoning and intent recognition in the design of such an agent, and
outline pathways towards the realization of these capabilities. We will
demonstrate some of these functionalities here in the context of automated
orchestration of a meeting in the CEL -- the Cognitive Environments Laboratory
at IBM's T.J. Watson Research Center.



The prediction of organic reaction outcomes is a fundamental problem in
computational chemistry. Since a reaction may involve hundreds of atoms, fully
exploring the space of possible transformations is intractable. The current
solution utilizes reaction templates to limit the space, but it suffers from
coverage and efficiency issues. In this paper, we propose a template-free
approach to efficiently explore the space of product molecules by first
pinpointing the reaction center -- the set of nodes and edges where graph edits
occur. Since only a small number of atoms contribute to reaction center, we can
directly enumerate candidate products. The generated candidates are scored by a
Weisfeiler-Lehman Difference Network that models high-order interactions
between changes occurring at nodes across the molecule. Our framework
outperforms the top-performing template-based approach with a 10\% margin,
while running orders of magnitude faster. Finally, we demonstrate that the
model accuracy rivals the performance of domain experts.



Reinforcement Learning AI commonly uses reward/penalty signals that are
objective and explicit in an environment -- e.g. game score, completion time,
etc. -- in order to learn the optimal strategy for task performance. However,
Human-AI interaction for such AI agents should include additional reinforcement
that is implicit and subjective -- e.g. human preferences for certain AI
behavior -- in order to adapt the AI behavior to idiosyncratic human
preferences. Such adaptations would mirror naturally occurring processes that
increase trust and comfort during social interactions. Here, we show how a
hybrid brain-computer-interface (hBCI), which detects an individual's level of
interest in objects/events in a virtual environment, can be used to adapt the
behavior of a Deep Reinforcement Learning AI agent that is controlling a
virtual autonomous vehicle. Specifically, we show that the AI learns a driving
strategy that maintains a safe distance from a lead vehicle, and most novelly,
preferentially slows the vehicle when the human passengers of the vehicle
encounter objects of interest. This adaptation affords an additional 20\%
viewing time for subjectively interesting objects. This is the first
demonstration of how an hBCI can be used to provide implicit reinforcement to
an AI agent in a way that incorporates user preferences into the control
system.



Random walks are at the heart of many existing deep learning algorithms for
graph data. However, such algorithms have many limitations that arise from the
use of random walks, e.g., the features resulting from these methods are unable
to transfer to new nodes and graphs as they are tied to node identity. In this
work, we introduce the notion of attributed random walks which serves as a
basis for generalizing existing methods such as DeepWalk, node2vec, and many
others that leverage random walks. Our proposed framework enables these methods
to be more widely applicable for both transductive and inductive learning as
well as for use on graphs with attributes (if available). This is achieved by
learning functions that generalize to new nodes and graphs. We show that our
proposed framework is effective with an average AUC improvement of 16.1% while
requiring on average 853 times less space than existing methods on a variety of
graphs from several domains.



We present a novel method to solve image analogy problems : it allows to
learn the relation between paired images present in training data, and then
generalize and generate images that correspond to the relation, but were never
seen in the training set. Therefore, we call the method Conditional Analogy
Generative Adversarial Network (CAGAN), as it is based on adversarial training
and employs deep convolutional neural networks. An especially interesting
application of that technique is automatic swapping of clothing on fashion
model photos. Our work has the following contributions. First, the definition
of the end-to-end trainable CAGAN architecture, which implicitly learns
segmentation masks without expensive supervised labeling data. Second,
experimental results show plausible segmentation masks and often convincing
swapped images, given the target article. Finally, we discuss the next steps
for that technique: neural network architecture improvements and more advanced
applications.



Despite the recent developments that allowed neural networks to achieve
impressive performance on a variety of applications, these models are
intrinsically affected by the problem of overgeneralization, due to their
partitioning of the full input space into the fixed set of target classes used
during training. Thus it is possible for novel inputs belonging to categories
unknown during training or even completely unrecognizable to humans to fool the
system into classifying them as one of the known classes, even with a high
degree of confidence. Solving this problem may help improve the security of
such systems in critical applications, and may further lead to applications in
the context of open set recognition and 1-class recognition. This paper
presents a novel way to compute a confidence score using denoising autoencoders
and shows that such confidence score can correctly identify the regions of the
input space close to the training distribution by approximately identifying its
local maxima.



High-dimensional data requires scalable algorithms. We propose and analyze
three scalable and related algorithms for semi-supervised discriminant analysis
(SDA). These methods are based on Krylov subspace methods which exploit the
data sparsity and the shift-invariance of Krylov subspaces. In addition, the
problem definition was improved by adding centralization to the semi-supervised
setting. The proposed methods are evaluated on a industry-scale data set from a
pharmaceutical company to predict compound activity on target proteins. The
results show that SDA achieves good predictive performance and our methods only
require a few seconds, significantly improving computation time on previous
state of the art.



In order for a robot to be a generalist that can perform a wide range of
jobs, it must be able to acquire a wide variety of skills quickly and
efficiently in complex unstructured environments. High-capacity models such as
deep neural networks can enable a robot to represent complex skills, but
learning each skill from scratch then becomes infeasible. In this work, we
present a meta-imitation learning method that enables a robot to learn how to
learn more efficiently, allowing it to acquire new skills from just a single
demonstration. Unlike prior methods for one-shot imitation, our method can
scale to raw pixel inputs and requires data from significantly fewer prior
tasks for effective learning of new skills. Our experiments on both simulated
and real robot platforms demonstrate the ability to learn new tasks,
end-to-end, from a single visual demonstration.



Today's general-purpose deep convolutional neural networks (CNN) for image
classification and object detection are trained offline on large static
datasets. Some applications, however, will require training in real-time on
live video streams with a human-in-the-loop. We refer to this class of problem
as Time-ordered Online Training (ToOT) - these problems will require a
consideration of not only the quantity of incoming training data, but the human
effort required to tag and use it. In this paper, we define training benefit as
a metric to measure the effectiveness of a sequence in using each user
interaction. We demonstrate and evaluate a system tailored to performing ToOT
in the field, capable of training an image classifier on a live video stream
through minimal input from a human operator. We show that by exploiting the
time-ordered nature of the video stream through optical flow-based object
tracking, we can increase the effectiveness of human actions by about 8 times.



Model compression is significant for the wide adoption of Recurrent Neural
Networks (RNNs) in both user devices possessing limited resources and business
clusters requiring quick responses to large-scale service requests. This work
aims to learn structurally-sparse Long Short-Term Memory (LSTM) by reducing the
sizes of basic structures within LSTM units, including input updates, gates,
hidden states, cell states and outputs. Independently reducing the sizes of
basic structures can result in inconsistent dimensions among them, and
consequently, end up with invalid LSTM units. To overcome the problem, we
propose Intrinsic Sparse Structures (ISS) in LSTMs. Removing a component of ISS
will simultaneously decrease the sizes of all basic structures by one and
thereby always maintain the dimension consistency. By learning ISS within LSTM
units, the obtained LSTMs remain regular while having much smaller basic
structures. Based on group Lasso regularization, our method achieves 10.59x
speedup without losing any perplexity of a language modeling of Penn TreeBank
dataset. It is also successfully evaluated through a compact model with only
2.69M weights for machine Question Answering of SQuAD dataset. Our approach is
successfully extended to non- LSTM RNNs, like Recurrent Highway Networks
(RHNs). Our source code is publicly available at
https://github.com/wenwei202/iss-rnns



Human action recognition refers to automatic recognizing human actions from a
video clip, which is one of the most challenging tasks in computer vision. In
reality, a video stream is often weakly-annotated with a set of relevant human
action labels at a global level rather than assigning each label to a specific
video episode corresponding to a single action, which leads to a multi-label
learning problem. Furthermore, there are a great number of meaningful human
actions in reality but it would be extremely difficult, if not impossible, to
collect/annotate video clips regarding all of various human actions, which
leads to a zero-shot learning scenario. To the best of our knowledge, there is
no work that has addressed all the above issues together in human action
recognition. In this paper, we formulate a real-world human action recognition
task as a multi-label zero-shot learning problem and propose a framework to
tackle this problem. Our framework simultaneously tackles the issue of unknown
temporal boundaries between different actions for multi-label learning and
exploits the side information regarding the semantic relationship between
different human actions for zero-shot learning. As a result, our framework
leads to a joint latent embedding representation for multi-label zero-shot
human action recognition. The joint latent embedding is learned with two
component models by exploring temporal coherence underlying video data and the
intrinsic relationship between visual and semantic domain. We evaluate our
framework with different settings, including a novel data split scheme designed
especially for evaluating multi-label zero-shot learning, on two weakly
annotated multi-label human action datasets: Breakfast and Charades. The
experimental results demonstrate the effectiveness of our framework in
multi-label zero-shot human action recognition.



Our understanding of the world depends highly on our capacity to produce
intuitive and simplified representations which can be easily used to solve
problems. We reproduce this simplification process using a neural network to
build a low dimensional state representation of the world from images acquired
by a robot. As in Jonschkowski et al. 2015, we learn in an unsupervised way
using prior knowledge about the world as loss functions called robotic priors
and extend this approach to high dimension richer images to learn a 3D
representation of the hand position of a robot from RGB images. We propose a
quantitative evaluation of the learned representation using nearest neighbors
in the state space that allows to assess its quality and show both the
potential and limitations of robotic priors in realistic environments. We
augment image size, add distractors and domain randomization, all crucial
components to achieve transfer learning to real robots. Finally, we also
contribute a new prior to improve the robustness of the representation. The
applications of such low dimensional state representation range from easing
reinforcement learning (RL) and knowledge transfer across tasks, to
facilitating learning from raw data with more efficient and compact high level
representations. The results show that the robotic prior approach is able to
extract high level representation as the 3D position of an arm and organize it
into a compact and coherent space of states in a challenging dataset.



We introduce a framework to leverage knowledge acquired from a repository of
(heterogeneous) supervised datasets to new unsupervised datasets. Our
perspective avoids the subjectivity inherent in unsupervised learning by
reducing it to supervised learning, and provides a principled way to evaluate
unsupervised algorithms. We demonstrate the versatility of our framework via
simple agnostic bounds on unsupervised problems. In the context of clustering,
our approach can help choose the number of clusters, the clustering algorithm,
and provably circumvents Kleinberg's impossibility result. Experimental results
across hundreds of problems demonstrate improved performance on unsupervised
data with simple algorithms, despite the fact problems come from different
domains. Additionally, a deep learning algorithm learns common features from
many small datasets across multiple domains.



Recommendation systems are recognised as being hugely important in industry,
and the area is now well understood. At News UK, there is a requirement to be
able to quickly generate recommendations for users on news items as they are
published. However, little has been published about systems that can generate
recommendations in response to changes in recommendable items and user
behaviour in a very short space of time. In this paper we describe a new
algorithm for updating collaborative filtering models incrementally, and
demonstrate its effectiveness on clickstream data from The Times. We also
describe the architecture that allows recommendations to be generated on the
fly, and how we have made each component scalable. The system is currently
being used in production at News UK.



We present a commonsense, qualitative model for the semantic grounding of
embodied visuo-spatial and locomotive interactions. The key contribution is an
integrative methodology combining low-level visual processing with high-level,
human-centred representations of space and motion rooted in artificial
intelligence. We demonstrate practical applicability with examples involving
object interactions, and indoor movement.



We consider the exploration/exploitation problem in reinforcement learning.
For exploitation, it is well known that the Bellman equation connects the value
at any time-step to the expected value at subsequent time-steps. In this paper
we consider a similar uncertainty Bellman equation (UBE), which connects the
uncertainty at any time-step to the expected uncertainties at subsequent
time-steps, thereby extending the potential exploratory benefit of a policy
beyond individual time-steps. We prove that the unique fixed point of the UBE
yields an upper bound on the variance of the estimated value of any fixed
policy. This bound can be much tighter than traditional count-based bonuses
that compound standard deviation rather than variance. Importantly, and unlike
several existing approaches to optimism, this method scales naturally to large
systems with complex generalization. Substituting our UBE-exploration strategy
for $\epsilon$-greedy improves DQN performance on 51 out of 57 games in the
Atari suite.



The vehicle to represent Knowledge Organization Systems (KOSs) in the
environment of the Semantic Web and linked data is the Simple Knowledge
Organization System (SKOS). SKOS provides a way to assign a URI to each
concept, and this URI functions as a surrogate for the concept. This fact makes
of main concern the need to clarify the URIs' ontological meaning. The aim of
this study is to investigate the relation between the ontological substance of
KOS concepts and concepts revealed through the grammatical and syntactic
formalisms of natural language. For this purpose, we examined the dividableness
of concepts in specific KOSs (i.e. a thesaurus, a subject headings system and a
classification scheme) by applying Natural Language Processing (NLP) techniques
(i.e. morphosyntactic analysis) to the lexical representations (i.e. RDF
literals) of SKOS concepts. The results of the comparative analysis reveal
that, despite the use of multi-word units, thesauri tend to represent concepts
in a way that can hardly be further divided conceptually, while Subject
Headings and Classification Schemes - to a certain extent - comprise terms that
can be decomposed into more conceptual constituents. Consequently, SKOS
concepts deriving from thesauri are more likely to represent atomic conceptual
units and thus be more appropriate tools for inference and reasoning. Since
identifiers represent the meaning of a concept, complex concepts are neither
the most appropriate nor the most efficient way of modelling a KOS for the
Semantic Web.



Latent factor models are increasingly popular for modeling multi-relational
knowledge graphs. By their vectorial nature, it is not only hard to interpret
why this class of models works so well, but also to understand where they fail
and how they might be improved. We conduct an experimental survey of
state-of-the-art models, not towards a purely comparative end, but as a means
to get insight about their inductive abilities. To assess the strengths and
weaknesses of each model, we create simple tasks that exhibit first, atomic
properties of binary relations, and then, common inter-relational inference
through synthetic genealogies. Based on these experimental results, we propose
new research directions to improve on existing models.



Recently, digital music libraries have been developed and can be plainly
accessed. Latest research showed that current organization and retrieval of
music tracks based on album information are inefficient. Moreover, they
demonstrated that people use emotion tags for music tracks in order to search
and retrieve them. In this paper, we discuss separability of a set of emotional
labels, proposed in the categorical emotion expression, using Fisher's
separation theorem. We determine a set of adjectives to tag music parts: happy,
sad, relaxing, exciting, epic and thriller. Temporal, frequency and energy
features have been extracted from the music parts. It could be seen that the
maximum separability within the extracted features occurs between relaxing and
epic music parts. Finally, we have trained a classifier using Support Vector
Machines to automatically recognize and generate emotional labels for a music
part. Accuracy for recognizing each label has been calculated; where the
results show that epic music can be recognized more accurately (77.4%),
comparing to the other types of music.



Object detection is considered one of the most challenging problems in this
field of computer vision, as it involves the combination of object
classification and object localization within a scene. Recently, deep neural
networks (DNNs) have been demonstrated to achieve superior object detection
performance compared to other approaches, with YOLOv2 (an improved You Only
Look Once model) being one of the state-of-the-art in DNN-based object
detection methods in terms of both speed and accuracy. Although YOLOv2 can
achieve real-time performance on a powerful GPU, it still remains very
challenging for leveraging this approach for real-time object detection in
video on embedded computing devices with limited computational power and
limited memory. In this paper, we propose a new framework called Fast YOLO, a
fast You Only Look Once framework which accelerates YOLOv2 to be able to
perform object detection in video on embedded devices in a real-time manner.
First, we leverage the evolutionary deep intelligence framework to evolve the
YOLOv2 network architecture and produce an optimized architecture (referred to
as O-YOLOv2 here) that has 2.8X fewer parameters with just a ~2% IOU drop. To
further reduce power consumption on embedded devices while maintaining
performance, a motion-adaptive inference method is introduced into the proposed
Fast YOLO framework to reduce the frequency of deep inference with O-YOLOv2
based on temporal motion characteristics. Experimental results show that the
proposed Fast YOLO framework can reduce the number of deep inferences by an
average of 38.13%, and an average speedup of ~3.3X for objection detection in
video compared to the original YOLOv2, leading Fast YOLO to run an average of
~18FPS on a Nvidia Jetson TX1 embedded system.



In this paper we focus on the linear algebra theory behind feedforward (FNN)
and recurrent (RNN) neural networks. We review backward propagation, including
backward propagation through time (BPTT). Also, we obtain a new exact
expression for Hessian, which represents second order effects. We show that for
$t$ time steps the weight gradient can be expressed as a rank-$t$ matrix, while
the weight Hessian is as a sum of $t^{2}$ Kronecker products of rank-$1$ and
$W^{T}AW$ matrices, for some matrix $A$ and weight matrix $W$. Also, we show
that for a mini-batch of size $r$, the weight update can be expressed as a
rank-$rt$ matrix. Finally, we briefly comment on the eigenvalues of the Hessian
matrix.



Online solvers for partially observable Markov decision processes have been
applied to problems with large discrete state spaces, but continuous state,
action, and observation spaces remain a challenge. This paper begins by
investigating double progressive widening (DPW) as a solution to this
challenge. However, we prove that this modification alone is not sufficient
because the belief representations in the search tree collapse to a single
particle causing the algorithm to converge to a policy that is suboptimal
regardless of the computation time. The main contribution of the paper is to
propose a new algorithm, POMCPOW, that incorporates DPW and weighted particle
filtering to overcome this deficiency and attack continuous problems.
Simulation results show that these modifications allow the algorithm to be
successful where previous approaches fail.



In recent years, a number of artificial intelligent services have been
developed such as defect detection system or diagnosis system for customer
services. Unfortunately, the core in these services is a black-box in which
human cannot understand the underlying decision making logic, even though the
inspection of the logic is crucial before launching a commercial service. Our
goal in this paper is to propose an analytic method of a model explanation that
is applicable to general classification models. To this end, we introduce the
concept of a contribution matrix and an explanation embedding in a constraint
space by using a matrix factorization. We extract a rule-like model explanation
from the contribution matrix with the help of the nonnegative matrix
factorization. To validate our method, the experiment results provide with open
datasets as well as an industry dataset of a LTE network diagnosis and the
results show our method extracts reasonable explanations.



In this paper, a sparse Markov decision process (MDP) with novel causal
sparse Tsallis entropy regularization is proposed.The proposed policy
regularization induces a sparse and multi-modal optimal policy distribution of
a sparse MDP. The full mathematical analysis of the proposed sparse MDP is
provided.We first analyze the optimality condition of a sparse MDP. Then, we
propose a sparse value iteration method which solves a sparse MDP and then
prove the convergence and optimality of sparse value iteration using the Banach
fixed point theorem. The proposed sparse MDP is compared to soft MDPs which
utilize causal entropy regularization. We show that the performance error of a
sparse MDP has a constant bound, while the error of a soft MDP increases
logarithmically with respect to the number of actions, where this performance
error is caused by the introduced regularization term. In experiments, we apply
sparse MDPs to reinforcement learning problems. The proposed method outperforms
existing methods in terms of the convergence speed and performance.



Generating music has a few notable differences from generating images and
videos. First, music is an art of time, necessitating a temporal model. Second,
music is usually composed of multiple instruments/tracks with their own
temporal dynamics, but collectively they unfold over time interdependently.
Lastly, musical notes are often grouped into chords, arpeggios or melodies in
polyphonic music, and thereby introducing a chronological ordering of notes is
not naturally suitable. In this paper, we propose three models for symbolic
multi-track music generation under the framework of generative adversarial
networks (GANs). The three models, which differ in the underlying assumptions
and accordingly the network architectures, are referred to as the jamming
model, the composer model and the hybrid model. We trained the proposed models
on a dataset of over one hundred thousand bars of rock music and applied them
to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings.
A few intra-track and inter-track objective metrics are also proposed to
evaluate the generative results, in addition to a subjective user study. We
show that our models can generate coherent music of four bars right from
scratch (i.e. without human inputs). We also extend our models to human-AI
cooperative music generation: given a specific track composed by human, we can
generate four additional tracks to accompany it. All code, the dataset and the
rendered audio samples are available at https://salu133445.github.io/musegan/ .



Recurrent Neural Networks (RNNS) are now widely used on sequence generation
tasks due to their ability to learn long-range dependencies and to generate
sequences of arbitrary length. However, their left-to-right generation
procedure only allows a limited control from a potential user which makes them
unsuitable for interactive and creative usages such as interactive music
generation. This paper introduces a novel architecture called Anticipation-RNN
which possesses the assets of the RNN-based generative models while allowing to
enforce user-defined positional constraints. We demonstrate its efficiency on
the task of generating melodies satisfying positional constraints in the style
of the soprano parts of the J.S. Bach chorale harmonizations. Sampling using
the Anticipation-RNN is of the same order of complexity than sampling from the
traditional RNN model. This fast and interactive generation of musical
sequences opens ways to devise real-time systems that could be used for
creative purposes.



Learning to remember long sequences remains a challenging task for recurrent
neural networks. Register memory and attention mechanisms were both proposed to
resolve the issue with either high computational cost to retain memory
differentiability, or by discounting the RNN representation learning towards
encoding shorter local contexts than encouraging long sequence encoding.
Associative memory, which studies the compression of multiple patterns in a
fixed size memory, were rarely considered in recent years. Although some recent
work tries to introduce associative memory in RNN and mimic the energy decay
process in Hopfield nets, it inherits the shortcoming of rule-based memory
updates, and the memory capacity is limited. This paper proposes a method to
learn the memory update rule jointly with task objective to improve memory
capacity for remembering long sequences. Also, we propose an architecture that
uses multiple such associative memory for more complex input encoding. We
observed some interesting facts when compared to other RNN architectures on
some well-studied sequence learning tasks.



Generative adversarial networks (GANs) are an exciting alternative to
algorithms for solving density estimation problems---using data to assess how
likely samples are to be drawn from the same distribution. Instead of
explicitly computing these probabilities, GANs learn a generator that can match
the given probabilistic source. This paper looks particularly at this matching
capability in the context of problems with one-dimensional outputs. We identify
a class of function decompositions with properties that make them well suited
to the critic role in a leading approach to GANs known as Wasserstein GANs. We
show that Taylor and Fourier series decompositions belong to our class, provide
examples of these critics outperforming standard GAN approaches, and suggest
how they can be scaled to higher dimensional problems in the future.



Understanding properties of deep neural networks is an important challenge in
deep learning. In this paper, we take a step in this direction by proposing a
rigorous way of verifying properties of a popular class of neural networks,
Binarized Neural Networks, using the well-developed means of Boolean
satisfiability. Our main contribution is a construction that creates a
representation of a binarized neural network as a Boolean formula. Our encoding
is the first exact Boolean representation of a deep neural network. Using this
encoding, we leverage the power of modern SAT solvers along with a proposed
counterexample-guided search procedure to verify various properties of these
networks. A particular focus will be on the critical property of robustness to
adversarial perturbations. For this property, our experimental results
demonstrate that our approach scales to medium-size deep neural networks used
in image classification tasks. To the best of our knowledge, this is the first
work on verifying properties of deep neural networks using an exact Boolean
encoding of the network.



Representing the semantic relations that exist between two given words (or
entities) is an important first step in a wide-range of NLP applications such
as analogical reasoning, knowledge base completion and relational information
retrieval. A simple, yet surprisingly accurate method for representing a
relation between two words is to compute the vector offset (\PairDiff) between
their corresponding word embeddings. Despite the empirical success, it remains
unclear as to whether \PairDiff is the best operator for obtaining a relational
representation from word embeddings. We conduct a theoretical analysis of
generalised bilinear operators that can be used to measure the $\ell_{2}$
relational distance between two word-pairs. We show that, if the word
embeddings are standardised and uncorrelated, such an operator will be
independent of bilinear terms, and can be simplified to a linear form, where
\PairDiff is a special case. For numerous word embedding types, we empirically
verify the uncorrelation assumption, demonstrating the general applicability of
our theoretical result. Moreover, we experimentally discover \PairDiff from the
bilinear relation composition operator on several benchmark analogy datasets.



We present a general approach to automating ethical decisions, drawing on
machine learning and computational social choice. In a nutshell, we propose to
learn a model of societal preferences, and, when faced with a specific ethical
dilemma at runtime, efficiently aggregate those preferences to identify a
desirable choice. We provide a concrete algorithm that instantiates our
approach; some of its crucial steps are informed by a new theory of
swap-dominance efficient voting rules. Finally, we implement and evaluate a
system for ethical decision making in the autonomous vehicle domain, using
preference data collected from 1.3 million people through the Moral Machine
website.



In knowledge bases such as Wikidata, it is possible to assert a large set of
properties for entities, ranging from generic ones such as name and place of
birth to highly profession-specific or background-specific ones such as
doctoral advisor or medical condition. Determining a preference or ranking in
this large set is a challenge in tasks such as prioritisation of edits or
natural-language generation. Most previous approaches to ranking knowledge base
properties are purely data-driven, that is, as we show, mistake frequency for
interestingness.
  In this work, we have developed a human-annotated dataset of 350 preference
judgments among pairs of knowledge base properties for fixed entities. From
this set, we isolate a subset of pairs for which humans show a high level of
agreement (87.5% on average). We show, however, that baseline and
state-of-the-art techniques achieve only 61.3% precision in predicting human
preferences for this subset.
  We then analyze what contributes to one property being rated as more
important than another one, and identify that at least three factors play a
role, namely (i) general frequency, (ii) applicability to similar entities and
(iii) semantic similarity between property and entity. We experimentally
analyze the contribution of each factor and show that a combination of
techniques addressing all the three factors achieves 74% precision on the task.
  The dataset is available at
www.kaggle.com/srazniewski/wikidatapropertyranking.



Can textual data be compressed intelligently without losing accuracy in
evaluating sentiment? In this study, we propose a novel evolutionary
compression algorithm, PARSEC (PARts-of-Speech for sEntiment Compression),
which makes use of Parts-of-Speech tags to compress text in a way that
sacrifices minimal classification accuracy when used in conjunction with
sentiment analysis algorithms. An analysis of PARSEC with eight commercial and
non-commercial sentiment analysis algorithms on twelve English sentiment data
sets reveals that accurate compression is possible with (0%, 1.3%, 3.3%) loss
in sentiment classification accuracy for (20%, 50%, 75%) data compression with
PARSEC using LingPipe, the most accurate of the sentiment algorithms. Other
sentiment analysis algorithms are more severely affected by compression. We
conclude that significant compression of text data is possible for sentiment
analysis depending on the accuracy demands of the specific application and the
specific sentiment analysis algorithm used.



Feature engineering is a crucial step in the process of predictive modeling.
It involves the transformation of given feature space, typically using
mathematical functions, with the objective of reducing the modeling error for a
given target. However, there is no well-defined basis for performing effective
feature engineering. It involves domain knowledge, intuition, and most of all,
a lengthy process of trial and error. The human attention involved in
overseeing this process significantly influences the cost of model generation.
We present a new framework to automate feature engineering. It is based on
performance driven exploration of a transformation graph, which systematically
and compactly enumerates the space of given options. A highly efficient
exploration strategy is derived through reinforcement learning on past
examples.



Deep learning algorithms offer a powerful means to automatically analyze the
content of medical images. However, many biological samples of interest are
primarily transparent to visible light and contain features that are difficult
to resolve with a standard optical microscope. Here, we use a convolutional
neural network (CNN) not only to classify images, but also to optimize the
physical layout of the imaging device itself. We increase the classification
accuracy of a microscope's recorded images by merging an optical model of image
formation into the pipeline of a CNN. The resulting network simultaneously
determines an ideal illumination arrangement to highlight important sample
features during image acquisition, along with a set of convolutional weights to
classify the detected images post-capture. We demonstrate our joint
optimization technique with an experimental microscope configuration that
automatically identifies malaria-infected cells with 5-10% higher accuracy than
standard and alternative microscope lighting designs.



We study the problem of learning description logic (DL) ontologies in Angluin
et al.'s framework of exact learning via queries. We admit membership queries
("is a given subsumption entailed by the target ontology?") and equivalence
queries ("is a given ontology equivalent to the target ontology?"). We present
three main results: (1) ontologies formulated in (two relevant versions of) the
description logic DL-Lite can be learned with polynomially many queries of
polynomial size; (2) this is not the case for ontologies formulated in the
description logic EL, even when only acyclic ontologies are admitted; and (3)
ontologies formulated in a fragment of EL related to the web ontology language
OWL 2 RL can be learned in polynomial time. We also show that neither
membership nor equivalence queries alone are sufficient in cases (1) and (3).



We study a unique network dataset including periodic surveys and electronic
logs of dyadic contacts via smartphones. The participants were a sample of
freshmen entering university in the Fall 2011. Their opinions on a variety of
political and social issues and lists of activities on campus were regularly
recorded at the beginning and end of each semester for the first three years of
study. We identify a behavioral network defined by call and text data, and a
cognitive network based on friendship nominations in ego-network surveys. Both
networks are limited to study participants. Since a wide range of attributes on
each node were collected in self-reports, we refer to these networks as
attribute-rich networks. We study whether student preferences for certain
attributes of friends can predict formation and dissolution of edges in both
networks. We introduce a method for computing student preferences for different
attributes which we use to predict link formation and dissolution. We then rank
these attributes according to their importance for making predictions. We find
that personal preferences, in particular political views, and preferences for
common activities help predict link formation and dissolution in both the
behavioral and cognitive networks.



We present an approach to automate the process of discovering optimization
methods, with a focus on deep learning architectures. We train a Recurrent
Neural Network controller to generate a string in a domain specific language
that describes a mathematical update equation based on a list of primitive
functions, such as the gradient, running average of the gradient, etc. The
controller is trained with Reinforcement Learning to maximize the performance
of a model after a few epochs. On CIFAR-10, our method discovers several update
rules that are better than many commonly used optimizers, such as Adam,
RMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two
new optimizers, named PowerSign and AddSign, which we show transfer well and
improve training on a variety of different tasks and architectures, including
ImageNet classification and Google's neural machine translation system.



We consider the problem of dense depth prediction from a sparse set of depth
measurements and a single RGB image. Since depth estimation from monocular
images alone is inherently ambiguous and unreliable, we introduce additional
sparse depth samples, which are either collected from a low-resolution depth
sensor or computed from SLAM, to attain a higher level of robustness and
accuracy. We propose the use of a single regression network to learn directly
from the RGB-D raw data, and explore the impact of number of depth samples on
prediction accuracy. Our experiments show that, as compared to using only RGB
images, the addition of 100 spatially random depth samples reduces the
prediction root-mean-square error by half in the NYU-Depth-v2 indoor dataset.
It also boosts the percentage of reliable prediction from 59% to 92% on the
more challenging KITTI driving dataset. We demonstrate two applications of the
proposed algorithm: serving as a plug-in module in SLAM to convert sparse maps
to dense maps, and creating much denser point clouds from low-resolution
LiDARs. Codes and video demonstration are publicly available.



E-commerce websites such as Amazon, Alibaba, Flipkart, and Walmart sell
billions of products. Machine learning (ML) algorithms involving products are
often used to improve the customer experience and increase revenue, e.g.,
product similarity, recommendation, and price estimation. The products are
required to be represented as features before training an ML algorithm. In this
paper, we propose an approach called MRNet-Product2Vec for creating generic
embeddings of products within an e-commerce ecosystem. We learn a dense and
low-dimensional embedding where a diverse set of signals related to a product
are explicitly injected into its representation. We train a Discriminative
Multi-task Bidirectional Recurrent Neural Network (RNN), where the input is a
product title fed through a Bidirectional RNN and at the output, product labels
corresponding to fifteen different tasks are predicted. The task set includes
several intrinsic characteristics about a product such as price, weight, size,
color, popularity, and material. We evaluate the proposed embedding
quantitatively and qualitatively. We demonstrate that they are almost as good
as sparse and extremely high-dimensional TF-IDF representation in spite of
having less than 3% of the TF-IDF dimension. We also use a multimodal
autoencoder for comparing products from different language-regions and show
preliminary yet promising qualitative results.



Automatic mesh-based shape generation is of great interest across a wide
range of disciplines, from industrial design to gaming, computer graphics and
various other forms of digital art. While most traditional methods focus on
primitive based model generation, advances in deep learning made it possible to
learn 3-dimensional geometric shape representations in an end-to-end manner.
However, most current deep learning based frameworks focus on the
representation and generation of voxel and point-cloud based shapes, making it
not directly applicable to design and graphics communities. This study
addresses the needs for automatic generation of mesh-based geometries, and
propose a novel framework that utilizes signed distance function representation
that generates detail preserving three-dimensional surface mesh by a deep
learning based approach.



This paper stands in the context of reinforcement learning with partial
observability and limited data. In this setting, we focus on the tradeoff
between asymptotic bias (suboptimality with unlimited data) and overfitting
(additional suboptimality due to limited data), and theoretically show that
while potentially increasing the asymptotic bias, a smaller state
representation decreases the risk of overfitting. Our analysis relies on
expressing the quality of a state representation by bounding L1 error terms of
the associated belief states. Theoretical results are empirically illustrated
when the state representation is a truncated history of observations. Finally,
we also discuss and empirically illustrate how using function approximators and
adapting the discount factor may enhance the tradeoff between asymptotic bias
and overfitting.



We propose a protocol to perform generalized quantum reinforcement learning
with quantum technologies. At variance with recent results on quantum
reinforcement learning with superconducting circuits [L. Lamata, Sci. Rep. 7,
1609 (2017)], in our current protocol coherent feedback during the learning
process is not required, enabling its implementation in a wide variety of
quantum systems. We consider diverse possible scenarios for an agent, an
environment, and a register that connects them, involving multiqubit and
multilevel systems, as well as open-system dynamics. We finally propose
possible implementations of this protocol in trapped ions and superconducting
circuits. The field of quantum reinforcement learning with quantum technologies
will enable enhanced quantum control, as well as more efficient machine
learning calculations.



Instrumenting and collecting annotated visual grasping datasets to train
modern machine learning algorithms can be extremely time-consuming and
expensive. An appealing alternative is to use off-the-shelf simulators to
render synthetic data for which ground-truth annotations are generated
automatically. Unfortunately, models trained purely on simulated data often
fail to generalize to the real world. We study how randomized simulated
environments and domain adaptation methods can be extended to train a grasping
system to grasp novel objects from raw monocular RGB images. We extensively
evaluate our approaches with a total of more than 25,000 physical test grasps,
studying a range of simulation conditions and domain adaptation methods,
including a novel extension of pixel-level domain adaptation that we term the
GraspGAN. We show that, by using synthetic data and domain adaptation, we are
able to reduce the number of real-world samples needed to achieve a given level
of performance by up to 50 times, using only randomly generated simulated
objects. We also show that by using only unlabeled real-world data and our
GraspGAN methodology, we obtain real-world grasping performance without any
real-world labels that is similar to that achieved with 939,777 labeled
real-world samples.



We introduce a general-purpose conditioning method for neural networks called
FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network
computation via a simple, feature-wise affine transformation based on
conditioning information. We show that FiLM layers are highly effective for
visual reasoning - answering image-related questions which require a
multi-step, high-level process - a task which has proven difficult for standard
deep learning methods that do not explicitly model reasoning. Specifically, we
show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error
for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are
robust to ablations and architectural modifications, and 4) generalize well to
challenging, new data from few examples or even zero-shot.



We present a method for efficient learning of control policies for multiple
related robotic motor skills. Our approach consists of two stages, joint
training and specialization training. During the joint training stage, a neural
network policy is trained with minimal information to disambiguate the motor
skills. This forces the policy to learn a common representation of the
different tasks. Then, during the specialization training stage we selectively
split the weights of the policy based on a per-weight metric that measures the
disagreement among the multiple tasks. By splitting part of the control policy,
it can be further trained to specialize to each task. To update the control
policy during learning, we use Trust Region Policy Optimization with
Generalized Advantage Function (TRPOGAE). We propose a modification to the
gradient update stage of TRPO to better accommodate multi-task learning
scenarios. We evaluate our approach on three continuous motor skill learning
problems in simulation: 1) a locomotion task where three single legged robots
with considerable difference in shape and size are trained to hop forward, 2) a
manipulation task where three robot manipulators with different sizes and joint
types are trained to reach different locations in 3D space, and 3) locomotion
of a two-legged robot, whose range of motion of one leg is constrained in
different ways. We compare our training method to three baselines. The first
baseline uses only joint training for the policy, the second trains independent
policies for each task, and the last randomly selects weights to split. We show
that our approach learns more efficiently than each of the baseline methods.



We analyse multimodal time-series data corresponding to weight, sleep and
steps measurements. We focus on predicting whether a user will successfully
achieve his/her weight objective. For this, we design several deep long
short-term memory (LSTM) architectures, including a novel cross-modal LSTM
(X-LSTM), and demonstrate their superiority over baseline approaches. The
X-LSTM improves parameter efficiency by processing each modality separately and
allowing for information flow between them by way of recurrent
cross-connections. We present a general hyperparameter optimisation technique
for X-LSTMs, which allows us to significantly improve on the LSTM and a prior
state-of-the-art cross-modal approach, using a comparable number of parameters.
Finally, we visualise the model's predictions, revealing implications about
latent variables in this task.



Self-supervised learning (SSL) is a reliable learning mechanism in which a
robot enhances its perceptual capabilities. Typically, in SSL a trusted,
primary sensor cue provides supervised training data to a secondary sensor cue.
In this article, a theoretical analysis is performed on the fusion of the
primary and secondary cue in a minimal model of SSL. A proof is provided that
determines the specific conditions under which it is favorable to perform
fusion. In short, it is favorable when (i) the prior on the target value is
strong or (ii) the secondary cue is sufficiently accurate. The theoretical
findings are validated with computational experiments. Subsequently, a
real-world case study is performed to investigate if fusion in SSL is also
beneficial when assumptions of the minimal model are not met. In particular, a
flying robot learns to map pressure measurements to sonar height measurements
and then fuses the two, resulting in better height estimation. Fusion is also
beneficial in the opposite case, when pressure is the primary cue. The analysis
and results are encouraging to study SSL fusion also for other robots and
sensors.



Transfer learning significantly accelerates the reinforcement learning
process by exploiting relevant knowledge from previous experiences. The problem
of optimally selecting source policies during the learning process is of great
importance yet challenging. There has been little theoretical analysis of this
problem. In this paper, we develop an optimal online method to select source
policies for reinforcement learning. This method formulates online source
policy selection as a multi-armed bandit problem and augments Q-learning with
policy reuse. We provide theoretical guarantees of the optimal selection
process and convergence to the optimal policy. In addition, we conduct
experiments on a grid-based robot navigation domain to demonstrate its
efficiency and robustness by comparing to the state-of-the-art transfer
learning method.



We present a robust multi-robot convoying approach that relies on visual
detection of the leading agent, thus enabling target following in unstructured
3-D environments. Our method is based on the idea of tracking-by-detection,
which interleaves efficient model-based object detection with temporal
filtering of image-based bounding box estimation. This approach has the
important advantage of mitigating tracking drift (i.e. drifting away from the
target object), which is a common symptom of model-free trackers and is
detrimental to sustained convoying in practice. To illustrate our solution, we
collected extensive footage of an underwater robot in ocean settings, and
hand-annotated its location in each frame. Based on this dataset, we present an
empirical comparison of multiple tracker variants, including the use of several
convolutional neural networks, both with and without recurrent connections, as
well as frequency-based model-free trackers. We also demonstrate the
practicality of this tracking-by-detection strategy in real-world scenarios by
successfully controlling a legged underwater robot in five degrees of freedom
to follow another robot's independent motion.



Cryptovirological augmentations present an immediate, incomparable threat.
Over the last decade, the substantial proliferation of crypto-ransomware has
had widespread consequences for consumers and organisations alike. Established
preventive measures perform well, however, the problem has not ceased. Reverse
engineering potentially malicious software is a cumbersome task due to platform
eccentricities and obfuscated transmutation mechanisms, hence requiring
smarter, more efficient detection strategies. The following manuscript presents
a novel approach for the classification of cryptographic primitives in compiled
binary executables using deep learning. The model blueprint, a DCNN, is
fittingly configured to learn from variable-length control flow diagnostics
output from a dynamic trace. To rival the size and variability of contemporary
data compendiums, hence feeding the model cognition, a methodology for the
procedural generation of synthetic cryptographic binaries is defined, utilising
core primitives from OpenSSL with multivariate obfuscation, to draw a vastly
scalable distribution. The library, CryptoKnight, rendered an algorithmic pool
of AES, RC4, Blowfish, MD5 and RSA to synthesis combinable variants which are
automatically fed in its core model. Converging at 91% accuracy, CryptoKnight
is successfully able to classify the sample algorithms with minimal loss.



In this paper we focus on developing a control algorithm for multi-terrain
tracked robots with flippers using a reinforcement learning (RL) approach. The
work is based on the deep deterministic policy gradient (DDPG) algorithm,
proven to be very successful in simple simulation environments. The algorithm
works in an end-to-end fashion in order to control the continuous position of
the flippers. This end-to-end approach makes it easy to apply the controller to
a wide array of circumstances, but the huge flexibility comes to the cost of an
increased difficulty of solution. The complexity of the task is enlarged even
more by the fact that real multi-terrain robots move in partially observable
environments. Notwithstanding these complications, being able to smoothly
control a multi-terrain robot can produce huge benefits in impaired people
daily lives or in search and rescue situations.



A new prior is proposed for representation learning, which can be combined
with other priors in order to help disentangling abstract factors from each
other. It is inspired by the phenomenon of consciousness seen as the formation
of a low-dimensional combination of a few concepts constituting a conscious
thought, i.e., consciousness as awareness at a particular time instant. This
provides a powerful constraint on the representation in that such
low-dimensional thought vectors can correspond to statements about reality
which are true, highly probable, or very useful for taking decisions. The fact
that a few elements of the current state can be combined into such a predictive
or useful statement is a strong constraint and deviates considerably from the
maximum likelihood approaches to modelling data and how states unfold in the
future based on an agent's actions. Instead of making predictions in the
sensory (e.g. pixel) space, the consciousness prior allows the agent to make
predictions in the abstract space, with only a few dimensions of that space
being involved in each of these predictions. The consciousness prior also makes
it natural to map conscious states to natural language utterances or to express
classical AI knowledge in the form of facts and rules, although the conscious
states may be richer than what can be expressed easily in the form of a
sentence, a fact or a rule.



Daily operation of a large-scale experiment is a challenging task,
particularly from perspectives of routine monitoring of quality for data being
taken. We describe an approach that uses Machine Learning for the automated
system to monitor data quality, which is based on partial use of data qualified
manually by detector experts. The system automatically classifies marginal
cases: both of good an bad data, and use human expert decision to classify
remaining "grey area" cases.
  This study uses collision data collected by the CMS experiment at LHC in
2010. We demonstrate that proposed workflow is able to automatically process at
least 20\% of samples without noticeable degradation of the result.



Automatically generating coherent and semantically meaningful text has many
applications in machine translation, dialogue systems, image captioning, etc.
Recently, by combining with policy gradient, Generative Adversarial Nets (GAN)
that use a discriminative model to guide the training of the generative model
as a reinforcement learning policy has shown promising results in text
generation. However, the scalar guiding signal is only available after the
entire text has been generated and lacks intermediate information about text
structure during the generative process. As such, it limits its success when
the length of the generated text samples is long (more than 20 words). In this
paper, we propose a new framework, called LeakGAN, to address the problem for
long text generation. We allow the discriminative net to leak its own
high-level extracted features to the generative net to further help the
guidance. The generator incorporates such informative signals into all
generation steps through an additional Manager module, which takes the
extracted features of current generated words and outputs a latent vector to
guide the Worker module for next-word generation. Our extensive experiments on
synthetic data and various real-world tasks with Turing test demonstrate that
LeakGAN is highly effective in long text generation and also improves the
performance in short text generation scenarios. More importantly, without any
supervision, LeakGAN would be able to implicitly learn sentence structures only
through the interaction between Manager and Worker.



Recently proposed models which learn to write computer programs from data use
either input/output examples or rich execution traces. Instead, we argue that a
novel alternative is to use a glass-box loss function, given as a program
itself that can be directly inspected. Glass-box optimization covers a wide
range of problems, from computing the greatest common divisor of two integers,
to learning-to-learn problems.
  In this paper, we present an intelligent search system which learns, given
the partial program and the glass-box problem, the probabilities over the space
of programs. We empirically demonstrate that our informed search procedure
leads to significant improvements compared to brute-force program search, both
in terms of accuracy and time. For our experiments we use rich context free
grammars inspired by number theory, text processing, and algebra. Our results
show that (i) performing 4 rounds of our framework typically solves about 70%
of the target problems, (ii) our framework can improve itself even in domain
agnostic scenarios, and (iii) it can solve problems that would be otherwise too
slow to solve with brute-force search.



Structured prediction is ubiquitous in applications of machine learning such
as knowledge extraction and natural language processing. Structure often can be
formulated in terms of logical constraints. We consider the question of how to
perform efficient active learning in the presence of logical constraints among
variables inferred by different classifiers. We propose several methods and
provide theoretical results that demonstrate the inappropriateness of employing
uncertainty guided sampling, a commonly used active learning method.
Furthermore, experiments on ten different datasets demonstrate that the methods
significantly outperform alternatives in practice. The results are of practical
significance in situations where labeled data is scarce.



We propose Object-oriented Neural Programming (OONP), a framework for
semantically parsing documents in specific domains. Basically, OONP reads a
document and parses it into a predesigned object-oriented data structure
(referred to as ontology in this paper) that reflects the domain-specific
semantics of the document. An OONP parser models semantic parsing as a decision
process: a neural net-based Reader sequentially goes through the document, and
during the process it builds and updates an intermediate ontology to summarize
its partial understanding of the text it covers. OONP supports a rich family of
operations (both symbolic and differentiable) for composing the ontology, and a
big variety of forms (both symbolic and differentiable) for representing the
state and the document. An OONP parser can be trained with supervision of
different forms and strength, including supervised learning (SL) ,
reinforcement learning (RL) and hybrid of the two. Our experiments on both
synthetic and real-world document parsing tasks have shown that OONP can learn
to handle fairly complicated ontology with training data of modest sizes.



With the use of ontologies in several domains such as semantic web,
information retrieval, artificial intelligence, the concept of similarity
measuring has become a very important domain of research. Therefore, in the
current paper, we propose our method of similarity measuring which uses the
Dijkstra algorithm to define and compute the shortest path. Then, we use this
one to compute the semantic distance between two concepts defined in the same
hierarchy of ontology. Afterward, we base on this result to compute the
semantic similarity. Finally, we present an experimental comparison between our
method and other methods of similarity measuring.



This paper provides an overview of evolutionary robotics techniques applied
to on-line distributed evolution for robot collectives -- namely, embodied
evolution. It provides a definition of embodied evolution as well as a thorough
description of the underlying concepts and mechanisms. The paper also presents
a comprehensive summary of research published in the field since its inception
(1999-2017), providing various perspectives to identify the major trends. In
particular, we identify a shift from considering embodied evolution as a
parallel search method within small robot collectives (fewer than 10 robots) to
embodied evolution as an on-line distributed learning method for designing
collective behaviours in swarm-like collectives. The paper concludes with a
discussion of applications and open questions, providing a milestone for past
and an inspiration for future research.



This paper is concerned with the problem of exact MAP inference in general
higher-order graphical models by means of a traditional linear programming
relaxation approach. In fact, the proof that we have developed in this paper is
a rather simple algebraic proof being made straightforward, above all, by the
introduction of two novel algebraic tools. Indeed, on the one hand, we
introduce the notion of delta-distribution which merely stands for the
difference of two arbitrary probability distributions, and which mainly serves
to alleviate the sign constraint inherent to a traditional probability
distribution. On the other hand, we develop an approximation framework of
general discrete functions by means of an orthogonal projection expressing in
terms of linear combinations of function margins with respect to a given
collection of point subsets, though, we rather exploit the latter approach for
the purpose of modeling locally consistent sets of discrete functions from a
global perspective. After that, as a first step, we develop from scratch the
expectation optimization framework which is nothing else than a reformulation,
on stochastic grounds, of the convex-hull approach, as a second step, we
develop the traditional LP relaxation of such an expectation optimization
approach, and we show that it enables to solve the MAP inference problem in
graphical models under rather general assumptions. Last but not least, we
describe an algorithm which allows to compute an exact MAP solution from a
perhaps fractional optimal (probability) solution of the proposed LP
relaxation.



Scholars and practitioners across domains are increasingly concerned with
algorithmic transparency and opacity, interrogating the values and assumptions
embedded in automated, black-boxed systems, particularly in user-generated
content platforms. I report from an ethnography of infrastructure in Wikipedia
to discuss an often understudied aspect of this topic: the local, contextual,
learned expertise involved in participating in a highly automated
social-technical environment. Today, the organizational culture of Wikipedia is
deeply intertwined with various data-driven algorithmic systems, which
Wikipedians rely on to help manage and govern the "anyone can edit"
encyclopedia at a massive scale. These bots, scripts, tools, plugins, and
dashboards make Wikipedia more efficient for those who know how to work with
them, but like all organizational culture, newcomers must learn them if they
want to fully participate. I illustrate how cultural and organizational
expertise is enacted around algorithmic agents by discussing two
autoethnographic vignettes, which relate my personal experience as a veteran in
Wikipedia. I present thick descriptions of how governance and gatekeeping
practices are articulated through and in alignment with these automated
infrastructures. Over the past 15 years, Wikipedian veterans and administrators
have made specific decisions to support administrative and editorial workflows
with automation in particular ways and not others. I use these cases of
Wikipedia's bot-supported bureaucracy to discuss several issues in the fields
of critical algorithms studies, critical data studies, and fairness,
accountability, and transparency in machine learning -- most principally
arguing that scholarship and practice must go beyond trying to "open up the
black box" of such systems and also examine sociocultural processes like
newcomer socialization.



This article discusses how the automation of tensor algorithms, based on A
Mathematics of Arrays and Psi Calculus, and a new way to represent numbers,
Unum Arithmetic, enables mechanically provable, scalable, portable, and more
numerically accurate software.



With the advancement of treatment modalities in radiation therapy, outcomes
haves greatly improved, but at the cost of increased treatment plan complexity
and planning time. The accurate prediction of dose distributions would
alleviate this issue by guiding clinical plan optimization to save time and
maintain high quality plans. We have developed a novel application of the fully
convolutional deep network model, U-net, for predicting dose from patient
contours. We show that with this model, we are able to accurately predict the
dose of prostate cancer patients, where the average dice similarity coefficient
is well over 0.9 when comparing the predicted vs. true isodose volumes between
0% and 100% of the prescription dose. The average differences in mean and max
dose for all structures were within 2.3% of the prescription dose.



In this dissertation the practical speech emotion recognition technology is
studied, including several cognitive related emotion types, namely fidgetiness,
confidence and tiredness. The high quality of naturalistic emotional speech
data is the basis of this research. The following techniques are used for
inducing practical emotional speech: cognitive task, computer game, noise
stimulation, sleep deprivation and movie clips.
  A practical speech emotion recognition system is studied based on Gaussian
mixture model. A two-class classifier set is adopted for performance
improvement under the small sample case. Considering the context information in
continuous emotional speech, a Gaussian mixture model embedded with Markov
networks is proposed.
  A further study is carried out for system robustness analysis. First, noise
reduction algorithm based on auditory masking properties is fist introduced to
the practical speech emotion recognition. Second, to deal with the complicated
unknown emotion types under real situation, an emotion recognition method with
rejection ability is proposed, which enhanced the system compatibility against
unknown emotion samples. Third, coping with the difficulties brought by a large
number of unknown speakers, an emotional feature normalization method based on
speaker-sensitive feature clustering is proposed. Fourth, by adding the
electrocardiogram channel, a bi-modal emotion recognition system based on
speech signals and electrocardiogram signals is first introduced.
  The speech emotion recognition methods studied in this dissertation may be
extended into the cross-language speech emotion recognition and the whispered
speech emotion recognition.



In this article, we investigate our early attempts at building an ontology
describing rehabilitation therapies following brain injury. These therapies are
wide-ranging, involving interventions of many different kinds. As a result,
these therapies are hard to describe. As well as restricting actual practice,
this is also a major impediment to evidence-based medicine as it is hard to
meaningfully compare two treatment plans.
  Ontology development requires significant effort from both ontologists and
domain experts. Knowledge elicited from domain experts forms the scope of the
ontology. The process of knowledge elicitation is expensive, consumes experts'
time and might have biases depending on the selection of the experts. Various
methodologies and techniques exist for enabling this knowledge elicitation,
including community groups and open development practices. A related problem is
that of defining scope. By defining the scope, we can decide whether a concept
(i.e. term) should be represented in the ontology. This is the opposite of
knowledge elicitation, in the sense that it defines what should not be in the
ontology. This can be addressed by pre-defining a set of competency questions.
  These approaches are, however, expensive and time-consuming. Here, we
describe our work toward an alternative approach, bootstrapping the ontology
from an initially small corpus of literature that will define the scope of the
ontology, expanding this to a set covering the domain, then using information
extraction to define an initial terminology to provide the basis and the
competencies for the ontology. Here, we discuss four approaches to building a
suitable corpus that is both sufficiently covering and precise.



In the research area of reinforcement learning (RL), frequently novel and
promising methods are developed and introduced to the RL community. However,
although many researchers are keen to apply their methods on real-world
problems, implementing such methods in real industry environments often is a
frustrating and tedious process. Generally, academic research groups have only
limited access to real industrial data and applications. For this reason, new
methods are usually developed, evaluated and compared by using artificial
software benchmarks. On one hand, these benchmarks are designed to provide
interpretable RL training scenarios and detailed insight into the learning
process of the method on hand. On the other hand, they usually do not share
much similarity with industrial real-world applications. For this reason we
used our industry experience to design a benchmark which bridges the gap
between freely available, documented, and motivated artificial benchmarks and
properties of real industrial problems. The resulting industrial benchmark (IB)
has been made publicly available to the RL community by publishing its Java and
Python code, including an OpenAI Gym wrapper, on Github. In this paper we
motivate and describe in detail the IB's dynamics and identify prototypic
experimental settings that capture common situations in real-world industry
control problems.



With the recent advancements in Artificial Intelligence (AI), various
organizations and individuals are debating about the progress of AI as a
blessing or a curse for the future of the society. This paper conducts an
investigation on how the public perceives the progress of AI by utilizing the
data shared on Twitter. Specifically, this paper performs a comparative
analysis on the understanding of users belonging to two categories -- general
AI-Tweeters (AIT) and expert AI-Tweeters (EAIT) who share posts about AI on
Twitter. Our analysis revealed that users from both the categories express
distinct emotions and interests towards AI. Users from both the categories
regard AI as positive and are optimistic about the progress of AI but the
experts are more negative than the general AI-Tweeters. Expert AI-Tweeters
share relatively large percentage of tweets about their personal news compared
to technical aspects of AI. However, the effects of automation on the future
are of primary concern to AIT than to EAIT. When the expert category is
sub-categorized, the emotion analysis revealed that students and industry
professionals have more insights in their tweets about AI than academicians.



Robot-assisted dressing offers an opportunity to benefit the lives of many
people with disabilities, such as some older adults. However, robots currently
lack common sense about the physical implications of their actions on people.
The physical implications of dressing are complicated by non-rigid garments,
which can result in a robot indirectly applying high forces to a person's body.
We present a deep recurrent model that, when given a proposed action by the
robot, predicts the forces a garment will apply to a person's body. We also
show that a robot can provide better dressing assistance by using this model
with model predictive control. The predictions made by our model only use
haptic and kinematic observations from the robot's end effector, which are
readily attainable. Collecting training data from real world physical
human-robot interaction can be time consuming, costly, and put people at risk.
Instead, we train our predictive model using data collected in an entirely
self-supervised fashion from a physics-based simulation. We evaluated our
approach with a PR2 robot that attempted to pull a hospital gown onto the arms
of 10 human participants. With a 0.2s prediction horizon, our controller
succeeded at high rates and lowered applied force while navigating the garment
around a persons fist and elbow without getting caught. Shorter prediction
horizons resulted in significantly reduced performance with the sleeve catching
on the participants' fists and elbows, demonstrating the value of our model's
predictions. These behaviors of mitigating catches emerged from our deep
predictive model and the controller objective function, which primarily
penalizes high forces.



Effective collaboration between a robot and a person requires natural
communication. When a robot travels with a human companion, the robot should be
able to explain its navigation behavior in natural language. This paper
explains how a cognitively-based, autonomous robot navigation system produces
informative, intuitive explanations for its decisions. Language generation here
is based upon the robot's commonsense, its qualitative reasoning, and its
learned spatial model. This approach produces natural explanations in real time
for a robot as it navigates in a large, complex indoor environment.



The reliable measurement of confidence in classifiers' predictions is very
important for many applications and is, therefore, an important part of
classifier design. Yet, although deep learning has received tremendous
attention in recent years, not much progress has been made in quantifying the
prediction confidence of neural network classifiers. Bayesian models offer a
mathematically grounded framework to reason about model uncertainty, but
usually come with prohibitive computational costs. In this paper we propose a
simple, scalable method to achieve a reliable confidence score, based on the
data embedding derived from the penultimate layer of the network. We
investigate two ways to achieve desirable embeddings, by using either a
distance-based loss or Adversarial Training. We then test the benefits of our
method when used for classification error prediction, weighting an ensemble of
classifiers, and novelty detection. In all tasks we show significant
improvement over traditional, commonly used confidence scores.



We report on an extensive study of the current benefits and limitations of
deep learning approaches to robot vision and introduce a novel dataset used for
our investigation. To avoid the biases in currently available datasets, we
consider a human-robot interaction setting to design a data-acquisition
protocol for visual object recognition on the iCub humanoid robot. Considering
the performance of off-the-shelf models trained on off-line large-scale image
retrieval datasets, we show the necessity for knowledge transfer. Indeed, we
analyze different ways in which this last step can be done, and identify the
major bottlenecks in robotics scenarios. By studying both object categorization
and identification tasks, we highlight the key differences between object
recognition in robotics and in image retrieval tasks, for which the considered
deep learning approaches have been originally designed. In a nutshell, our
results confirm also in the considered setting the remarkable improvements
yield by deep learning, while pointing to specific open challenges that need to
be addressed for seamless deployment in robotics.



The excellent performance of deep neural networks has enabled us to solve
several automatization problems, opening an era of autonomous devices. However,
current deep net architectures are heavy with millions of parameters and
require billions of floating point operations. Several works have been
developed to compress a pre-trained deep network to reduce memory footprint
and, possibly, computation. Instead of compressing a pre-trained network, in
this work, we propose a generic neural network layer structure employing
multilinear projection as the primary feature extractor. The proposed
architecture requires several times less memory as compared to the traditional
Convolutional Neural Networks (CNN), while inherits the similar design
principles of a CNN. In addition, the proposed architecture is equipped with
two computation schemes that enable computation reduction or scalability.
Experimental results show the effectiveness of our compact projection that
outperforms traditional CNN, while requiring far fewer parameters.



We propose a deep learning-based approach to the problem of premise
selection: selecting mathematical statements relevant for proving a given
conjecture. We represent a higher-order logic formula as a graph that is
invariant to variable renaming but still fully preserves syntactic and semantic
information. We then embed the graph into a vector via a novel embedding method
that preserves the information of edge ordering. Our approach achieves
state-of-the-art results on the HolStep dataset, improving the classification
accuracy from 83% to 90.3%.



Developing a safe and efficient collision avoidance policy for multiple
robots is challenging in the decentralized scenarios where each robot generate
its paths without observing other robots' states and intents. While other
distributed multi-robot collision avoidance systems exist, they often require
extracting agent-level features to plan a local collision-free action, which
can be computationally prohibitive and not robust. More importantly, in
practice the performance of these methods are much lower than their centralized
counterparts.
  We present a decentralized sensor-level collision avoidance policy for
multi-robot systems, which directly maps raw sensor measurements to an agent's
steering commands in terms of movement velocity. As a first step toward
reducing the performance gap between decentralized and centralized methods, we
present a multi-scenario multi-stage training framework to find an optimal
policy which is trained over a large number of robots on rich, complex
environments simultaneously using a policy gradient based reinforcement
learning algorithm. We validate the learned sensor-level collision avoidance
policy in a variety of simulated scenarios with thorough performance
evaluations and show that the final learned policy is able to find time
efficient, collision-free paths for a large-scale robot system. We also
demonstrate that the learned policy can be well generalized to new scenarios
that do not appear in the entire training period, including navigating a
heterogeneous group of robots and a large-scale scenario with 100 robots.
Videos are available at https://sites.google.com/view/drlmaca



Dexterous multi-fingered hands are extremely versatile and provide a generic
way to perform multiple tasks in human-centric environments. However,
effectively controlling them remains challenging due to their high
dimensionality and large number of potential contacts. Deep reinforcement
learning (DRL) provides a model-agnostic approach to control complex dynamical
systems, but has not been shown to scale to high-dimensional dexterous
manipulation. Furthermore, deployment of DRL on physical systems remains
challenging due to sample inefficiency. Thus, the success of DRL in robotics
has thus far been limited to simpler manipulators and tasks. In this work, we
show that model-free DRL with natural policy gradients can effectively scale up
to complex manipulation tasks with a high-dimensional 24-DoF hand, and solve
them from scratch in simulated experiments. Furthermore, with the use of a
small number of human demonstrations, the sample complexity can be
significantly reduced, and enable learning within the equivalent of a few hours
of robot experience. We demonstrate successful policies for multiple complex
tasks: object relocation, in-hand manipulation, tool use, and door opening.



Exploration in environments with sparse rewards has been a persistent problem
in reinforcement learning (RL). Many tasks are natural to specify with a sparse
reward, and manually shaping a reward function can result in suboptimal
performance. However, finding a non-zero reward is exponentially more difficult
with increasing task horizon or action dimensionality. This puts many
real-world tasks out of practical reach of RL methods. In this work, we use
demonstrations to overcome the exploration problem and successfully learn to
perform long-horizon, multi-step robotics tasks with continuous control such as
stacking blocks with a robot arm. Our method, which builds on top of Deep
Deterministic Policy Gradients and Hindsight Experience Replay, provides an
order of magnitude of speedup over RL on simulated robotics tasks. It is simple
to implement and makes only the additional assumption that we can collect a
small set of demonstrations. Furthermore, our method is able to solve tasks not
solvable by either RL or behavior cloning alone, and often ends up
outperforming the demonstrator policy.



This paper proposes a novel neural machine reading model for open-domain
question answering at scale. Existing machine comprehension models typically
assume that a short piece of relevant text containing answers is already
identified and given to the models, from which the models are designed to
extract answers. This assumption, however, is not realistic for building a
large-scale open-domain question answering system which requires both deep text
understanding and identifying relevant text from corpus simultaneously.
  In this paper, we introduce Neural Comprehensive Ranker (NCR) that integrates
both passage ranking and answer extraction in one single framework. A Q&A
system based on this framework allows users to issue an open-domain question
without needing to provide a piece of text that must contain the answer.
Experiments show that the unified NCR model is able to outperform the
states-of-the-art in both retrieval of relevant text and answer extraction.



The ability to deploy neural networks in real-world, safety-critical systems
is severely limited by the presence of adversarial examples: slightly perturbed
inputs that are misclassified by the network. In recent years, several
techniques have been proposed for training networks that are robust to such
examples; and each time stronger attacks have been devised, demonstrating the
shortcomings of existing defenses. This highlights a key difficulty in
designing an effective defense: the inability to assess a network's robustness
against future attacks. We propose to address this difficulty through formal
verification techniques. We construct ground truths: adversarial examples with
provably minimal perturbation. We demonstrate how ground truths can serve to
assess the effectiveness of attack techniques, by comparing the adversarial
examples produced to the ground truths; and also of defense techniques, by
measuring the increase in distortion to ground truths in the hardened network
versus the original. We use this technique to assess recently suggested attack
and defense techniques.



We present an optimised multi-modal dialogue agent for interactive learning
of visually grounded word meanings from a human tutor, trained on real
human-human tutoring data. Within a life-long interactive learning period, the
agent, trained using Reinforcement Learning (RL), must be able to handle
natural conversations with human users and achieve good learning performance
(accuracy) while minimising human effort in the learning process. We train and
evaluate this system in interaction with a simulated human tutor, which is
built on the BURCHAK corpus -- a Human-Human Dialogue dataset for the visual
learning task. The results show that: 1) The learned policy can coherently
interact with the simulated user to achieve the goal of the task (i.e. learning
visual attributes of objects, e.g. colour and shape); and 2) it finds a better
trade-off between classifier accuracy and tutoring costs than hand-crafted
rule-based policies, including ones with dynamic policies.



We present a multi-modal dialogue system for interactive learning of
perceptually grounded word meanings from a human tutor. The system integrates
an incremental, semantic parsing/generation framework - Dynamic Syntax and Type
Theory with Records (DS-TTR) - with a set of visual classifiers that are
learned throughout the interaction and which ground the meaning representations
that it produces. We use this system in interaction with a simulated human
tutor to study the effects of different dialogue policies and capabilities on
the accuracy of learned meanings, learning rates, and efforts/costs to the
tutor. We show that the overall performance of the learning agent is affected
by (1) who takes initiative in the dialogues; (2) the ability to express/use
their confidence level about visual attributes; and (3) the ability to process
elliptical and incrementally constructed dialogue turns. Ultimately, we train
an adaptive dialogue policy which optimises the trade-off between classifier
accuracy and tutoring costs.



We motivate and describe a new freely available human-human dialogue dataset
for interactive learning of visually grounded word meanings through ostensive
definition by a tutor to a learner. The data has been collected using a novel,
character-by-character variant of the DiET chat tool (Healey et al., 2003;
Mills and Healey, submitted) with a novel task, where a Learner needs to learn
invented visual attribute words (such as " burchak " for square) from a tutor.
As such, the text-based interactions closely resemble face-to-face conversation
and thus contain many of the linguistic phenomena encountered in natural,
spontaneous dialogue. These include self-and other-correction, mid-sentence
continuations, interruptions, overlaps, fillers, and hedges. We also present a
generic n-gram framework for building user (i.e. tutor) simulations from this
type of incremental data, which is freely available to researchers. We show
that the simulations produce outputs that are similar to the original data
(e.g. 78% turn match similarity). Finally, we train and evaluate a
Reinforcement Learning dialogue control agent for learning visually grounded
word meanings, trained from the BURCHAK corpus. The learned policy shows
comparable performance to a rule-based system built previously.



Enabling robots to autonomously navigate complex environments is essential
for real-world deployment. Prior methods approach this problem by having the
robot maintain an internal map of the world, and then use a localization and
planning method to navigate through the internal map. However, these approaches
often include a variety of assumptions, are computationally intensive, and do
not learn from failures. In contrast, learning-based methods improve as the
robot acts in the environment, but are difficult to deploy in the real-world
due to their high sample complexity. To address the need to learn complex
policies with few samples, we propose a generalized computation graph that
subsumes value-based model-free methods and model-based methods, with specific
instantiations interpolating between model-free and model-based. We then
instantiate this graph to form a navigation model that learns from raw images
and is sample efficient. Our simulated car experiments explore the design
decisions of our navigation model, and show our approach outperforms
single-step and $N$-step double Q-learning. We also evaluate our approach on a
real-world RC car and show it can learn to navigate through a complex indoor
environment with a few hours of fully autonomous, self-supervised training.
Videos of the experiments and code can be found at github.com/gkahn13/gcg



We present a novel framework for the automatic discovery and recognition of
human motion primitives from motion capture data. Human motion primitives are
discovered by optimizing the 'motion flux', a quantity which depends on the
motion of a group of skeletal joints. Models of each primitive category are
computed via non-parametric Bayes methods and recognition is performed based on
their geometric properties. A normalization of the primitives is proposed in
order to make them invariant with respect to anatomical variations and data
sampling rate. Using our framework we build a publicly available dataset of
human motion primitives based on motion capture sequences taken from well-known
datasets. We expect that our framework, by providing an objective way for
discovering and categorizing human motion, will be a useful tool in numerous
research fields related to Robotics including human inspired motion generation,
learning by demonstration, and intuitive human-robot interaction.



Execution monitor of high-level robot actions can be effectively improved by
visual monitoring the state of the world in terms of preconditions and
postconditions that hold before and after the execution of an action.
Furthermore a policy for searching where to look at, either for verifying the
relations that specify the pre and postconditions or to refocus in case of a
failure, can tremendously improve the robot execution in an uncharted
environment. It is now possible to strongly rely on visual perception in order
to make the assumption that the environment is observable, by the amazing
results of deep learning. In this work we present visual execution monitoring
for a robot executing tasks in an uncharted Lab environment. The execution
monitor interacts with the environment via a visual stream that uses two DCNN
for recognizing the objects the robot has to deal with and manipulate, and a
non-parametric Bayes estimation to discover the relations out of the DCNN
features. To recover from lack of focus and failures due to missed objects we
resort to visual search policies via deep reinforcement learning.



Speech recognition is largely taking advantage of deep learning, showing that
substantial benefits can be obtained by modern Recurrent Neural Networks
(RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which
typically reach state-of-the-art performance in many tasks thanks to their
ability to learn long-term dependencies and robustness to vanishing gradients.
Nevertheless, LSTMs have a rather complex design with three multiplicative
gates, that might impair their efficient implementation. An attempt to simplify
LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just
two multiplicative gates.
  This paper builds on these efforts by further revising GRUs and proposing a
simplified architecture potentially more suitable for speech recognition. The
contribution of this work is two-fold. First, we suggest to remove the reset
gate in the GRU design, resulting in a more efficient single-gate architecture.
Second, we propose to replace tanh with ReLU activations in the state update
equations. Results show that, in our implementation, the revised architecture
reduces the per-epoch training time with more than 30% and consistently
improves recognition performance across different tasks, input features, and
noisy conditions when compared to a standard GRU.



Using a recently proposed privacy definition of R\'enyi Differential Privacy
(RDP), we re-examine the inherent privacy of releasing a single sample from a
posterior distribution. We exploit the impact of the prior distribution in
mitigating the influence of individual data points. In particular, we focus on
sampling from an exponential family and specific generalized linear models,
such as logistic regression. We propose novel RDP mechanisms as well as
offering a new RDP analysis for an existing method in order to add value to the
RDP framework. Each method is capable of achieving arbitrary RDP privacy
guarantees, and we offer experimental results of their efficacy.



Deep Neural Networks (DNNs) require very large amounts of computation both
for training and for inference when deployed in the field. Many different
algorithms have been proposed to implement the most computationally expensive
layers of DNNs. Further, each of these algorithms has a large number of
variants, which offer different trade-offs of parallelism, data locality,
memory footprint, and execution time. In addition, specific algorithms operate
much more efficiently on specialized data layouts and formats.
  We state the problem of optimal primitive selection in the presence of data
format transformations, and show that it is NP-hard by demonstrating an
embedding in the Partitioned Boolean Quadratic Assignment problem (PBQP).
  We propose an analytic solution via a PBQP solver, and evaluate our approach
experimentally by optimizing several popular DNNs using a library of more than
70 DNN primitives, on an embedded platform and a general purpose platform. We
show experimentally that significant gains are possible versus the state of the
art vendor libraries by using a principled analytic solution to the problem of
layout selection in the presence of data format transformations.



Multi-agent path finding (MAPF) is a well-studied problem in artificial
intelligence, where one needs to find collision-free paths for agents with
given start and goal locations. In video games, agents of different types often
form teams. In this paper, we demonstrate the usefulness of MAPF algorithms
from artificial intelligence for moving such non-homogeneous teams in congested
video game environments.



Low dimensional embeddings that capture the main variations of interest in
collections of data are important for many applications. One way to construct
these embeddings is to acquire estimates of similarity from the crowd. However,
similarity is a multi-dimensional concept that varies from individual to
individual. Existing models for learning embeddings from the crowd typically
make simplifying assumptions such as all individuals estimate similarity using
the same criteria, the list of criteria is known in advance, or that the crowd
workers are not influenced by the data that they see. To overcome these
limitations we introduce Context Embedding Networks (CENs). In addition to
learning interpretable embeddings from images, CENs also model worker biases
for different attributes along with the visual context i.e. the visual
attributes highlighted by a set of images. Experiments on two noisy crowd
annotated datasets show that modeling both worker bias and visual context
results in more interpretable embeddings compared to existing approaches.



IQ tests are an accepted method for assessing human intelligence. The tests
consist of several parts that must be solved under a time constraint. Of all
the tested abilities, pattern recognition has been found to have the highest
correlation with general intelligence. This is primarily because pattern
recognition is the ability to find order in a noisy environment, a necessary
skill for intelligent agents. In this paper, we propose a convolutional neural
network (CNN) model for solving geometric pattern recognition problems. The CNN
receives as input multiple ordered input images and outputs the next image
according to the pattern. Our CNN is able to solve problems involving rotation,
reflection, color, size and shape patterns and score within the top 5% of human
performance.



Deep neural networks are increasingly being used in a variety of machine
learning applications applied to rich user data on the cloud. However, this
approach introduces a number of privacy and efficiency challenges, as the cloud
operator can perform secondary inferences on the available data. Recently,
advances in edge processing have paved the way for more efficient, and private,
data processing at the source for simple tasks and lighter models, though they
remain a challenge for larger, and more complicated models. In this paper, we
present a hybrid approach for breaking down large, complex deep models for
cooperative, privacy-preserving analytics. We do this by breaking down the
popular deep architectures and fine-tune them in a particular way. We then
evaluate the privacy benefits of this approach based on the information exposed
to the cloud service. We also asses the local inference cost of different
layers on a modern handset for mobile applications. Our evaluations show that
by using certain kind of fine-tuning and embedding techniques and at a small
processing costs, we can greatly reduce the level of information available to
unintended tasks applied to the data feature on the cloud, and hence achieving
the desired tradeoff between privacy and performance.



In this work, we propose a novel robot learning framework called Neural Task
Programming (NTP), which bridges the idea of few-shot learning from
demonstration and neural program induction. NTP takes as input a task
specification (e.g., video demonstration of a task) and recursively decomposes
it into finer sub-task specifications. These specifications are fed to a
hierarchical neural program, where bottom-level programs are callable
subroutines that interact with the environment. We validate our method in three
robot manipulation tasks. NTP achieves strong generalization across sequential
tasks that exhibit hierarchal and compositional structures. The experimental
results show that NTP learns to generalize well to- wards unseen tasks with
increasing lengths, variable topologies, and changing objectives.



Mobile Ad hoc Network (MANET) is an infrastructure-less network formed
between a set of mobile nodes. The discovery of services in MANET is a
challenging job due to the unique properties of network. In this paper, a novel
service discovery framework called Hybrid Association Rules Based Network Layer
Discovery of Services for Ad hoc Networks (HANDY) has been proposed. HANDY
provides three major research contributions. At first, it adopts a cross-layer
optimized design for discovery of services that is based on simultaneous
discovery of services and corresponding routes. Secondly, it provides a
multi-level ontology-based approach to describe the services. This resolves the
issue of semantic interoperability among the service consumers in a scalable
fashion. Finally, to further optimize the performance of the discovery process,
HANDY recommends exploiting the inherent associations present among the
services. These associations are used in two ways. First, periodic service
advertisements are performed based on these associations. In addition, when a
response of a service discovery request is generated, correlated services are
also attached with the response. The proposed service discovery scheme has been
implemented in JIST/SWANS simulator. The results demonstrate that the proposed
modifications give rise to improvement in hit ratio of the service consumers
and latency of discovery process.



A current challenge for data management systems is to support the
construction and maintenance of machine learning models over data that is
large, multi-dimensional, and evolving. While systems that could support these
tasks are emerging, the need to scale to distributed, streaming data requires
new models and algorithms. In this setting, as well as computational
scalability and model accuracy, we also need to minimize the amount of
communication between distributed processors, which is the chief component of
latency. We study Bayesian networks, the workhorse of graphical models, and
present a communication-efficient method for continuously learning and
maintaining a Bayesian network model over data that is arriving as a
distributed stream partitioned across multiple processors. We show a strategy
for maintaining model parameters that leads to an exponential reduction in
communication when compared with baseline approaches to maintain the exact MLE
(maximum likelihood estimation). Meanwhile, our strategy provides similar
prediction errors for the target distribution and for classification tasks.



Lifted Relational Neural Networks (LRNNs) describe relational domains using
weighted first-order rules which act as templates for constructing feed-forward
neural networks. While previous work has shown that using LRNNs can lead to
state-of-the-art results in various ILP tasks, these results depended on
hand-crafted rules. In this paper, we extend the framework of LRNNs with
structure learning, thus enabling a fully automated learning process. Similarly
to many ILP methods, our structure learning algorithm proceeds in an iterative
fashion by top-down searching through the hypothesis space of all possible Horn
clauses, considering the predicates that occur in the training examples as well
as invented soft concepts entailed by the best weighted rules found so far. In
the experiments, we demonstrate the ability to automatically induce useful
hierarchical soft concepts leading to deep LRNNs with a competitive predictive
power.



In the last few years, we have seen the rise of deep learning applications in
a broad range of chemistry research problems. Recently, we reported on the
development of Chemception, a deep convolutional neural network (CNN)
architecture for general-purpose small molecule property prediction. In this
work, we investigate the effects of systematically removing and adding basic
chemical information to the image channels of the 2D images used to train
Chemception. By augmenting images with only 3 additional basic chemical
information, we demonstrate that Chemception now outperforms contemporary deep
learning models trained on more sophisticated chemical representations
(molecular fingerprints) for the prediction of toxicity, activity, and
solvation free energy, as well as physics-based free energy simulation methods.
Thus, our work demonstrates that a firm grasp of first-principles chemical
knowledge is not a pre-requisite for deep learning models to accurately predict
chemical properties. Lastly, by altering the chemical information content in
the images, and examining the resulting performance of Chemception, we also
identify two different learning patterns in predicting toxicity/activity as
compared to solvation free energy, and these patterns suggest that Chemception
is learning about its tasks in the manner that is consistent with established
knowledge.



In this paper, we study two aspects of the variational autoencoder (VAE): the
prior distribution over the latent variables and its corresponding posterior.
First, we decompose the learning of VAEs into layerwise density estimation, and
argue that having a flexible prior is beneficial to both sample generation and
inference. Second, we analyze the family of inverse autoregressive flows
(inverse AF) and show that with further improvement, inverse AF could be used
as universal approximation to any complicated posterior. Our analysis results
in a unified approach to parameterizing a VAE, without the need to restrict
ourselves to use factorial Gaussians in the latent real space.



Recurrent neural networks have shown remarkable success in modeling
sequences. However low resource situations still adversely affect the
generalizability of these models. We introduce a new family of models, called
Lattice Recurrent Units (LRU), to address the challenge of learning deep
multi-layer recurrent models with limited resources. LRU models achieve this
goal by creating distinct (but coupled) flow of information inside the units: a
first flow along time dimension and a second flow along depth dimension. It
also offers a symmetry in how information can flow horizontally and vertically.
We analyze the effects of decoupling three different components of our LRU
model: Reset Gate, Update Gate and Projected State. We evaluate this family on
new LRU models on computational convergence rates and statistical efficiency.
Our experiments are performed on four publicly-available datasets, comparing
with Grid-LSTM and Recurrent Highway networks. Our results show that LRU has
better empirical computational convergence rates and statistical efficiency
values, along with learning more accurate language models.



We present a hybrid neural network and rule-based system that generates pop
music. Music produced by pure rule-based systems often sounds mechanical. Music
produced by machine learning sounds better, but still lacks hierarchical
temporal structure. We restore temporal hierarchy by augmenting machine
learning with a temporal production grammar, which generates the music's
overall structure and chord progressions. A compatible melody is then generated
by a conditional variational recurrent autoencoder. The autoencoder is trained
with eight-measure segments from a corpus of 10,000 MIDI files, each of which
has had its melody track and chord progressions identified heuristically. The
autoencoder maps melody into a multi-dimensional feature space, conditioned by
the underlying chord progression. A melody is then generated by feeding a
random sample from that space to the autoencoder's decoder, along with the
chord progression generated by the grammar. The autoencoder can make musically
plausible variations on an existing melody, suitable for recurring motifs. It
can also reharmonize a melody to a new chord progression, keeping the rhythm
and contour. The generated music compares favorably with that generated by
other academic and commercial software designed for the music-as-a-service
industry.



Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned
problem. We observe that the scaling-based weight space symmetry property in
rectified nonlinear network will cause this negative effect. Therefore, we
propose to constrain the incoming weights of each neuron to be unit-norm, which
is formulated as an optimization problem over Oblique manifold. A simple yet
efficient method referred to as projection based weight normalization (PBWN) is
also developed to solve this problem. PBWN executes standard gradient updates,
followed by projecting the updated weight back to Oblique manifold. This
proposed method has the property of regularization and collaborates well with
the commonly used batch normalization technique. We conduct comprehensive
experiments on several widely-used image datasets including CIFAR-10,
CIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art
convolutional neural networks, such as Inception, VGG and residual networks.
The results show that our method is able to improve the performance of DNNs
with different architectures consistently. We also apply our method to Ladder
network for semi-supervised learning on permutation invariant MNIST dataset,
and our method outperforms the state-of-the-art methods: we obtain test errors
as 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples,
respectively.



We present an approach for mobile robots to learn to navigate in
pedestrian-rich environments via raw depth inputs, in a social-compliant
manner. To achieve this, we adopt a generative adversarial imitation learning
(GAIL) strategy for motion planning, which improves upon a supervised policy
model pre-trained via behavior cloning. Our approach overcomes the
disadvantages of previous methods, as they heavily depend on the full knowledge
of the location and velocity information of nearby pedestrians, which not only
requires specific sensors but also consumes much computation time for
extracting such state information from raw sensor input. In this paper, our
proposed GAIL-based model performs directly on raw depth inputs and plans in
real-time. Experiments show that our GAIL-based approach greatly improves the
behavior of mobile robots from pure behavior cloning both safely and
efficiently. Real-world implementation also shows that our method is capable of
guiding autonomous vehicles to navigate in a social-compliant manner directly
through raw depth inputs.



Digital image segmentation is the process of assigning distinct labels to
different objects in a digital image, and the fuzzy segmentation algorithm has
been successfully used in the segmentation of images from a wide variety of
sources. However, the traditional fuzzy segmentation algorithm fails to segment
objects that are characterized by textures whose patterns cannot be
successfully described by simple statistics computed over a very restricted
area. In this paper, we propose an extension of the fuzzy segmentation
algorithm that uses adaptive textural affinity functions to perform the
segmentation of such objects on bidimensional images. The adaptive affinity
functions compute their appropriate neighborhood size as they compute the
texture descriptors surrounding the seed spels (spatial elements), according to
the characteristics of the texture being processed. The algorithm then segments
the image with an appropriate neighborhood for each object. We performed
experiments on mosaic images that were composed using images from the Brodatz
database, and compared our results with the ones produced by a recently
published texture segmentation algorithm, showing the applicability of our
method.



In this paper, we propose an information-theoretic exploration strategy for
stochastic, discrete multi-armed bandits that achieves optimal regret. Our
strategy is based on the value of information criterion. This criterion
measures the trade-off between policy information and obtainable rewards. High
amounts of policy information are associated with exploration-dominant searches
of the space and yield high rewards. Low amounts of policy information favor
the exploitation of existing knowledge. Information, in this criterion, is
quantified by a parameter that can be varied during search. We demonstrate that
a simulated-annealing-like update of this parameter, with a sufficiently fast
cooling schedule, leads to an optimal regret that is logarithmic with respect
to the number of episodes.



Next-generation wireless networks must support ultra-reliable, low-latency
communication and intelligently manage a massive number of Internet of Things
(IoT) devices in real-time, within a highly dynamic environment. This need for
stringent communication quality-of-service (QoS) requirements as well as mobile
edge and core intelligence can only be realized by integrating fundamental
notions of artificial intelligence (AI) and machine learning across the
wireless infrastructure and end-user devices. In this context, this paper
provides a comprehensive tutorial that introduces the main concepts of machine
learning, in general, and artificial neural networks (ANNs), in particular, and
their potential applications in wireless communications. For this purpose, we
present a comprehensive overview on a number of key types of neural networks
that include feed-forward, recurrent, spiking, and deep neural networks. For
each type of neural network, we present the basic architecture and training
procedure, as well as the associated challenges and opportunities. Then, we
provide an in-depth overview on the variety of wireless communication problems
that can be addressed using ANNs, ranging from communication using unmanned
aerial vehicles to virtual reality and edge caching.For each individual
application, we present the main motivation for using ANNs along with the
associated challenges while also providing a detailed example for a use case
scenario and outlining future works that can be addressed using ANNs. In a
nutshell, this article constitutes one of the first holistic tutorials on the
development of machine learning techniques tailored to the needs of future
wireless networks.



Machine learning algorithms for prediction are increasingly being used in
critical decisions affecting human lives. Various fairness formalizations, with
no firm consensus yet, are employed to prevent such algorithms from
systematically discriminating against people based on certain attributes
protected by law. The aim of this article is to survey how fairness is
formalized in the machine learning literature for the task of prediction and
present these formalizations with their corresponding notions of distributive
justice from the social sciences literature. We provide theoretical as well as
empirical critiques of these notions from the social sciences literature and
explain how these critiques limit the suitability of the corresponding fairness
formalizations to certain domains. We also suggest two notions of distributive
justice which address some of these critiques and discuss avenues for
prospective fairness formalizations.



A smart grid can be considered as a complex network where each node
represents a generation unit or a consumer. Whereas links can be used to
represent transmission lines. One way to study complex systems is by using the
agent-based modeling (ABM) paradigm. An ABM is a way of representing a complex
system of autonomous agents interacting with each other. Previously, a number
of studies have been presented in the smart grid domain making use of the ABM
paradigm. However, to the best of our knowledge, none of these studies have
focused on the specification aspect of ABM. An ABM specification is important
not only for understanding but also for replication of the model. In this
study, we focus on development as well as specification of ABM for smart grid.
We propose an ABM by using a combination of agent-based and complex
network-based approaches. For ABM specification, we use ODD and DREAM
specification approaches. We analyze these two specification approaches
qualitatively as well as quantitatively. Extensive experiments demonstrate that
DREAM is a most useful approach as compared with ODD for modeling as well as
for replication of models for smart grid.



In this paper we propose a function space approach to Representation Learning
and the analysis of the representation layers in deep learning architectures.
We show how to compute a weak-type Besov smoothness index that quantifies the
geometry of the clustering in the feature space. This approach was already
applied successfully to improve the performance of machine learning algorithms
such as the Random Forest and tree-based Gradient Boosting. Our experiments
demonstrate that in well-known and well-performing trained networks, the Besov
smoothness of the training set, measured in the corresponding hidden layer
feature map representation, increases from layer to layer. We also contribute
to the understanding of generalization by showing how the Besov smoothness of
the representations, decreases as we add more mis-labeling to the training
data. We hope this approach will contribute to the de-mystification of some
aspects of deep learning.



Many applications infer the structure of a probabilistic graphical model from
data to elucidate the relationships between variables. But how can we train
graphical models on a massive data set? In this paper, we show how to construct
coresets -compressed data sets which can be used as proxy for the original data
and have provably bounded worst case error- for Gaussian dependency networks
(DNs), i.e., cyclic directed graphical models over Gaussians, where the parents
of each variable are its Markov blanket. Specifically, we prove that Gaussian
DNs admit coresets of size independent of the size of the data set.
Unfortunately, this does not extend to DNs over members of the exponential
family in general. As we will prove, Poisson DNs do not admit small coresets.
Despite this worst-case result, we will provide an argument why our coreset
construction for DNs can still work well in practice on count data. To
corroborate our theoretical results, we empirically evaluated the resulting
Core DNs on real data sets. The results



An adversarial example is an example that has been adjusted to produce the
wrong label when presented to a system at test time. If adversarial examples
existed that could fool a detector, they could be used to (for example) wreak
havoc on roads populated with smart vehicles. Recently, we described our
difficulties creating physical adversarial stop signs that fool a detector.
More recently, Evtimov et al. produced a physical adversarial stop sign that
fools a proxy model of a detector. In this paper, we show that these physical
adversarial stop signs do not fool two standard detectors (YOLO and Faster
RCNN) in standard configuration. Evtimov et al.'s construction relies on a crop
of the image to the stop sign; this crop is then resized and presented to a
classifier. We argue that the cropping and resizing procedure largely
eliminates the effects of rescaling and of view angle. Whether an adversarial
attack is robust under rescaling and change of view direction remains moot. We
argue that attacking a classifier is very different from attacking a detector,
and that the structure of detectors - which must search for their own bounding
box, and which cannot estimate that box very accurately - likely makes it
difficult to make adversarial patterns. Finally, an adversarial pattern on a
physical object that could fool a detector would have to be adversarial in the
face of a wide family of parametric distortions (scale; view angle; box shift
inside the detector; illumination; and so on). Such a pattern would be of great
theoretical and practical interest. There is currently no evidence that such
patterns exist.



Monotonic policy improvement and off-policy learning are two main desirable
properties for reinforcement learning algorithms. In this paper, by lower
bounding the performance difference of two policies, we show that the monotonic
policy improvement is guaranteed from on- and off-policy mixture samples. An
optimization procedure which applies the proposed bound can be regarded as an
off-policy natural policy gradient method. In order to support the theoretical
result, we provide a trust region policy optimization method using experience
replay as a naive application of our bound, and evaluate its performance in two
classical benchmark problems.



The paper reports on some results concerning Aqvist's dyadic logic known as
system G, which is one of the most influential logics for reasoning with dyadic
obligations ("it ought to be the case that ... if it is the case that ...").
Although this logic has been known in the literature for a while, many of its
properties still await in-depth consideration. In this short paper we show:
that any formula in system G including nested modal operators is equivalent to
some formula with no nesting; that the universal modality introduced by Aqvist
in the first presentation of the system is definable in terms of the deontic
modality.



Deep neural networks have enabled progress in a wide variety of applications.
Growing the size of the neural network typically results in improved accuracy.
As model sizes grow, the memory and compute requirements for training these
models also increases. We introduce a technique to train deep neural networks
using half precision floating point numbers. In our technique, weights,
activations and gradients are stored in IEEE half-precision format.
Half-precision floating numbers have limited numerical range compared to
single-precision numbers. We propose two techniques to handle this loss of
information. Firstly, we recommend maintaining a single-precision copy of the
weights that accumulates the gradients after each optimizer step. This
single-precision copy is rounded to half-precision format during training.
Secondly, we propose scaling the loss appropriately to handle the loss of
information with half-precision gradients. We demonstrate that this approach
works for a wide variety of models including convolution neural networks,
recurrent neural networks and generative adversarial networks. This technique
works for large scale models with more than 100 million parameters trained on
large datasets. Using this approach, we can reduce the memory consumption of
deep learning models by nearly 2x. In future processors, we can also expect a
significant computation speedup using half-precision hardware units.



Explanation of the hot topic "multi-agent path finding".



Learning from expert demonstrations has received a lot of attention in
artificial intelligence and machine learning. The goal is to infer the
underlying reward function that an agent is optimizing given a set of
observations of the agent's behavior over time in a variety of circumstances,
the system state trajectories, and a plant model specifying the evolution of
the system state for different agent's actions. The system is often modeled as
a Markov decision process, that is, the next state depends only on the current
state and agent's action, and the the agent's choice of action depends only on
the current state. While the former is a Markovian assumption on the evolution
of system state, the later assumes that the target reward function is itself
Markovian. In this work, we explore learning a class of non-Markovian reward
functions, known in the formal methods literature as specifications. These
specifications offer better composition, transferability, and interpretability.
We then show that inferring the specification can be done efficiently without
unrolling the transition system. We demonstrate on a 2-d grid world example.



We propose a deep semantic characterization of space and motion categorically
from the viewpoint of grounding embodied human-object interactions. Our key
focus is on an ontological model that would be adept to formalisation from the
viewpoint of commonsense knowledge representation, relational learning, and
qualitative reasoning about space and motion in cognitive robotics settings. We
demonstrate key aspects of the space & motion ontology and its formalization as
a representational framework in the backdrop of select examples from a dataset
of everyday activities. Furthermore, focussing on human-object interaction data
obtained from RGBD sensors, we also illustrate how declarative
(spatio-temporal) reasoning in the (constraint) logic programming family may be
performed with the developed deep semantic abstractions.



We present Synkhronos, an extension to Theano for multi-GPU computations
leveraging data parallelism. Our framework provides automated execution and
synchronization across devices, allowing users to continue to write serial
programs without risk of race conditions. The NVIDIA Collective Communication
Library is used for high-bandwidth inter-GPU communication. Further
enhancements to the Theano function interface include input slicing (with
aggregation) and input indexing, which perform common data-parallel computation
patterns efficiently. One example use case is synchronous SGD, which has
recently been shown to scale well for a growing set of deep learning problems.
When training ResNet-50, we achieve a near-linear speedup of 7.5x on an NVIDIA
DGX-1 using 8 GPUs, relative to Theano-only code running a single GPU in
isolation. Yet Synkhronos remains general to any data-parallel computation
programmable in Theano. By implementing parallelism at the level of individual
Theano functions, our framework uniquely addresses a niche between manual
multi-device programming and prescribed multi-GPU training routines.



We propose Marve, a system for extracting measurement values, units, and
related words from natural language text. Marve uses conditional random fields
(CRF) to identify measurement values and units, followed by a rule-based system
to find related entities, descriptors and modifiers within a sentence. Sentence
tokens are represented by an undirected graphical model, and rules are based on
part-of-speech and word dependency patterns connecting values and units to
contextual words. Marve is unique in its focus on measurement context and early
experimentation demonstrates Marve's ability to generate high-precision
extractions with strong recall. We also discuss Marve's role in refining
measurement requirements for NASA's proposed HyspIRI mission, a hyperspectral
infrared imaging satellite that will study the world's ecosystems. In general,
our work with HyspIRI demonstrates the value of semantic measurement
extractions in characterizing quantitative discussion contained in large
corpuses of natural language text. These extractions accelerate broad,
cross-cutting research and expose scientists new algorithmic approaches and
experimental nuances. They also facilitate identification of scientific
opportunities enabled by HyspIRI leading to more efficient scientific
investment and research.



The Epicurean Philosophy is commonly thought as simplistic and hedonistic.
Here I discuss how this is a misconception and explore its link to
Reinforcement Learning. Based on the letters of Epicurus, I construct an
objective function for hedonism which turns out to be equivalent of the
Reinforcement Learning objective function when omitting the discount factor. I
then discuss how Plato and Aristotle 's views that can be also loosely linked
to Reinforcement Learning, as well as their weaknesses in relationship to it.
Finally, I emphasise the close affinity of the Epicurean views and the Bellman
equation.



The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is
the main computational bottleneck in spectral clustering. In this work, we
introduce a highly-scalable, spectrum-preserving graph sparsification algorithm
that enables to build ultra-sparse NN (u-NN) graphs with guaranteed
preservation of the original graph spectrums, such as the first few
eigenvectors of the original graph Laplacian. Our approach can immediately lead
to scalable spectral clustering of large data networks without sacrificing
solution quality. The proposed method starts from constructing low-stretch
spanning trees (LSSTs) from the original graphs, which is followed by
iteratively recovering small portions of "spectrally critical" off-tree edges
to the LSSTs by leveraging a spectral off-tree embedding scheme. To determine
the suitable amount of off-tree edges to be recovered to the LSSTs, an
eigenvalue stability checking scheme is proposed, which enables to robustly
preserve the first few Laplacian eigenvectors within the sparsified graph.
Additionally, an incremental graph densification scheme is proposed for
identifying extra edges that have been missing in the original NN graphs but
can still play important roles in spectral clustering tasks. Our experimental
results for a variety of well-known data sets show that the proposed method can
dramatically reduce the complexity of NN graphs, leading to significant
speedups in spectral clustering.



In Crowdfunding platforms, people turn their prototype ideas into real
products by raising money from the crowd, or invest in someone else's projects.
In reward-based crowdfunding platforms such as Kickstarter and Indiegogo,
selecting accurate reward delivery duration becomes crucial for creators,
backers, and platform providers to keep the trust between the creators and the
backers, and the trust between the platform providers and users. According to
Kickstarter, 35% backers did not receive rewards on time. Unfortunately, little
is known about on-time and late reward delivery projects, and there is no prior
work to estimate reward delivery duration. To fill the gap, in this paper, we
(i) extract novel features that reveal latent difficulty levels of project
rewards; (ii) build predictive models to identify whether a creator will
deliver all rewards in a project on time or not; and (iii) build a regression
model to estimate accurate reward delivery duration (i.e., how long it will
take to produce and deliver all the rewards). Experimental results show that
our models achieve good performance -- 82.5% accuracy, 78.1 RMSE, and 0.108
NRMSE at the first 5% of the longest reward delivery duration.



Although aviation accidents are rare, safety incidents occur more frequently
and require careful analysis for providing actionable recommendations to
improve safety. Automatically analyzing safety incidents using flight data is
challenging because of the absence of labels on timestep-wise events in a
flight, complexity of multi-dimensional data, and lack of scalable tools to
perform analysis over large number of events. In this work, we propose a
precursor mining algorithm that identifies correlated patterns in
multidimensional time series to explain an adverse event. Precursors are
valuable to systems health and safety monitoring in explaining and forecasting
anomalies. Current precursor mining methods suffer from poor scalability to
high dimensional time series data and in capturing long-term memory. We propose
an approach by combining multiple-instance learning (MIL) and deep recurrent
neural networks (DRNN) to take advantage of MIL's ability to model
weakly-supervised data and DRNN's ability to model long term memory processes,
to scale well to high dimensional data and to large volumes of data using GPU
parallelism. We apply the proposed method to find precursors and offer
explanations to high speed exceedance safety incidents using commercial flight
data.



We propose Bayesian hypernetworks: a framework for approximate Bayesian
inference in neural networks. A Bayesian hypernetwork, $h$, is a neural network
which learns to transform a simple noise distribution, $p(\epsilon) =
\mathcal{N}(0,I)$, to a distribution $q(\theta) \doteq q(h(\epsilon))$ over the
parameters $\theta$ of another neural network (the "primary network"). We train
$q$ with variational inference, using an invertible $h$ to enable efficient
estimation of the variational lower bound on the posterior $p(\theta |
\mathcal{D})$ via sampling. In contrast to most methods for Bayesian deep
learning, Bayesian hypernets can represent a complex multimodal approximate
posterior with correlations between parameters, while enabling cheap i.i.d.
sampling of $q(\theta)$. We demonstrate these qualitative advantages of
Bayesian hypernets, which also achieve competitive performance on a suite of
tasks that demonstrate the advantage of estimating model uncertainty, including
active learning and anomaly detection.



Deep neural networks are widely used for classification. These deep models
often suffer from a lack of interpretability -- they are particularly difficult
to understand because of their non-linear nature. As a result, neural networks
are often treated as "black box" models, and in the past, have been trained
purely to optimize the accuracy of predictions. In this work, we create a novel
network architecture for deep learning that naturally explains its own
reasoning for each prediction. This architecture contains an autoencoder and a
special prototype layer, where each unit of that layer stores a weight vector
that resembles an encoded training input. The encoder of the autoencoder allows
us to do comparisons within the latent space, while the decoder allows us to
visualize the learned prototypes. The training objective has four terms: an
accuracy term, a term that encourages every prototype to be similar to at least
one encoded input, a term that encourages every encoded input to be close to at
least one prototype, and a term that encourages faithful reconstruction by the
autoencoder. The distances computed in the prototype layer are used as part of
the classification process. Since the prototypes are learned during training,
the learned network naturally comes with explanations for each prediction, and
the explanations are loyal to what the network actually computes.



Algorithmic decision making process now affects many aspects of our lives.
Standard tools for machine learning, such as classification and regression, are
subject to the bias in data, and thus direct application of such off-the-shelf
tools could lead to a specific group being unfairly discriminated. Removing
sensitive attributes of data does not solve this problem because a
\textit{disparate impact} can arise when non-sensitive attributes and sensitive
attributes are correlated. Here, we study a fair machine learning algorithm
that avoids such a disparate impact when making a decision. Inspired by the
two-stage least squares method that is widely used in the field of economics,
we propose a two-stage algorithm that removes bias in the training data. The
proposed algorithm is conceptually simple. Unlike most of existing fair
algorithms that are designed for classification tasks, the proposed method is
able to (i) deal with regression tasks, (ii) combine explanatory attributes to
remove reverse discrimination, and (iii) deal with numerical sensitive
attributes. The performance and fairness of the proposed algorithm are
evaluated in simulations with synthetic and real-world datasets.



Social network analysis provides meaningful information about behavior of
network members that can be used in diverse applications such as
classification, link prediction, etc. however, network analysis is
computationally expensive because of feature learning for different
applications. In recent years, many researches have focused on feature learning
methods in social networks. Network embedding represents the network in a lower
dimensional representation space with the same properties which presents a
compressed representation of the input network. In this paper, we introduce a
novel algorithm named "CARE" for network embedding that can be used for
different types of networks including weighted, directed and complex. While
current methods try to preserve local neighborhood information of nodes, we
utilize local neighborhood and community information of network nodes to cover
both local and global structure of social networks. CARE builds customized
paths, which are consisted of local and global structure of network nodes, as a
basis for network embedding and uses skip-gram model to learn representation
vector of nodes. Then, stochastic gradient descent is used to optimize our
objective function and learn the final representation of nodes. Our method can
be scalable when new nodes are appended to network without information loss.
Parallelize generation of customized random walks is also used for speeding up
CARE. We evaluate the performance of CARE on multi label classification and
link prediction tasks. Experimental results on different networks indicate that
the proposed method outperforms others in both Micro-f1 and Macro-f1 measures
for different size of training data.



In order to autonomously learn wide repertoires of complex skills, robots
must be able to learn from their own autonomously collected data, without human
supervision. One learning signal that is always available for autonomously
collected data is prediction: if a robot can learn to predict the future, it
can use this predictive model to take actions to produce desired outcomes, such
as moving an object to a particular location. However, in complex open-world
scenarios, designing a representation for prediction is difficult. In this
work, we instead aim to enable self-supervised robotic learning through direct
video prediction: instead of attempting to design a good representation, we
directly predict what the robot will see next, and then use this model to
achieve desired goals. A key challenge in video prediction for robotic
manipulation is handling complex spatial arrangements such as occlusions. To
that end, we introduce a video prediction model that can keep track of objects
through occlusion by incorporating temporal skip-connections. Together with a
novel planning criterion and action space formulation, we demonstrate that this
model substantially outperforms prior work on video prediction-based control.
Our results show manipulation of objects not seen during training, handling
multiple objects, and pushing objects around obstructions. These results
represent a significant advance in the range and complexity of skills that can
be performed entirely with self-supervised robotic learning.



In this work, we propose an infinite restricted Boltzmann machine~(RBM),
whose maximum likelihood estimation~(MLE) corresponds to a constrained convex
optimization. We consider the Frank-Wolfe algorithm to solve the program, which
provides a sparse solution that can be interpreted as inserting a hidden unit
at each iteration, so that the optimization process takes the form of a
sequence of finite models of increasing complexity. As a side benefit, this can
be used to easily and efficiently identify an appropriate number of hidden
units during the optimization. The resulting model can also be used as an
initialization for typical state-of-the-art RBM training algorithms such as
contrastive divergence, leading to models with consistently higher test
likelihood than random initialization.



Policy evaluation or value function or Q-function approximation is a key
procedure in reinforcement learning (RL). It is a necessary component of policy
iteration and can be used for variance reduction in policy gradient methods.
Therefore its quality has a significant impact on most RL algorithms. Motivated
by manifold regularized learning, we propose a novel kernelized policy
evaluation method that takes advantage of the intrinsic geometry of the state
space learned from data, in order to achieve better sample efficiency and
higher accuracy in Q-function approximation. Applying the proposed method in
the Least-Squares Policy Iteration (LSPI) framework, we observe superior
performance compared to widely used parametric basis functions on two standard
benchmarks in terms of policy quality.



Flow is a new computational framework, built to support a key need triggered
by the rapid growth of autonomy in ground traffic: controllers for autonomous
vehicles in the presence of complex nonlinear dynamics in traffic. Leveraging
recent advances in deep Reinforcement Learning (RL), Flow enables the use of RL
methods such as policy gradient for traffic control and enables benchmarking
the performance of classical (including hand-designed) controllers with learned
policies (control laws). Flow integrates traffic microsimulator SUMO with deep
reinforcement learning library rllab and enables the easy design of traffic
tasks, including different networks configurations and vehicle dynamics. We use
Flow to develop reliable controllers for complex problems, such as controlling
mixed-autonomy traffic (involving both autonomous and human-driven vehicles) in
a ring road. For this, we first show that state-of-the-art hand-designed
controllers excel when in-distribution, but fail to generalize; then, we show
that even simple neural network policies can solve the stabilization task
across density settings and generalize to out-of-distribution settings.



This paper explains why deep learning can generalize well, despite large
capacity and possible algorithmic instability, nonrobustness, and sharp minima,
effectively addressing an open problem in the literature. Based on our
theoretical insight, this paper also proposes a family of new regularization
methods. Its simplest member was empirically shown to improve base models and
achieve competitive performance on MNIST and CIFAR-10 benchmarks. Moreover,
this paper presents both data-dependent and data-independent generalization
guarantees with improved convergence rates. Our results suggest several new
open areas of research.



Optical Character Recognition (OCR) has been a topic of interest for many
years. It is defined as the process of digitizing a document image into its
constituent characters. Despite decades of intense research, developing OCR
with capabilities comparable to that of human still remains an open challenge.
Due to this challenging nature, researchers from industry and academic circles
have directed their attentions towards Optical Character Recognition. Over the
last few years, the number of academic laboratories and companies involved in
research on Character Recognition has increased dramatically. This research
aims at summarizing the research so far done in the field of OCR. It provides
an overview of different aspects of OCR and discusses corresponding proposals
aimed at resolving issues of OCR.



Modern socio-technical systems are increasingly complex. A fundamental
problem is that the borders of such systems are often not well-defined
a-priori, which among other problems can lead to unwanted behavior during
runtime. Ideally, unwanted behavior should be prevented. If this is not
possible the system shall at least be able to help determine potential cause(s)
a-posterori, identify responsible parties and make them accountable for their
behavior. Recently, several algorithms addressing these concepts have been
proposed. However, the applicability of the corresponding approaches,
specifically their effectiveness and performance, is mostly unknown. Therefore,
in this paper, we propose ACCBench, a benchmark tool that allows to compare and
evaluate causality algorithms under a consistent setting. Furthermore, we
contribute an implementation of the two causality algorithms by G\"o{\ss}ler
and Metayer and G\"o{\ss}ler and Astefanoaei as well as of a policy compliance
approach based on some concepts of Main et al. Lastly, we conduct a case study
of an Intelligent Door Control System, which exposes concrete strengths and
weaknesses of all algorithms under different aspects. In the course of this, we
show that the effectiveness of the algorithms in terms of cause detection as
well as their performance differ to some extent. In addition, our analysis
reports on some qualitative aspects that should be considered when evaluating
each algorithm. For example, the human effort needed to configure the algorithm
and model the use case is analyzed.



Finding semantically rich and computer-understandable representations for
textual dialogues, utterances and words is crucial for dialogue systems (or
conversational agents), as their performance mostly depends on understanding
the context of conversations. Recent research aims at finding distributed
vector representations (embeddings) for words, such that semantically similar
words are relatively close within the vector-space. Encoding the "meaning" of
text into vectors is a current trend, and text can range from words, phrases
and documents to actual human-to-human conversations. In recent research
approaches, responses have been generated utilizing a decoder architecture,
given the vector representation of the current conversation. In this paper, the
utilization of embeddings for answer retrieval is explored by using
Locality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor
(ANN) model, to find similar conversations in a corpus and rank possible
candidates. Experimental results on the well-known Ubuntu Corpus (in English)
and a customer service chat dataset (in Dutch) show that, in combination with a
candidate selection method, retrieval-based approaches outperform generative
ones and reveal promising future research directions towards the usability of
such a system.



We develop a method for policy architecture search and adaptation via
gradient-free optimization which can learn to perform autonomous driving tasks.
By learning from both demonstration and environmental reward we develop a model
that can learn with relatively few early catastrophic failures. We first learn
an architecture of appropriate complexity to perceive aspects of world state
relevant to the expert demonstration, and then mitigate the effect of
domain-shift during deployment by adapting a policy demonstrated in a source
domain to rewards obtained in a target environment. We show that our approach
allows safer learning than baseline methods, offering a reduced cumulative
crash metric over the agent's lifetime as it learns to drive in a realistic
simulated environment.



We present PubMed 200k RCT, a new dataset based on PubMed for sequential
sentence classification. The dataset consists of approximately 200,000
abstracts of randomized controlled trials, totaling 2.3 million sentences. Each
sentence of each abstract is labeled with their role in the abstract using one
of the following classes: background, objective, method, result, or conclusion.
The purpose of releasing this dataset is twofold. First, the majority of
datasets for sequential short-text classification (i.e., classification of
short texts that appear in sequences) are small: we hope that releasing a new
large dataset will help develop more accurate algorithms for this task. Second,
from an application perspective, researchers need better tools to efficiently
skim through the literature. Automatically classifying each sentence in an
abstract would help researchers read abstracts more efficiently, especially in
fields where abstracts may be long, such as the medical field.



We propose a framework to understand the unprecedented performance and
robustness of deep neural networks using field theory. Correlations between the
weights within the same layer can be described by symmetries in that layer, and
networks generalize better if such symmetries are broken to reduce the
redundancies of the weights. Using a two parameter field theory, we find that
the network can break such symmetries itself towards the end of training in a
process commonly known in physics as spontaneous symmetry breaking. This
corresponds to a network generalizing itself without any user input layers to
break the symmetry, but by communication with adjacent layers. In the layer
decoupling limit applicable to residual networks (He et al., 2015), we show
that the remnant symmetries that survive the non-linear layers are
spontaneously broken. The Lagrangian for the non-linear and weight layers
together has striking similarities with the one in quantum field theory of a
scalar. Using results from quantum field theory we show that our framework is
able to explain many experimentally observed phenomena,such as training on
random labels with zero error (Zhang et al., 2017), the information bottleneck,
the phase transition out of it and gradient variance explosion (Shwartz-Ziv &
Tishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more.



In order for robots to perform mission-critical tasks, it is essential that
they are able to quickly adapt to changes in their environment as well as to
injuries and or other bodily changes. Deep reinforcement learning has been
shown to be successful in training robot control policies for operation in
complex environments. However, existing methods typically employ only a single
policy. This can limit the adaptability since a large environmental
modification might require a completely different behavior compared to the
learning environment. To solve this problem, we propose Map-based Multi-Policy
Reinforcement Learning (MMPRL), which aims to search and store multiple
policies that encode different behavioral features while maximizing the
expected reward in advance of the environment change. Thanks to these policies,
which are stored into a multi-dimensional discrete map according to its
behavioral feature, adaptation can be performed within reasonable time without
retraining the robot. An appropriate pre-trained policy from the map can be
recalled using Bayesian optimization. Our experiments show that MMPRL enables
robots to quickly adapt to large changes without requiring any prior knowledge
on the type of injuries that could occur. A highlight of the learned behaviors
can be found here: https://youtu.be/QwInbilXNOE .



Black-box risk scoring models permeate our lives, yet are typically
proprietary and opaque. We propose a transparent model distillation approach to
detect bias in such models. Model distillation was originally designed to
distill knowledge from a large, complex teacher model to a faster, simpler
student model without significant loss in prediction accuracy. We add a third
restriction - transparency. In this paper we use data sets that contain two
labels to train on: the risk score predicted by a black-box model, as well as
the actual outcome the risk score was intended to predict. This allows us to
compare models that predict each label. For a particular class of student
models - interpretable tree additive models with pairwise interactions (GA2Ms)
- we provide confidence intervals for the difference between the risk score and
actual outcome models. This presents a new method for detecting bias in
black-box risk scores by assessing if contributions of protected features to
the risk score are statistically different from their contributions to the
actual outcome.



In this paper, an original heuristic algorithm of empty vehicles management
in personal rapid transit network is presented. The algorithm is used for the
delivery of empty vehicles for waiting passengers, for balancing the
distribution of empty vehicles within the network, and for providing an empty
space for vehicles approaching a station. Each of these tasks involves a
decision on the trip that has to be done by a selected empty vehicle from its
actual location to some determined destination. The decisions are based on a
multi-parameter function involving a set of factors and thresholds. An
important feature of the algorithm is that it does not use any central database
of passenger input (demand) and locations of free vehicles. Instead, it is
based on the local exchange of data between stations: on their states and on
the vehicles they expect. Therefore, it seems well-tailored for a distributed
implementation. The algorithm is uniform, meaning that the same basic procedure
is used for multiple tasks using a task-specific set of parameters.



We describe the adaptation and refinement of a graphical user interface
designed to facilitate a Wizard-of-Oz (WoZ) approach to collecting human-robot
dialogue data. The data collected will be used to develop a dialogue system for
robot navigation. Building on an interface previously used in the development
of dialogue systems for virtual agents and video playback, we add templates
with open parameters which allow the wizard to quickly produce a wide variety
of utterances. Our research demonstrates that this approach to data collection
is viable as an intermediate step in developing a dialogue system for physical
robots in remote locations from their users - a domain in which the human and
robot need to regularly verify and update a shared understanding of the
physical environment. We show that our WoZ interface and the fixed set of
utterances and templates therein provide for a natural pace of dialogue with
good coverage of the navigation domain.



Learning-based approaches to robotic manipulation are limited by the
scalability of data collection and accessibility of labels. In this paper, we
present a multi-task domain adaptation framework for instance grasping in
cluttered scenes by utilizing simulated robot experiments. Our neural network
takes monocular RGB images and the instance segmentation mask of a specified
target object as inputs, and predicts the probability of successfully grasping
the specified object for each candidate motor command. The proposed transfer
learning framework trains a model for instance grasping in simulation and uses
a domain-adversarial loss to transfer the trained model to real robots using
indiscriminate grasping data, which is available both in simulation and the
real world. We evaluate our model in real-world robot experiments, comparing it
with alternative model architectures as well as an indiscriminate grasping
baseline.



This paper tackles two related questions at the heart of machine learning;
how can we predict if a minimum will generalize to the test set, and why does
stochastic gradient descent find minima that generalize well? Our work is
inspired by Zhang et al. (2017), who showed deep networks can easily memorize
randomly labeled training data, despite generalizing well when shown real
labels of the same inputs. We show here that the same phenomenon occurs in
small linear models. These observations are explained by evaluating the
Bayesian evidence, which penalizes sharp minima but is invariant to model
parameterization. We also explore the "generalization gap" between small and
large batch training, identifying an optimum batch size which maximizes the
test set accuracy. Interpreting stochastic gradient descent as a stochastic
differential equation, we identify a "noise scale" $g = \epsilon (\frac{N}{B} -
1) \approx \epsilon N/B$, where $\epsilon$ is the learning rate, $N$ training
set size and $B$ batch size. Consequently the optimum batch size is
proportional to the learning rate and the training set size, $B_{opt} \propto
\epsilon N$. We verify these predictions empirically.



Deep reinforcement learning (RL) has proven a powerful technique in many
sequential decision making domains. However, Robotics poses many challenges for
RL, most notably training on a physical system can be expensive and dangerous,
which has sparked significant interest in learning control policies using a
physics simulator. While several recent works have shown promising results in
transferring policies trained in simulation to the real world, they often do
not fully utilize the advantage of working with a simulator. In this work, we
exploit the full state observability in the simulator to train better policies
which take as input only partial observations (RGBD images). We do this by
employing an actor-critic training algorithm in which the critic is trained on
full states while the actor (or policy) gets rendered images as input. We show
experimentally on a range of simulated tasks that using these asymmetric inputs
significantly improves performance. Finally, we combine this method with domain
randomization and show real robot experiments for several tasks like picking,
pushing, and moving a block. We achieve this simulation to real world transfer
without training on any real world data.



Experience replay is a key technique behind many recent advances in deep
reinforcement learning. Allowing the agent to learn from earlier memories can
speed up learning and break undesirable temporal correlations. Despite its
wide-spread application, very little is understood about the properties of
experience replay. How does the amount of memory kept affect learning dynamics?
Does it help to prioritize certain experiences? In this paper, we address these
questions by formulating a dynamical systems ODE model of Q-learning with
experience replay. We derive analytic solutions of the ODE for a simple
setting. We show that even in this very simple setting, the amount of memory
kept can substantially affect the agent's performance. Too much or too little
memory both slow down learning. Moreover, we characterize regimes where
prioritized replay harms the agent's learning. We show that our analytic
solutions have excellent agreement with experiments. Finally, we propose a
simple algorithm for adaptively changing the memory buffer size which achieves
consistently good empirical performance.



Social dilemmas, where mutual cooperation can lead to high payoffs but
participants face incentives to cheat, are ubiquitous in multi-agent
interaction. We wish to construct agents that cooperate with pure cooperators,
avoid exploitation by pure defectors, and incentivize cooperation from the
rest. However, often the actions taken by a partner are (partially) unobserved
or the consequences of individual actions are hard to predict. We show that in
a large class of games good strategies can be constructed by conditioning one's
behavior solely on outcomes (ie. one's past rewards). We call this
consequentialist conditional cooperation. We show how to construct such
strategies using deep reinforcement learning techniques and demonstrate, both
analytically and experimentally, that they are effective in social dilemmas
beyond simple matrix games. We also show the limitations of relying purely on
consequences and discuss the need for understanding both the consequences of
and the intentions behind an action.



This paper presents a novel differential evolution algorithm for protein
folding optimization that is applied to a three-dimensional AB off-lattice
model. The proposed algorithm includes two new mechanisms. A local search is
used to improve convergence speed and to reduce the runtime complexity of the
energy calculation. For this purpose, a local movement is introduced within the
local search. The designed evolutionary algorithm has fast convergence and,
therefore, when it is trapped into local optimum or a relatively good solution
is located, it is hard to locate a better similar solution. The similar
solution is different from the good solution in only a few components. A
component reinitialization method is designed to mitigate this problem. Both
the new mechanisms and the proposed algorithm were analyzed on well-known
amino-acid sequences that are used frequently in the literature. Experimental
results show that the employed new mechanisms improve the efficiency of our
algorithm and the proposed algorithm is superior to other state-of-the-art
algorithms. It obtained a hit ratio of 100 % for sequences up to 18 monomers
within a budget of $10^{11}$ solution evaluations. New best-known solutions
were obtained for most of the sequences. The existence of the symmetric
best-known solutions is also demonstrated in the paper.



In this study we developed an automated system that evaluates speech and
language features from audio recordings of neuropsychological examinations of
92 subjects in the Framingham Heart Study. A total of 265 features were used in
an elastic-net regularized binomial logistic regression model to classify the
presence of cognitive impairment, and to select the most predictive features.
We compared performance with a demographic model from 6,258 subjects in the
greater study cohort (0.79 AUC), and found that a system that incorporated both
audio and text features performed the best (0.92 AUC), with a True Positive
Rate of 29% (at 0% False Positive Rate) and a good model fit (Hosmer-Lemeshow
test > 0.05). We also found that decreasing pitch and jitter, shorter segments
of speech, and responses phrased as questions were positively associated with
cognitive impairment.



The mathematical model underlying the Neural Engineering Framework (NEF)
expresses neuronal input as a linear combination of synaptic currents. However,
in biology, synapses are not perfect current sources and are thus nonlinear.
Detailed synapse models are based on channel conductances instead of currents,
which require independent handling of excitatory and inhibitory synapses. This,
in particular, significantly affects the influence of inhibitory signals on the
neuronal dynamics. In this technical report we first summarize the relevant
portions of the NEF and conductance-based synapse models. We then discuss a
na\"ive translation between populations of LIF neurons with current- and
conductance-based synapses based on an estimation of an average membrane
potential. Experiments show that this simple approach works relatively well for
feed-forward communication channels, yet performance degrades for NEF networks
describing more complex dynamics, such as integration.



Similar to convolution neural networks, recurrent neural networks (RNNs)
typically suffer from over-parameterization. Quantizing bit-widths of weights
and activations results in runtime efficiency on hardware, yet it often comes
at the cost of reduced accuracy. This paper proposes a quantization approach
that increases model size with bit-width reduction. This approach will allow
networks to perform at their baseline accuracy while still maintaining the
benefits of reduced precision and overall model size reduction.



The use of random perturbations of ground truth data, such as random
translation or scaling of bounding boxes, is a common heuristic used for data
augmentation that has been shown to prevent overfitting and improve
generalization. Since the design of data augmentation is largely guided by
reported best practices, it is difficult to understand if those design choices
are optimal. To provide a more principled perspective, we develop a
game-theoretic interpretation of data augmentation in the context of object
detection. We aim to find an optimal adversarial perturbations of the ground
truth data (i.e., the worst case perturbations) that forces the object bounding
box predictor to learn from the hardest distribution of perturbed examples for
better test-time performance. We establish that the game theoretic solution,
the Nash equilibrium, provides both an optimal predictor and optimal data
augmentation distribution. We show that our adversarial method of training a
predictor can significantly improve test time performance for the task of
object detection. On the ImageNet object detection task, our adversarial
approach improves performance by over 16\% compared to the best performing data
augmentation method



SGD (Stochastic Gradient Descent) is a popular algorithm for large scale
optimization problems due to its low iterative cost. However, SGD can not
achieve linear convergence rate as FGD (Full Gradient Descent) because of the
inherent gradient variance. To attack the problem, mini-batch SGD was proposed
to get a trade-off in terms of convergence rate and iteration cost. In this
paper, a general CVI (Convergence-Variance Inequality) equation is presented to
state formally the interaction of convergence rate and gradient variance. Then
a novel algorithm named SSAG (Stochastic Stratified Average Gradient) is
introduced to reduce gradient variance based on two techniques, stratified
sampling and averaging over iterations that is a key idea in SAG (Stochastic
Average Gradient). Furthermore, SSAG can achieve linear convergence rate of
$\mathcal {O}((1-\frac{\mu}{8CL})^k)$ at smaller storage and iterative costs,
where $C\geq 2$ is the category number of training data. This convergence rate
depends mainly on the variance between classes, but not on the variance within
the classes. In the case of $C\ll N$ ($N$ is the training data size), SSAG's
convergence rate is much better than SAG's convergence rate of $\mathcal
{O}((1-\frac{\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG
and many other algorithms.



Identifying arbitrary topologies of power networks in real time is a
computationally hard problem due to the number of hypotheses that grows
exponentially with the network size. A new "Learning-to-Infer" variational
inference method is developed for efficient inference of every line status in
the network. Optimizing the variational model is transformed to and solved as a
discriminative learning problem based on Monte Carlo samples generated with
power flow simulations. A major advantage of the developed Learning-to-Infer
method is that the labeled data used for training can be generated in an
arbitrarily large amount fast and at very little cost. As a result, the power
of offline training is fully exploited to learn very complex classifiers for
effective real-time topology identification. The proposed methods are evaluated
in the IEEE 30, 118 and 300 bus systems. Excellent performance in identifying
arbitrary power network topologies in real time is achieved even with
relatively simple variational models and a reasonably small amount of data.



Deep neural networks are powerful learning models that achieve
state-of-the-art performance on many computer vision, speech, and language
processing tasks. In this paper, we study a fundamental question that arises
when designing deep network architectures: Given a target network architecture
can we design a smaller network architecture that approximates the operation of
the target network? The question is, in part, motivated by the challenge of
parameter reduction (compression) in modern deep neural networks, as the ever
increasing storage and memory requirements of these networks pose a problem in
resource constrained environments.
  In this work, we focus on deep convolutional neural network architectures,
and propose a novel randomized tensor sketching technique that we utilize to
develop a unified framework for approximating the operation of both the
convolutional and fully connected layers. By applying the sketching technique
along different tensor dimensions, we design changes to the convolutional and
fully connected layers that substantially reduce the number of effective
parameters in a network. We show that the resulting smaller network can be
trained directly, and has a classification accuracy that is comparable to the
original network.



Due to the lack of enough generalization in the state-space, common methods
in Reinforcement Learning (RL) suffer from slow learning speed especially in
the early learning trials. This paper introduces a model-based method in
discrete state-spaces for increasing learning speed in terms of required
experience (but not required computational time) by exploiting generalization
in the experiences of the subspaces. A subspace is formed by choosing a subset
of features in the original state representation (full-space). Generalization
and faster learning in a subspace are due to many-to-one mapping of experiences
from the full-space to each state in the subspace. Nevertheless, due to
inherent perceptual aliasing in the subspaces, the policy suggested by each
subspace does not generally converge to the optimal policy. Our approach,
called Model Based Learning with Subspaces (MoBLeS), calculates confidence
intervals of the estimated Q-values in the full-space and in the subspaces.
These confidence intervals are used in the decision making, such that the agent
benefits the most from the possible generalization while avoiding from
detriment of the perceptual aliasing in the subspaces. Convergence of MoBLeS to
the optimal policy is theoretically investigated. Additionally, we show through
several experiments that MoBLeS improves the learning speed in the early
trials.



We study transfer learning in convolutional network architectures applied to
the task of recognizing audio, such as environmental sound events and speech
commands. Our key finding is that not only is it possible to transfer
representations from an unrelated task like environmental sound classification
to a voice-focused task like speech command recognition, but also that doing so
improves accuracies significantly. We also investigate the effect of increased
model capacity for transfer learning audio, by first validating known results
from the field of Computer Vision of achieving better accuracies with
increasingly deeper networks on two audio datasets: UrbanSound8k and the newly
released Google Speech Commands dataset. Then we propose a simple multiscale
input representation using dilated convolutions and show that it is able to
aggregate larger contexts and increase classification performance. Further, the
models trained using a combination of transfer learning and multiscale input
representations need only 40% of the training data to achieve similar
accuracies as a freshly trained model with 100% of the training data. Finally,
we demonstrate a positive interaction effect for the multiscale input and
transfer learning, making a case for the joint application of the two
techniques.



The study of representations invariant to common transformations of the data
is important to learning. Most techniques have focused on local approximate
invariance implemented within expensive optimization frameworks lacking
explicit theoretical guarantees. In this paper, we study kernels that are
invariant to a unitary group while having theoretical guarantees in addressing
the important practical issue of unavailability of transformed versions of
labelled data. A problem we call the Unlabeled Transformation Problem which is
a special form of semi-supervised learning and one-shot learning. We present a
theoretically motivated alternate approach to the invariant kernel SVM based on
which we propose Max-Margin Invariant Features (MMIF) to solve this problem. As
an illustration, we design an framework for face recognition and demonstrate
the efficacy of our approach on a large scale semi-synthetic dataset with
153,000 images and a new challenging protocol on Labelled Faces in the Wild
(LFW) while out-performing strong baselines.



This paper presents a practical approach for identifying unknown mechanical
parameters, such as mass and friction models of manipulated rigid objects or
actuated robotic links, in a succinct manner that aims to improve the
performance of policy search algorithms. Key features of this approach are the
use of off-the-shelf physics engines and the adaptation of a black-box Bayesian
optimization framework for this purpose. The physics engine is used to
reproduce in simulation experiments that are performed on a real robot, and the
mechanical parameters of the simulated system are automatically fine-tuned so
that the simulated trajectories match with the real ones. The optimized model
is then used for learning a policy in simulation, before safely deploying it on
the real robot. Given the well-known limitations of physics engines in modeling
real-world objects, it is generally not possible to find a mechanical model
that reproduces in simulation the real trajectories exactly. Moreover, there
are many scenarios where a near-optimal policy can be found without having a
perfect knowledge of the system. Therefore, searching for a perfect model may
not be worth the computational effort in practice. The proposed approach aims
then to identify a model that is good enough to approximate the value of a
locally optimal policy with a certain confidence, instead of spending all the
computational resources on searching for the most accurate model. Empirical
evaluations, performed in simulation and on a real robotic manipulation task,
show that model identification via physics engines can significantly boost the
performance of policy search algorithms that are popular in robotics, such as
TRPO, PoWER and PILCO, with no additional real-world data.



This paper describes a novel text-to-speech (TTS) technique based on deep
convolutional neural networks (CNN), without any recurrent units. Recurrent
neural network (RNN) has been a standard technique to model sequential data
recently, and this technique has been used in some cutting-edge neural TTS
techniques. However, training RNN component often requires a very powerful
computer, or very long time typically several days or weeks. Recent other
studies, on the other hand, have shown that CNN-based sequence synthesis can be
much faster than RNN-based techniques, because of high parallelizability. The
objective of this paper is to show an alternative neural TTS system, based only
on CNN, that can alleviate these economic costs of training. In our experiment,
the proposed Deep Convolutional TTS can be sufficiently trained only in a night
(15 hours), using an ordinary gaming PC equipped with two GPUs, while the
quality of the synthesized speech was almost acceptable.



Optimization problems pervade essentially every scientific discipline and
industry. Many such problems require finding a solution that maximizes the
number of constraints satisfied. Often, these problems are particularly
difficult to solve because they belong to the NP-hard class, namely algorithms
that always find a solution in polynomial time are not known. Over the past
decades, research has focused on developing heuristic approaches that attempt
to find an approximation to the solution. However, despite numerous research
efforts, in many cases even approximations to the optimal solution are hard to
find, as the computational time for further refining a candidate solution grows
exponentially with input size. Here, we show a non-combinatorial approach to
hard optimization problems that achieves an exponential speed-up and finds
better approximations than the current state-of-the-art. First, we map the
optimization problem into a boolean circuit made of specially designed,
self-organizing logic gates, which can be built with (non-quantum) electronic
components; the equilibrium points of the circuit represent the approximation
to the problem at hand. Then, we solve its associated non-linear ordinary
differential equations numerically, towards the equilibrium points. We
demonstrate this exponential gain by comparing a sequential MatLab
implementation of our solver with the winners of the 2016 Max-SAT competition
on a variety of hard optimization instances. We show empirical evidence that
our solver scales linearly with the size of the problem, both in time and
memory, and argue that this property derives from the collective behavior of
the simulated physical circuit. Our approach can be applied to other types of
optimization problems and the results presented here have far-reaching
consequences in many fields.



Data and knowledge representation are fundamental concepts in machine
learning. The quality of the representation impacts the performance of the
learning model directly. Feature learning transforms or enhances raw data to
structures that are effectively exploited by those models. In recent years,
several works have been using complex networks for data representation and
analysis. However, no feature learning method has been proposed for such
category of techniques. Here, we present an unsupervised feature learning
mechanism that works on datasets with binary features. First, the dataset is
mapped into a feature--sample network. Then, a multi-objective optimization
process selects a set of new vertices to produce an enhanced version of the
network. The new features depend on a nonlinear function of a combination of
preexisting features. Effectively, the process projects the input data into a
higher-dimensional space. To solve the optimization problem, we design two
metaheuristics based on the lexicographic genetic algorithm and the improved
strength Pareto evolutionary algorithm (SPEA2). We show that the enhanced
network contains more information and can be exploited to improve the
performance of machine learning methods. The advantages and disadvantages of
each optimization strategy are discussed.



Graphs (networks) are ubiquitous and allow us to model entities (nodes) and
the dependencies (edges) between them. Learning a useful feature representation
from graph data lies at the heart and success of many machine learning tasks
such as classification, anomaly detection, link prediction, among many others.
Many existing techniques use random walks as a basis for learning features or
estimating the parameters of a graph model for a downstream prediction task.
Examples include recent node embedding methods such as DeepWalk, node2vec, as
well as graph-based deep learning algorithms. However, the simple random walk
used by these methods is fundamentally tied to the identity of the node. This
has three main disadvantages. First, these approaches are inherently
transductive and do not generalize to unseen nodes and other graphs. Second,
they are not space-efficient as a feature vector is learned for each node which
is impractical for large graphs. Third, most of these approaches lack support
for attributed graphs.
  To make these methods more generally applicable, we propose a framework for
inductive network representation learning based on the notion of attributed
random walk that is not tied to node identity and is instead based on learning
a function $\Phi : \mathrm{\rm \bf x} \rightarrow w$ that maps a node attribute
vector $\mathrm{\rm \bf x}$ to a type $w$. This framework serves as a basis for
generalizing existing methods such as DeepWalk, node2vec, and many other
previous methods that leverage traditional random walks.



We consider the composition optimization with two expected-value functions in
the form of $\frac{1}{n}\sum\nolimits_{i = 1}^n F_i(\frac{1}{m}\sum\nolimits_{j
= 1}^m G_j(x))+R(x)$, { which formulates many important problems in statistical
learning and machine learning such as solving Bellman equations in
reinforcement learning and nonlinear embedding}. Full Gradient or classical
stochastic gradient descent based optimization algorithms are unsuitable or
computationally expensive to solve this problem due to the inner expectation
$\frac{1}{m}\sum\nolimits_{j = 1}^m G_j(x)$. We propose a duality-free based
stochastic composition method that combines variance reduction methods to
address the stochastic composition problem. We apply SVRG and SAGA based
methods to estimate the inner function, and duality-free method to estimate the
outer function. We prove the linear convergence rate not only for the convex
composition problem, but also for the case that the individual outer functions
are non-convex while the objective function is strongly-convex. We also provide
the results of experiments that show the effectiveness of our proposed methods.



The Advent of the Internet-of-Things (IoT) paradigm has brought opportunities
to solve many real-world problems. Energy management, for example, has
attracted huge interest from academia, industries, governments and regulatory
bodies. It involves collecting energy usage data, analyzing it, and optimizing
the energy consumption by applying control strategies. However, in industrial
environments, performing such optimization is not trivial. The changes in
business rules, process control, and customer requirements make it much more
challenging. In this paper, a Semantic Rules Engine (SRE) for industrial
gateways is presented that allows implementing dynamic and flexible rule-based
control strategies. It is simple, expressive, and allows managing rules
on-the-fly without causing any service interruption. Additionally, it can
handle semantic queries and provide results by inferring additional knowledge
from previously defined concepts in ontologies. SRE has been validated and
tested on different hardware platforms and in commercial products. Performance
evaluations are also presented to validate its conformance to the customer
requirements.



This paper presents Klout Topics, a lightweight ontology to describe social
media users' topics of interest and expertise. Klout Topics is designed to: be
human-readable and consumer-friendly; cover multiple domains of knowledge in
depth; and promote data extensibility via knowledge base entities. We discuss
why this ontology is well-suited for text labeling and interest modeling
applications, and how it compares to available alternatives. We show its
coverage against common social media interest sets, and examples of how it is
used to model the interests of over 780M social media users on Klout.com.
Finally, we open the ontology for external use.



Neural network-based systems can now learn to locate the referents of words
and phrases in images, answer questions about visual scenes, and even execute
symbolic instructions as first-person actors in partially-observable worlds. To
achieve this so-called grounded language learning, models must overcome certain
well-studied learning challenges that are also fundamental to infants learning
their first words. While it is notable that models with no meaningful prior
knowledge overcome these learning obstacles, AI researchers and practitioners
currently lack a clear understanding of exactly how they do so. Here we address
this question as a way of achieving a clearer general understanding of grounded
language learning, both to inform future research and to improve confidence in
model predictions. For maximum control and generality, we focus on a simple
neural network-based language learning agent trained via policy-gradient
methods to interpret synthetic linguistic instructions in a simulated 3D world.
We apply experimental paradigms from developmental psychology to this agent,
exploring the conditions under which established human biases and learning
effects emerge. We further propose a novel way to visualise and analyse
semantic representation in grounded language learning agents that yields a
plausible computational account of the observed effects.



In reinforcement learning an agent interacts with the environment by taking
actions and observing the next state and reward. When sampled
probabilistically, these state transitions, rewards, and actions can all induce
randomness in the observed long-term return. Traditionally, reinforcement
learning algorithms average over this randomness to estimate the value
function. In this paper, we build on recent work advocating a distributional
approach to reinforcement learning in which the distribution over returns is
modeled explicitly instead of only estimating the mean. That is, we examine
methods of learning the value distribution instead of the value function. We
give results that close a number of gaps between the theoretical and
algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we
extend existing results to the approximate distribution setting. Second, we
present a novel distributional reinforcement learning algorithm consistent with
our theoretical formulation. Finally, we evaluate this new algorithm on the
Atari 2600 games, observing that it significantly outperforms many of the
recent improvements on DQN, including the related distributional algorithm C51.



We study multiwinner voting problems when there is an additional requirement
that the selected committee should be fair with respect to attributes such as
gender, ethnicity, or political parties. Every setting of an attribute gives
rise to a group, and the goal is to ensure that each group is neither over nor
under represented in the selected committee. Prior work has largely focused on
designing specialized score functions that lead to a precise level of
representation with respect to disjoint attributes (e.g., only political
affiliation). Here we propose a general algorithmic framework that allows the
use of any score function and can guarantee flexible notions of fairness with
respect to multiple, non-disjoint attributes (e.g., political affiliation and
gender). Technically, we study the complexity of this constrained multiwinner
voting problem subject to group-fairness constraints for monotone submodular
score functions. We present approximation algorithms and hardness of
approximation results for various attribute set structures and score functions.



We consider the problem of performing inverse reinforcement learning when the
trajectory of the expert is not perfectly observed by the learner. Instead, a
noisy continuous-time observation of the trajectory is provided to the learner.
This problem exhibits wide-ranging applications and the specific application we
consider here is the scenario in which the learner seeks to penetrate a
perimeter patrolled by a robot. The learner's field of view is limited due to
which it cannot observe the patroller's complete trajectory. Instead, we allow
the learner to listen to the expert's movement sound, which it can also use to
estimate the expert's state and action using an observation model. We treat the
expert's state and action as hidden data and present an algorithm based on
expectation maximization and maximum entropy principle to solve the non-linear,
non-convex problem. Related work considers discrete-time observations and an
observation model that does not include actions. In contrast, our technique
takes expectations over both state and action of the expert, enabling learning
even in the presence of extreme noise and broader applications.



Multi-label classification is an important learning problem with many
applications. In this work, we propose a principled similarity-based approach
for multi-label learning called SML. We also introduce a similarity-based
approach for predicting the label set size. The experimental results
demonstrate the effectiveness of SML for multi-label classification where it is
shown to compare favorably with a wide variety of existing algorithms across a
range of evaluation criterion.



In this paper, we provide an approach to clustering relational matrices whose
entries correspond to either similarities or dissimilarities between objects.
Our approach is based on the value of information, a parameterized,
information-theoretic criterion that measures the change in costs associated
with changes in information. Optimizing the value of information yields a
deterministic annealing style of clustering with many benefits. For instance,
investigators avoid needing to a priori specify the number of clusters, as the
partitions naturally undergo phase changes, during the annealing process,
whereby the number of clusters changes in a data-driven fashion. The
global-best partition can also often be identified.



Recent work has addressed using formulas in linear temporal logic (LTL) as
specifications for agents planning in Markov Decision Processes (MDPs). We
consider the inverse problem: inferring an LTL specification from demonstrated
behavior trajectories in MDPs. We formulate this as a multiobjective
optimization problem, and describe state-based ("what actually happened") and
action-based ("what the agent expected to happen") objective functions based on
a notion of "violation cost". We demonstrate the efficacy of the approach by
employing genetic programming to solve this problem in two simple domains.



The support vector machine (SVM) is a widely used machine learning tool for
classification based on statistical learning theory. Given a set of training
data, the SVM finds a hyperplane that separates two different classes of data
points by the largest distance. While the standard form of SVM uses L2-norm
regularization, other regularization approaches are particularly attractive for
biomedical datasets where, for example, sparsity and interpretability of the
classifier's coefficient values are highly desired features. Therefore, in this
paper we consider different types of regularization approaches for SVMs, and
explore them in both synthetic and real biomedical datasets.



Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown
considerable promise in recent years as a potential tool for improving clinical
decision support in medical oncology, particularly those based around the
concept of Discovery Radiomics, where radiomic sequencers are discovered
through the analysis of medical imaging data. One of the main limitations with
current CAD approaches is that it is very difficult to gain insight or
rationale as to how decisions are made, thus limiting their utility to
clinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable
CAD system based on the notion of CLass-Enhanced Attentive Response Discovery
Radiomics for the purpose of clinical decision support for diabetic
retinopathy. Results: In addition to disease grading via the discovered deep
radiomic sequencer, the CLEAR-DR system also produces a visual interpretation
of the decision-making process to provide better insight and understanding into
the decision-making process of the system. Conclusion: We demonstrate the
effectiveness and utility of the proposed CLEAR-DR system of enhancing the
interpretability of diagnostic grading results for the application of diabetic
retinopathy grading. Significance: CLEAR-DR can act as a potential powerful
tool to address the uninterpretability issue of current CAD systems, thus
improving their utility to clinicians.



Third-generation neural networks, or Spiking Neural Networks (SNNs), aim at
harnessing the energy efficiency of spike-domain processing by building on
computing elements that operate on, and exchange, spikes. In this paper, the
problem of training a two-layer SNNs is studied for the purpose of
classification, under a Generalized Linear Model (GLM) probabilistic neural
model that was previously considered within the computational neuroscience
literature. Conventional classification rules for SNNs operate offline based on
the number of output spikes at each output neuron. In contrast, a novel
training method is proposed here for a first-to-spike decoding rule, whereby
the SNN can perform an early classification decision once spike firing is
detected at an output neuron. Numerical results bring insights into the optimal
parameter selection for the GLM neuron and on the accuracy-complexity trade-off
performance of conventional and first-to-spike decoding.



We discuss the connection between computational social choice (comsoc) and
computational complexity. We stress the work so far on, and urge continued
focus on, two less-recognized aspects of this connection. Firstly, this is very
much a two-way street: Everyone knows complexity classification is used in
comsoc, but we also highlight benefits to complexity that have arisen from its
use in comsoc. Secondly, more subtle, less-known complexity tools often can be
very productively used in comsoc.



Generative Adversarial Network (GAN) and its variants demonstrate
state-of-the-art performance in the class of generative models. To capture
higher dimensional distributions, the common learning procedure requires high
computational complexity and large number of parameters. In this paper, we
present a new generative adversarial framework by representing each layer as a
tensor structure connected by multilinear operations, aiming to reduce the
number of model parameters by a large factor while preserving the quality of
generalized performance. To learn the model, we develop an efficient algorithm
by alternating optimization of the mode connections. Experimental results
demonstrate that our model can achieve high compression rate for model
parameters up to 40 times as compared to the existing GAN.



Deep learning models require extensive architecture design exploration and
hyperparameter optimization to perform well on a given task. The exploration of
the model design space is often made by a human expert, and optimized using a
combination of grid search and search heuristics over a large space of possible
choices. Neural Architecture Search (NAS) is a Reinforcement Learning approach
that has been proposed to automate architecture design. NAS has been
successfully applied to generate Neural Networks that rival the best
human-designed architectures. However, NAS requires sampling, constructing, and
training hundreds to thousands of models to achieve well-performing
architectures. This procedure needs to be executed from scratch for each new
task. The application of NAS to a wide set of tasks currently lacks a way to
transfer generalizable knowledge across tasks. In this paper, we present the
Multitask Neural Model Search (MNMS) controller. Our goal is to learn a
generalizable framework that can condition model construction on successful
model searches for previously seen tasks, thus significantly speeding up the
search for new tasks. We demonstrate that MNMS can conduct an automated
architecture search for multiple tasks simultaneously while still learning
well-performing, specialized models for each task. We then show that
pre-trained MNMS controllers can transfer learning to new tasks. By leveraging
knowledge from previous searches, we find that pre-trained MNMS models start
from a better location in the search space and reduce search time on unseen
tasks, while still discovering models that outperform published human-designed
models.



Extreme learning machine (ELM) is a new single hidden layer feedback neural
network. The weights of the input layer and the biases of neurons in hidden
layer are randomly generated, the weights of the output layer can be
analytically determined. ELM has been achieved good results for a large number
of classification tasks. In this paper, a new extreme learning machine called
rough extreme learning machine (RELM) was proposed. RELM uses rough set to
divide data into upper approximation set and lower approximation set, and the
two approximation sets are utilized to train upper approximation neurons and
lower approximation neurons. In addition, an attribute reduction is executed in
this algorithm to remove redundant attributes. The experimental results showed,
comparing with the comparison algorithms, RELM can get a better accuracy and
repeatability in most cases, RELM can not only maintain the advantages of fast
speed, but also effectively cope with the classification task for
high-dimensional data.



We present graph attention networks (GATs), novel neural network
architectures that operate on graph-structured data, leveraging masked
self-attentional layers to address the shortcomings of prior methods based on
graph convolutions or their approximations. By stacking layers in which nodes
are able to attend over their neighborhoods' features, we enable (implicitly)
specifying different weights to different nodes in a neighborhood, without
requiring any kind of costly matrix operation (such as inversion) or depending
on knowing the graph structure upfront. In this way, we address several key
challenges of spectral-based graph neural networks simultaneously, and make our
model readily applicable to inductive as well as transductive problems. Our GAT
models have achieved or matched state-of-the-art results across four
established transductive and inductive graph benchmarks: the Cora, Citeseer and
Pubmed citation network datasets, as well as a protein-protein interaction
dataset (wherein test graphs remain unseen during training).



Although Generative Adversarial Networks (GANs) have shown remarkable success
in various tasks, they still face challenges in generating high quality images.
In this paper, we propose Stacked Generative Adversarial Networks (StackGAN)
aiming at generating high-resolution photo-realistic images. First, we propose
a two-stage generative adversarial network architecture, StackGAN-v1, for
text-to-image synthesis. The Stage-I GAN sketches the primitive shape and
colors of the object based on given text description, yielding low-resolution
images. The Stage-II GAN takes Stage-I results and text descriptions as inputs,
and generates high-resolution images with photo-realistic details. Second, an
advanced multi-stage generative adversarial network architecture, StackGAN-v2,
is proposed for both conditional and unconditional generative tasks. Our
StackGAN-v2 consists of multiple generators and discriminators in a tree-like
structure; images at multiple scales corresponding to the same scene are
generated from different branches of the tree. StackGAN-v2 shows more stable
training behavior than StackGAN-v1 by jointly approximating multiple
distributions. Extensive experiments demonstrate that the proposed stacked
generative adversarial networks significantly outperform other state-of-the-art
methods in generating photo-realistic images.



Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, including the classical games of chess, shogi, and Go, but
understanding and explaining AI remain challenging. This paper studies the
machine-learning algorithms for developing the game AIs, and provides their
structural interpretations. Specifically, chess-playing Deep Blue is a
calibrated value function, whereas shogi-playing Bonanza represents an
estimated value function via Rust's (1987) nested fixed-point method. AlphaGo's
"supervised-learning policy network" is a deep neural network (DNN) version of
Hotz and Miller's (1993) conditional choice probability estimates; its
"reinforcement-learning value network" is equivalent to Hotz, Miller, Sanders,
and Smith's (1994) simulation method for estimating the value function. Their
performances suggest DNNs are a useful functional form when the state space is
large and data are sparse. Explicitly incorporating strategic interactions and
unobserved heterogeneity in the data-generating process would further improve
AIs' explicability.



Endowing robots with the capability of assessing risk and making risk-aware
decisions is widely considered a key step toward ensuring safety for robots
operating under uncertainty. But, how should a robot quantify risk? A natural
and common approach is to consider the framework whereby costs are assigned to
stochastic outcomes - an assignment captured by a cost random variable.
Quantifying risk then corresponds to evaluating a risk metric, i.e., a mapping
from the cost random variable to a real number. Yet, the question of what
constitutes a "good" risk metric has received little attention within the
robotics community. The goal of this paper is to explore and partially address
this question by advocating axioms that risk metrics in robotics applications
should satisfy in order to be employed as rational assessments of risk. We
discuss general representation theorems that precisely characterize the class
of metrics that satisfy these axioms (referred to as distortion risk metrics),
and provide instantiations that can be used in applications. We further discuss
pitfalls of commonly used risk metrics in robotics, and discuss additional
properties that one must consider in sequential decision making tasks. Our hope
is that the ideas presented here will lead to a foundational framework for
quantifying risk (and hence safety) in robotics applications.



In spite of the recent success of neural machine translation (NMT) in
standard benchmarks, the lack of large parallel corpora poses a major practical
problem for many language pairs. There have been several proposals to alleviate
this issue with, for instance, triangulation and semi-supervised learning
techniques, but they still require a strong cross-lingual signal. In this work,
we completely remove the need of parallel data and propose a novel method to
train an NMT system in a completely unsupervised manner, relying on nothing but
monolingual corpora. Our model builds upon the recent work on unsupervised
embedding mappings, and consists of a slightly modified attentional
encoder-decoder model that can be trained on monolingual corpora alone using a
combination of denoising and backtranslation. Despite the simplicity of the
approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014
French-to-English and German-to-English translation. The model can also profit
from small parallel corpora, and attains 21.81 and 15.24 points when combined
with 100,000 parallel sentences, respectively. Our approach is a breakthrough
in unsupervised NMT, and opens exciting opportunities for future research.



We study the problem of semantic code repair, which can be broadly defined as
automatically fixing non-syntactic bugs in source code. The majority of past
work in semantic code repair assumed access to unit tests against which
candidate repairs could be validated. In contrast, the goal here is to develop
a strong statistical model to accurately predict both bug locations and exact
fixes without access to information about the intended correct behavior of the
program. Achieving such a goal requires a robust contextual repair model, which
we train on a large corpus of real-world source code that has been augmented
with synthetically injected bugs. Our framework adopts a two-stage approach
where first a large set of repair candidates are generated by rule-based
processors, and then these candidates are scored by a statistical model using a
novel neural network architecture which we refer to as Share, Specialize, and
Compete. Specifically, the architecture (1) generates a shared encoding of the
source code using an RNN over the abstract syntax tree, (2) scores each
candidate repair using specialized network modules, and (3) then normalizes
these scores together so they can compete against one another in comparable
probability space. We evaluate our model on a real-world test set gathered from
GitHub containing four common categories of bugs. Our model is able to predict
the exact correct repair 41\% of the time with a single guess, compared to 13\%
accuracy for an attentional sequence-to-sequence model.



Recent work on quantum machine learning has demonstrated that quantum
computers can offer dramatic improvements over classical devices for data
mining, prediction and classification. However, less is known about the
advantages using quantum computers may bring in the more general setting of
reinforcement learning, where learning is achieved via interaction with a task
environment that provides occasional rewards. Reinforcement learning can
incorporate data-analysis-oriented learning settings as special cases, but also
includes more complex situations where, e.g., reinforcing feedback is delayed.
In a few recent works, Grover-type amplification has been utilized to construct
quantum agents that achieve up-to-quadratic improvements in learning
efficiency. These encouraging results have left open the key question of
whether super-polynomial improvements in learning times are possible for
genuine reinforcement learning problems, that is problems that go beyond the
other more restricted learning paradigms. In this work, we provide a family of
such genuine reinforcement learning tasks. We construct quantum-enhanced
learners which learn super-polynomially, and even exponentially faster than any
classical reinforcement learning model, and we discuss the potential impact our
results may have on future technologies.



We focus on the problem of estimating the change in the dependency structures
of two $p$-dimensional Gaussian Graphical models (GGMs). Previous studies for
sparse change estimation in GGMs involve expensive and difficult non-smooth
optimization. We propose a novel method, DIFFEE for estimating DIFFerential
networks via an Elementary Estimator under a high-dimensional situation. DIFFEE
is solved through a faster and closed form solution that enables it to work in
large-scale settings. We conduct a rigorous statistical analysis showing that
surprisingly DIFFEE achieves the same asymptotic convergence rates as the
state-of-the-art estimators that are much more difficult to compute. Our
experimental results on multiple synthetic datasets and one real-world data
about brain connectivity show strong performance improvements over baselines,
as well as significant computational benefits.



One of the fundamental tasks in understanding genomics is the problem of
predicting Transcription Factor Binding Sites (TFBSs). With more than hundreds
of Transcription Factors (TFs) as labels, genomic-sequence based TFBS
prediction is a challenging multi-label classification task. There are two
major biological mechanisms for TF binding: (1) sequence-specific binding
patterns on genomes known as "motifs" and (2) interactions among TFs known as
co-binding effects. In this paper, we propose a novel deep architecture, the
Prototype Matching Network (PMN) to mimic the TF binding mechanisms. Our PMN
model automatically extracts prototypes ("motif"-like features) for each TF
through a novel prototype-matching loss. Borrowing ideas from few-shot matching
models, we use the notion of support set of prototypes and an LSTM to learn how
TFs interact and bind to genomic sequences. On a reference TFBS dataset with
$2.1$ $million$ genomic sequences, PMN significantly outperforms baselines and
validates our design choices empirically. To our knowledge, this is the first
deep learning architecture that introduces prototype learning and considers
TF-TF interactions for large-scale TFBS prediction. Not only is the proposed
architecture accurate, but it also models the underlying biology.



This paper presents a new method --- adversarial advantage actor-critic
(Adversarial A2C), which significantly improves the efficiency of dialogue
policy learning in task-completion dialogue systems. Inspired by generative
adversarial networks (GAN), we train a discriminator to differentiate
responses/actions generated by dialogue agents from responses/actions by
experts. Then, we incorporate the discriminator as another critic into the
advantage actor-critic (A2C) framework, to encourage the dialogue agent to
explore state-action within the regions where the agent takes actions similar
to those of the experts. Experimental results in a movie-ticket booking domain
show that the proposed Adversarial A2C can accelerate policy exploration
efficiently.



We consider the problems of learning forward models that map state to
high-dimensional images and inverse models that map high-dimensional images to
state in robotics. Specifically, we present a perceptual model for generating
video frames from state with deep networks, and provide a framework for its use
in tracking and prediction tasks. We show that our proposed model greatly
outperforms standard deconvolutional methods and GANs for image generation,
producing clear, photo-realistic images. We also develop a convolutional neural
network model for state estimation and compare the result to an Extended Kalman
Filter to estimate robot trajectories. We validate all models on a real robotic
system.



Due to their complex nature, it is hard to characterize the ways in which
machine learning models can misbehave or be exploited when deployed. Recent
work on adversarial examples, i.e. inputs with minor perturbations that result
in substantially different model predictions, is helpful in evaluating the
robustness of these models by exposing the adversarial scenarios where they
fail. However, these malicious perturbations are often unnatural, not
semantically meaningful, and not applicable to complicated domains such as
language. In this paper, we propose a framework to generate natural and legible
adversarial examples by searching in semantic space of dense and continuous
data representation, utilizing the recent advances in generative adversarial
networks. We present generated adversaries to demonstrate the potential of the
proposed approach for black-box classifiers in a wide range of applications
such as image classification, textual entailment, and machine translation. We
include experiments to show that the generated adversaries are natural, legible
to humans, and useful in evaluating and analyzing black-box classifiers.



It is commonly agreed that the use of relevant invariances as a good
statistical bias is important in machine-learning. However, most approaches
that explicitly incorporate invariances into a model architecture only make use
of very simple transformations, such as translations and rotations. Hence,
there is a need for methods to model and extract richer transformations that
capture much higher-level invariances. To that end, we introduce a tool
allowing to parametrize the set of filters of a trained convolutional neural
network with the latent space of a generative adversarial network. We then show
that the method can capture highly non-linear invariances of the data by
visualizing their effect in the data space.



Combining deep model-free reinforcement learning with on-line planning is a
promising approach to building on the successes of deep RL. On-line planning
with look-ahead trees has proven successful in environments where transition
models are known a priori. However, in complex environments where transition
models need to be learned from data, the deficiencies of learned models have
limited their utility for planning. To address these challenges, we propose
TreeQN, a differentiable, recursive, tree-structured model that serves as a
drop-in replacement for any value function network in deep RL with discrete
actions. TreeQN dynamically constructs a tree by recursively applying a
transition model in a learned abstract state space and then aggregating
predicted rewards and state-values using a tree backup to estimate Q-values. We
also propose ATreeC, an actor-critic variant that augments TreeQN with a
softmax layer to form a stochastic policy network. Both approaches are trained
end-to-end, such that the learned model is optimised for its actual use in the
planner. We show that TreeQN and ATreeC outperform n-step DQN and A2C on a
box-pushing task, as well as n-step DQN and value prediction networks (Oh et
al., 2017) on multiple Atari games, with deeper trees often outperforming
shallower ones. We also present a qualitative analysis that sheds light on the
trees learned by TreeQN.



Convolution Neural Network (CNN) has gained tremendous success in computer
vision tasks with its outstanding ability to capture the local latent features.
Recently, there has been an increasing interest in extending CNNs to the
general spatial domain. Although various types of graph and geometric
convolution methods have been proposed, their connections to traditional
2D-convolution are not well-understood. In this paper, we show that depthwise
separable convolution is the key to close the gap, based on which we derive a
novel Depthwise Separable Graph Convolution that subsumes existing graph
convolution methods as special cases of our formulation. Experiments show that
the proposed approach consistently outperforms other graph and geometric
convolution baselines on benchmark datasets in multiple domains.



In this paper we argue that crime drama exemplified in television programs
such as CSI:Crime Scene Investigation is an ideal testbed for approximating
real-world natural language understanding and the complex inferences associated
with it. We propose to treat crime drama as a new inference task, capitalizing
on the fact that each episode poses the same basic question (i.e., who
committed the crime) and naturally provides the answer when the perpetrator is
revealed. We develop a new dataset based on CSI episodes, formalize perpetrator
identification as a sequence labeling problem, and develop an LSTM-based model
which learns from multi-modal data. Experimental results show that an
incremental inference strategy is key to making accurate guesses as well as
learning from representations fusing textual, visual, and acoustic input.



Learning to learn is a powerful paradigm for enabling models to learn from
data more effectively and efficiently. A popular approach to meta-learning is
to train a recurrent model to read in a training dataset as input and output
the parameters of a learned model, or output predictions for new test inputs.
Alternatively, a more recent approach to meta-learning aims to acquire deep
representations that can be effectively fine-tuned, via standard gradient
descent, to new tasks. In this paper, we consider the meta-learning problem
from the perspective of universality, formalizing the notion of learning
algorithm approximation and comparing the expressive power of the
aforementioned recurrent models to the more recent approaches that embed
gradient descent into the meta-learner. In particular, we seek to answer the
following question: does deep representation combined with standard gradient
descent have sufficient capacity to approximate any learning algorithm? We find
that this is indeed true, and further find, in our experiments, that
gradient-based meta-learning consistently leads to learning strategies that
generalize more widely compared to those represented by recurrent models.



Recurrent neural networks (RNNs) are important class of architectures among
neural networks useful for language modeling and sequential prediction.
However, optimizing RNNs is known to be harder compared to feed-forward neural
networks. A number of techniques have been proposed in literature to address
this problem. In this paper we propose a simple technique called fraternal
dropout that takes advantage of dropout to achieve this goal. Specifically, we
propose to train two identical copies of an RNN (that share parameters) with
different dropout masks while minimizing the difference between their
(pre-softmax) predictions. In this way our regularization encourages the
representations of RNNs to be invariant to dropout mask, thus being robust. We
show that our regularization term is upper bounded by the expectation-linear
dropout objective which has been shown to address the gap due to the difference
between the train and inference phases of dropout. We evaluate our model and
achieve state-of-the-art results in sequence modeling tasks on two benchmark
datasets - Penn Treebank and Wikitext-2. We also show that our approach leads
to performance improvement by a significant margin in image captioning
(Microsoft COCO) and semi-supervised (CIFAR-10) tasks.



Purpose: A new method for magnetic resonance (MR) imaging water-fat
separation using a convolutional neural network (ConvNet) and deep learning
(DL) is presented. Feasibility of the method with complex and magnitude images
is demonstrated with a series of patient studies and accuracy of predicted
quantitative values is analyzed.
  Methods: Water-fat separation of 1200 gradient-echo acquisitions from 90
imaging sessions (normal, acute and chronic myocardial infarction) was
performed using a conventional model based method with modeling of R2* and
off-resonance and a multi-peak fat spectrum. A U-Net convolutional neural
network for calculation of water-only, fat-only, R2* and off-resonance images
was trained with 900 gradient-echo Multiple and single-echo complex and
magnitude input data algorithms were studied and compared to conventional
extended echo modeling.
  Results: The U-Net ConvNet was easily trained and provided water-fat
separation results visually comparable to conventional methods. Myocardial fat
deposition in chronic myocardial infarction and intramyocardial hemorrhage in
acute myocardial infarction were well visualized in the DL results. Predicted
values for R2*, off-resonance, water and fat signal intensities were well
correlated with conventional model based water fat separation (R2>=0.97,
p<0.001). DL images had a 14% higher signal-to-noise ratio (p<0.001) when
compared to the conventional method.
  Conclusion: Deep learning utilizing ConvNets is a feasible method for MR
water-fat separationimaging with complex, magnitude and single echo image data.
A trained U-Net can be efficiently used for MR water-fat separation, providing
results comparable to conventional model based methods.



Existing deep multitask learning (MTL) approaches align layers shared between
tasks in a parallel ordering. Such an organization significantly constricts the
types of shared structure that can be learned. The necessity of parallel
ordering for deep MTL is first tested by comparing it with permuted ordering of
shared layers. The results indicate that a flexible ordering can enable more
effective sharing, thus motivating the development of a soft ordering approach,
which learns how shared layers are applied in different ways for different
tasks. Deep MTL with soft ordering outperforms parallel ordering methods across
a series of domains. These results suggest that the power of deep MTL comes
from learning highly general building blocks that can be assembled to meet the
demands of each task.



We present pomegranate, an open source machine learning package for
probabilistic modeling in Python. Probabilistic modeling encompasses a wide
range of methods that explicitly describe uncertainty using probability
distributions. Three widely used probabilistic models implemented in
pomegranate are general mixture models, hidden Markov models, and Bayesian
networks. A primary focus of pomegranate is to abstract away the complexities
of training models from their definition. This allows users to focus on
specifying the correct model for their application instead of being limited by
their understanding of the underlying algorithms. An aspect of this focus
involves the collection of additive sufficient statistics from data sets as a
strategy for training models. This approach trivially enables many useful
learning strategies, such as out-of-core learning, minibatch learning, and
semi-supervised learning, without requiring the user to consider how to
partition data or modify the algorithms to handle these tasks themselves.
pomegranate is written in Cython to speed up calculations and releases the
global interpreter lock to allow for built-in multithreaded parallelism, making
it competitive with---or outperform---other implementations of similar
algorithms. This paper presents an overview of the design choices in
pomegranate, and how they have enabled complex features to be supported by
simple code.



Understanding physical phenomena is a key component of human intelligence and
enables physical interaction with previously unseen environments. In this
paper, we study how an artificial agent can autonomously acquire this intuition
through interaction with the environment. We created a synthetic block stacking
environment with physics simulation in which the agent can learn a policy
end-to-end through trial and error. Thereby, we bypass to explicitly model
physical knowledge within the policy. We are specifically interested in tasks
that require the agent to reach a given goal state that may be different for
every new trial. To this end, we propose a deep reinforcement learning
framework that learns policies which are parametrized by a goal. We validated
the model on a toy example navigating in a grid world with different target
positions and in a block stacking task with different target structures of the
final tower. In contrast to prior work, our policies show better generalization
across different goals.



Humans can understand and produce new utterances effortlessly, thanks to
their systematic compositional skills. Once a person learns the meaning of a
new verb "dax," he or she can immediately understand the meaning of "dax twice"
or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a
set of simple compositional navigation commands paired with the corresponding
action sequences. We then test the zero-shot generalization capabilities of a
variety of recurrent neural networks (RNNs) trained on SCAN with
sequence-to-sequence methods. We find that RNNs can generalize well when the
differences between training and test commands are small, so that they can
apply "mix-and-match" strategies to solve the task. However, when
generalization requires systematic compositional skills (as in the "dax"
example above), RNNs fail spectacularly. We conclude with a proof-of-concept
experiment in neural machine translation, supporting the conjecture that lack
of systematicity is an important factor explaining why neural networks need
very large training sets.



This paper introduces and addresses a wide class of stochastic bandit
problems where the function mapping the arm to the corresponding reward
exhibits some known structural properties. Most existing structures (e.g.
linear, Lipschitz, unimodal, combinatorial, dueling, ...) are covered by our
framework. We derive an asymptotic instance-specific regret lower bound for
these problems, and develop OSSB, an algorithm whose regret matches this
fundamental limit. OSSB is not based on the classical principle of "optimism in
the face of uncertainty" or on Thompson sampling, and rather aims at matching
the minimal exploration rates of sub-optimal arms as characterized in the
derivation of the regret lower bound. We illustrate the efficiency of OSSB
using numerical experiments in the case of the linear bandit problem and show
that OSSB outperforms existing algorithms, including Thompson sampling.



This paper presents the design of the machine learning architecture that
underlies the Alexa Skills Kit (ASK), which was the first Spoken Language
Understanding (SLU) Software Development Kit (SDK) for a virtual digital
assistant, as far as we are aware. At Amazon, the infrastructure powers more
than 25,000 skills built through the ASK, as well as AWS's Amazon Lex SLU
Service. The ASK emphasizes flexibility, predictability and a rapid iteration
cycle for third party developers. It imposes inductive biases that allow it to
learn robust SLU models from extremely small and sparse datasets and, in doing
so, removes significant barriers to entry for software developers and dialog
systems researchers.



Learning tasks on source code (i.e., formal languages) have been considered
recently, but most work has tried to transfer natural language methods and does
not capitalize on the unique opportunities offered by code's known syntax. For
example, long-range dependencies induced by using the same variable or function
in distant locations are often not considered. We propose to use graphs to
represent both the syntactic and semantic structure of code and use graph-based
deep learning methods to learn to reason over program structures.
  In this work, we present how to construct graphs from source code and how to
scale Gated Graph Neural Networks training to such large graphs. We evaluate
our method on two tasks: VarNaming, in which a network attempts to predict the
name of a variable given its usage, and VarMisuse, in which the network learns
to reason about selecting the correct variable that should be used at a given
program location. Our comparison to methods that use less structured program
representations shows the advantages of modeling known structure, and suggests
that our models learn to infer meaningful names and to solve the VarMisuse task
in many cases. Additionally, our testing showed that VarMisuse identifies a
number of bugs in mature open-source projects.



The largest source of sound events is web videos. Most videos lack sound
event labels at segment level, however, a significant number of them do respond
to text queries, from a match found to their metadata by the search engine. In
this paper we explore the extent to which a search query could be used as the
true label for the presence of sound events in the videos. For this, we
developed a framework for large-scale sound event recognition on web videos.
The framework crawls videos using search queries corresponding to 78 sound
event labels drawn from three datasets. The datasets are used to train three
classifiers, which were then run on 3.7 million video segments. We evaluated
performance using the search query as the true label and compare it (on a
subset) with human labeling. Both types exhibited close performance, to within
10%, and similar performance trends as the number of evaluated segments
increased. Hence, our experiments show potential for using search query as a
preliminary true label for sound events in web videos.



To achieve general intelligence, agents must learn how to interact with
others in a shared environment: this is the challenge of multiagent
reinforcement learning (MARL). The simplest form is independent reinforcement
learning (InRL), where each agent treats its experience as part of its
(non-stationary) environment. In this paper, we first observe that policies
learned using InRL can overfit to the other agents' policies during training,
failing to sufficiently generalize during execution. We introduce a new metric,
joint-policy correlation, to quantify this effect. We describe an algorithm for
general MARL, based on approximate best responses to mixtures of policies
generated using deep reinforcement learning, and empirical game-theoretic
analysis to compute meta-strategies for policy selection. The algorithm
generalizes previous ones such as InRL, iterated best response, double oracle,
and fictitious play. Then, we present a scalable implementation which reduces
the memory requirement using decoupled meta-solvers. Finally, we demonstrate
the generality of the resulting policies in two partially observable settings:
gridworld coordination games and poker.



Disentangled representations, where the higher level data generative factors
are reflected in disjoint latent dimensions, offer several benefits such as
ease of deriving invariant representations, transferability to other tasks,
interpretability, etc. We consider the problem of unsupervised learning of
disentangled representations from large pool of unlabeled observations, and
propose a variational inference based approach to infer disentangled latent
factors. We introduce a regularizer on the expectation of the approximate
posterior over observed data that encourages the disentanglement. We evaluate
the proposed approach using several quantitative metrics and empirically
observe significant gains over existing methods in terms of both
disentanglement and data likelihood (reconstruction quality).



We propose a method to learn deep ReLU-based classifiers that are provably
robust against norm-bounded adversarial perturbations (on the training data;
for previously unseen examples, the approach will be guaranteed to detect all
adversarial examples, though it may flag some non-adversarial examples as
well). The basic idea of the approach is to consider a convex outer
approximation of the set of activations reachable through a norm-bounded
perturbation, and we develop a robust optimization procedure that minimizes the
worst case loss over this outer region (via a linear program). Crucially, we
show that the dual problem to this linear program can be represented itself as
a deep network similar to the backpropagation network, leading to very
efficient optimization approaches that produce guaranteed bounds on the robust
loss. The end result is that by executing a few more forward and backward
passes through a slightly modified version of the original network (though
possibly with much larger batch sizes), we can learn a classifier that is
provably robust to any norm-bounded adversarial attack. We illustrate the
approach on a toy 2D robust classification task, and on a simple convolutional
architecture applied to MNIST, where we produce a classifier that provably has
less than 8.4% test error for any adversarial attack with bounded $\ell_\infty$
norm less than $\epsilon = 0.1$. This represents the largest verified network
that we are aware of, and we discuss future challenges in scaling the approach
to much larger domains.



In many real-world optimization problems, the objective function evaluation
is subject to noise, and we cannot obtain the exact objective value.
Evolutionary algorithms (EAs), a type of general-purpose randomized
optimization algorithm, have shown able to solve noisy optimization problems
well. However, previous theoretical analyses of EAs mainly focused on
noise-free optimization, which makes the theoretical understanding largely
insufficient. Meanwhile, the few existing theoretical studies under noise often
considered the one-bit noise model, which flips a randomly chosen bit of a
solution before evaluation; while in many realistic applications, several bits
of a solution can be changed simultaneously. In this paper, we study a natural
extension of one-bit noise, the bit-wise noise model, which independently flips
each bit of a solution with some probability. We analyze the running time of
the (1+1)-EA solving OneMax and LeadingOnes under bit-wise noise for the first
time, and derive the ranges of the noise level for polynomial and
super-polynomial running time bounds. The analysis on LeadingOnes under
bit-wise noise can be easily transferred to one-bit noise, and improves the
previously known results. Since our analysis discloses that the (1+1)-EA can be
efficient only under low noise levels, we also study whether the sampling
strategy can bring robustness to noise. We prove that using sampling can
significantly increase the largest noise level allowing a polynomial running
time, that is, sampling is robust to noise.



Program analysis is a technique to reason about programs without executing
them, and it has various applications in compilers, integrated development
environments, and security. In this work, we present a machine learning
pipeline that induces a security analyzer for programs by example. The security
analyzer determines whether a program is either secure or insecure based on
symbolic rules that were deduced by our machine learning pipeline. The machine
pipeline is two-staged consisting of a Recurrent Neural Networks (RNN) and an
Extractor that converts an RNN to symbolic rules.
  To evaluate the quality of the learned symbolic rules, we propose a
sampling-based similarity measurement between two infinite regular languages.
We conduct a case study using real-world data. In this work, we discuss the
limitations of existing techniques and possible improvements in the future. The
results show that with sufficient training data and a fair distribution of
program paths it is feasible to deducing symbolic security rules for the
OpenJDK library with millions lines of code.



Bayesian inference is an effective approach for solving statistical learning
problems especially with uncertainty and incompleteness. However, inference
efficiencies are physically limited by the bottlenecks of conventional
computing platforms. In this paper, an emerging Bayesian inference system is
proposed by exploiting spintronics based stochastic computing. A stochastic
bitstream generator is realized as the kernel components by leveraging the
inherent randomness of spintronics devices. The proposed system is evaluated by
typical applications of data fusion and Bayesian belief networks. Simulation
results indicate that the proposed approach could achieve significant
improvement on inference efficiencies in terms of power consumption and
inference speed.



In representational lifelong learning an agent aims to learn to solve novel
tasks while updating its representation in light of previous tasks. Under the
assumption that future tasks are `related' to previous tasks, representations
should be learned in such a way that they capture the common structure across
learned tasks, while allowing the learner sufficient flexibility to adapt to
novel aspects of a new task. We develop a framework for lifelong learning in
deep neural networks that is based on generalization bounds, developed within
the PAC-Bayes framework. Learning takes place through the construction of a
distribution over networks based on the tasks seen so far, and its utilization
for learning a new task. Thus, prior knowledge is incorporated through setting
a history-dependent prior for novel tasks. We develop a gradient-based
algorithm implementing these ideas, based on minimizing an objective function
motivated by generalization bounds, and demonstrate its effectiveness through
numerical examples.



In robotics, it is essential to be able to plan efficiently in
high-dimensional continuous state-action spaces for long horizons. For such
complex planning problems, unguided uniform sampling of actions until a path to
a goal is found is hopelessly inefficient, and gradient-based approaches often
fall short when the optimization manifold of a given problem is not smooth. In
this paper we present an approach that guides the search of a state-space
planner, such as A*, by learning an action-sampling distribution that can
generalize across different instances of a planning problem. The motivation is
that, unlike typical learning approaches for planning for continuous action
space that estimate a policy, an estimated action sampler is more robust to
error since it has a planner to fall back on. We use a Generative Adversarial
Network (GAN), and address an important issue: search experience consists of a
relatively large number of actions that are not on a solution path and a
relatively small number of actions that actually are on a solution path. We
introduce a new technique, based on an importance-ratio estimation method, for
using samples from a non-target distribution to make GAN learning more
data-efficient. We provide theoretical guarantees and empirical evaluation in
three challenging continuous robot planning problems to illustrate the
effectiveness of our algorithm.



Machine learning is usually defined in behaviourist terms, where external
validation is the primary mechanism of learning. In this paper, I argue for a
more holistic interpretation in which finding more probable, efficient and
abstract representations is as central to learning as performance. In other
words, machine learning should be extended with strategies to reason over its
own learning process, leading to so-called meta-cognitive machine learning. As
such, the de facto definition of machine learning should be reformulated in
these intrinsically multi-objective terms, taking into account not only the
task performance but also internal learning objectives. To this end, we suggest
a "model entropy function" to be defined that quantifies the efficiency of the
internal learning processes. It is conjured that the minimization of this model
entropy leads to concept formation. Besides philosophical aspects, some initial
illustrations are included to support the claims.



Individual Neurons in the nervous systems exploit various dynamics. To
capture these dynamics for single neurons, we tune the parameters of an
electrophysiological model of nerve cells, to fit experimental data obtained by
calcium imaging. A search for the biophysical parameters of this model is
performed by means of a genetic algorithm, where the model neuron is exposed to
a predefined input current representing overall inputs from other parts of the
nervous system. The algorithm is then constrained for keeping the ion-channel
currents within reasonable ranges, while producing the best fit to a calcium
imaging time series of the AVA interneuron, from the brain of the soil-worm, C.
elegans. Our settings enable us to project a set of biophysical parameters to
the the neuron kinetics observed in neuronal imaging.



Deep learning approaches such as convolutional neural nets have consistently
outperformed previous methods on challenging tasks such as dense, semantic
segmentation. However, the various proposed networks perform differently, with
behaviour largely influenced by architectural choices and training settings.
This paper explores Ensembles of Multiple Models and Architectures (EMMA) for
robust performance through aggregation of predictions from a wide range of
methods. The approach reduces the influence of the meta-parameters of
individual models and the risk of overfitting the configuration to a particular
database. EMMA can be seen as an unbiased, generic deep learning model which is
shown to yield excellent performance, winning the first position in the BRATS
2017 competition among 50+ participating teams.



The performance of many parallel applications depends on loop-level
parallelism. However, manually parallelizing all loops may result in degrading
parallel performance, as some of them cannot scale desirably to a large number
of threads. In addition, the overheads of manually tuning loop parameters might
prevent an application from reaching its maximum parallel performance. We
illustrate how machine learning techniques can be applied to address these
challenges. In this research, we develop a framework that is able to
automatically capture the static and dynamic information of a loop. Moreover,
we advocate a novel method by introducing HPX smart executors for determining
the execution policy, chunk size, and prefetching distance of an HPX loop to
achieve higher possible performance by feeding static information captured
during compilation and runtime-based dynamic information to our learning model.
Our evaluated execution results show that using these smart executors can speed
up the HPX execution process by around 12%-35% for the Matrix Multiplication,
Stream and $2D$ Stencil benchmarks compared to setting their HPX loop's
execution policy/parameters manually or using HPX auto-parallelization
techniques.



We study the relationship between geometry and capacity measures for deep
neural networks from an invariance viewpoint. We introduce a new notion of
capacity --- the Fisher-Rao norm --- that possesses desirable invariance
properties and is motivated by Information Geometry. We discover an analytical
characterization of the new capacity measure, through which we establish
norm-comparison inequalities and further show that the new measure serves as an
umbrella for several existing norm-based complexity measures. We discuss upper
bounds on the generalization error induced by the proposed measure. Extensive
numerical experiments on CIFAR-10 support our theoretical findings. Our
theoretical analysis rests on a key structural lemma about partial derivatives
of multi-layer rectifier networks.



Temporal-difference (TD) learning is an important field in reinforcement
learning. Sarsa and Q-Learning are among the most used TD algorithms. The
Q($\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper
extends the Q($\sigma$) algorithm to an online multi-step algorithm Q($\sigma,
\lambda$) using eligibility traces and introduces Double Q($\sigma$) as the
extension of Q($\sigma$) to double learning. Experiments suggest that the new
Q($\sigma, \lambda$) algorithm can outperform the classical TD control methods
Sarsa($\lambda$), Q($\lambda$) and Q($\sigma$).



Long Short-Term Memory (LSTM) is a popular approach to boosting the ability
of Recurrent Neural Networks to store longer term temporal information. The
capacity of an LSTM network can be increased by widening and adding layers.
However, usually the former introduces additional parameters, while the latter
increases the runtime. As an alternative we propose the Tensorized LSTM in
which the hidden states are represented by tensors and updated via a
cross-layer convolution. By increasing the tensor size, the network can be
widened efficiently without additional parameters since the parameters are
shared across different locations in the tensor; by delaying the output, the
network can be deepened implicitly with little additional runtime since deep
computations for each timestep are merged into temporal computations of the
sequence. Experiments conducted on five challenging sequence learning tasks
show the potential of the proposed model.



Training a conventional automatic speech recognition (ASR) system to support
multiple languages is challenging because the sub-word unit, lexicon and word
inventories are typically language specific. In contrast, sequence-to-sequence
models are well suited for multilingual ASR because they encapsulate an
acoustic, pronunciation and language model jointly in a single network. In this
work we present a single sequence-to-sequence ASR model trained on 9 different
Indian languages, which have very little overlap in their scripts.
Specifically, we take a union of language-specific grapheme sets and train a
grapheme-based sequence-to-sequence model jointly on data from all languages.
We find that this model, which is not explicitly given any information about
language identity, improves recognition performance by 21% relative compared to
analogous sequence-to-sequence models trained on each language individually. By
modifying the model to accept a language identifier as an additional input
feature, we further improve performance by an additional 7% relative and
eliminate confusion between different languages.



RoboCup is an international scientific robot competition in which teams of
multiple robots compete against each other. Its different leagues provide many
sources of robotics data, that can be used for further analysis and application
of machine learning. This paper describes a large dataset from games of some of
the top teams (from 2016 and 2017) in RoboCup Soccer Simulation League (2D),
where teams of 11 robots (agents) compete against each other. Overall, we used
10 different teams to play each other, resulting in 45 unique pairings. For
each pairing, we ran 25 matches (of 10mins), leading to 1125 matches or more
than 180 hours of game play. The generated CSV files are 17GB of data (zipped),
or 229GB (unzipped). The dataset is unique in the sense that it contains both
the ground truth data (global, complete, noise-free information of all objects
on the field), as well as the noisy, local and incomplete percepts of each
robot. These data are made available as CSV files, as well as in the original
soccer simulator formats.



Previously referred to as `miraculous' in the scientific literature because
of its powerful properties and its wide application as optimal solution to the
problem of induction/inference, (approximations to) Algorithmic Probability
(AP) and the associated Universal Distribution are (or should be) of the
greatest importance in science. Here we investigate the emergence, the rates of
emergence and convergence, and the Coding-theorem like behaviour of AP in
Turing-subuniversal models of computation. We investigate empirical
distributions of computing models in the Chomsky hierarchy. We introduce
measures of algorithmic probability and algorithmic complexity based upon
resource-bounded computation, in contrast to previously thoroughly investigated
distributions produced from the output distribution of Turing machines. This
approach allows for numerical approximations to algorithmic
(Kolmogorov-Chaitin) complexity-based estimations at each of the levels of a
computational hierarchy. We demonstrate that all these estimations are
correlated in rank and that they converge both in rank and values as a function
of computational power, despite fundamental differences between computational
models. In the context of natural processes that operate below the Turing
universal level because of finite resources and physical degradation, the
investigation of natural biases stemming from algorithmic rules may shed light
on the distribution of outcomes. We show that up to 60\% of the
simplicity/complexity bias in distributions produced even by the weakest of the
computational models can be accounted for by Algorithmic Probability in its
approximation to the Universal Distribution.



Generative Adversarial Networks (GANs) were intuitively and attractively
explained under the perspective of game theory, wherein two involving parties
are a discriminator and a generator. In this game, the task of the
discriminator is to discriminate the real and generated (i.e., fake) data,
whilst the task of the generator is to generate the fake data that maximally
confuses the discriminator. In this paper, we propose a new viewpoint for GANs,
which is termed as the minimizing general loss viewpoint. This viewpoint shows
a connection between the general loss of a classification problem regarding a
convex loss function and a f-divergence between the true and fake data
distributions. Mathematically, we proposed a setting for the classification
problem of the true and fake data, wherein we can prove that the general loss
of this classification problem is exactly the negative f-divergence for a
certain convex function f. This allows us to interpret the problem of learning
the generator for dismissing the f-divergence between the true and fake data
distributions as that of maximizing the general loss which is equivalent to the
min-max problem in GAN if the Logistic loss is used in the classification
problem. However, this viewpoint strengthens GANs in two ways. First, it allows
us to employ any convex loss function for the discriminator. Second, it
suggests that rather than limiting ourselves in NN-based discriminators, we can
alternatively utilize other powerful families. Bearing this viewpoint, we then
propose using the kernel-based family for discriminators. This family has two
appealing features: i) a powerful capacity in classifying non-linear nature
data and ii) being convex in the feature space. Using the convexity of this
family, we can further develop Fenchel duality to equivalently transform the
max-min problem to the max-max dual problem.



Neural networks (NNs) have begun to have a pervasive impact on various
applications of machine learning. However, the problem of finding an optimal NN
architecture for large applications has remained open for several decades.
Conventional approaches search for the optimal NN architecture through
extensive trial-and-error. Such a procedure is quite inefficient. In addition,
the generated NN architectures incur substantial redundancy. To address these
problems, we propose an NN synthesis tool (NeST) that automatically generates
very compact architectures for a given dataset. NeST starts with a seed NN
architecture. It iteratively tunes the architecture with gradient-based growth
and magnitude-based pruning of neurons and connections. Our experimental
results show that NeST yields accurate yet very compact NNs with a wide range
of seed architecture selection. For example, for the LeNet-300-100 (LeNet-5) NN
architecture derived from the MNIST dataset, we reduce network parameters by
34.1x (74.3x) and floating-point operations (FLOPs) by 35.8x (43.7x). For the
AlexNet NN architecture derived from the ImageNet dataset, we reduce network
parameters by 15.7x and FLOPs by 4.6x. All these results are the current
state-of-the-art for these architectures.



We present a novel technique for learning the mass matrices in samplers
obtained from discretized dynamics that preserve some energy function. Existing
adaptive samplers use Riemannian preconditioning techniques, where the mass
matrices are functions of the parameters being sampled. This leads to
significant complexities in the energy reformulations and resultant dynamics,
often leading to implicit systems of equations and requiring inversion of
high-dimensional matrices in the leapfrog steps. Our approach provides a
simpler alternative, by using existing dynamics in the sampling step of a Monte
Carlo EM framework, and learning the mass matrices in the M step with a novel
online technique. We also propose a way to adaptively set the number of samples
gathered in the E step, using sampling error estimates from the leapfrog
dynamics. Along with a novel stochastic sampler based on Nos\'{e}-Poincar\'{e}
dynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as
well as newer stochastic algorithms such as SGHMC and SGNHT, and show strong
performance on synthetic and real high-dimensional sampling scenarios; we
achieve sampling accuracies comparable to Riemannian samplers while being
significantly faster.



Approximate algorithms for structured prediction problems---such as the
popular alpha-expansion algorithm (Boykov et al. 2001) in computer
vision---typically far exceed their theoretical performance guarantees on
real-world instances. These algorithms often find solutions that are very close
to optimal. The goal of this paper is to partially explain the performance of
alpha-expansion on MAP inference in Ferromagnetic Potts models (FPMs). Our main
results use the connection between energy minimization in FPMs and the Uniform
Metric Labeling problem to give a stability condition under which the
alpha-expansion algorithm provably recovers the optimal MAP solution. This
theoretical result complements the numerous empirical observations of
alpha-expansion's performance. Additionally, we give a different stability
condition under which an LP-based algorithm recovers the optimal solution.



Deep reinforcement learning has achieved many recent successes, but our
understanding of its strengths and limitations is hampered by the lack of rich
environments in which we can fully characterize optimal behavior, and
correspondingly diagnose individual actions against such a characterization.
Here we consider a family of combinatorial games, arising from work of Erdos,
Selfridge, and Spencer, and we propose their use as environments for evaluating
and comparing different approaches to reinforcement learning. These games have
a number of appealing features: they are challenging for current learning
approaches, but they form (i) a low-dimensional, simply parametrized
environment where (ii) there is a linear closed form solution for optimal
behavior from any state, and (iii) the difficulty of the game can be tuned by
changing environment parameters in an interpretable way. We use these
Erdos-Selfridge-Spencer games not only to compare different algorithms, but
also to compare approaches based on supervised and reinforcement learning, to
analyze the power of multi-agent approaches in improving performance, and to
evaluate generalization to environments outside the training set.



We study the problem of learning overcomplete HMMs---those that have many
hidden states but a small output alphabet. Despite having significant practical
importance, such HMMs are poorly understood with no known positive or negative
results for efficient learning. In this paper, we present several new
results---both positive and negative---which help define the boundaries between
the tractable and intractable settings. Specifically, we show positive results
for a large subclass of HMMs whose transition matrices are sparse,
well-conditioned, and have small probability mass on short cycles. On the other
hand, we show that learning is impossible given only a polynomial number of
samples for HMMs with a small output alphabet and whose transition matrices are
random regular graphs with large degree. We also discuss these results in the
context of learning HMMs which can capture long-term dependencies.



A major drawback of backpropagation through time (BPTT) is the difficulty of
learning long-term dependencies, coming from having to propagate credit
information backwards through every single step of the forward computation.
This makes BPTT both computationally impractical and biologically implausible.
For this reason, full backpropagation through time is rarely used on long
sequences, and truncated backpropagation through time is used as a heuristic.
However, this usually leads to biased estimates of the gradient in which longer
term dependencies are ignored. Addressing this issue, we propose an alternative
algorithm, Sparse Attentive Backtracking, which might also be related to
principles used by brains to learn long-term dependencies. Sparse Attentive
Backtracking learns an attention mechanism over the hidden states of the past
and selectively backpropagates through paths with high attention weights. This
allows the model to learn long term dependencies while only backtracking for a
small number of time steps, not just from the recent past but also from
attended relevant past states.



The importance of interpretability of machine learning models has been
increasing due to emerging enterprise predictive analytics, threat of data
privacy, accountability of artificial intelligence in society, and so on.
Piecewise linear models have been actively studied to achieve both accuracy and
interpretability. They often produce competitive accuracy against
state-of-the-art non-linear methods. In addition, their representations (i.e.,
rule-based segmentation plus sparse linear formula) are often preferred by
domain experts. A disadvantage of such models, however, is high computational
cost for simultaneous determinations of the number of "pieces" and cardinality
of each linear predictor, which has restricted their applicability to
middle-scale data sets. This paper proposes a distributed factorized asymptotic
Bayesian (FAB) inference of learning piece-wise sparse linear models on
distributed memory architectures. The distributed FAB inference solves the
simultaneous model selection issue without communicating $O(N)$ data where N is
the number of training samples and achieves linear scale-out against the number
of CPU cores. Experimental results demonstrate that the distributed FAB
inference achieves high prediction accuracy and performance scalability with
both synthetic and benchmark data.



DR-submodular continuous functions are important objectives with wide
real-world applications spanning MAP inference in determinantal point processes
(DPPs), and mean-field inference for probabilistic submodular models, amongst
others. DR-submodularity captures a subclass of non-convex functions that
enables both exact minimization and approximate maximization in polynomial
time.
  In this work we study the problem of maximizing non-monotone DR-submodular
continuous functions under general down-closed convex constraints. We start by
investigating geometric properties that underlie such objectives, e.g., a
strong relation between (approximately) stationary points and global optimum is
proved. These properties are then used to devise two optimization algorithms
with provable guarantees. Concretely, we first devise a "two-phase" algorithm
with $1/4$ approximation guarantee. This algorithm allows the use of existing
methods for finding (approximately) stationary points as a subroutine, thus,
harnessing recent progress in non-convex optimization. Then we present a
non-monotone Frank-Wolfe variant with $1/e$ approximation guarantee and
sublinear convergence rate. Finally, we extend our approach to a broader class
of generalized DR-submodular continuous functions, which captures a wider
spectrum of applications. Our theoretical findings are validated on synthetic
and real-world problem instances.



The main challenge of online multi-object tracking is to reliably associate
object trajectories with detections in each video frame based on their tracking
history. In this work, we propose the Recurrent Autoregressive Network (RAN), a
temporal generative modeling framework to characterize the appearance and
motion dynamics of multiple objects over time. The RAN couples an external
memory and an internal memory. The external memory explicitly stores previous
inputs of each trajectory in a time window, while the internal memory learns to
summarize long-term tracking history and associate detections by processing the
external memory. We conduct experiments on the MOT 2015 and 2016 datasets to
demonstrate the robustness of our tracking method in highly crowded and
occluded scenes. Our method achieves top-ranked results on the two benchmarks.



Recurrent Neural Networks (RNNs) are used in state-of-the-art models in
domains such as speech recognition, machine translation, and language
modelling. Sparsity is a technique to reduce compute and memory requirements of
deep learning models. Sparse RNNs are easier to deploy on devices and high-end
server processors. Even though sparse operations need less compute and memory
relative to their dense counterparts, the speed-up observed by using sparse
operations is less than expected on different hardware platforms. In order to
address this issue, we investigate two different approaches to induce block
sparsity in RNNs: pruning blocks of weights in a layer and using group lasso
regularization to create blocks of weights with zeros. Using these techniques,
we demonstrate that we can create block-sparse RNNs with sparsity ranging from
80% to 90% with small loss in accuracy. This allows us to reduce the model size
by roughly 10x. Additionally, we can prune a larger dense network to recover
this loss in accuracy while maintaining high block sparsity and reducing the
overall parameter count. Our technique works with a variety of block sizes up
to 32x32. Block-sparse RNNs eliminate overheads related to data storage and
irregular memory accesses while increasing hardware efficiency compared to
unstructured sparsity.



Non-availability of reliable and sustainable electric power is a major
problem in the developing world. Renewable energy sources like solar are not
very lucrative in the current stage due to various uncertainties like weather,
storage, land use among others. There also exists various other issues like
mis-commitment of power, absence of intelligent fault analysis, congestion,
etc. In this paper, we propose a novel deep learning-based system for
predicting faults and selecting power generators optimally so as to reduce
costs and ensure higher reliability in solar power systems. The results are
highly encouraging and they suggest that the approaches proposed in this paper
have the potential to be applied successfully in the developing world.



An elementary mathematical example proves, thanks to the Routh-Hurwitz
criterion, a result that is intriguing with respect to today's practical
understanding of model-free control, i.e., an "intelligent" proportional
controller (iP) may turn to be more difficult to tune than an intelligent
proportional-derivative one (iPD). The vast superiority of iPDs when compared
to classic PIDs is shown via computer simulations. The introduction as well as
the conclusion analyse model-free control in the light of recent advances.



This paper deals with unsupervised clustering with feature selection. The
problem is to estimate both labels and a sparse projection matrix of weights.
To address this combinatorial non-convex problem maintaining a strict control
on the sparsity of the matrix of weights, we propose an alternating
minimization of the Frobenius norm criterion. We provide a new efficient
algorithm named K-sparse which alternates k-means with projection-gradient
minimization. The projection-gradient step is a method of splitting type, with
exact projection on the $\ell^1$ ball to promote sparsity. The convergence of
the gradient-projection step is addressed, and a preliminary analysis of the
alternating minimization is made. The Frobenius norm criterion converges as the
number of iterates in Algorithm K-sparse goes to infinity. Experiments on
Single Cell RNA sequencing datasets show that our method significantly improves
the results of PCA k-means, spectral clustering, SIMLR, and Sparcl methods, and
achieves a relevant selection of genes. The complexity of K-sparse is linear in
the number of samples (cells), so that the method scales up to large datasets.



Power grids are one of the most important components of infrastructure in
today's world. Every nation is dependent on the security and stability of its
own power grid to provide electricity to the households and industries. A
malfunction of even a small part of a power grid can cause loss of
productivity, revenue and in some cases even life. Thus, it is imperative to
design a system which can detect the health of the power grid and take
protective measures accordingly even before a serious anomaly takes place. To
achieve this objective, we have set out to create an artificially intelligent
system which can analyze the grid information at any given time and determine
the health of the grid through the usage of sophisticated formal models and
novel machine learning techniques like recurrent neural networks. Our system
simulates grid conditions including stimuli like faults, generator output
fluctuations, load fluctuations using Siemens PSS/E software and this data is
trained using various classifiers like SVM, LSTM and subsequently tested. The
results are excellent with our methods giving very high accuracy for the data.
This model can easily be scaled to handle larger and more complex grid
architectures.



Embedding methods such as word embedding have become pillars for many
applications containing discrete structures. Conventional embedding methods
directly associate each symbol with a continuous embedding vector, which is
equivalent to applying linear transformation based on "one-hot" encoding of the
discrete symbols. Despite its simplicity, such approach yields number of
parameters that grows linearly with the vocabulary size and can lead to
overfitting. In this work we propose a much more compact K-way D-dimensional
discrete encoding scheme to replace the "one-hot" encoding. In "KD encoding",
each symbol is represented by a $D$-dimensional code, and each of its dimension
has a cardinality of $K$. The final symbol embedding vector can be generated by
composing the code embedding vectors. To learn the semantically meaningful
code, we derive a relaxed discrete optimization technique based on stochastic
gradient descent. By adopting the new coding system, the efficiency of
parameterization can be significantly improved (from linear to logarithmic),
and this can also mitigate the over-fitting problem. In our experiments with
language modeling, the number of embedding parameters can be reduced by 97\%
while achieving similar or better performance.



In this paper we analyse the benefits of incorporating interval-valued fuzzy
sets into the Bousi-Prolog system. A syntax, declarative semantics and im-
plementation for this extension is presented and formalised. We show, by using
potential applications, that fuzzy logic programming frameworks enhanced with
them can correctly work together with lexical resources and ontologies in order
to improve their capabilities for knowledge representation and reasoning.



We consider stochastic multi-armed bandit problems with graph feedback, where
the decision maker is allowed to observe the neighboring actions of the chosen
action. We allow the graph structure to vary with time and consider both
deterministic and Erd\H{o}s-R\'enyi random graph models. For such a graph
feedback model, we first present a novel analysis of Thompson sampling that
leads to tighter performance bound than existing work. Next, we propose new
Information Directed Sampling based policies that are graph-aware in their
decision making. Under the deterministic graph case, we establish a Bayesian
regret bound for the proposed policies that scales with the clique cover number
of the graph instead of the number of actions. Under the random graph case, we
provide a Bayesian regret bound for the proposed policies that scales with the
ratio of the number of actions over the expected number of observations per
iteration. To the best of our knowledge, this is the first analytical result
for stochastic bandits with random graph feedback. Finally, using numerical
evaluations, we demonstrate that our proposed IDS policies outperform existing
approaches, including adaptions of upper confidence bound, $\epsilon$-greedy
and Exp3 algorithms.



For applications as varied as Bayesian neural networks, determinantal point
processes, elliptical graphical models, and kernel learning for Gaussian
processes (GPs), one must compute a log determinant of an $n \times n$ positive
definite matrix, and its derivatives - leading to prohibitive
$\mathcal{O}(n^3)$ computations. We propose novel $\mathcal{O}(n)$ approaches
to estimating these quantities from only fast matrix vector multiplications
(MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and
surrogate models, and converge quickly even for kernel matrices that have
challenging spectra. We leverage these approximations to develop a scalable
Gaussian process approach to kernel learning. We find that Lanczos is generally
superior to Chebyshev for kernel learning, and that a surrogate approach can be
highly efficient and accurate with popular kernels.



Representing the semantics of words is a long-standing problem for the
natural language processing community. Most methods compute word semantics
given their textual context in large corpora. More recently, researchers
attempted to integrate perceptual and visual features. Most of these works
consider the visual appearance of objects to enhance word representations but
they ignore the visual environment and context in which objects appear. We
propose to unify text-based techniques with vision-based techniques by
simultaneously leveraging textual and visual context to learn multimodal word
embeddings. We explore various choices for what can serve as a visual context
and present an end-to-end method to integrate visual context elements in a
multimodal skip-gram model. We provide experiments and extensive analysis of
the obtained results.



This paper proposes a computational approach for analysis of strokes in line
drawings by artists. We aim at developing an AI methodology that facilitates
attribution of drawings of unknown authors in a way that is not easy to be
deceived by forged art. The methodology used is based on quantifying the
characteristics of individual strokes in drawings. We propose a novel algorithm
for segmenting individual strokes. We designed and compared different
hand-crafted and learned features for the task of quantifying stroke
characteristics. We also propose and compare different classification methods
at the drawing level. We experimented with a dataset of 300 digitized drawings
with over 80 thousands strokes. The collection mainly consisted of drawings of
Pablo Picasso, Henry Matisse, and Egon Schiele, besides a small number of
representative works of other artists. The experiments shows that the proposed
methodology can classify individual strokes with accuracy 70%-90%, and
aggregate over drawings with accuracy above 80%, while being robust to be
deceived by fakes (with accuracy 100% for detecting fakes in most settings).



Scientific publishing conveys the outputs of an academic or research
activity, in this sense; it also reflects the efforts and issues in which
people engage. To identify potential collaborative networks one of the simplest
approaches is to leverage the co-authorship relations. In this approach,
semantic and hierarchic relationships defined by a Knowledge Organization
System are used in order to improve the system's ability to recommend potential
networks beyond the lexical or syntactic analysis of the topics or concepts
that are of interest to academics.



The multi-armed bandit problem has been extensively studied under the
stationary assumption. However in reality, this assumption often does not hold
because the distributions of rewards themselves may change over time. In this
paper, we propose a change-detection (CD) based framework for multi-armed
bandit problems under the piecewise-stationary setting, and study a class of
change-detection based UCB (Upper Confidence Bound) policies, CD-UCB, that
actively detects change points and restarts the UCB indices. We then develop
CUSUM-UCB and PHT-UCB, that belong to the CD-UCB class and use cumulative sum
(CUSUM) and Page-Hinkley Test (PHT) to detect changes. We show that CUSUM-UCB
obtains the best known regret upper bound under mild assumptions. We also
demonstrate the regret reduction of the CD-UCB policies over arbitrary
Bernoulli rewards and Yahoo! datasets of webpage click-through rates.



With an abundance of research papers in deep learning, reproducibility or
adoption of the existing works becomes a challenge. This is due to the lack of
open source implementations provided by the authors. Further, re-implementing
research papers in a different library is a daunting task. To address these
challenges, we propose a novel extensible approach, DLPaper2Code, to extract
and understand deep learning design flow diagrams and tables available in a
research paper and convert them to an abstract computational graph. The
extracted computational graph is then converted into execution ready source
code in both Keras and Caffe, in real-time. An arXiv-like website is created
where the automatically generated designs is made publicly available for 5,000
research papers. The generated designs could be rated and edited using an
intuitive drag-and-drop UI framework in a crowdsourced manner. To evaluate our
approach, we create a simulated dataset with over 216,000 valid design
visualizations using a manually defined grammar. Experiments on the simulated
dataset show that the proposed framework provide more than $93\%$ accuracy in
flow diagram content extraction.



We describe a novel spiking neural network (SNN) for automated, real-time
handwritten digit classification and its implementation on a GP-GPU platform.
Information processing within the network, from feature extraction to
classification is implemented by mimicking the basic aspects of neuronal spike
initiation and propagation in the brain. The feature extraction layer of the
SNN uses fixed synaptic weight maps to extract the key features of the image
and the classifier layer uses the recently developed NormAD approximate
gradient descent based supervised learning algorithm for spiking neural
networks to adjust the synaptic weights. On the standard MNIST database images
of handwritten digits, our network achieves an accuracy of 99.80% on the
training set and 98.06% on the test set, with nearly 7x fewer parameters
compared to the state-of-the-art spiking networks. We further use this network
in a GPU based user-interface system demonstrating real-time SNN simulation to
infer digits written by different users. On a test set of 500 such images, this
real-time platform achieves an accuracy exceeding 97% while making a prediction
within an SNN emulation time of less than 100ms.



We study the performance of stochastically trained deep neural networks
(DNNs) whose synaptic weights are implemented using emerging memristive devices
that exhibit limited dynamic range, resolution, and variability in their
programming characteristics. We show that a key device parameter to optimize
the learning efficiency of DNNs is the variability in its programming
characteristics. DNNs with such memristive synapses, even with dynamic range as
low as $15$ and only $32$ discrete levels, when trained based on stochastic
updates suffer less than $3\%$ loss in accuracy compared to floating point
software baseline. We also study the performance of stochastic memristive DNNs
when used as inference engines with noise corrupted data and find that if the
device variability can be minimized, the relative degradation in performance
for the Stochastic DNN is better than that of the software baseline. Hence, our
study presents a new optimization corner for memristive devices for building
large noise-immune deep learning systems.



This work presents an overarching perspective on the role that machine
intelligence can play in enhancing human abilities, especially those that have
been diminished due to injury or illness. As a primary contribution, we develop
the hypothesis that assistive devices, and specifically artificial arms and
hands, can and should be viewed as agents in order for us to most effectively
improve their collaboration with their human users. We believe that increased
agency will enable more powerful interactions between human users and next
generation prosthetic devices, especially when the sensorimotor space of the
prosthetic technology greatly exceeds the conventional control and
communication channels available to a prosthetic user. To more concretely
examine an agency-based view on prosthetic devices, we propose a new schema for
interpreting the capacity of a human-machine collaboration as a function of
both the human's and machine's degrees of agency. We then introduce the idea of
communicative capital as a way of thinking about the communication resources
developed by a human and a machine during their ongoing interaction. Using this
schema of agency and capacity, we examine the benefits and disadvantages of
increasing the agency of a prosthetic limb. To do so, we present an analysis of
examples from the literature where building communicative capital has enabled a
progression of fruitful, task-directed interactions between prostheses and
their human users. We then describe further work that is needed to concretely
evaluate the hypothesis that prostheses are best thought of as agents. The
agent-based viewpoint developed in this article significantly extends current
thinking on how best to support the natural, functional use of increasingly
complex prosthetic enhancements, and opens the door for more powerful
interactions between humans and their assistive technologies.



Intrinsic decomposition from a single image is a highly challenging task, due
to its inherent ambiguity and the scarcity of training data. In contrast to
traditional fully supervised learning approaches, in this paper we propose
learning intrinsic image decomposition by explaining the input image. Our
model, the Rendered Intrinsics Network (RIN), joins together an image
decomposition pipeline, which predicts reflectance, shape, and lighting
conditions given a single image, with a recombination function, a learned
shading model used to recompose the original input based off of intrinsic image
predictions. Our network can then use unsupervised reconstruction error as an
additional signal to improve its intermediate representations. This allows
large-scale unlabeled data to be useful during training, and also enables
transferring learned knowledge to images of unseen object categories, lighting
conditions, and shapes. Extensive experiments demonstrate that our method
performs well on both intrinsic image decomposition and knowledge transfer.



We introduce CARLA, an open-source simulator for autonomous driving research.
CARLA has been developed from the ground up to support development, training,
and validation of autonomous urban driving systems. In addition to open-source
code and protocols, CARLA provides open digital assets (urban layouts,
buildings, vehicles) that were created for this purpose and can be used freely.
The simulation platform supports flexible specification of sensor suites and
environmental conditions. We use CARLA to study the performance of three
approaches to autonomous driving: a classic modular pipeline, an end-to-end
model trained via imitation learning, and an end-to-end model trained via
reinforcement learning. The approaches are evaluated in controlled scenarios of
increasing difficulty, and their performance is examined via metrics provided
by CARLA, illustrating the platform's utility for autonomous driving research.
The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E



Within-Class Covariance Normalization (WCCN) is a powerful post-processing
method for normalizing the within-class covariance of a set of data points.
WCCN projects the observations into a linear sub-space where the within-class
variability is reduced. This property has proven to be beneficial in subsequent
recognition tasks. The central idea of this paper is to reformulate the classic
WCCN as a Deep Neural Network (DNN) compatible version. We propose the Deep
WithinClass Covariance Analysis (DWCCA) which can be incorporated in a DNN
architecture. This formulation enables us to exploit the beneficial properties
of WCCN, and still allows for training with Stochastic Gradient Descent (SGD)
in an end-to-end fashion. We investigate the advantages of DWCCA on deep neural
networks with convolutional layers for supervised learning. Our results on
Acoustic Scene Classification show that via DWCCA we can achieves equal or
superior performance in a VGG-style deep neural network.



Differential performance debugging is a technique to find performance
problems. It applies in situations where the performance of a program is
(unexpectedly) different for different classes of inputs. The task is to
explain the differences in asymptotic performance among various input classes
in terms of program internals. We propose a data-driven technique based on
discriminant regression tree (DRT) learning problem where the goal is to
discriminate among different classes of inputs. We propose a new algorithm for
DRT learning that first clusters the data into functional clusters, capturing
different asymptotic performance classes, and then invokes off-the-shelf
decision tree learning algorithms to explain these clusters. We focus on linear
functional clusters and adapt classical clustering algorithms (K-means and
spectral) to produce them. For the K-means algorithm, we generalize the notion
of the cluster centroid from a point to a linear function. We adapt spectral
clustering by defining a novel kernel function to capture the notion of linear
similarity between two data points. We evaluate our approach on benchmarks
consisting of Java programs where we are interested in debugging performance.
We show that our algorithm significantly outperforms other well-known
regression tree learning algorithms in terms of running time and accuracy of
classification.



We search for digital biomarkers from Parkinson's Disease by observing
approximate repetitive patterns matching hypothesized step and stride periodic
cycles. These observations were modeled as a cycle of hidden states with
randomness allowing deviation from a canonical pattern of transitions and
emissions, under the hypothesis that the averaged features of hidden states
would serve to informatively characterize classes of patients/controls. We
propose a Hidden Semi-Markov Model (HSMM), a latent-state model, emitting
3D-acceleration vectors. Transitions and emissions are inferred from data. We
fit separate models per unique device and training label. Hidden Markov Models
(HMM) force geometric distributions of the duration spent at each state before
transition to a new state. Instead, our HSMM allows us to specify the
distribution of state duration. This modified version is more effective because
we are interested more in each state's duration than the sequence of distinct
states, allowing inclusion of these durations the feature vector.



Recommender engines have become an integral component in today's e-commerce
systems. From recommending books in Amazon to finding friends in social
networks such as Facebook, they have become omnipresent.
  Generally, recommender systems can be classified into two main categories:
content based and collaborative filtering based models. Both these models build
relationships between users and items to provide recommendations. Content based
systems achieve this task by utilizing features extracted from the context
available, whereas collaborative systems use shared interests between user-item
subsets.
  There is another relatively unexplored approach for providing recommendations
that utilizes a stochastic process named random walks. This study is a survey
exploring use cases of random walks in recommender systems and an attempt at
classifying them.



The quest for algorithms that enable cognitive abilities is an important part
of machine learning. A common trait in many recently investigated
cognitive-like tasks is that they take into account different data modalities,
such as visual and textual input. In this paper we propose a novel and
generally applicable form of attention mechanism that learns high-order
correlations between various data modalities. We show that high-order
correlations effectively direct the appropriate attention to the relevant
elements in the different data modalities that are required to solve the joint
task. We demonstrate the effectiveness of our high-order attention mechanism on
the task of visual question answering (VQA), where we achieve state-of-the-art
performance on the standard VQA dataset.



A primary goal of computational phenotype research is to conduct medical
diagnosis. In hospital, physicians rely on massive clinical data to make
diagnosis decisions, among which laboratory tests are one of the most important
resources. However, the longitudinal and incomplete nature of laboratory test
data casts a significant challenge on its interpretation and usage, which may
result in harmful decisions by both human physicians and automatic diagnosis
systems. In this work, we take advantage of deep generative models to deal with
the complex laboratory tests. Specifically, we propose an end-to-end
architecture that involves a deep generative variational recurrent neural
networks (VRNN) to learn robust and generalizable features, and a
discriminative neural network (NN) model to learn diagnosis decision making,
and the two models are trained jointly. Our experiments are conducted on a
dataset involving 46,252 patients, and the 50 most frequent tests are used to
predict the 50 most common diagnoses. The results show that our model, VRNN+NN,
significantly (p<0.001) outperforms other baseline models. Moreover, we
demonstrate that the representations learned by the joint training are more
informative than those learned by pure generative models. Finally, we find that
our model offers a surprisingly good imputation for missing values.



Synthesizing SQL queries from natural language is a long-standing open
problem and has been attracting considerable interest recently. Toward solving
the problem, the de facto approach is to employ a sequence-to-sequence-style
model. Such an approach will necessarily require the SQL queries to be
serialized. Since the same SQL query may have multiple equivalent
serializations, training a sequence-to-sequence-style model is sensitive to the
choice from one of them. This phenomenon is documented as the "order-matters"
problem. Existing state-of-the-art approaches rely on reinforcement learning to
reward the decoder when it generates any of the equivalent serializations.
However, we observe that the improvement from reinforcement learning is
limited.
  In this paper, we propose a novel approach, i.e., SQLNet, to fundamentally
solve this problem by avoiding the sequence-to-sequence structure when the
order does not matter. In particular, we employ a sketch-based approach where
the sketch contains a dependency graph so that one prediction can be done by
taking into consideration only the previous predictions that it depends on. In
addition, we propose a sequence-to-set model as well as the column attention
mechanism to synthesize the query based on the sketch. By combining all these
novel techniques, we show that SQLNet can outperform the prior art by 9% to 13%
on the WikiSQL task.



Audience interest, demography, purchase behavior and other possible
classifications are ex- tremely important factors to be carefully studied in a
targeting campaign. This information can help advertisers and publishers
deliver advertisements to the right audience group. How- ever, it is not easy
to collect such information, especially for the online audience with whom we
have limited interaction and minimum deterministic knowledge. In this paper, we
pro- pose a predictive framework that can estimate online audience demographic
attributes based on their browsing histories. Under the proposed framework,
first, we retrieve the content of the websites visited by audience, and
represent the content as website feature vectors; second, we aggregate the
vectors of websites that audience have visited and arrive at feature vectors
representing the users; finally, the support vector machine is exploited to
predict the audience demographic attributes. The key to achieving good
prediction performance is preparing representative features of the audience.
Word Embedding, a widely used tech- nique in natural language processing tasks,
together with term frequency-inverse document frequency weighting scheme is
used in the proposed method. This new representation ap- proach is unsupervised
and very easy to implement. The experimental results demonstrate that the new
audience feature representation method is more powerful than existing baseline
methods, leading to a great improvement in prediction accuracy.



Neural networks have recently had a lot of success for many tasks. However,
neural network architectures that perform well are still typically designed
manually by experts in a cumbersome trial-and-error process. We propose a new
method to automatically search for well-performing CNN architectures based on a
simple hill climbing procedure whose operators apply network morphisms,
followed by short optimization runs by cosine annealing. Surprisingly, this
simple method yields competitive results, despite only requiring resources in
the same order of magnitude as training a single network. E.g., on CIFAR-10,
our method designs and trains networks with an error rate below 6% in only 12
hours on a single GPU; training for one day reduces this error further, to
almost 5%.



Training automatic speech recognition (ASR) systems requires large amounts of
data in the target language in order to achieve good performance. Whereas large
training corpora are readily available for languages like English, there exists
a long tail of languages which do suffer from a lack of resources. One method
to handle data sparsity is to use data from additional source languages and
build a multilingual system. Recently, ASR systems based on recurrent neural
networks (RNNs) trained with connectionist temporal classification (CTC) have
gained substantial research interest. In this work, we extended our previous
approach towards training CTC-based systems multilingually. Our systems feature
a global phone set, based on the joint phone sets of each source language. We
evaluated the use of different language combinations as well as the addition of
Language Feature Vectors (LFVs). As contrastive experiment, we built systems
based on graphemes as well. Systems having a multilingual phone set are known
to suffer in performance compared to their monolingual counterparts. With our
proposed approach, we could reduce the gap between these mono- and multilingual
setups, using either graphemes or phonemes.



A large amount of data is required for automatic speech recognition (ASR)
systems achieving good performance. While such data is readily available for
languages like English, there exists a long tail of languages with only limited
language resources. By using data from additional source languages, this
problem can be mitigated. In this work, we focus on multilingual systems based
on recurrent neural networks (RNNs), trained using the Connectionist Temporal
Classification (CTC) loss function. Using a multilingual set of acoustic units
to train systems jointly on multiple languages poses difficulties: While the
same phones share the same symbols across languages, they are pronounced
slightly different because of, e.g., small shifts in tongue positions. To
address this issue, we proposed Language Feature Vectors (LFVs) to train
language adaptive multilingual systems. In this work, we extended this approach
by introducing a novel technique which we call "modulation" to add LFVs . We
evaluated our approach in multiple conditions, showing improvements in both
full and low resource conditions as well as for grapheme and phone based
systems.



We study the properties of the endpoint of stochastic gradient descent (SGD).
By approximating SGD as a stochastic differential equation (SDE) we consider
the Boltzmann-Gibbs equilibrium distribution of that SDE under the assumption
of isotropic variance in loss gradients. Through this analysis, we find that
three factors - learning rate, batch size and the variance of the loss
gradients - control the trade-off between the depth and width of the minima
found by SGD, with wider minima favoured by a higher ratio of learning rate to
batch size. We have direct control over the learning rate and batch size, while
the variance is determined by the choice of model architecture, model
parameterization and dataset. In the equilibrium distribution only the ratio of
learning rate to batch size appears, implying that the equilibrium distribution
is invariant under a simultaneous rescaling of learning rate and batch size by
the same amount. We then explore experimentally how learning rate and batch
size affect SGD from two perspectives: the endpoint of SGD and the dynamics
that lead up to it. For the endpoint, the experiments suggest the endpoint of
SGD is invariant under simultaneous rescaling of batch size and learning rate,
and also that a higher ratio leads to flatter minima, both findings are
consistent with our theoretical analysis. We note experimentally that the
dynamics also seem to be invariant under the same rescaling of learning rate
and batch size, which we explore showing that one can exchange batch size and
learning rate for cyclical learning rate schedule. Next, we illustrate how
noise affects memorization, showing that high noise levels lead to better
generalization. Finally, we find experimentally that the invariance under
simultaneous rescaling of learning rate and batch size breaks down if the
learning rate gets too large or the batch size gets too small.



Geosciences is a field of great societal relevance that requires solutions to
several urgent problems facing our humanity and the planet. As geosciences
enters the era of big data, machine learning (ML) -- that has been widely
successful in commercial domains -- offers immense potential to contribute to
problems in geosciences. However, problems in geosciences have several unique
challenges that are seldom found in traditional applications, requiring novel
problem formulations and methodologies in machine learning. This article
introduces researchers in the machine learning (ML) community to these
challenges offered by geoscience problems and the opportunities that exist for
advancing both machine learning and geosciences. We first highlight typical
sources of geoscience data and describe their properties that make it
challenging to use traditional machine learning techniques. We then describe
some of the common categories of geoscience problems where machine learning can
play a role, and discuss some of the existing efforts and promising directions
for methodological development in machine learning. We conclude by discussing
some of the emerging research themes in machine learning that are applicable
across all problems in the geosciences, and the importance of a deep
collaboration between machine learning and geosciences for synergistic
advancements in both disciplines.



Large volumes of spatio-temporal data are increasingly collected and studied
in diverse domains including, climate science, social sciences, neuroscience,
epidemiology, transportation, mobile health, and Earth sciences.
Spatio-temporal data differs from relational data for which computational
approaches are developed in the data mining community for multiple decades, in
that both spatial and temporal attributes are available in addition to the
actual measurements/attributes. The presence of these attributes introduces
additional challenges that needs to be dealt with. Approaches for mining
spatio-temporal data have been studied for over a decade in the data mining
community. In this article we present a broad survey of this relatively young
field of spatio-temporal data mining. We discuss different types of
spatio-temporal data and the relevant data mining questions that arise in the
context of analyzing each of these datasets. Based on the nature of the data
mining problem studied, we classify literature on spatio-temporal data mining
into six major categories: clustering, predictive learning, change detection,
frequent pattern mining, anomaly detection, and relationship mining. We discuss
the various forms of spatio-temporal data mining problems in each of these
categories.



We discuss practical methods to ensure near wirespeed performance from
clusters with either one or two Intel(R) Omni-Path host fabric interfaces (HFI)
per node, and Intel(R) Xeon Phi(TM) 72xx (Knight's Landing) processors, and
using the Linux operating system.
  The study evaluates the performance improvements achievable and the required
programming approaches in two distinct example problems: firstly in Cartesian
communicator halo exchange problems, appropriate for structured grid PDE
solvers that arise in quantum chromodynamics simulations of particle physics,
and secondly in gradient reduction appropriate to synchronous stochastic
gradient descent for machine learning. As an example, we accelerate a published
Baidu Research reduction code and obtain a factor of ten speedup over the
original code using the techniques discussed in this paper. This displays how a
factor of ten speedup in strongly scaled distributed machine learning could be
achieved when synchronous stochastic gradient descent is massively parallelised
with a fixed mini-batch size.
  We find a significant improvement in performance robustness when memory is
obtained using carefully allocated 2MB "huge" virtual memory pages, implying
that either non-standard allocation routines should be used for communication
buffers. These can be accessed via a LD\_PRELOAD override in the manner
suggested by libhugetlbfs. We make use of a the Intel(R) MPI 2019 library
"Technology Preview" and underlying software to enable thread concurrency
throughout the communication software stake via multiple PSM2 endpoints per
process and use of multiple independent MPI communicators. When using a single
MPI process per node, we find that this greatly accelerates delivered bandwidth
in many core Intel(R) Xeon Phi processors.



Selecting the appropriate visual presentation of the data such that it
preserves the semantics of the underlying data and at the same time provides an
intuitive summary of the data is an important, often the final step of data
analytics. Unfortunately, this is also a step involving significant human
effort starting from selection of groups of columns in the structured results
from analytics stages, to the selection of right visualization by experimenting
with various alternatives. In this paper, we describe our \emph{DataVizard}
system aimed at reducing this overhead by automatically recommending the most
appropriate visual presentation for the structured result. Specifically, we
consider the following two scenarios: first, when one needs to visualize the
results of a structured query such as SQL; and the second, when one has
acquired a data table with an associated short description (e.g., tables from
the Web). Using a corpus of real-world database queries (and their results) and
a number of statistical tables crawled from the Web, we show that DataVizard is
capable of recommending visual presentations with high accuracy. We also
present the results of a user survey that we conducted in order to assess user
views of the suitability of the presented charts vis-a-vis the plain text
captions of the data.



This article presents the use of Answer Set Programming (ASP) to mine
sequential patterns. ASP is a high-level declarative logic programming paradigm
for high level encoding combinatorial and optimization problem solving as well
as knowledge representation and reasoning. Thus, ASP is a good candidate for
implementing pattern mining with background knowledge, which has been a data
mining issue for a long time. We propose encodings of the classical sequential
pattern mining tasks within two representations of embeddings (fill-gaps vs
skip-gaps) and for various kinds of patterns: frequent, constrained and
condensed. We compare the computational performance of these encodings with
each other to get a good insight into the efficiency of ASP encodings. The
results show that the fill-gaps strategy is better on real problems due to
lower memory consumption. Finally, compared to a constraint programming
approach (CPSM), another declarative programming paradigm, our proposal showed
comparable performance.



Semantic parsers translate language utterances to programs, but are often
trained from utterance-denotation pairs only. Consequently, parsers must
overcome the problem of spuriousness at training time, where an incorrect
program found at search time accidentally leads to a correct denotation. We
propose that in small well-typed domains, we can semi-automatically generate an
abstract representation for examples that facilitates information sharing
across examples. This alleviates spuriousness, as the probability of randomly
obtaining a correct answer from a program decreases across multiple examples.
We test our approach on CNLVR, a challenging visual reasoning dataset, where
spuriousness is central because denotations are either TRUE or FALSE, and thus
random programs have high probability of leading to a correct denotation. We
develop the first semantic parser for this task and reach 83.5% accuracy, a
15.7% absolute accuracy improvement compared to the best reported accuracy so
far.



We study the problem of multiset prediction. The goal of multiset prediction
is to train a predictor that maps an input to a multiset consisting of multiple
items. Unlike existing problems in supervised learning, such as classification,
ranking and sequence generation, there is no known order among items in a
target multiset, and each item in the multiset may appear more than once,
making this problem extremely challenging. In this paper, we propose a novel
multiset loss function by viewing this problem from the perspective of
sequential decision making. The proposed multiset loss function is empirically
evaluated on two families of datasets, one synthetic and the other real, with
varying levels of difficulty, against various baseline loss functions including
reinforcement learning, sequence, and aggregated distribution matching loss
functions. The experiments reveal the effectiveness of the proposed loss
function over the others.



We address the problem of learning vector representations for entities and
relations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This
problem has received significant attention in the past few years and multiple
methods have been proposed. Most of the existing methods in the literature use
a predefined characteristic scoring function for evaluating the correctness of
KG triples. These scoring functions distinguish correct triples (high score)
from incorrect ones (low score). However, their performance vary across
different datasets. In this work, we demonstrate that a simple neural network
based score function can consistently achieve near start-of-the-art performance
on multiple datasets. We also quantitatively demonstrate biases in standard
benchmark datasets, and highlight the need to perform evaluation spanning
various datasets.



Automatic generation of caption to describe the content of an image has been
gaining a lot of research interests recently, where most of the existing works
treat the image caption as pure sequential data. Natural language, however
possess a temporal hierarchy structure, with complex dependencies between each
subsequence. In this paper, we propose a phrase-based hierarchical Long
Short-Term Memory (phi-LSTM) model to generate image description. In contrast
to the conventional solutions that generate caption in a pure sequential
manner, our proposed model decodes image caption from phrase to sentence. It
consists of a phrase decoder at the bottom hierarchy to decode noun phrases of
variable length, and an abbreviated sentence decoder at the upper hierarchy to
decode an abbreviated form of the image description. A complete image caption
is formed by combining the generated phrases with sentence during the inference
stage. Empirically, our proposed model shows a better or competitive result on
the Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the
art models. We also show that our proposed model is able to generate more novel
captions (not seen in the training data) which are richer in word contents in
all these three datasets.



We present a new algorithm that significantly improves the efficiency of
exploration for deep Q-learning agents in dialogue systems. Our agents explore
via Thompson sampling, drawing Monte Carlo samples from a Bayes-by-Backprop
neural network. Our algorithm learns much faster than common exploration
strategies such as \epsilon-greedy, Boltzmann, bootstrapping, and
intrinsic-reward-based ones. Additionally, we show that spiking the replay
buffer with experiences from just a few successful episodes can make Q-learning
feasible when it might otherwise fail.



We consider a reinforcement learning (RL) setting in which the agent
interacts with a sequence of episodic MDPs. At the start of each episode the
agent has access to some side-information or context that determines the
dynamics of the MDP for that episode. Our setting is motivated by applications
in healthcare where baseline measurements of a patient at the start of a
treatment episode form the context that may provide information about how the
patient might respond to treatment decisions. We propose algorithms for
learning in such Contextual Markov Decision Processes (CMDPs) under an
assumption that the unobserved MDP parameters vary smoothly with the observed
context. We also give lower and upper PAC bounds under the smoothness
assumption. Because our lower bound has an exponential dependence on the
dimension, we consider a tractable linear setting where the context is used to
create linear combinations of a finite set of MDPs. For the linear setting, we
give a PAC learning algorithm based on KWIK learning techniques.



We study the multi-armed bandit problem with multiple plays and a budget
constraint for both the stochastic and the adversarial setting. At each round,
exactly $K$ out of $N$ possible arms have to be played (with $1\leq K \leq N$).
In addition to observing the individual rewards for each arm played, the player
also learns a vector of costs which has to be covered with an a-priori defined
budget $B$. The game ends when the sum of current costs associated with the
played arms exceeds the remaining budget.
  Firstly, we analyze this setting for the stochastic case, for which we assume
each arm to have an underlying cost and reward distribution with support
$[c_{\min}, 1]$ and $[0, 1]$, respectively. We derive an Upper Confidence Bound
(UCB) algorithm which achieves $O(NK^4 \log B)$ regret.
  Secondly, for the adversarial case in which the entire sequence of rewards
and costs is fixed in advance, we derive an upper bound on the regret of order
$O(\sqrt{NB\log(N/K)})$ utilizing an extension of the well-known
$\texttt{Exp3}$ algorithm. We also provide upper bounds that hold with high
probability and a lower bound of order $\Omega((1 - K/N)^2 \sqrt{NB/K})$.



Search engines rely heavily on term-based approaches that represent queries
and documents as bags of words. Text---a document or a query---is represented
by a bag of its words that ignores grammar and word order, but retains word
frequency counts. When presented with a search query, the engine then ranks
documents according to their relevance scores by computing, among other things,
the matching degrees between query and document terms. While term-based
approaches are intuitive and effective in practice, they are based on the
hypothesis that documents that exactly contain the query terms are highly
relevant regardless of query semantics. Inversely, term-based approaches assume
documents that do not contain query terms as irrelevant. However, it is known
that a high matching degree at the term level does not necessarily mean high
relevance and, vice versa, documents that match null query terms may still be
relevant. Consequently, there exists a vocabulary gap between queries and
documents that occurs when both use different words to describe the same
concepts. It is the alleviation of the effect brought forward by this
vocabulary gap that is the topic of this dissertation. More specifically, we
propose (1) methods to formulate an effective query from complex textual
structures and (2) latent vector space models that circumvent the vocabulary
gap in information retrieval.



Goal-conditional policies allow reinforcement learning agents to pursue
specific goals during different episodes. In addition to their potential to
generalize desired behavior to unseen goals, such policies may also help in
defining options for arbitrary subgoals, enabling higher-level planning. While
trying to achieve a specific goal, an agent may also be able to exploit
information about the degree to which it has achieved alternative goals.
Reinforcement learning agents have only recently been endowed with such
capacity for hindsight, which is highly valuable in environments with sparse
rewards. In this paper, we show how hindsight can be introduced to
likelihood-ratio policy gradient methods, generalizing this capacity to an
entire class of highly successful algorithms. Our preliminary experiments
suggest that hindsight may increase the sample efficiency of policy gradient
methods.



Pandemic influenza has the epidemic potential to kill millions of people.
While various preventive measures exist (i.a., vaccination and school
closures), deciding on strategies that lead to their most effective and
efficient use, remains challenging. To this end, individual-based
epidemiological models are essential to assist decision makers in determining
the best strategy to curve epidemic spread. However, individual-based models
are computationally intensive and therefore it is pivotal to identify the
optimal strategy using a minimal amount of model evaluations. Additionally, as
epidemiological modeling experiments need to be planned, a computational budget
needs to be specified a priori. Consequently, we present a new sampling method
to optimize the evaluation of preventive strategies using fixed budget best-arm
identification algorithms. We use epidemiological modeling theory to derive
knowledge about the reward distribution which we exploit using Bayesian
best-arm identification algorithms (i.e., Top-two Thompson sampling and
BayesGap). We evaluate these algorithms in a realistic experimental setting and
demonstrate that it is possible to identify the optimal strategy using only a
limited number of model evaluations, i.e., 2-to-3 times faster compared to the
uniform sampling method, the predominant technique used for epidemiological
decision making in the literature. Finally, we contribute and evaluate a
statistic for Top-two Thompson sampling to inform the decision makers about the
confidence of an arm recommendation.



Active queue control aims to improve the overall communication network
throughput while providing lower delay and small packet loss rate. The basic
idea is to actively trigger packet dropping (or marking provided by explicit
congestion notification (ECN)) before buffer overflow. In this paper, two
artificial neural networks (ANN)-based control schemes are proposed for
adaptive queue control in TCP communication networks. The structure of these
controllers is optimized using genetic algorithm (GA) and the output weights of
ANNs are optimized using particle swarm optimization (PSO) algorithm. The
controllers are radial bias function (RBF)-based, but to improve the robustness
of RBF controller, an error-integral term is added to RBF equation in the
second scheme. Experimental results show that GA- PSO-optimized improved RBF
(I-RBF) model controls network congestion effectively in terms of link
utilization with a low packet loss rate and outperform Drop Tail,
proportional-integral (PI), random exponential marking (REM), and adaptive
random early detection (ARED) controllers.



There is an increasing interest in exploiting mobile sensing technologies and
machine learning techniques for mental health monitoring and intervention.
Researchers have effectively used contextual information, such as mobility,
communication and mobile phone usage patterns for quantifying individuals' mood
and wellbeing. In this paper, we investigate the effectiveness of neural
network models for predicting users' level of stress by using the location
information collected by smartphones. We characterize the mobility patterns of
individuals using the GPS metrics presented in the literature and employ these
metrics as input to the network. We evaluate our approach on the open-source
StudentLife dataset. Moreover, we discuss the challenges and trade-offs
involved in building machine learning models for digital mental health and
highlight potential future work in this direction.



A hallmark of human intelligence is the ability to ask rich, creative, and
revealing questions. Here we introduce a cognitive model capable of
constructing human-like questions. Our approach treats questions as formal
programs that, when executed on the state of the world, output an answer. The
model specifies a probability distribution over a complex, compositional space
of programs, favoring concise programs that help the agent learn in the current
context. We evaluate our approach by modeling the types of open-ended questions
generated by humans who were attempting to learn about an ambiguous situation
in a game. We find that our model predicts what questions people will ask, and
can creatively produce novel questions that were not present in the training
set. In addition, we compare a number of model variants, finding that both
question informativeness and complexity are important for producing human-like
questions.



We present a method for explaining the image classification predictions of
deep convolution neural networks, by highlighting the pixels in the image which
influence the final class prediction. Our method requires the identification of
a heuristic method to select parameters hypothesized to be most relevant in
this prediction, and here we use Kullback-Leibler divergence to provide this
focus. Overall, our approach helps in understanding and interpreting deep
network predictions and we hope contributes to a foundation for such
understanding of deep learning networks. In this brief paper, our experiments
evaluate the performance of two popular networks in this context of
interpretability.



We propose a simple yet effective technique to simplify the training and the
resulting model of neural networks. In back propagation, only a small subset of
the full gradient is computed to update the model parameters. The gradient
vectors are sparsified in such a way that only the top-$k$ elements (in terms
of magnitude) are kept. As a result, only $k$ rows or columns (depending on the
layout) of the weight matrix are modified, leading to a linear reduction in the
computational cost. Based on the sparsified gradients, we further simplify the
model by eliminating the rows or columns that are seldom updated, which will
reduce the computational cost both in the training and decoding, and
potentially accelerate decoding in real-world applications. Surprisingly,
experimental results demonstrate that most of time we only need to update fewer
than 5% of the weights at each back propagation pass. More interestingly, the
accuracy of the resulting models is actually improved rather than degraded, and
a detailed analysis is given. The model simplification results show that we
could adaptively simplify the model which could often be reduced by around 9x,
without any loss on accuracy or even with improved accuracy.



Achieving superhuman playing level by AlphaGo corroborated the capabilities
of convolutional neural architectures (CNNs) for capturing complex spatial
patterns. This result was to a great extent due to several analogies between Go
board states and 2D images CNNs have been designed for, in particular
translational invariance and a relatively large board. In this paper, we verify
whether CNN-based move predictors prove effective for Othello, a game with
significantly different characteristics, including a much smaller board size
and complete lack of translational invariance. We compare several CNN
architectures and board encodings, augment them with state-of-the-art
extensions, train on an extensive database of experts' moves, and examine them
with respect to move prediction accuracy and playing strength. The empirical
evaluation confirms high capabilities of neural move predictors and suggests a
strong correlation between prediction accuracy and playing strength. The best
CNNs not only surpass all other 1-ply Othello players proposed to date but
defeat (2-ply) Edax, the best open-source Othello player.



Designing soft robots poses considerable challenges: automated design
approaches may be particularly appealing in this field, as they promise to
optimize complex multi-material machines with very little or no human
intervention. Evolutionary soft robotics is concerned with the application of
optimization algorithms inspired by natural evolution in order to let soft
robots (both morphologies and controllers) spontaneously evolve within
physically-realistic simulated environments, figuring out how to satisfy a set
of objectives defined by human designers. In this paper a powerful evolutionary
system is put in place in order to perform a broad investigation on the
free-form evolution of walking and swimming soft robots in different
environments. Three sets of experiments are reported, tackling different
aspects of the evolution of soft locomotion. The first two sets explore the
effects of different material properties on the evolution of terrestrial and
aquatic soft locomotion: particularly, we show how different materials lead to
the evolution of different morphologies, behaviors, and energy-performance
tradeoffs. It is found that within our simplified physics world stiffer robots
evolve more sophisticated and effective gaits and morphologies on land, while
softer ones tend to perform better in water. The third set of experiments
starts investigating the effect and potential benefits of major environmental
transitions (land - water) during evolution. Results provide interesting
morphological exaptation phenomena, and point out a potential asymmetry between
land-water and water-land transitions: while the first type of transition
appears to be detrimental, the second one seems to have some beneficial
effects.



We present a self-supervised approach to ignoring "distractors" in camera
images for the purposes of robustly estimating vehicle motion in cluttered
urban environments. We leverage offline multi-session mapping approaches to
automatically generate a per-pixel ephemerality mask and depth map for each
input image, which we use to train a deep convolutional network. At run-time we
use the predicted ephemerality and depth as an input to a monocular visual
odometry (VO) pipeline, using either sparse features or dense photometric
matching. Our approach yields metric-scale VO using only a single camera and
can recover the correct egomotion even when 90% of the image is obscured by
dynamic, independently moving objects. We evaluate our robust VO methods on
more than 400km of driving from the Oxford RobotCar Dataset and demonstrate
reduced odometry drift and significantly improved egomotion estimation in the
presence of large moving vehicles in urban traffic.



Deep lifelong learning systems need to efficiently manage resources to scale
to large numbers of experiences and non-stationary goals. In this paper, we
explore the relationship between lossy compression and the resource constrained
lifelong learning problem of function transferability. We demonstrate that
lossy episodic experience storage can enable efficient function transferability
between different architectures and algorithms at a fraction of the storage
cost of lossless storage. This is achieved by introducing a generative
knowledge distillation strategy that does not store any full training examples.
As an important extension of this idea, we show that lossy recollections
stabilize deep networks much better than lossless sampling in resource
constrained settings of lifelong learning while avoiding catastrophic
forgetting. For this setting, we propose a novel dual purpose recollection
buffer used to both stabilize the recollection generator itself and an
accompanying reasoning model.



Spatial understanding is a fundamental problem with wide-reaching real-world
applications. The representation of spatial knowledge is often modeled with
spatial templates, i.e., regions of acceptability of two objects under an
explicit spatial relationship (e.g., "on", "below", etc.). In contrast with
prior work that restricts spatial templates to explicit spatial prepositions
(e.g., "glass on table"), here we extend this concept to implicit spatial
language, i.e., those relationships (generally actions) for which the spatial
arrangement of the objects is only implicitly implied (e.g., "man riding
horse"). In contrast with explicit relationships, predicting spatial
arrangements from implicit spatial language requires significant common sense
spatial understanding. Here, we introduce the task of predicting spatial
templates for two objects under a relationship, which can be seen as a spatial
question-answering task with a (2D) continuous output ("where is the man w.r.t.
a horse when the man is walking the horse?"). We present two simple
neural-based models that leverage annotated images and structured text to learn
this task. The good performance of these models reveals that spatial locations
are to a large extent predictable from implicit spatial language. Crucially,
the models attain similar performance in a challenging generalized setting,
where the object-relation-object combinations (e.g.,"man walking dog") have
never been seen before. Next, we go one step further by presenting the models
with unseen objects (e.g., "dog"). In this scenario, we show that leveraging
word embeddings enables the models to output accurate spatial predictions,
proving that the models acquire solid common sense spatial knowledge allowing
for such generalization.



This paper proposes a novel game-theoretical autonomous decision-making
framework to address a task allocation problem for a swarm of multiple agents.
We consider cooperation of self-interested agents and show that agents who have
social inhibition can converge to a Nash stable partition (i.e., social
agreement) using our proposed decentralised algorithm within polynomial time.
The algorithm is simple and executable based on local interactions with
neighbour agents under a strongly-connected communication network and even in
asynchronous environments. We analytically present a mathematical formulation
for computing the lower bound of a converged solution's suboptimality and
additionally show that 50 % of suboptimality can be minimally guaranteed if
social utilities are non-decreasing functions with respect to the number of
co-working agents. Through numerical experiments, it is confirmed that the
proposed framework is scalable, fast adaptable against dynamical environments,
and robust even in a realistic situation where some of the agents temporarily
somehow do not operate during a mission.



In this paper, we present our approach to solve a physics-based reinforcement
learning challenge "Learning to Run" with objective to train
physiologically-based human model to navigate a complex obstacle course as
quickly as possible. The environment is computationally expensive, has a
high-dimensional continuous action space and is stochastic. We benchmark state
of the art policy-gradient methods and test several improvements, such as layer
normalization, parameter noise, action and state reflecting, to stabilize
training and improve its sample-efficiency. We found that the Deep
Deterministic Policy Gradient method is the most efficient method for this
environment and the improvements we have introduced help to stabilize training.
Learned models are able to generalize to new physical scenarios, e.g. different
obstacle courses.



In this paper, we study the problem of learning image classification models
with label noise. Existing approaches depending on human supervision are
generally not scalable as manually identifying correct or incorrect labels is
timeconsuming, whereas approaches not relying on human supervision are scalable
but less effective. To reduce the amount of human supervision for label noise
cleaning, we introduce CleanNet, a joint neural embedding network, which only
requires a fraction of the classes being manually verified to provide the
knowledge of label noise that can be transferred to other classes. We further
integrate CleanNet and conventional convolutional neural network classifier
into one framework for image classification learning. We demonstrate the
effectiveness of the proposed algorithm on both of the label noise detection
task and the image classification on noisy data task on several large-scale
datasets. Experimental results show that CleanNet can reduce label noise
detection error rate on held-out classes where no human supervision available
by 41.5% compared to current weakly supervised methods. It also achieves 47% of
the performance gain of verifying all images with only 3.2% images verified on
an image classification task.



Geometry theorem proving forms a major and challenging component in the K-12
mathematics curriculum. A particular difficult task is to add auxiliary
constructions (i.e, additional lines or points) to aid proof discovery.
Although there exist many intelligent tutoring systems proposed for geometry
proofs, few teach students how to find auxiliary constructions. And the few
exceptions are all limited by their underlying reasoning processes for
supporting auxiliary constructions. This paper tackles these weaknesses of
prior systems by introducing an interactive geometry tutor, the Advanced
Geometry Proof Tutor (AGPT). It leverages a recent automated geometry prover to
provide combined benefits that any geometry theorem prover or intelligent
tutoring system alone cannot accomplish. In particular, AGPT not only can
automatically process images of geometry problems directly, but also can
interactively train and guide students toward discovering auxiliary
constructions on their own. We have evaluated AGPT via a pilot study with 78
high school students. The study results show that, on training students how to
find auxiliary constructions, there is no significant perceived difference
between AGPT and human tutors, and AGPT is significantly more effective than
the state-of-the-art geometry solver that produces human-readable proofs.



Evolutionary algorithms (EAs) are a kind of nature-inspired general-purpose
optimization algorithm, and have shown empirically good performance in solving
various real-word optimization problems. However, due to the highly randomized
and complex behavior, the theoretical analysis of EAs is difficult and is an
ongoing challenge, which has attracted a lot of research attentions. During the
last two decades, promising results on the running time analysis (one essential
theoretical aspect) of EAs have been obtained, while most of them focused on
isolated combinatorial optimization problems, which do not reflect the
general-purpose nature of EAs. To provide a general theoretical explanation of
the behavior of EAs, it is desirable to study the performance of EAs on a
general class of combinatorial optimization problems. To the best of our
knowledge, this direction has been rarely touched and the only known result is
the provably good approximation guarantees of EAs for the problem class of
maximizing monotone submodular set functions with matroid constraints, which
includes many NP-hard combinatorial optimization problems. The aim of this work
is to contribute to this line of research. As many combinatorial optimization
problems also involve non-monotone or non-submodular objective functions, we
consider these two general problem classes, maximizing non-monotone submodular
functions without constraints and maximizing monotone non-submodular functions
with a size constraint. We prove that a simple multi-objective EA called GSEMO
can generally achieve good approximation guarantees in polynomial expected
running time.



A robot that can carry out a natural-language instruction has been a dream
since before the Jetsons cartoon series imagined a life of leisure mediated by
a fleet of attentive robot helpers. It is a dream that remains stubbornly
distant. However, recent advances in vision and language methods have made
incredible progress in closely related areas. This is significant because a
robot interpreting a natural-language navigation instruction on the basis of
what it sees is carrying out a vision and language process that is similar to
Visual Question Answering. Both tasks can be interpreted as visually grounded
sequence-to-sequence translation problems, and many of the same methods are
applicable. To enable and encourage the application of vision and language
methods to the problem of interpreting visually-grounded navigation
instructions, we present the Matterport3D Simulator -- a large-scale
reinforcement learning environment based on real imagery. Using this simulator,
which can in future support a range of embodied vision and language tasks, we
provide the first benchmark dataset for visually-grounded natural language
navigation in real buildings -- the Room-to-Room (R2R) dataset.



We study a classification problem where each feature can be acquired for a
cost and the goal is to optimize the trade-off between classification precision
and the total feature cost. We frame the problem as a sequential
decision-making problem, where we classify one sample in each episode. At each
step, an agent can use values of acquired features to decide whether to
purchase another one or whether to classify the sample. We use vanilla Double
Deep Q-learning, a standard reinforcement learning technique, to find a
classification policy. We show that this generic approach outperforms
Adapt-Gbrt, currently the best-performing algorithm developed specifically for
classification with costly features.



Organisms result from multiple adaptive processes occurring and interacting
at different time scales. One such interaction is that between development and
evolution. In modeling studies, it has been shown that development sweeps over
a series of traits in a single agent, and sometimes exposes promising static
traits. Subsequent evolution can then canalize these rare traits. Thus,
development can, under the right conditions, increase evolvability. Here, we
report on a previously unknown phenomenon when embodied agents are allowed to
develop and evolve: Evolution discovers body plans which are robust to control
changes, these body plans become genetically assimilated, yet controllers for
these agents are not assimilated. This allows evolution to continue climbing
fitness gradients by tinkering with the developmental programs for controllers
within these permissive body plans. This exposes a previously unknown detail
about the Baldwin effect: instead of all useful traits becoming genetically
assimilated, only phenotypic traits that render the agent robust to changes in
other traits become assimilated. We refer to this phenomenon as differential
canalization. This finding also has important implications for the evolutionary
design of artificial and embodied agents such as robots: robots that are robust
to internal changes in their controllers may also be robust to external changes
in their environment, such as transferal from simulation to reality, or
deployment in novel environments.



Transparency, user trust, and human comprehension are popular ethical
motivations for interpretable machine learning. In support of these goals,
researchers evaluate model explanation performance using humans and real world
applications. This alone presents a challenge in many areas of artificial
intelligence. In this position paper, we propose a distinction between
descriptive and persuasive explanations. We discuss reasoning suggesting that
functional interpretability may be correlated with cognitive function and user
preferences. If this is indeed the case, evaluation and optimization using
functional metrics could perpetuate implicit cognitive bias in explanations
that threaten transparency. Finally, we propose two potential research
directions to disambiguate cognitive function and explanation models, retaining
control over the tradeoff between accuracy and interpretability.



A core aspect of human intelligence is the ability to learn new tasks quickly
and switch between them flexibly. Here, we describe a modular continual
reinforcement learning paradigm inspired by these abilities. We first introduce
a visual interaction environment that allows many types of tasks to be unified
in a single framework. We then describe a reward map prediction scheme that
learns new tasks robustly in the very large state and action spaces required by
such an environment. We investigate how properties of module architecture
influence efficiency of task learning, showing that a module motif
incorporating specific design principles (e.g. early bottlenecks, low-order
polynomial nonlinearities, and symmetry) significantly outperforms more
standard neural network motifs, needing fewer training examples and fewer
neurons to achieve high levels of performance. Finally, we present a
meta-controller architecture for task switching based on a dynamic neural
voting scheme, which allows new modules to use information learned from
previously-seen tasks to substantially improve their own learning efficiency.



The Morris Water Maze is commonly used in behavioural neuroscience for the
study of spatial learning with rodents. Over the years, various methods of
analysing rodent data collected in this task have been proposed. These methods
span from classical performance measurements (e.g. escape latency, rodent
speed, quadrant preference) to more sophisticated methods of categorisation
which classify the animal swimming path into behavioural classes known as
strategies. Classification techniques provide additional insight in relation to
the actual animal behaviours but still only a limited amount of studies utilise
them mainly because they highly depend on machine learning knowledge. We have
previously demonstrated that the animals implement various strategies and by
classifying whole trajectories can lead to the loss of important information.
In this work, we developed a generalised and robust classification methodology
which implements majority voting to boost the classification performance and
successfully nullify the need of manual tuning. Based on this framework, we
built a complete software, capable of performing the full analysis described in
this paper. The software provides an easy to use graphical user interface (GUI)
through which users can enter their trajectory data, segment and label them and
finally generate reports and figures of the results.



While deep neural networks have been shown in recent years to outperform
other machine learning methods in a wide range of applications, one of the
biggest challenges with enabling deep neural networks for widespread deployment
on edge devices such as mobile and other consumer devices is high computational
and memory requirements. Recently, there has been greater exploration into
small deep neural network architectures that are more suitable for edge
devices, with one of the most popular architectures being SqueezeNet, with an
incredibly small model size of 4.8MB. Taking further advantage of the notion
that many applications of machine learning on edge devices are often
characterized by a low number of target classes, this study explores the
utility of combining architectural modifications and an evolutionary synthesis
strategy for synthesizing even smaller deep neural architectures based on the
more recent SqueezeNet v1.1 macroarchitecture for applications with fewer
target classes. In particular, architectural modifications are first made to
SqueezeNet v1.1 to accommodate for a 10-class ImageNet-10 dataset, and then an
evolutionary synthesis strategy is leveraged to synthesize more efficient deep
neural networks based on this modified macroarchitecture. The resulting
SquishedNets possess model sizes ranging from 2.4MB to 0.95MB (~5.17X smaller
than SqueezeNet v1.1, or 253X smaller than AlexNet). Furthermore, the
SquishedNets are still able to achieve accuracies ranging from 81.2% to 77%,
and able to process at speeds of 156 images/sec to as much as 256 images/sec on
a Nvidia Jetson TX1 embedded chip. These preliminary results show that a
combination of architectural modifications and an evolutionary synthesis
strategy can be a useful tool for producing very small deep neural network
architectures that are well-suited for edge device scenarios.



The ability to use a 2D map to navigate a complex 3D environment is quite
remarkable, and even difficult for many humans. Localization and navigation is
also an important problem in domains such as robotics, and has recently become
a focus of the deep reinforcement learning community. In this paper we teach a
reinforcement learning agent to read a map in order to find the shortest way
out of a random maze it has never seen before. Our system combines several
state-of-the-art methods such as A3C and incorporates novel elements such as a
recurrent localization cell. Our agent learns to localize itself based on 3D
first person images and an approximate orientation angle. The agent generalizes
well to bigger mazes, showing that it learned useful localization and
navigation capabilities.



The Visual Dialogue task requires an agent to engage in a conversation about
an image with a human. It represents an extension of the Visual Question
Answering task in that the agent needs to answer a question about an image, but
it needs to do so in light of the previous dialogue that has taken place. The
key challenge in Visual Dialogue is thus maintaining a consistent, and natural
dialogue while continuing to answer questions correctly. We present a novel
approach that combines Reinforcement Learning and Generative Adversarial
Networks (GANs) to generate more human-like responses to questions. The GAN
helps overcome the relative paucity of training data, and the tendency of the
typical MLE-based approach to generate overly terse answers. Critically, the
GAN is tightly integrated into the attention mechanism that generates
human-interpretable reasons for each answer. This means that the discriminative
model of the GAN has the task of assessing whether a candidate answer is
generated by a human or not, given the provided reason. This is significant
because it drives the generative model to produce high quality answers that are
well supported by the associated reasoning. The method also generates the
state-of-the-art results on the primary benchmark.



Despite significant progress in a variety of vision-and-language problems,
developing a method capable of asking intelligent, goal-oriented questions
about images is proven to be an inscrutable challenge. Towards this end, we
propose a Deep Reinforcement Learning framework based on three new intermediate
rewards, namely goal-achieved, progressive and informativeness that encourage
the generation of succinct questions, which in turn uncover valuable
information towards the overall goal. By directly optimizing for questions that
work quickly towards fulfilling the overall goal, we avoid the tendency of
existing methods to generate long series of insane queries that add little
value. We evaluate our model on the GuessWhat?! dataset and show that the
resulting questions can help a standard Guesser identify a specific object in
an image at a much higher success rate.



Computer poetry generation is our first step towards computer writing.
Writing must have a theme. The current approaches of using sequence-to-sequence
models with attention often produce non-thematic poems. We present a novel
conditional variational autoencoder with a hybrid decoder adding the
deconvolutional neural networks to the general recurrent neural networks to
fully learn topic information via latent variables. This approach significantly
improves the relevance of the generated poems by representing each line of the
poem not only in a context-sensitive manner but also in a holistic way that is
highly related to the given keyword and the learned topic. A proposed augmented
word2vec model further improves the rhythm and symmetry. Tests show that the
generated poems by our approach are mostly satisfying with regulated rules and
consistent themes, and 73.42% of them receive an Overall score no less than 3
(the highest score is 5).



Temporal gates play a significant role in modern recurrent-based neural
encoders, enabling fine-grained control over recursive compositional operations
over time. In recurrent models such as the long short-term memory (LSTM),
temporal gates control the amount of information retained or discarded over
time, not only playing an important role in influencing the learned
representations but also serving as a protection against vanishing gradients.
This paper explores the idea of learning temporal gates for sequence pairs
(question and answer), jointly influencing the learned representations in a
pairwise manner. In our approach, temporal gates are learned via 1D
convolutional layers and then subsequently cross applied across question and
answer for joint learning. Empirically, we show that this conceptually simple
sharing of temporal gates can lead to competitive performance across multiple
benchmarks. Intuitively, what our network achieves can be interpreted as
learning representations of question and answer pairs that are aware of what
each other is remembering or forgetting, i.e., pairwise temporal gating. Via
extensive experiments, we show that our proposed model achieves
state-of-the-art performance on two community-based QA datasets and competitive
performance on one factoid-based QA dataset.



Multimodal features play a key role in wearable sensor based Human Activity
Recognition (HAR). Selecting the most salient features adaptively is a
promising way to maximize the effectiveness of multimodal sensor data. In this
regard, we propose a "collect fully and select wisely (Fullie and Wiselie)"
principle as well as a dual-stream recurrent convolutional attention model,
Recurrent Attention and Activity Frame (RAAF), to improve the recognition
performance. We first collect modality features and the relations between each
pair of features to generate activity frames, and then introduce an attention
mechanism to select the most prominent regions from activity frames precisely.
The selected frames not only maximize the utilization of valid features but
also reduce the number of features to be computed effectively. We further
analyze the hyper-parameters, accuracy, interpretability, and annotation
dependency of the proposed model based on extensive experiments. The results
show that RAAF achieves competitive performance on two benchmarked datasets and
works well in real life scenarios.



A major bottleneck for developing general reinforcement learning agents is
determining rewards that will yield desirable behaviors under various
circumstances. We introduce a general mechanism for automatically specifying
meaningful behaviors from raw pixels. In particular, we train a generative
adversarial network to produce short sub-goals represented through motion
templates. We demonstrate that this approach generates visually meaningful
behaviors in unknown environments with novel agents and describe how these
motions can be used to train reinforcement learning agents.



The paper introduces the Hidden Tree Markov Network (HTN), a
neuro-probabilistic hybrid fusing the representation power of generative models
for trees with the incremental and discriminative learning capabilities of
neural networks. We put forward a modular architecture in which multiple
generative models of limited complexity are trained to learn structural feature
detectors whose outputs are then combined and integrated by neural layers at a
later stage. In this respect, the model is both deep, thanks to the unfolding
of the generative models on the input structures, as well as wide, given the
potentially large number of generative modules that can be trained in parallel.
Experimental results show that the proposed approach can outperform
state-of-the-art syntactic kernels as well as generative kernels built on the
same probabilistic model as the HTN.



Spinal cord stimulation has enabled humans with motor complete spinal cord
injury (SCI) to independently stand and recover some lost autonomic function.
Quantifying the quality of bipedal standing under spinal stimulation is
important for spinal rehabilitation therapies and for new strategies that seek
to combine spinal stimulation and rehabilitative robots (such as exoskeletons)
in real time feedback. To study the potential for automated electromyography
(EMG) analysis in SCI, we evaluated the standing quality of paralyzed patients
undergoing electrical spinal cord stimulation using both video and
multi-channel surface EMG recordings during spinal stimulation therapy
sessions. The quality of standing under different stimulation settings was
quantified manually by experienced clinicians. By correlating features of the
recorded EMG activity with the expert evaluations, we show that multi-channel
EMG recording can provide accurate, fast, and robust estimation for the quality
of bipedal standing in spinally stimulated SCI patients. Moreover, our analysis
shows that the total number of EMG channels needed to effectively predict
standing quality can be reduced while maintaining high estimation accuracy,
which provides more flexibility for rehabilitation robotic systems to
incorporate EMG recordings.



We consider the use of Deep Learning methods for modeling complex phenomena
like those occurring in natural physical processes. With the large amount of
data gathered on these phenomena the data intensive paradigm could begin to
challenge more traditional approaches elaborated over the years in fields like
maths or physics. However, despite considerable successes in a variety of
application domains, the machine learning field is not yet ready to handle the
level of complexity required by such problems. Using an example application,
namely Sea Surface Temperature Prediction, we show how general background
knowledge gained from physics could be used as a guideline for designing
efficient Deep Learning models. In order to motivate the approach and to assess
its generality we demonstrate a formal link between the solution of a class of
differential equations underlying a large family of physical phenomena and the
proposed model. Experiments and comparison with series of baselines including a
state of the art numerical approach is then provided.



Many current methods to interpret convolutional neural networks (CNNs) use
visualization techniques and words to highlight concepts of the input seemingly
relevant to a CNN's decision. The methods hypothesize that the recognition of
these concepts are instrumental in the decision a CNN reaches, but the nature
of this relationship has not been well explored. To address this gap, this
paper examines the quality of a concept's recognition by a CNN and the degree
to which the recognitions are associated with CNN decisions. The study
considers a CNN trained for scene recognition over the ADE20k dataset. It uses
a novel approach to find and score the strength of minimally distributed
representations of input concepts (defined by objects in scene images) across
late stage feature maps. Subsequent analysis finds evidence that concept
recognition impacts decision making. Strong recognition of concepts
frequently-occurring in few scenes are indicative of correct decisions, but
recognizing concepts common to many scenes may mislead the network.



Stackelberg equilibria have become increasingly important as a solution
concept in computational game theory, largely inspired by practical problems
such as security settings. In practice, however, there is typically uncertainty
regarding the model about the opponent. This paper is, to our knowledge, the
first to investigate Stackelberg equilibria under uncertainty in extensive-form
games, one of the broadest classes of game. We introduce robust Stackelberg
equilibria, where the uncertainty is about the opponent's payoffs, as well as
ones where the opponent has limited lookahead and the uncertainty is about the
opponent's node evaluation function. We develop a new mixed-integer program for
the deterministic limited-lookahead setting. We then extend the program to the
robust setting for Stackelberg equilibrium under unlimited and under limited
lookahead by the opponent. We show that for the specific case of interval
uncertainty about the opponent's payoffs (or about the opponent's node
evaluations in the case of limited lookahead), robust Stackelberg equilibria
can be computed with a mixed-integer program that is of the same asymptotic
size as that for the deterministic setting.



The dynamics of infectious diseases spread is crucial in determining their
risk and offering ways to contain them. We study sequential vaccination of
individuals in networks. In the original (deterministic) version of the
Firefighter problem, a fire breaks out at some node of a given graph. At each
time step, b nodes can be protected by a firefighter and then the fire spreads
to all unprotected neighbors of the nodes on fire. The process ends when the
fire can no longer spread. We extend the Firefighter problem to a probabilistic
setting, where the infection is stochastic. We devise a simple policy that only
vaccinates neighbors of infected nodes and is optimal on regular trees and on
general graphs for a sufficiently large budget. We derive methods for
calculating upper and lower bounds of the expected number of infected
individuals, as well as provide estimates on the budget needed for containment
in expectation. We calculate these explicitly on trees, d-dimensional grids,
and Erd\H{o}s R\'{e}nyi graphs. Finally, we construct a state-dependent budget
allocation strategy and demonstrate its superiority over constant budget
allocation on real networks following a first order acquaintance vaccination
policy.



A first step to reach Theory of Mind (ToM) abilities (attribution of beliefs
to others) in synthetic agents through sensorimotor interactions, would be to
tag sensory data with agent typology and action intentions: autonomous agent X
moved an object under the box. We propose a dual arm robotic setup in which ToM
could be probed. We then discuss what measures can be extracted from
sensorimotor interaction data (based on a correlation analysis) in the proposed
setup that allow to distinguish self than other and other/inanimate from
other/active with intentions. We finally discuss what elements are missing in
current cognitive architectures to be able to acquire ToM abilities in
synthetic agents from sensorimotor interactions, bottom-up from reactive agent
interaction behaviors and top-down from the optimization of social behaviour
and cooperation.



MagNet and "Efficient Defenses..." were recently proposed as a defense to
adversarial examples. We find that we can construct adversarial examples that
defeat these defenses with only a slight increase in distortion.



The discriminative approach to classification using deep neural networks has
become the de-facto standard in various fields. Complementing recent
reservations about safety against adversarial examples, we show that
conventional discriminative methods can easily be fooled to provide incorrect
labels with very high confidence to out of distribution examples. We posit that
a generative approach is the natural remedy for this problem, and propose a
method for classification using generative models. At training time, we learn a
generative model for each class, while at test time, given an example to
classify, we query each generator for its most similar generation, and select
the class corresponding to the most similar one. Our approach is general and
can be used with expressive models such as GANs and VAEs. At test time, our
method accurately "knows when it does not know," and provides resilience to out
of distribution examples while maintaining competitive performance for standard
examples.



In this paper, we build upon previous work on designing informative and
efficient Exploratory Landscape Analysis features for characterizing problems'
landscapes and show their effectiveness in automatically constructing algorithm
selection models in continuous black-box optimization problems. Focussing on
algorithm performance results of the COCO platform of several years, we
construct a representative set of high-performing complementary solvers and
present an algorithm selection model that manages to outperform the single best
solver out of the portfolio by factor two. Acting on the assumption that the
function set of the Black-Box Optimization Benchmark is representative enough
for practical applications the model allows for selecting the best suited
optimization algorithm within the considered set for unseen problems prior to
the optimization itself based on a small sample of function evaluations. Note
that such a sample can even be reused for the initial algorithm population so
that feature costs become negligible.



Reinforcement Learning agents often need to solve not a single task, but
several tasks pertaining to a same domain; in particular, each task corresponds
to an MDP drawn from a family of related MDPs (a domain). An agent learning in
this setting should be able exploit policies it has learned in the past, for a
given set of sample tasks, in order to more rapidly acquire policies for novel
tasks. Consider, for instance, a navigation problem where an agent may have to
learn to navigate different (but related) mazes. Even though these correspond
to distinct tasks (since the goal and starting locations of the agent may
change, as well as the maze configuration itself), their solutions do share
common properties---e.g. in order to reach distant areas of the maze, an agent
should not move in circles. After an agent has learned to solve a few sample
tasks, it may be possible to leverage the acquired experience to facilitate
solving novel tasks from the same domain. Our work is motivated by the
observation that trajectory samples from optimal policies for tasks belonging
to a common domain, often reveal underlying useful patterns for solving novel
tasks. We propose an optimization objective that characterizes the problem of
learning reusable temporally extended actions (macros). We introduce a
computationally tractable surrogate objective that is equivalent to finding
macros that allow for maximal compression of a given set of sampled
trajectories. We develop a compression-based approach for obtaining such macros
and propose an exploration strategy that takes advantage of them. We show that
meaningful behavioral patterns can be identified from sample policies over
discrete and continuous action spaces, and present evidence that the proposed
exploration strategy improves learning time on novel tasks.



We present a general-purpose method to train Markov chain Monte Carlo
kernels, parameterized by deep neural networks, that converge and mix quickly
to their target distribution. Our method generalizes Hamiltonian Monte Carlo
and is trained to maximize expected squared jumped distance, a proxy for mixing
speed. We demonstrate large empirical gains on a collection of simple but
challenging distributions, for instance achieving a 106x improvement in
effective sample size in one case, and mixing when standard HMC makes no
measurable progress in a second. Finally, we show quantitative and qualitative
gains on a real-world task: latent-variable generative modeling. We release an
open source TensorFlow implementation of the algorithm.



Text attribute transfer using non-parallel data requires methods that can
perform disentanglement of content and linguistic attributes. In this work, we
propose multiple improvements over the existing approaches that enable the
encoder-decoder framework to cope with the text attribute transfer from
non-parallel data. We perform experiments on the sentiment transfer task using
two datasets. For both datasets, our proposed method outperforms a strong
baseline in two of the three employed evaluation metrics.



We introduce a one-shot learning approach for video object tracking. The
proposed algorithm requires seeing the object to be tracked only once, and
employs an external memory to store and remember the evolving features of the
foreground object as well as backgrounds over time during tracking. With the
relevant memory retrieved and updated in each tracking, our tracking model is
capable of maintaining long-term memory of the object, and thus can naturally
deal with hard tracking scenarios including partial and total occlusion, motion
changes and large scale and shape variations. In our experiments we use the
ImageNet ILSVRC2015 video detection dataset to train and use the VOT-2016
benchmark to test and compare our Memory-Augmented Video Object Tracking
(MAVOT) model. From the results, we conclude that given its oneshot property
and simplicity in design, MAVOT is an attractive approach in visual tracking
because it shows good performance on VOT-2016 benchmark and is among the top 5
performers in accuracy and robustness in occlusion, motion changes and empty
target.



We present the first experimental realization of a quantum artificial life
algorithm in a quantum computer. The quantum biomimetic protocol encodes
tailored quantum behaviors belonging to living systems, namely,
self-replication, mutation, interaction between individuals, and death, into
the cloud quantum computer IBM ibmqx4. In this experiment, entanglement spreads
throughout generations of individuals, where genuine quantum information
features are inherited through genealogical networks. As a pioneering
proof-of-principle, experimental data fits the ideal model with accuracy.
Thereafter, these and other models of quantum artificial life, for which no
classical device may predict its quantum supremacy evolution, can be further
explored in novel generations of quantum computers. Quantum biomimetics,
quantum machine learning, and quantum artificial intelligence will move forward
hand in hand through more elaborate levels of quantum complexity.



Predicting and understanding human motion dynamics has many applications,
such as motion synthesis, augmented reality, security, and autonomous vehicles.
Due to the recent success of generative adversarial networks (GAN), there has
been much interest in probabilistic estimation and synthetic data generation
using deep neural network architectures and learning algorithms.
  We propose a novel sequence-to-sequence model for probabilistic human motion
prediction, trained with a modified version of improved Wasserstein generative
adversarial networks (WGAN-GP), in which we use a custom loss function designed
for human motion prediction. Our model, which we call HP-GAN, learns a
probability density function of future human poses conditioned on previous
poses. It predicts multiple sequences of possible future human poses, each from
the same input sequence but a different vector z drawn from a random
distribution. Furthermore, to quantify the quality of the non-deterministic
predictions, we simultaneously train a motion-quality-assessment model that
learns the probability that a given skeleton sequence is a real human motion.
  We test our algorithm on two of the largest skeleton datasets: NTURGB-D and
Human3.6M. We train our model on both single and multiple action types. Its
predictive power for long-term motion estimation is demonstrated by generating
multiple plausible futures of more than 30 frames from just 10 frames of input.
We show that most sequences generated from the same input have more than 50\%
probabilities of being judged as a real human sequence. We will release all the
code used in this paper to Github.



Humans can learn in a continuous manner. Old rarely utilized knowledge can be
overwritten by new incoming information while important, frequently used
knowledge is prevented from being erased. In artificial learning systems,
lifelong learning so far has focused mainly on accumulating knowledge over
tasks and overcoming catastrophic forgetting. In this paper, we argue that,
given the limited model capacity and the unlimited new information to be
learned, knowledge has to be preserved or erased selectively. Inspired by
neuroplasticity, we propose an online method to compute the importance of the
parameters of a neural network, based on the data that the network is actively
applied to, in an unsupervised manner. After learning a task, whenever a sample
is fed to the network, we accumulate an importance measure for each parameter
of the network, based on how sensitive the predicted output is to a change in
this parameter. When learning a new task, changes to important parameters are
penalized. We show that a local version of our method is a direct application
of Hebb's rule in identifying the important connections between neurons. We
test our method on a sequence of object recognition tasks and on the
challenging problem of learning an embedding in a continuous manner. We show
state of the art performance and the ability to adapt the importance of the
parameters towards what the network needs (not) to forget, which may be
different for different test conditions.



This paper proposes a new algorithm for controlling classification results by
generating a small additive perturbation without changing the classifier
network. Our work is inspired by existing works generating adversarial
perturbation that worsens classification performance. In contrast to the
existing methods, our work aims to generate perturbations that can enhance
overall classification performance. To solve this performance enhancement
problem, we newly propose a perturbation generation network (PGN) influenced by
the adversarial learning strategy. In our problem, the information in a large
external dataset is summarized by a small additive perturbation, which helps to
improve the performance of the classifier trained with the target dataset. In
addition to this performance enhancement problem, we show that the proposed PGN
can be adopted to solve the classical adversarial problem without utilizing the
information on the target classifier. The mentioned characteristics of our
method are verified through extensive experiments on publicly available visual
datasets.



Recent research has demonstrated the ability to estimate gaze on mobile
devices by performing inference on the image from the phone's front-facing
camera, and without requiring specialized hardware. While this offers wide
potential applications such as in human-computer interaction, medical diagnosis
and accessibility (e.g., hands free gaze as input for patients with motor
disorders), current methods are limited as they rely on collecting data from
real users, which is a tedious and expensive process that is hard to scale
across devices. There have been some attempts to synthesize eye region data
using 3D models that can simulate various head poses and camera settings,
however these lack in realism.
  In this paper, we improve upon a recently suggested method, and propose a
generative adversarial framework to generate a large dataset of high resolution
colorful images with high diversity (e.g., in subjects, head pose, camera
settings) and realism, while simultaneously preserving the accuracy of gaze
labels. The proposed approach operates on extended regions of the eye, and even
completes missing parts of the image. Using this rich synthesized dataset, and
without using any additional training data from real users, we demonstrate
improvements over state-of-the-art for estimating 2D gaze position on mobile
devices. We further demonstrate cross-device generalization of model
performance, as well as improved robustness to diverse head pose, blur and
distance.



Deep neural networks have proved to be a very effective way to perform
classification tasks. They excel when the input data is high dimensional, the
relationship between the input and the output is complicated, and the number of
labeled training examples is large. But it is hard to explain why a learned
network makes a particular classification decision on a particular test case.
This is due to their reliance on distributed hierarchical representations. If
we could take the knowledge acquired by the neural net and express the same
knowledge in a model that relies on hierarchical decisions instead, explaining
a particular decision would be much easier. We describe a way of using a
trained neural net to create a type of soft decision tree that generalizes
better than one learned directly from the training data.



Recovering images from undersampled linear measurements typically leads to an
ill-posed linear inverse problem, that asks for proper statistical priors.
Building effective priors is however challenged by the low train and test
overhead dictated by real-time tasks; and the need for retrieving visually
"plausible" and physically "feasible" images with minimal hallucination. To
cope with these challenges, we design a cascaded network architecture that
unrolls the proximal gradient iterations by permeating benefits from generative
residual networks (ResNet) to modeling the proximal operator. A mixture of
pixel-wise and perceptual costs is then deployed to train proximals. The
overall architecture resembles back-and-forth projection onto the intersection
of feasible and plausible images. Extensive computational experiments are
examined for a global task of reconstructing MR images of pediatric patients,
and a more local task of superresolving CelebA faces, that are insightful to
design efficient architectures. Our observations indicate that for MRI
reconstruction, a recurrent ResNet with a single residual block effectively
learns the proximal. This simple architecture appears to significantly
outperform the alternative deep ResNet architecture by 2dB SNR, and the
conventional compressed-sensing MRI by 4dB SNR with 100x faster inference. For
image superresolution, our preliminary results indicate that modeling the
denoising proximal demands deep ResNets.



The literature on Inverse Reinforcement Learning (IRL) typically assumes that
humans take actions in order to minimize the expected value of a cost function,
i.e., that humans are risk neutral. Yet, in practice, humans are often far from
being risk neutral. To fill this gap, the objective of this paper is to devise
a framework for risk-sensitive IRL in order to explicitly account for a human's
risk sensitivity. To this end, we propose a flexible class of models based on
coherent risk measures, which allow us to capture an entire spectrum of risk
preferences from risk-neutral to worst-case. We propose efficient
non-parametric algorithms based on linear programming and semi-parametric
algorithms based on maximum likelihood for inferring a human's underlying risk
measure and cost function for a rich class of static and dynamic
decision-making settings. The resulting approach is demonstrated on a simulated
driving game with ten human participants. Our method is able to infer and mimic
a wide range of qualitatively different driving styles from highly risk-averse
to risk-neutral in a data-efficient manner. Moreover, comparisons of the
Risk-Sensitive (RS) IRL approach with a risk-neutral model show that the RS-IRL
framework more accurately captures observed participant behavior both
qualitatively and quantitatively, especially in scenarios where catastrophic
outcomes such as collisions can occur.



Tensor completion is a problem of filling the missing or unobserved entries
of partially observed tensors. Due to the multidimensional character of tensors
in describing complex datasets, tensor completion algorithms and their
applications have received wide attention and achievement in data mining,
computer vision, signal processing, and neuroscience, etc. In this survey, we
provide a modern overview of recent advances in tensor completion algorithms
from the perspective of big data analytics characterized by diverse variety,
large volume, and high velocity. Towards a better comprehension and comparison
of vast existing advances, we summarize and categorize them into four groups
including general tensor completion algorithms, tensor completion with
auxiliary information (variety), scalable tensor completion algorithms (volume)
and dynamic tensor completion algorithms (velocity). Besides, we introduce
their applications on real-world data-driven problems and present an
open-source package covering several widely used tensor decomposition and
completion algorithms. Our goal is to summarize these popular methods and
introduce them to researchers for promoting the research process in this field
and give an available repository for practitioners. In the end, we also discuss
some challenges and promising research directions in this community for future
explorations.



Distributed training of deep neural networks has received significant
research interest, and its major approaches include implementations on multiple
GPUs and clusters. Parallelization can dramatically improve the efficiency of
training deep and complicated models with large-scale data. A fundamental
barrier against the speedup of DNN training, however, is the trade-off between
computation and communication time. In other words, increasing the number of
worker nodes decreases the time consumed in computation while simultaneously
increasing communication overhead under constrained network bandwidth,
especially in commodity hardware environments. To alleviate this trade-off, we
suggest the idea of homomorphic parameter compression, which compresses
parameters with the least expense and trains the DNN with the compressed
representation. Although the specific method is yet to be discovered, we
demonstrate that there is a high probability that the homomorphism can reduce
the communication overhead, thanks to little compression and decompression
times. We also provide theoretical speedup of homomorphic compression.



This paper introduces a novel method to perform transfer learning across
domains and tasks, formulating it as a problem of learning to cluster. The key
insight is that, in addition to features, we can transfer similarity
information and this is sufficient to learn a similarity function and
clustering network to perform both domain adaptation and cross-task transfer
learning. We begin by reducing categorical information to pairwise constraints,
which only considers whether two instances belong to the same class or not.
This similarity is category-agnostic and can be learned from data in the source
domain using a similarity network. We then present two novel approaches for
performing transfer learning using this similarity function. First, for
unsupervised domain adaptation, we design a new loss function to regularize
classification with a constrained clustering loss, hence learning a clustering
network with the transferred similarity metric generating the training inputs.
Second, for cross-task learning (i.e., unsupervised clustering with unseen
categories), we propose a framework to reconstruct and estimate the number of
semantic clusters, again using the clustering network. Since the similarity
network is noisy, the key is to use a robust clustering algorithm, and we show
that our formulation is more robust than the alternative constrained and
unconstrained clustering approaches. Using this method, we first show state of
the art results for the challenging cross-task problem, applied on Omniglot and
ImageNet. Our results show that we can reconstruct semantic clusters with high
accuracy. We then evaluate the performance of cross-domain transfer using
images from the Office-31 and SVHN-MNIST tasks and present top accuracy on both
datasets. Our approach doesn't explicitly deal with domain discrepancy. If we
combine with a domain adaptation loss, it shows further improvement.



Recently, model-free reinforcement learning algorithms have been shown to
solve challenging problems by learning from extensive interaction with the
environment. A significant issue with transferring this success to the robotics
domain is that interaction with the real world is costly, but training on
limited experience is prone to overfitting. We present a method for learning to
navigate, to a fixed goal and in a known environment, on a mobile robot. The
robot leverages an interactive world model built from a single traversal of the
environment, a pre-trained visual feature encoder, and stochastic environmental
augmentation, to demonstrate successful zero-shot transfer under real-world
environmental variations without fine-tuning.



Quantitative CBA is a postprocessing algorithm for association rule
classification algorithm CBA (Liu et al, 1998). QCBA uses original,
undiscretized numerical attributes to optimize the discovered association
rules, refining the boundaries of literals in the antecedent of the rules
produced by CBA. Some rules as well as literals from the rules can consequently
be removed, which makes the resulting classifier smaller. One-rule
classification and crisp rules make CBA classification models possibly most
comprehensible among all association rule classification algorithms. These
viable properties are retained by QCBA. The postprocessing is conceptually
fast, because it is performed on a relatively small number of rules that passed
data coverage pruning in CBA. Benchmark of our QCBA approach on 22 UCI datasets
shows average 53% decrease in the total size of the model as measured by the
total number of conditions in all rules. Model accuracy remains on the same
level as for CBA.



Object ranking or "learning to rank" is an important problem in the realm of
preference learning. On the basis of training data in the form of a set of
rankings of objects represented as feature vectors, the goal is to learn a
ranking function that predicts a linear order of any new set of objects. In
this paper, we propose a new approach to object ranking based on principles of
analogical reasoning. More specifically, our inference pattern is formalized in
terms of so-called analogical proportions and can be summarized as follows:
Given objects $A,B,C,D$, if object $A$ is known to be preferred to $B$, and $C$
relates to $D$ as $A$ relates to $B$, then $C$ is (supposedly) preferred to
$D$. Our method applies this pattern as a main building block and combines it
with ideas and techniques from instance-based learning and rank aggregation.
Based on first experimental results for data sets from various domains (sports,
education, tourism, etc.), we conclude that our approach is highly competitive.
It appears to be specifically interesting in situations in which the objects
are coming from different subdomains, and which hence require a kind of
knowledge transfer.



Recent systems on structured prediction focus on increasing the level of
structural dependencies within the model. However, our study suggests that
complex structures entail high overfitting risks. To control the
structure-based overfitting, we propose to conduct structure regularization
decoding (SR decoding). The decoding of the complex structure model is
regularized by the additionally trained simple structure model. We
theoretically analyze the quantitative relations between the structural
complexity and the overfitting risk. The analysis shows that complex structure
models are prone to the structure-based overfitting. Empirical evaluations show
that the proposed method improves the performance of the complex structure
models by reducing the structure-based overfitting. On the sequence labeling
tasks, the proposed method substantially improves the performance of the
complex neural network models. The maximum F1 error rate reduction is 36.4% for
the third-order model. The proposed method also works for the parsing task. The
maximum UAS improvement is 5.5% for the tri-sibling model. The results are
competitive with or better than the state-of-the-art results.



A supervised learning algorithm searches over a set of functions $A \to B$
parametrised by a space $P$ to find the best approximation to some ideal
function $f\colon A \to B$. It does this by taking examples $(a,f(a)) \in
A\times B$, and updating the parameter according to some rule. We define a
category where these update rules may be composed, and show that gradient
descent---with respect to a fixed step size and an error function satisfying a
certain property---defines a monoidal functor from a category of parametrised
functions to this category of update rules. This provides a structural
perspective on backpropagation, as well as a broad generalisation of neural
networks.



Recommender systems take inputs from user history, use an internal ranking
algorithm to generate results and possibly optimize this ranking based on
feedback. However, often the recommender system is unaware of the actual intent
of the user and simply provides recommendations dynamically without properly
understanding the thought process of the user. An intelligent recommender
system is not only useful for the user but also for businesses which want to
learn the tendencies of their users. Finding out tendencies or intents of a
user is a difficult problem to solve.
  Keeping this in mind, we sought out to create an intelligent system which
will keep track of the user's activity on a web-application as well as
determine the intent of the user in each session. We devised a way to encode
the user's activity through the sessions. Then, we have represented the
information seen by the user in a high dimensional format which is reduced to
lower dimensions using tensor factorization techniques. The aspect of intent
awareness (or scoring) is dealt with at this stage. Finally, combining the user
activity data with the contextual information gives the recommendation score.
The final recommendations are then ranked using filtering and collaborative
recommendation techniques to show the top-k recommendations to the user. A
provision for feedback is also envisioned in the current system which informs
the model to update the various weights in the recommender system. Our overall
model aims to combine both frequency-based and context-based recommendation
systems and quantify the intent of a user to provide better recommendations.
  We ran experiments on real-world timestamped user activity data, in the
setting of recommending reports to the users of a business analytics tool and
the results are better than the baselines. We also tuned certain aspects of our
model to arrive at optimized results.



Incremental class learning involves sequentially learning classes in bursts
of examples from the same class. This violates the assumptions that underlie
methods for training standard deep neural networks, and will cause them to
suffer from catastrophic forgetting. Arguably, the best method for incremental
class learning is iCaRL, but it requires storing training examples for each
class, making it challenging to scale. Here, we propose FearNet for incremental
class learning. FearNet is a generative model that does not store previous
examples, making it memory efficient. FearNet uses a brain-inspired dual-memory
system in which new memories are consolidated from a network for recent
memories inspired by the mammalian hippocampal complex to a network for
long-term storage inspired by medial prefrontal cortex. Memory consolidation is
inspired by mechanisms that occur during sleep. FearNet also uses a module
inspired by the basolateral amygdala for determining which memory system to use
for recall. FearNet achieves state-of-the-art performance at incremental class
learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet)
benchmarks.



The TensorFlow Distributions library implements a vision of probability
theory adapted to the modern deep-learning paradigm of end-to-end
differentiable computation. Building on two basic abstractions, it offers
flexible building blocks for probabilistic computation. Distributions provide
fast, numerically stable methods for generating samples and computing
statistics, e.g., log density. Bijectors provide composable volume-tracking
transformations with automatic caching. Together these enable modular
construction of high dimensional distributions and transformations not possible
with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible
residual networks). They are the workhorse behind deep probabilistic
programming systems like Edward and empower fast black-box inference in
probabilistic models built on deep-network components. TensorFlow Distributions
has proven an important part of the TensorFlow toolkit within Google and in the
broader deep learning community.



Predicting unseen weather phenomena is an important issue for disaster
management. In this paper, we suggest a model for a convolutional
sequence-to-sequence autoencoder for predicting undiscovered weather situations
from previous satellite images. We also propose a symmetric skip connection
between encoder and decoder modules to produce more comprehensive image
predictions. To examine our model performance, we conducted experiments for
each suggested model to predict future satellite images from historical
satellite images. A specific combination of skip connection and
sequence-to-sequence autoencoder was able to generate closest prediction from
the ground truth image.



Network embedding aims to learn the low-dimensional representations of
vertexes in a network, while structure and inherent properties of the network
is preserved. Existing network embedding works primarily focus on preserving
the microscopic structure, such as the first- and second-order proximity of
vertexes, while the macroscopic scale-free property is largely ignored.
Scale-free property depicts the fact that vertex degrees follow a heavy-tailed
distribution (i.e., only a few vertexes have high degrees) and is a critical
property of real-world networks, such as social networks. In this paper, we
study the problem of learning representations for scale-free networks. We first
theoretically analyze the difficulty of embedding and reconstructing a
scale-free network in the Euclidean space, by converting our problem to the
sphere packing problem. Then, we propose the "degree penalty" principle for
designing scale-free property preserving network embedding algorithm: punishing
the proximity between high-degree vertexes. We introduce two implementations of
our principle by utilizing the spectral techniques and a skip-gram model
respectively. Extensive experiments on six datasets show that our algorithms
are able to not only reconstruct heavy-tailed distributed degree distribution,
but also outperform state-of-the-art embedding models in various network mining
tasks, such as vertex classification and link prediction.



This paper studies directed exploration for reinforcement learning agents by
tracking uncertainty about the value of each available action. We identify two
sources of uncertainty that are relevant for exploration. The first originates
from limited data (parametric uncertainty), while the second originates from
the distribution of the returns (return uncertainty). We identify methods to
learn these distributions with deep neural networks, where we estimate
parametric uncertainty with Bayesian drop-out, while return uncertainty is
propagated through the Bellman equation as a Gaussian distribution. Then, we
identify that both can be jointly estimated in one network, which we call the
Double Uncertain Value Network. The policy is directly derived from the learned
distributions based on Thompson sampling. Experimental results show that both
types of uncertainty may vastly improve learning in domains with a strong
exploration challenge.



This work explores attention models to weight the contribution of local
convolutional representations for the instance search task. We present a
retrieval framework based on bags of local convolutional features (BLCF) that
benefits from saliency weighting to build an efficient image representation.
The use of human visual attention models (saliency) allows significant
improvements in retrieval performance without the need to conduct region
analysis or spatial verification, and without requiring any feature fine
tuning. We investigate the impact of different saliency models, finding that
higher performance on saliency benchmarks does not necessarily equate to
improved performance when used in instance search tasks. The proposed approach
outperforms the state-of-the-art on the challenging INSTRE benchmark by a large
margin, and provides similar performance on the Oxford and Paris benchmarks
compared to more complex methods that use off-the-shelf representations. The
source code used in this project is available at
https://imatge-upc.github.io/salbow/



The recent popularity of post-denitrification processes in the greater Paris
area wastewater treatment plants has caused a resurgence of the presence of
nitrite in the Seine river. Controlling the production of nitrite during the
post-denitrification has thus become a major technical issue. Research studies
have been led in the MOCOPEE program (www.mocopee.com) to better understand the
underlying mechanisms behind the production of nitrite during wastewater
denitrification and to develop technical tools (measurement and control
solutions) to assist on-site reductions of nitrite productions. Prior studies
have shown that typical methanol dosage strategies produce a varying
carbon-to-nitrogen ratio in the reactor, which in turn leads to unstable
nitrite concentrations in the effluent. The possibility of adding a model-free
control to the actual classical dosage strategy has thus been tested on the
SimBio model, which simulates the behavior of wastewater biofilters. The
corresponding "intelligent" feedback loop, which is using effluent nitrite
concentrations, compensates the classical strategy only when needed. Simulation
results show a clear improvement in average nitrite concentration level and
level stability in the effluent, without a notable overcost in methanol.



We propose a novel computational strategy based on deep and reinforcement
learning techniques for de-novo design of molecules with desired properties.
This strategy integrates two deep neural networks -generative and predictive -
that are trained separately but employed jointly to generate novel chemical
structures with the desired properties. Generative models are trained to
produce chemically feasible SMILES, and predictive models are derived to
forecast the desired compound properties. In the first phase of the method,
generative and predictive models are separately trained with supervised
learning algorithms. In the second phase, both models are trained jointly with
reinforcement learning approach to bias newly generated chemical structures
towards those with desired physical and biological properties. In this
proof-of-concept study, we have employed this integrative strategy to design
chemical libraries biased toward compounds with either maximal, minimal, or
specific range of physical properties, such as melting point and
hydrophobicity, as well as to develop novel putative inhibitors of JAK2. This
new approach can find a general use for generating targeted chemical libraries
optimized for a single desired property or multiple properties.



In the covariate shift learning scenario, the training and test covariate
distributions differ, so that a predictor's average loss over the training and
test distributions also differ. The importance weighting approach handles this
shift by minimizing an estimate of test loss over predictors, obtained via a
weighted sum over training sample losses. However, as the dimension of the
covariates increases, this test loss estimator increases in variance. In this
work, we adapt the importance weighting approach to more robustly handle higher
dimensional covariates by incorporating dimension reduction into the learning
process.



Existing music recognition applications require a connection to a server that
performs the actual recognition. In this paper we present a low-power music
recognizer that runs entirely on a mobile device and automatically recognizes
music without user interaction. To reduce battery consumption, a small music
detector runs continuously on the mobile device's DSP chip and wakes up the
main application processor only when it is confident that music is present.
Once woken, the recognizer on the application processor is provided with a few
seconds of audio which is fingerprinted and compared to the stored fingerprints
in the on-device fingerprint database of tens of thousands of songs. Our
presented system, Now Playing, has a daily battery usage of less than 1% on
average, respects user privacy by running entirely on-device and can passively
recognize a wide range of music.



We introduce a method for embedding words as probability densities in a
low-dimensional space. Rather than assuming that a word embedding is fixed
across the entire text collection, as in standard word embedding methods, in
our Bayesian model we generate it from a word-specific prior density for each
occurrence of a given word. Intuitively, for each word, the prior density
encodes the distribution of its potential 'meanings'. These prior densities are
conceptually similar to Gaussian embeddings. Interestingly, unlike the Gaussian
embeddings, we can also obtain context-specific densities: they encode
uncertainty about the sense of a word given its context and correspond to
posterior distributions within our model. The context-dependent densities have
many potential applications: for example, we show that they can be directly
used in the lexical substitution task. We describe an effective estimation
method based on the variational autoencoding framework. We also demonstrate
that our embeddings achieve competitive results on standard benchmarks.



Modern social platforms are characterized by the presence of rich
user-behavior data associated with the publication, sharing and consumption of
textual content. Users interact with content and with each other in a complex
and dynamic social environment while simultaneously evolving over time. In
order to effectively characterize users and predict their future behavior in
such a setting, it is necessary to overcome several challenges. Content
heterogeneity and temporal inconsistency of behavior data result in severe
sparsity at the user level. In this paper, we propose a novel
mutual-enhancement framework to simultaneously partition and learn latent
activity profiles of users. We propose a flexible user partitioning approach to
effectively discover rare behaviors and tackle user-level sparsity. We
extensively evaluate the proposed framework on massive datasets from real-world
platforms including Q&A networks and interactive online courses (MOOCs). Our
results indicate significant gains over state-of-the-art behavior models ( 15%
avg ) in a varied range of tasks and our gains are further magnified for users
with limited interaction data. The proposed algorithms are amenable to
parallelization, scale linearly in the size of datasets, and provide
flexibility to model diverse facets of user behavior.



Video captioning is the task of automatically generating a textual
description of the actions in a video. Although previous work (e.g.
sequence-to-sequence model) has shown promising results in abstracting a coarse
description of a short video, it is still very challenging to caption a video
containing multiple fine-grained actions with a detailed description. This
paper aims to address the challenge by proposing a novel hierarchical
reinforcement learning framework for video captioning, where a high-level
Manager module learns to design sub-goals and a low-level Worker module
recognizes the primitive actions to fulfill the sub-goal. With this
compositional framework to reinforce video captioning at different levels, our
approach significantly outperforms all the baseline methods on a newly
introduced large-scale dataset for fine-grained video captioning. Furthermore,
our non-ensemble model has already achieved the state-of-the-art results on the
widely-used MSR-VTT dataset.



This paper develops a novel methodology for using symbolic knowledge in deep
learning. From first principles, we derive a semantic loss function that
bridges between neural output vectors and logical constraints. This loss
function captures how close the neural network is to satisfying the constraints
on its output. An experimental evaluation shows that our semantic loss function
effectively guides the learner to achieve (near-)state-of-the-art results on
semi-supervised multi-class classification. Moreover, it significantly
increases the ability of the neural network to predict structured objects, such
as rankings and paths. These discrete concepts are tremendously difficult to
learn, and benefit from a tight integration of deep learning and symbolic
reasoning methods.



This paper proposes a real-time embedded fall detection system using a
DVS(Dynamic Vision Sensor) that has never been used for traditional fall
detection, a dataset for fall detection using that, and a DVS-TN(DVS-Temporal
Network). The first contribution is building a DVS Falls Dataset, which made
our network to recognize a much greater variety of falls than the existing
datasets that existed before and solved privacy issues using the DVS. Secondly,
we introduce the DVS-TN : optimized deep learning network to detect falls using
DVS. Finally, we implemented a fall detection system which can run on
low-computing H/W with real-time, and tested on DVS Falls Dataset that takes
into account various falls situations. Our approach achieved 95.5% on the
F1-score and operates at 31.25 FPS on NVIDIA Jetson TX1 board.



We propose a framework that directly tackles the probability distribution of
the value function parameters in Deep Q Network (DQN), with powerful
variational inference subroutines to approximate the posterior of the
parameters. We will establish the equivalence between our proposed surrogate
objective and variational inference loss. Our new algorithm achieves efficient
exploration and performs well on large scale chain Markov Decision Process
(MDP).



In this paper, we propose a method for training neural networks when we have
a large set of data with weak labels and a small amount of data with true
labels. In our proposed model, we train two neural networks: a target network,
the learner and a confidence network, the meta-learner. The target network is
optimized to perform a given task and is trained using a large set of unlabeled
data that are weakly annotated. We propose to control the magnitude of the
gradient updates to the target network using the scores provided by the second
confidence network, which is trained on a small amount of supervised data. Thus
we avoid that the weight updates computed from noisy labels harm the quality of
the target network model.



Determining semantic similarity between academic documents is crucial to many
tasks such as plagiarism detection, automatic technical survey and semantic
search. Current studies mostly focus on semantic similarity between concepts,
sentences and short text fragments. However, document-level semantic matching
is still based on statistical information in surface level, neglecting article
structures and global semantic meanings, which may cause the deviation in
document understanding. In this paper, we focus on the document-level semantic
similarity issue for academic literatures with a novel method. We represent
academic articles with topic events that utilize multiple information profiles,
such as research purposes, methodologies and domains to integrally describe the
research work, and calculate the similarity between topic events based on the
domain ontology to acquire the semantic similarity between articles.
Experiments show that our approach achieves significant performance compared to
state-of-the-art methods.



We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where
an agent is spawned at a random location in a 3D environment and asked a
question ("What color is the car?"). In order to answer, the agent must first
intelligently navigate to explore the environment, gather information through
first-person (egocentric) vision, and then answer the question ("orange").
  This challenging task requires a range of AI skills -- active perception,
language understanding, goal-driven navigation, commonsense reasoning, and
grounding of language into actions. In this work, we develop the environments,
end-to-end-trained reinforcement learning agents, and evaluation protocols for
EmbodiedQA.



A number of studies have found that today's Visual Question Answering (VQA)
models are heavily driven by superficial correlations in the training data and
lack sufficient image grounding. To encourage development of models geared
towards the latter, we propose a new setting for VQA where for every question
type, train and test sets have different prior distributions of answers.
Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we
call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2
respectively). First, we evaluate several existing VQA models under this new
setting and show that their performance degrades significantly compared to the
original VQA setting. Second, we propose a novel Grounded Visual Question
Answering model (GVQA) that contains inductive biases and restrictions in the
architecture specifically designed to prevent the model from 'cheating' by
primarily relying on priors in the training data. Specifically, GVQA explicitly
disentangles the recognition of visual concepts present in the image from the
identification of plausible answer space for a given question, enabling the
model to more robustly generalize across different distributions of answers.
GVQA is built off an existing VQA model -- Stacked Attention Networks (SAN).
Our experiments demonstrate that GVQA significantly outperforms SAN on both
VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more
powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in
several cases. GVQA offers strengths complementary to SAN when trained and
evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more
transparent and interpretable than existing VQA models.



We introduce MAgent, a platform to support research and development of
many-agent reinforcement learning. Unlike previous research platforms on single
or multi-agent reinforcement learning, MAgent focuses on supporting the tasks
and the applications that require hundreds to millions of agents. Within the
interactions among a population of agents, it enables not only the study of
learning algorithms for agents' optimal polices, but more importantly, the
observation and understanding of individual agent's behaviors and social
phenomena emerging from the AI society, including communication languages,
leaderships, altruism. MAgent is highly scalable and can host up to one million
agents on a single GPU server. MAgent also provides flexible configurations for
AI researchers to design their customized environments and agents. In this
demo, we present three environments designed on MAgent and show emerged
collective intelligence by learning from scratch.



Alzheimer's disease is the most common cause of dementia, yet hard to
diagnose precisely without invasive techniques, particularly at the onset of
the disease. This work approaches image analysis and classification of
synthetic multispectral images composed by diffusion-weighted magnetic
resonance (MR) cerebral images for the evaluation of cerebrospinal fluid area
and measuring the advance of Alzheimer's disease. A clinical 1.5 T MR imaging
system was used to acquire all images presented. The classification methods are
based on multilayer perceptrons and Kohonen Self-Organized Map classifiers. We
assume the classes of interest can be separated by hyperquadrics. Therefore, a
2-degree polynomial network is used to classify the original image, generating
the ground truth image. The classification results are used to improve the
usual analysis of the apparent diffusion coefficient map.



We propose a hybrid architecture for systematically computing robust visual
explanation(s) encompassing hypothesis formation, belief revision, and default
reasoning with video data. The architecture consists of two tightly integrated
synergistic components: (1) (functional) answer set programming based abductive
reasoning with space-time tracklets as native entities; and (2) a visual
processing pipeline for detection based object tracking and motion analysis.
  We present the formal framework, its general implementation as a
(declarative) method in answer set programming, and an example application and
evaluation based on two diverse video datasets: the MOTChallenge benchmark
developed by the vision community, and a recently developed Movie Dataset.



This volume of EPTCS contains the proceedings of the Fifth Workshop on Proof
Exchange for Theorem Proving (PxTP 2017), held on September 23-24, 2017 as part
of the Tableaux, FroCoS and ITP conferences in Brasilia, Brazil. The PxTP
workshop series brings together researchers working on various aspects of
communication, integration, and cooperation between reasoning systems and
formalisms, with a special focus on proofs. The progress in computer-aided
reasoning, both automated and interactive, during the past decades, made it
possible to build deduction tools that are increasingly more applicable to a
wider range of problems and are able to tackle larger problems progressively
faster. In recent years, cooperation between such tools in larger systems has
demonstrated the potential to reduce the amount of manual intervention.
Cooperation between reasoning systems relies on availability of theoretical
formalisms and practical tools to exchange problems, proofs, and models. The
PxTP workshop series strives to encourage such cooperation by inviting
contributions on all aspects of cooperation between reasoning tools, whether
automatic or interactive.



Can artificial intelligence (AI) learn complicated non-linear physics? Here
we propose a novel deep learning approach that learns non-linear photon
scattering physics and obtains accurate 3D distribution of optical anomalies.
In contrast to the traditional black-box deep learning approaches to inverse
problems, our deep network learns to invert the Lippmann-Schwinger integral
equation which describes the essential physics of photon migration of diffuse
near-infrared (NIR) photons in turbid media. As an example for clinical
relevance, we applied the method to our prototype diffuse optical tomography
(DOT). We show that our deep neural network, trained with only simulation data,
can accurately recover the location of anomalies within biomimetic phantoms and
live animals without the use of an exogenous contrast agent.



We present a novel approach to hierarchical reinforcement learning called
Hierarchical Actor-Critic (HAC). HAC aims to make learning tasks with sparse
binary rewards more efficient by enabling agents to learn how to break down
tasks from scratch. The technique uses of a set of actor-critic networks that
learn to decompose tasks into a hierarchy of subgoals. We demonstrate that HAC
significantly improves sample efficiency in a series of tasks that involve
sparse binary rewards and require behavior over a long time horizon.



A correspondence between database tuples as causes for query answers in
databases and tuple-based repairs of inconsistent databases with respect to
denial constraints has already been established. In this work, answer-set
programs that specify repairs of databases are used as a basis for solving
computational and reasoning problems about causes. Here, causes are also
introduced at the attribute level by appealing to a both null-based and
attribute-based repair semantics. The corresponding repair programs are
presented, and they are used as a basis for computation and reasoning about
attribute-level causes.



We view intersection handling on autonomous vehicles as a reinforcement
learning problem, and study its behavior in a transfer learning setting. We
show that a network trained on one type of intersection generally is not able
to generalize to other intersections. However, a network that is pre-trained on
one intersection and fine-tuned on another performs better on the new task
compared to training in isolation. This network also retains knowledge of the
prior task, even though some forgetting occurs. Finally, we show that the
benefits of fine-tuning hold when transferring simulated intersection handling
knowledge to a real autonomous vehicle.



This article demonstrates that convolutional operation can be converted to
matrix multiplication, which has the same calculation way with fully connected
layer. The article is helpful for the beginners of the neural network to
understand how fully connected layer and the convolutional layer work in the
backend. To be concise and to make the article more readable, we only consider
the linear case. It can be extended to the non-linear case easily through
plugging in a non-linear encapsulation to the values like this $\sigma(x)$
denoted as $x^{\prime}$.



Compatibility between items, such as clothes and shoes, is a major factor
among customer's purchasing decisions. However, learning "compatibility" is
challenging due to (1) broader notions of compatibility than those of
similarity, (2) the asymmetric nature of compatibility, and (3) only a small
set of compatible and incompatible items are observed. We propose an end-to-end
trainable system to embed each item into a latent vector and project a query
item into K compatible prototypes in the same space. These prototypes reflect
the broad notions of compatibility. We refer to both the embedding and
prototypes as "Compatibility Family". In our learned space, we introduce a
novel Projected Compatibility Distance (PCD) function which is differentiable
and ensures diversity by aiming for at least one prototype to be close to a
compatible item, whereas none of the prototypes are close to an incompatible
item. We evaluate our system on a toy dataset, two Amazon product datasets, and
Polyvore outfit dataset. Our method consistently achieves state-of-the-art
performance. Finally, we show that we can visualize the candidate compatible
prototypes using a Metric-regularized Conditional Generative Adversarial
Network (MrCGAN), where the input is a projected prototype and the output is a
generated image of a compatible item. We ask human evaluators to judge the
relative compatibility between our generated images and images generated by
CGANs conditioned directly on query items. Our generated images are
significantly preferred, with roughly twice the number of votes as others.



Deriving event storylines is an effective summarization method to succinctly
organize extensive information, which can significantly alleviate the pain of
information overload. The critical challenge is the lack of widely recognized
definition of storyline metric. Prior studies have developed various approaches
based on different assumptions about users' interests. These works can extract
interesting patterns, but their assumptions do not guarantee that the derived
patterns will match users' preference. On the other hand, their exclusiveness
of single modality source misses cross-modality information. This paper
proposes a method, multimodal imitation learning via generative adversarial
networks(MIL-GAN), to directly model users' interests as reflected by various
data. In particular, the proposed model addresses the critical challenge by
imitating users' demonstrated storylines. Our proposed model is designed to
learn the reward patterns given user-provided storylines and then applies the
learned policy to unseen data. The proposed approach is demonstrated to be
capable of acquiring the user's implicit intent and outperforming competing
methods by a substantial margin with a user study.



Autonomous lifelong development and learning is a fundamental capability of
humans, differentiating them from current deep learning systems. However, other
branches of artificial intelligence have designed crucial ingredients towards
autonomous learning: curiosity and intrinsic motivation, social learning and
natural interaction with peers, and embodiment. These mechanisms guide
exploration and autonomous choice of goals, and integrating them with deep
learning opens stimulating perspectives. Deep learning (DL) approaches made
great advances in artificial intelligence, but are still far away from human
learning. As argued convincingly by Lake et al., differences include human
capabilities to learn causal models of the world from very little data,
leveraging compositional representations and priors like intuitive physics and
psychology. However, there are other fundamental differences between current DL
systems and human learning, as well as technical ingredients to fill this gap,
that are either superficially, or not adequately, discussed by Lake et al.
These fundamental mechanisms relate to autonomous development and learning.
They are bound to play a central role in artificial intelligence in the future.
Current DL systems require engineers to manually specify a task-specific
objective function for every new task, and learn through off-line processing of
large training databases. On the contrary, humans learn autonomously open-ended
repertoires of skills, deciding for themselves which goals to pursue or value,
and which skills to explore, driven by intrinsic motivation/curiosity and
social learning through natural interaction with peers. Such learning processes
are incremental, online, and progressive. Human child development involves a
progressive increase of complexity in a curriculum of learning where skills are
explored, acquired, and built on each other, through particular ordering and
timing. Finally, human learning happens in the physical world, and through
bodily and physical experimentation, under severe constraints on energy, time,
and computational resources. In the two last decades, the field of
Developmental and Cognitive Robotics (Cangelosi and Schlesinger, 2015, Asada et
al., 2009), in strong interaction with developmental psychology and
neuroscience, has achieved significant advances in computational



Personal electronic devices such as smartphones give access to a broad range
of behavioral signals that can be used to learn about the characteristics and
preferences of individuals. In this study we explore the connection between
demographic and psychological attributes and digital records for a cohort of
7,633 people, closely representative of the US population with respect to
gender, age, geographical distribution, education, and income. We collected
self-reported assessments on validated psychometric questionnaires based on
both the Moral Foundations and Basic Human Values theories, and combined this
information with passively-collected multi-modal digital data from web browsing
behavior, smartphone usage and demographic data. Then, we designed a machine
learning framework to infer both the demographic and psychological attributes
from the behavioral data. In a cross-validated setting, our model is found to
predict demographic attributes with good accuracy (weighted AUC scores of 0.90
for gender, 0.71 for age, 0.74 for ethnicity). Our weighted AUC scores for
Moral Foundation attributes (0.66) and Human Values attributes (0.60) suggest
that accurate prediction of complex psychometric attributes is more challenging
but feasible. This connection might prove useful for designing personalized
services, communication strategies, and interventions, and can be used to
sketch a portrait of people with similar worldviews.



Attention-based sequence-to-sequence models for automatic speech recognition
jointly train an acoustic model, language model, and alignment mechanism. Thus,
the language model component is only trained on transcribed audio-text pairs.
This leads to the use of shallow fusion with an external language model at
inference time. Shallow fusion refers to log-linear interpolation with a
separately trained language model at each step of the beam search. In this
work, we investigate the behavior of shallow fusion across a range of
conditions: different types of language models, different decoding units, and
different tasks. On Google Voice Search, we demonstrate that the use of shallow
fusion with a neural LM with wordpieces yields a 9.1% relative word error rate
reduction (WERR) over our competitive attention-based sequence-to-sequence
model, obviating the need for second-pass rescoring.



Chemical databases store information in text representations, and the SMILES
format is a universal standard used in many cheminformatics software. Encoded
in each SMILES string is structural information that can be used to predict
complex chemical properties. In this work, we develop SMILES2Vec, a deep RNN
that automatically learns features from SMILES strings to predict chemical
properties, without the need for additional explicit chemical information, or
the "grammar" of how SMILES encode structural data. Using Bayesian optimization
methods to tune the network architecture, we show that an optimized SMILES2Vec
model can serve as a general-purpose neural network for learning a range of
distinct chemical properties including toxicity, activity, solubility and
solvation energy, while outperforming contemporary MLP networks that uses
engineered features. Furthermore, we demonstrate proof-of-concept of
interpretability by developing an explanation mask that localizes on the most
important characters used in making a prediction. When tested on the solubility
dataset, this localization identifies specific parts of a chemical that is
consistent with established first-principles knowledge of solubility with an
accuracy of 88%, demonstrating that neural networks can learn technically
accurate chemical concepts. The fact that SMILES2Vec validates established
chemical facts, while providing state-of-the-art accuracy, makes it a potential
tool for widespread adoption of interpretable deep learning by the chemistry
community.



In this paper, we propose a new algorithm for learning general
latent-variable probabilistic graphical models using the techniques of
predictive state representation, instrumental variable regression, and
reproducing-kernel Hilbert space embeddings of distributions. Under this new
learning framework, we first convert latent-variable graphical models into
corresponding latent-variable junction trees, and then reduce the hard
parameter learning problem into a pipeline of supervised learning problems,
whose results will then be used to perform predictive belief propagation over
the latent junction tree during the actual inference procedure. We then give
proofs of our algorithm's correctness, and demonstrate its good performance in
experiments on one synthetic dataset and two real-world tasks from
computational biology and computer vision - classifying DNA splice junctions
and recognizing human actions in videos.



Humans are routinely asked to evaluate the performance of other individuals,
separating success from failure and affecting outcomes from science to
education and sports. Yet, in many contexts, the metrics driving the human
evaluation process remain unclear. Here we analyse a massive dataset capturing
players' evaluations by human judges to explore human perception of performance
in soccer, the world's most popular sport. We use machine learning to design an
artificial judge which accurately reproduces human evaluation, allowing us to
demonstrate how human observers are biased towards diverse contextual features.
By investigating the structure of the artificial judge, we uncover the aspects
of the players' behavior which attract the attention of human judges,
demonstrating that human evaluation is based on a noticeability heuristic where
only feature values far from the norm are considered to rate an individual's
performance.



Person Re-identification (re-id) faces two major challenges: the lack of
cross-view paired training data and learning discriminative identity-sensitive
and view-invariant features in the presence of large pose variations. In this
work, we address both problems by proposing a novel deep person image
generation model for synthesizing realistic person images conditional on pose.
The model is based on a generative adversarial network (GAN) and used
specifically for pose normalization in re-id, thus termed pose-normalization
GAN (PN-GAN). With the synthesized images, we can learn a new type of deep
re-id feature free of the influence of pose variations. We show that this
feature is strong on its own and highly complementary to features learned with
the original images. Importantly, we now have a model that generalizes to any
new re-id dataset without the need for collecting any training data for model
fine-tuning, thus making a deep re-id model truly scalable. Extensive
experiments on five benchmarks show that our model outperforms the
state-of-the-art models, often significantly. In particular, the features
learned on Market-1501 can achieve a Rank-1 accuracy of 68.67% on VIPeR without
any model fine-tuning, beating almost all existing models fine-tuned on the
dataset.



Named Entity Recognition (NER) aims at locating and classifying named
entities in text. In some use cases of NER, including cases where detected
named entities are used in creating content recommendations, it is crucial to
have a reliable confidence level for the detected named entities. In this work
we study the problem of finding confidence levels for detected named entities.
We refer to this problem as Named Entity Sequence Classification (NESC). We
frame NESC as a binary classification problem and we use NER as well as
recurrent neural networks to find the probability of candidate named entity is
a real named entity. We apply this approach to Tweet texts and we show how we
could find named entities with high confidence levels from Tweets.



An adversarial example is an example that has been adjusted to produce a
wrong label when presented to a system at test time. To date, adversarial
example constructions have been demonstrated for classifiers, but not for
detectors. If adversarial examples that could fool a detector exist, they could
be used to (for example) maliciously create security hazards on roads populated
with smart vehicles. In this paper, we demonstrate a construction that
successfully fools two standard detectors, Faster RCNN and YOLO. The existence
of such examples is surprising, as attacking a classifier is very different
from attacking a detector, and that the structure of detectors - which must
search for their own bounding box, and which cannot estimate that box very
accurately - makes it quite likely that adversarial patterns are strongly
disrupted. We show that our construction produces adversarial examples that
generalize well across sequences digitally, even though large perturbations are
needed. We also show that our construction yields physical objects that are
adversarial.



With access to large datasets, deep neural networks (DNN) have achieved
human-level accuracy in image and speech recognition tasks. However, in
chemistry, availability of large standardized and labelled datasets is scarce,
and many chemical properties of research interest, chemical data is inherently
small and fragmented. In this work, we explore transfer learning techniques in
conjunction with the existing Chemception CNN model, to create a transferable
and generalizable deep neural network for small-molecule property prediction.
Our latest model, ChemNet learns in a semi-supervised manner from inexpensive
labels computed from the ChEMBL database. When fine-tuned to the Tox21, HIV and
FreeSolv dataset, which are 3 separate chemical properties that ChemNet was not
originally trained on, we demonstrate that ChemNet exceeds the performance of
existing Chemception models and other contemporary DNN models. Furthermore, as
ChemNet has been pre-trained on a large diverse chemical database, it can be
used as a general-purpose plug-and-play deep neural network for the prediction
of novel small-molecule chemical properties.



This paper is concerned with paraphrase detection. The ability to detect
similar sentences written in natural language is crucial for several
applications, such as text mining, text summarization, plagiarism detection,
authorship authentication and question answering. Given two sentences, the
objective is to detect whether they are semantically identical. An important
insight from this work is that existing paraphrase systems perform well when
applied on clean texts, but they do not necessarily deliver good performance
against noisy texts. Challenges with paraphrase detection on user generated
short texts, such as Twitter, include language irregularity and noise. To cope
with these challenges, we propose a novel deep neural network-based approach
that relies on coarse-grained sentence modeling using a convolutional neural
network and a long short-term memory model, combined with a specific
fine-grained word-level similarity matching model. Our experimental results
show that the proposed approach outperforms existing state-of-the-art
approaches on user-generated noisy social media data, such as Twitter texts,
and achieves highly competitive performance on a cleaner corpus.



Learning a goal-oriented dialog policy is generally performed offline with
supervised learning algorithms or online with reinforcement learning (RL).
Additionally, as companies accumulate massive quantities of dialog transcripts
between customers and trained human agents, encoder-decoder methods have gained
popularity as agent utterances can be directly treated as supervision without
the need for utterance-level annotations. However, one potential drawback of
such approaches is that they myopically generate the next agent utterance
without regard for dialog-level considerations. To resolve this concern, this
paper describes an offline RL method for learning from unannotated corpora that
can optimize a goal-oriented policy at both the utterance and dialog level. We
introduce a novel reward function and use both on-policy and off-policy policy
gradient to learn a policy offline without requiring online user interaction or
an explicit state space definition.



In Positional-Slotted Object-Applicative (PSOA) RuleML, a predicate
application (atom) can have an Object IDentifier (OID) and descriptors that may
be positional arguments (tuples) or attribute-value pairs (slots). PSOA RuleML
1.0 specifies for each descriptor whether it is to be interpreted under the
perspective of the predicate in whose scope it occurs. This perspectivity
dimension refines the space between oidless, positional atoms (relationships)
and oidful, slotted atoms (frames): While relationships use only a
predicate-scope-sensitive (predicate-dependent) tuple and frames use only
predicate-scope-insensitive (predicate-independent) slots, PSOA RuleML 1.0 uses
a systematics of orthogonal constructs also permitting atoms with
(predicate-)independent tuples and atoms with (predicate-)dependent slots. This
supports data and knowledge representation where a slot attribute can have
different values depending on the predicate. PSOA thus extends object-oriented
multi-membership and multiple inheritance. Based on objectification, PSOA laws
are given: Besides unscoping and centralization, the semantic restriction and
transformation of describution permits rescoping of one atom's independent
descriptors to another atom with the same OID but a different predicate. For
inheritance, default descriptors are realized by rules. On top of a metamodel
and a Grailog visualization, PSOA's atom systematics for facts, queries, and
rules is explained. The presentation and (XML-)serialization syntaxes of PSOA
RuleML 1.0 are introduced. Its model-theoretic semantics is formalized by
extending the earlier interpretation functions for dependent descriptors. The
open-source PSOATransRun 1.3 system realizes PSOA RuleML 1.0 by a translator to
runtime predicates, including for dependent tuples (prdtupterm) and slots
(prdsloterm). Our tests show efficiency advantages of dependent and tupled
modeling.



Coordinate descent methods minimize a cost function by updating a single
decision variable (corresponding to one coordinate) at a time. Ideally, one
would update the decision variable that yields the largest marginal decrease in
the cost function. However, finding this coordinate would require checking all
of them, which is not computationally practical. We instead propose a new
adaptive method for coordinate descent. First, we define a lower bound on the
decrease of the cost function when a coordinate is updated and, instead of
calculating this lower bound for all coordinates, we use a multi-armed bandit
algorithm to learn which coordinates result in the largest marginal decrease
while simultaneously performing coordinate descent. We show that our approach
improves the convergence of the coordinate methods (including parallel
versions) both theoretically and experimentally.



In this paper, we describe and study the indicator mining problem in the
online sex advertising domain. We present an in-development system, FlagIt
(Flexible and adaptive generation of Indicators from text), which combines the
benefits of both a lightweight expert system and classical semi-supervision
(heuristic re-labeling) with recently released state-of-the-art unsupervised
text embeddings to tag millions of sentences with indicators that are highly
correlated with human trafficking. The FlagIt technology stack is open source.
On preliminary evaluations involving five indicators, FlagIt illustrates
promising performance compared to several alternatives. The system is being
actively developed, refined and integrated into a domain-specific search system
used by over 200 law enforcement agencies to combat human trafficking, and is
being aggressively extended to mine at least six more indicators with minimal
programming effort. FlagIt is a good example of a system that operates in
limited label settings, and that requires creative combinations of established
machine learning techniques to produce outputs that could be used by real-world
non-technical analysts.



An outstanding challenge in nonlinear systems theory is identification or
learning of a given nonlinear system's Koopman operator directly from data or
models. Advances in extended dynamic mode decomposition approaches and machine
learning methods have enabled data-driven discovery of Koopman operators, for
both continuous and discrete-time systems. Since Koopman operators are often
infinite-dimensional, they are approximated in practice using
finite-dimensional systems. The fidelity and convergence of a given
finite-dimensional Koopman approximation is a subject of ongoing research. In
this paper we introduce a class of Koopman observable functions that confer an
approximate closure property on their corresponding finite-dimensional
approximations of the Koopman operator. We derive error bounds for the fidelity
of this class of observable functions, as well as identify two key learning
parameters which can be used to tune performance. We illustrate our approach on
two classical nonlinear system models: the Van Der Pol oscillator and the
bistable toggle switch.



Direct acoustics-to-word (A2W) models in the end-to-end paradigm have
received increasing attention compared to conventional sub-word based automatic
speech recognition models using phones, characters, or context-dependent hidden
Markov model states. This is because A2W models recognize words from speech
without any decoder, pronunciation lexicon, or externally-trained language
model, making training and decoding with such models simple. Prior work has
shown that A2W models require orders of magnitude more training data in order
to perform comparably to conventional models. Our work also showed this
accuracy gap when using the English Switchboard-Fisher data set. This paper
describes a recipe to train an A2W model that closes this gap and is at-par
with state-of-the-art sub-word based models. We achieve a word error rate of
8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder
or language model. We find that model initialization, training data order, and
regularization have the most impact on the A2W model performance. Next, we
present a joint word-character A2W model that learns to first spell the word
and then recognize it. This model provides a rich output to the user instead of
simple word hypotheses, making it especially useful in the case of words unseen
or rarely-seen during training.



As of February 2016 Facebook allows users to express their experienced
emotions about a post by using five so-called `reactions'. This research paper
proposes and evaluates alternative methods for predicting these reactions to
user posts on public pages of firms/companies (like supermarket chains). For
this purpose, we collected posts (and their reactions) from Facebook pages of
large supermarket chains and constructed a dataset which is available for other
researches. In order to predict the distribution of reactions of a new post,
neural network architectures (convolutional and recurrent neural networks) were
tested using pretrained word embeddings. Results of the neural networks were
improved by introducing a bootstrapping approach for sentiment and emotion
mining on the comments for each post. The final model (a combination of neural
network and a baseline emotion miner) is able to predict the reaction
distribution on Facebook posts with a mean squared error (or misclassification
rate) of 0.135.



Adversarial perturbations can pose a serious threat for deploying machine
learning systems. Recent works have shown existence of image-agnostic
perturbations that can fool classifiers over most natural images. Existing
methods present optimization approaches that solve for a fooling objective with
an imperceptibility constraint to craft the perturbations. However, for a given
classifier, they generate one perturbation at a time, which is a single
instance from the manifold of adversarial perturbations. Also, in order to
build robust models, it is essential to explore the manifold of adversarial
perturbations. In this paper, we propose for the first time, a generative
approach to model the distribution of adversarial perturbations. The
architecture of the proposed model is inspired from that of GANs and is trained
using fooling and diversity objectives. Our trained generator network attempts
to capture the distribution of adversarial perturbations for a given classifier
and readily generates a wide variety of such perturbations. Our experimental
evaluation demonstrates that perturbations crafted by our model (i) achieve
state-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver
excellent cross model generalizability. Our work can be deemed as an important
step in the process of inferring about the complex manifolds of adversarial
perturbations.



This paper proposes adversarial attacks for Reinforcement Learning (RL) and
then improves the robustness of Deep Reinforcement Learning algorithms (DRL) to
parameter uncertainties with the help of these attacks. We show that even a
naively engineered attack successfully degrades the performance of DRL
algorithm. We further improve the attack using gradient information of an
engineered loss function which leads to further degradation in performance.
These attacks are then leveraged during training to improve the robustness of
RL within robust control framework. We show that this adversarial training of
DRL algorithms like Deep Double Q learning and Deep Deterministic Policy
Gradients leads to significant increase in robustness to parameter variations
for RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah
environment.



Research in Artificial Intelligence is breaking technology barriers every
day. New algorithms and high performance computing are making things possible
which we could only have imagined earlier. Though the enhancements in AI are
making life easier for human beings day by day, there is constant fear that AI
based systems will pose a threat to humanity. People in AI community have
diverse set of opinions regarding the pros and cons of AI mimicking human
behavior. Instead of worrying about AI advancements, we propose a novel idea of
cognitive agents, including both human and machines, living together in a
complex adaptive ecosystem, collaborating on human computation for producing
essential social goods while promoting sustenance, survival and evolution of
the agents' life cycle. We highlight several research challenges and technology
barriers in achieving this goal. We propose a governance mechanism around this
ecosystem to ensure ethical behaviors of all cognitive agents. Along with a
novel set of use-cases of Cogniculture, we discuss the road map ahead for this
journey.



Recently, there have been increasing demands to construct compact deep
architectures to remove unnecessary redundancy and to improve the inference
speed. While many recent works focus on reducing the redundancy by eliminating
unneeded weight parameters, it is not possible to apply a single deep
architecture for multiple devices with different resources. When a new device
or circumstantial condition requires a new deep architecture, it is necessary
to construct and train a new network from scratch. In this work, we propose a
novel deep learning framework, called a nested sparse network, which exploits
an n-in-1-type nested structure in a neural network. A nested sparse network
consists of multiple levels of networks with a different sparsity ratio
associated with each level, and higher level networks share parameters with
lower level networks to enable stable nested learning. The proposed framework
realizes a resource-aware versatile architecture as the same network can meet
diverse resource requirements. Moreover, the proposed nested network can learn
different forms of knowledge in its internal networks at different levels,
enabling multiple tasks using a single network, such as coarse-to-fine
hierarchical classification. In order to train the proposed nested sparse
network, we propose efficient weight connection learning and channel and layer
scheduling strategies. We evaluate our network in multiple tasks, including
adaptive deep compression, knowledge distillation, and learning class
hierarchy, and demonstrate that nested sparse networks perform competitively,
but more efficiently, than existing methods.



In recent years, many techniques have been developed to improve the
performance and efficiency of data center networks. While these techniques
provide high accuracy, they are often designed using heuristics that leverage
domain-specific properties of the workload or hardware.
  In this vision paper, we argue that many data center networking techniques,
e.g., routing, topology augmentation, energy savings, with diverse goals
actually share design and architectural similarity. We present a design for
developing general intermediate representations of network topologies using
deep learning that is amenable to solving classes of data center problems. We
develop a framework, DeepConfig, that simplifies the processing of configuring
and training deep learning agents that use the intermediate representation to
learns different tasks. To illustrate the strength of our approach, we
configured, implemented, and evaluated a DeepConfig-Agent that tackles the data
center topology augmentation problem. Our initial results are promising ---
DeepConfig performs comparably to the optimal.



The performance of optimization algorithms relies crucially on their
parameterizations. Finding good parameter settings is called algorithm tuning.
Using a simple simulated annealing algorithm, we will demonstrate how
optimization algorithms can be tuned using the sequential parameter
optimization toolbox (SPOT). SPOT provides several tools for automated and
interactive tuning. The underling concepts of the SPOT approach are explained.
This includes key techniques such as exploratory fitness landscape analysis and
response surface methodology. Many examples illustrate how SPOT can be used for
understanding the performance of algorithms and gaining insight into
algorithm's behavior. Furthermore, we demonstrate how SPOT can be used as an
optimizer and how a sophisticated ensemble approach is able to combine several
meta models via stacking.



In this paper, we present a comprehensive study and evaluation of existing
single image dehazing algorithms, using a new large-scale benchmark consisting
of both synthetic and real-world hazy images, called REalistic Single Image
DEhazing (RESIDE). RESIDE highlights diverse data sources and image contents,
and is divided into five subsets, each serving different training or evaluation
purposes. We further provide a rich variety of criteria for dehazing algorithm
evaluation, ranging from full-reference metrics, to no-reference metrics, to
subjective evaluation and the novel task-driven evaluation. Experiments on
RESIDE sheds light on the comparisons and limitations of state-of-the-art
dehazing algorithms, and suggest promising future directions.



Sequential pattern mining techniques extract patterns corresponding to
frequent subsequences from a sequence database. A practical limitation of these
techniques is that they overload the user with too many patterns. Local Process
Model (LPM) mining is an alternative approach coming from the field of process
mining. While in traditional sequential pattern mining, a pattern describes one
subsequence, an LPM captures a set of subsequences. Also, while traditional
sequential patterns only match subsequences that are observed in the sequence
database, an LPM may capture subsequences that are not explicitly observed, but
that are related to observed subsequences. In other words, LPMs generalize the
behavior observed in the sequence database. These properties make it possible
for a set of LPMs to cover the behavior of a much larger set of sequential
patterns. Yet, existing LPM mining techniques still suffer from the pattern
explosion problem because they produce sets of redundant LPMs. In this paper,
we propose several heuristics to mine a set of non-redundant LPMs either from a
set of redundant LPMs or from a set of sequential patterns. We empirically
compare the proposed heuristics between them and against existing (local)
process mining techniques in terms of coverage, precision, and complexity of
the produced sets of LPMs.



The search for interpretable reinforcement learning policies is of high
academic and industrial interest. Especially for industrial systems, domain
experts are more likely to deploy autonomously learned controllers if they are
understandable and convenient to evaluate. Basic algebraic equations are
supposed to meet these requirements, as long as they are restricted to an
adequate complexity. Here we introduce the genetic programming for
reinforcement learning (GPRL) approach based on model-based batch reinforcement
learning and genetic programming, which autonomously learns policy equations
from pre-existing default state-action trajectory samples. GPRL is compared to
a straight-forward method which utilizes genetic programming for symbolic
regression, yielding policies imitating an existing well-performing, but
non-interpretable policy. Experiments on three reinforcement learning
benchmarks, i.e., mountain car, cart-pole balancing, and industrial benchmark,
demonstrate the superiority of our GPRL approach compared to the symbolic
regression method. GPRL is capable of producing well-performing interpretable
reinforcement learning policies from pre-existing default trajectory data.



The study of deep recurrent neural networks (RNNs) and, in particular, of
deep Reservoir Computing (RC) is gaining an increasing research attention in
the neural networks community. The recently introduced deep Echo State Network
(deepESN) model opened the way to an extremely efficient approach for designing
deep neural networks for temporal data. At the same time, the study of deepESNs
allowed to shed light on the intrinsic properties of state dynamics developed
by hierarchical compositions of recurrent layers, i.e. on the bias of depth in
RNNs architectural design. In this paper, we summarize the advancements in the
development, analysis and applications of deepESNs.



Prediction of popularity has profound impact for social media, since it
offers opportunities to reveal individual preference and public attention from
evolutionary social systems. Previous research, although achieves promising
results, neglects one distinctive characteristic of social data, i.e.,
sequentiality. For example, the popularity of online content is generated over
time with sequential post streams of social media. To investigate the
sequential prediction of popularity, we propose a novel prediction framework
called Deep Temporal Context Networks (DTCN) by incorporating both temporal
context and temporal attention into account. Our DTCN contains three main
components, from embedding, learning to predicting. With a joint embedding
network, we obtain a unified deep representation of multi-modal user-post data
in a common embedding space. Then, based on the embedded data sequence over
time, temporal context learning attempts to recurrently learn two adaptive
temporal contexts for sequential popularity. Finally, a novel temporal
attention is designed to predict new popularity (the popularity of a new
user-post pair) with temporal coherence across multiple time-scales.
Experiments on our released image dataset with about 600K Flickr photos
demonstrate that DTCN outperforms state-of-the-art deep prediction algorithms,
with an average of 21.51% relative performance improvement in the popularity
prediction (Spearman Ranking Correlation).



Deep reinforcement learning (DRL) has shown incredible performance in
learning various tasks to the human level. However, unlike human perception,
current DRL models connect the entire low-level sensory input to the
state-action values rather than exploiting the relationship between and among
entities that constitute the sensory input. Because of this difference, DRL
needs vast amount of experience samples to learn. In this paper, we propose a
Multi-focus Attention Network (MANet) which mimics human ability to spatially
abstract the low-level sensory input into multiple entities and attend to them
simultaneously. The proposed method first divides the low-level input into
several segments which we refer to as partial states. After this segmentation,
parallel attention layers attend to the partial states relevant to solving the
task. Our model estimates state-action values using these attended partial
states. In our experiments, MANet attains highest scores with significantly
less experience samples. Additionally, the model shows higher performance
compared to the Deep Q-network and the single attention model as benchmarks.
Furthermore, we extend our model to attentive communication model for
performing multi-agent cooperative tasks. In multi-agent cooperative task
experiments, our model shows 20% faster learning than existing state-of-the-art
model.



We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source
python libraries for building conversational software. Their purpose is to make
machine-learning based dialogue management and language understanding
accessible to non-specialist software developers. In terms of design
philosophy, we aim for ease of use, and bootstrapping from minimal (or no)
initial training data. Both packages are extensively documented and ship with a
comprehensive suite of tests. The code is available at
https://github.com/RasaHQ/



With the advent of the Internet, large amount of digital text is generated
everyday in the form of news articles, research publications, blogs, question
answering forums and social media. It is important to develop techniques for
extracting information automatically from these documents, as lot of important
information is hidden within them. This extracted information can be used to
improve access and management of knowledge hidden in large text corpora.
Several applications such as Question Answering, Information Retrieval would
benefit from this information. Entities like persons and organizations, form
the most basic unit of the information. Occurrences of entities in a sentence
are often linked through well-defined relations; e.g., occurrences of person
and organization in a sentence may be linked through relations such as employed
at. The task of Relation Extraction (RE) is to identify such relations
automatically. In this paper, we survey several important supervised,
semi-supervised and unsupervised RE techniques. We also cover the paradigms of
Open Information Extraction (OIE) and Distant Supervision. Finally, we describe
some of the recent trends in the RE techniques and possible future research
directions. This survey would be useful for three kinds of readers - i)
Newcomers in the field who want to quickly learn about RE; ii) Researchers who
want to know how the various RE techniques evolved over time and what are
possible future research directions and iii) Practitioners who just need to
know which RE technique works best in various settings.



To harness the complexity of their high-dimensional bodies during
sensorimotor development, infants are guided by patterns of freezing and
freeing of degrees of freedom. For instance, when learning to reach, infants
free the degrees of freedom in their arm proximodistally, i.e. from joints that
are closer to the body to those that are more distant. Here, we formulate and
study computationally the hypothesis that such patterns can emerge
spontaneously as the result of a family of stochastic optimization processes
(evolution strategies with covariance-matrix adaptation), without an innate
encoding of a maturational schedule. In particular, we present simulated
experiments with an arm where a computational learner progressively acquires
reaching skills through adaptive exploration, and we show that a proximodistal
organization appears spontaneously, which we denote PDFF (ProximoDistal
Freezing and Freeing of degrees of freedom). We also compare this emergent
organization between different arm morphologies -- from human-like to quite
unnatural ones -- to study the effect of different kinematic structures on the
emergence of PDFF. Keywords: human motor learning; proximo-distal exploration;
stochastic optimization; modelling; evolution strategies; cross-entropy
methods; policy search; morphology.}



Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a
given document with respect to a given aspect entity. While neural network
architectures have been successful in predicting the overall polarity of
sentences, aspect-specific sentiment analysis still remains as an open problem.
In this paper, we propose a novel method for integrating aspect information
into the neural model. More specifically, we incorporate aspect information
into the neural model by modeling word-aspect relationships. Our novel model,
\textit{Aspect Fusion LSTM} (AF-LSTM) learns to attend based on associative
relationships between sentence words and aspect which allows our model to
adaptively focus on the correct words given an aspect term. This ameliorates
the flaws of other state-of-the-art models that utilize naive concatenations to
model word-aspect similarity. Instead, our model adopts circular convolution
and circular correlation to model the similarity between aspect and words and
elegantly incorporates this within a differentiable neural attention framework.
Finally, our model is end-to-end differentiable and highly related to
convolution-correlation (holographic like) memories. Our proposed neural model
achieves state-of-the-art performance on benchmark datasets, outperforming
ATAE-LSTM by $4\%-5\%$ on average across multiple datasets.



We introduce The House Of inteRactions (THOR), a framework for visual AI
research, available at http://ai2thor.allenai.org. AI2-THOR consists of near
photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes
and interact with objects to perform tasks. AI2-THOR enables research in many
different domains including but not limited to deep reinforcement learning,
imitation learning, learning by interaction, planning, visual question
answering, unsupervised representation learning, object detection and
segmentation, and learning models of cognition. The goal of AI2-THOR is to
facilitate building visually intelligent models and push the research forward
in this domain.



In this work, we propose a goal-driven collaborative task that contains
vision, language, and action in a virtual environment as its core components.
Specifically, we develop a collaborative `Image Drawing' game between two
agents, called CoDraw. Our game is grounded in a virtual world that contains
movable clip art objects. Two players, Teller and Drawer, are involved. The
Teller sees an abstract scene containing multiple clip arts in a semantically
meaningful configuration, while the Drawer tries to reconstruct the scene on an
empty canvas using available clip arts. The two players communicate via two-way
communication using natural language. We collect the CoDraw dataset of ~10K
dialogs consisting of 138K messages exchanged between a Teller and a Drawer
from Amazon Mechanical Turk (AMT). We analyze our dataset and present three
models to model the players' behaviors, including an attention model to
describe and draw multiple clip arts at each round. The attention models are
quantitatively compared to the other models to show how the conventional
approaches work for this new task. We also present qualitative visualizations.



Most of the weights in a Lightweight Neural Network have a value of zero,
while the remaining ones are either +1 or -1. These universal approximators
require approximately 1.1 bits/weight of storage, posses a quick forward pass
and achieve classification accuracies similar to conventional continuous-weight
networks. Their training regimen focuses on error reduction initially, but
later emphasizes discretization of weights. They ignore insignificant inputs,
remove unnecessary weights, and drop unneeded hidden neurons. We have
successfully tested them on the MNIST, credit card fraud, and credit card
defaults data sets using networks having 2 to 16 hidden layers and up to 4.4
million weights.



We suggest a model of a multi-agent society of decision makers taking
decisions being based on two criteria, one is the utility of the prospects and
the other is the attractiveness of the considered prospects. The model is the
generalization of quantum decision theory, developed earlier for single
decision makers realizing one-step decisions, in two principal aspects. First,
several decision makers are considered simultaneously, who interact with each
other through information exchange. Second, a multistep procedure is treated,
when the agents exchange information many times. Several decision makers
exchanging information and forming their judgement, using quantum rules, form a
kind of a quantum information network, where collective decisions develop in
time as a result of information exchange. In addition to characterizing
collective decisions that arise in human societies, such networks can describe
dynamical processes occurring in artificial quantum intelligence composed of
several parts or in a cluster of quantum computers. The practical usage of the
theory is illustrated on the dynamic disjunction effect for which three
quantitative predictions are made: (i) the probabilistic behavior of decision
makers at the initial stage of the process is described; (ii) the decrease of
the difference between the initial prospect probabilities and the related
utility factors is proved; (iii) the existence of a common consensus after
multiple exchange of information is predicted. The predicted numerical values
are in very good agreement with empirical data.



In this work, we present our findings and experiments for stock-market
prediction using various textual sentiment analysis tools, such as mood
analysis and event extraction, as well as prediction models, such as LSTMs and
specific convolutional architectures.



The next generation of AI applications will continuously interact with the
environment and learn from these interactions. These applications impose new
and demanding systems requirements, both in terms of performance and
flexibility. In this paper, we consider these requirements and present Ray---a
distributed system to address them. Ray implements a dynamic task graph
computation model that supports both the task-parallel and the actor
programming models. To meet the performance requirements of AI applications, we
propose an architecture that logically centralizes the system's control state
using a sharded storage system and a novel bottom-up distributed scheduler. In
our experiments, we demonstrate sub-millisecond remote task latencies and
linear throughput scaling beyond 1.8 million tasks per second. We empirically
validate that Ray speeds up challenging benchmarks and serves as both a natural
and performant fit for an emerging class of reinforcement learning applications
and algorithms.



In portable, three dimensional, and ultra-fast ultrasound (US) imaging
systems, there is an increasing need to reconstruct high quality images from a
limited number of RF data from receiver (Rx) or scan-line (SC) sub-sampling.
However, due to the severe side lobe artifacts from RF sub-sampling, the
standard beam-former often produces blurry images with less contrast that are
not suitable for diagnostic purpose. To address this problem, some researchers
have studied compressed sensing (CS) to exploit the sparsity of the image or RF
data in some domains. However, the existing CS approaches require either
hardware changes or computationally expensive algorithms. To overcome these
limitations, here we propose a novel deep learning approach that directly
interpolates the missing RF data by utilizing redundancy in the Rx-SC plane. In
particular, the network design principle derives from a novel interpretation of
the deep neural network as a cascaded convolution framelets that learns the
data-driven bases for Hankel matrix decomposition. Our extensive experimental
results from sub-sampled RF data from a real US system confirmed that the
proposed method can effectively reduce the data rate without sacrificing the
image quality.



The visual explanation of learned representation of models helps to
understand the fundamentals of learning. The attentional models of previous
works used to visualize the attended regions over an image or text using their
learned weights to confirm their intended mechanism. Kim et al. (2016) show
that the Hadamard product in multimodal deep networks, which is well-known for
the joint function of visual question answering tasks, implicitly performs an
attentional mechanism for visual inputs. In this work, we extend their work to
show that the Hadamard product in multimodal deep networks performs not only
for visual inputs but also for textual inputs simultaneously using the proposed
gradient-based visualization technique. The attentional effect of Hadamard
product is visualized for both visual and textual inputs by analyzing the two
inputs and an output of the Hadamard product with the proposed method and
compared with learned attentional weights of a visual question answering model.



Deep convolutional neural networks (CNN) based solutions are the current
state- of-the-art for computer vision tasks. Due to the large size of these
models, they are typically run on clusters of CPUs or GPUs. However, power
requirements and cost budgets can be a major hindrance in adoption of CNN for
IoT applications. Recent research highlights that CNN contain significant
redundancy in their structure and can be quantized to lower bit-width
parameters and activations, while maintaining acceptable accuracy. Low
bit-width and especially single bit-width (binary) CNN are particularly
suitable for mobile applications based on FPGA implementation, due to the
bitwise logic operations involved in binarized CNN. Moreover, the transition to
lower bit-widths opens new avenues for performance optimizations and model
improvement. In this paper, we present an automatic flow from trained
TensorFlow models to FPGA system on chip implementation of binarized CNN. This
flow involves quantization of model parameters and activations, generation of
network and model in embedded-C, followed by automatic generation of the FPGA
accelerator for binary convolutions. The automated flow is demonstrated through
implementation of binarized "YOLOV2" on the low cost, low power Cyclone- V FPGA
device. Experiments on object detection using binarized YOLOV2 demonstrate
significant performance benefit in terms of model size and inference speed on
FPGA as compared to CPU and mobile CPU platforms. Furthermore, the entire
automated flow from trained models to FPGA synthesis can be completed within
one hour.



We would like to learn latent representations that are low-dimensional and
highly interpretable. A model that has these characteristics is the Gaussian
Process Latent Variable Model. The benefits and negative of the GP-LVM are
complementary to the Variational Autoencoder, the former provides interpretable
low-dimensional latent representations while the latter is able to handle large
amounts of data and can use non-Gaussian likelihoods. Our inspiration for this
paper is to marry these two approaches and reap the benefits of both. In order
to do so we will introduce a novel approximate inference scheme inspired by the
GP-LVM and the VAE. We show experimentally that the approximation allows the
capacity of the generative bottle-neck (Z) of the VAE to be arbitrarily large
without losing a highly interpretable representation, allowing reconstruction
quality to be unlimited by Z at the same time as a low-dimensional space can be
used to perform ancestral sampling from as well as a means to reason about the
embedded data.



We show that the forward and backward propagation can be formulated as a
solution of lower and upper triangular systems of equations. For standard
feedforward (FNNs) and recurrent neural networks (RNNs) the triangular systems
are always block bi-diagonal, while for a general computation graph (directed
acyclic graph) they can have a more complex triangular sparsity pattern. We
discuss direct and iterative parallel algorithms that can be used for their
solution and interpreted as different ways of performing model parallelism.
Also, we show that for FNNs and RNNs with $k$ layers and $\tau$ time steps the
backward propagation can be performed in parallel in O($\log k$) and O($\log k
\log \tau$) steps, respectively. Finally, we outline the generalization of this
technique using Jacobians that potentially allows us to handle arbitrary
layers.



A common goal in Reinforcement Learning is to derive a good strategy given a
limited batch of data. In this paper, we adopt the safe policy improvement
(SPI) approach: we compute a target policy guaranteed to perform at least as
well as a given baseline policy. Our SPI strategy, inspired by the
knows-what-it-knows paradigms, consists in bootstrapping the target policy with
the baseline policy when it does not know. We develop two computationally
efficient bootstrapping algorithms, a value-based and a policy-based, both
accompanied with theoretical SPI bounds. Three algorithm variants are proposed.
We empirically show the literature algorithms limits on a small stochastic
gridworld problem, and then demonstrate that our five algorithms not only
improve the worst case scenarios, but also the mean performance.



Any-gram kernels are a flexible and efficient way to employ bag-of-n-gram
features when learning from textual data. They are also compatible with the use
of word embeddings so that word similarities can be accounted for. While the
original any-gram kernels are implemented on top of tree kernels, we propose a
new approach which is independent of tree kernels and is more efficient. We
also propose a more effective way to make use of word embeddings than the
original any-gram formulation. When applied to the task of sentiment
classification, our new formulation achieves significantly better performance.



Mobile ad-hoc network (MANET) is a dynamic collection of mobile computers
without the need for any existing infrastructure. Nodes in a MANET act as hosts
and routers. Designing of robust routing algorithms for MANETs is a challenging
task. Disjoint multipath routing protocols address this problem and increase
the reliability, security and lifetime of network. However, selecting an
optimal multipath is an NP-complete problem. In this paper, Hopfield neural
network (HNN) which its parameters are optimized by particle swarm optimization
(PSO) algorithm is proposed as multipath routing algorithm. Link expiration
time (LET) between each two nodes is used as the link reliability estimation
metric. This approach can find either node-disjoint or link-disjoint paths in
single phase route discovery. Simulation results confirm that PSO-HNN routing
algorithm has better performance as compared to backup path set selection
algorithm (BPSA) in terms of the path set reliability and number of paths in
the set.



Reading comprehension (RC)---in contrast to information retrieval---requires
integrating information and reasoning about events, entities, and their
relations across a full document. Question answering is conventionally used to
assess RC ability, in both artificial agents and children learning to read.
However, existing RC datasets and tasks are dominated by questions that can be
solved by selecting answers using superficial information (e.g., local context
similarity or global term frequency); they thus fail to test for the essential
integrative aspect of RC. To encourage progress on deeper comprehension of
language, we present a new dataset and set of tasks in which the reader must
answer questions about stories by reading entire books or movie scripts. These
tasks are designed so that successfully answering their questions requires
understanding the underlying narrative rather than relying on shallow pattern
matching or salience. We show that although humans solve the tasks easily,
standard RC models struggle on the tasks presented here. We provide an analysis
of the dataset and the challenges it presents.



We propose Cognitive Databases, an approach for transparently enabling
Artificial Intelligence (AI) capabilities in relational databases. A novel
aspect of our design is to first view the structured data source as meaningful
unstructured text, and then use the text to build an unsupervised neural
network model using a Natural Language Processing (NLP) technique called word
embedding. This model captures the hidden inter-/intra-column relationships
between database tokens of different types. For each database token, the model
includes a vector that encodes contextual semantic relationships. We seamlessly
integrate the word embedding model into existing SQL query infrastructure and
use it to enable a new class of SQL-based analytics queries called cognitive
intelligence (CI) queries. CI queries use the model vectors to enable complex
queries such as semantic matching, inductive reasoning queries such as
analogies, predictive queries using entities not present in a database, and,
more generally, using knowledge from external sources. We demonstrate unique
capabilities of Cognitive Databases using an Apache Spark based prototype to
execute inductive reasoning CI queries over a multi-modal database containing
text and images. We believe our first-of-a-kind system exemplifies using AI
functionality to endow relational databases with capabilities that were
previously very hard to realize in practice.



Second-order methods for neural network optimization have several advantages
over methods based on first-order gradient descent, including better scaling to
large mini-batch sizes and fewer updates needed for convergence. But they are
rarely applied to deep learning in practice because of high computational cost
and the need for model-dependent algorithmic variations. We introduce a variant
of the Hessian-free method that leverages a block-diagonal approximation of the
generalized Gauss-Newton matrix. Our method computes the curvature
approximation matrix only for pairs of parameters from the same layer or block
of the neural network and performs conjugate gradient updates independently for
each block. Experiments on deep autoencoders, deep convolutional networks, and
multilayer LSTMs demonstrate better convergence and generalization compared to
the original Hessian-free approach and the Adam method.



Breast cancer is already one of the most common form of cancer worldwide.
Mammography image analysis is still the most effective diagnostic method to
promote the early detection of breast cancer. Accurately segmenting tumors in
digital mammography images is important to improve diagnosis capabilities of
health specialists and avoid misdiagnosis. In this work, we evaluate the
feasibility of applying GrowCut to segment regions of tumor and we propose two
GrowCut semi-supervised versions. All the analysis was performed by evaluating
the application of segmentation techniques to a set of images obtained from the
Mini-MIAS mammography image database. GrowCut segmentation was compared to
Region Growing, Active Contours, Random Walks and Graph Cut techniques.
Experiments showed that GrowCut, when compared to the other techniques, was
able to acquire better results for the metrics analyzed. Moreover, the proposed
semi-supervised versions of GrowCut was proved to have a clinically
satisfactory quality of segmentation.



Learning algorithms for implicit generative models can optimize a variety of
criteria that measure how the data distribution differs from the implicit model
distribution, including the Wasserstein distance, the Energy distance, and the
Maximum Mean Discrepancy criterion. A careful look at the geometries induced by
these distances on the space of probability measures reveals interesting
differences. In particular, we can establish surprising approximate global
convergence guarantees for the $1$-Wasserstein distance,even when the
parametric generator has a nonconvex parametrization.



In the context of post-hoc interpretability, this paper addresses the task of
explaining the prediction of a classifier, considering the case where no
information is available, neither on the classifier itself, nor on the
processed data (neither the training nor the test data). It proposes an
instance-based approach whose principle consists in determining the minimal
changes needed to alter a prediction: given a data point whose classification
must be explained, the proposed method consists in identifying a close
neighbour classified differently, where the closeness definition integrates a
sparsity constraint. This principle is implemented using observation generation
in the Growing Spheres algorithm. Experimental results on two datasets
illustrate the relevance of the proposed approach that can be used to gain
knowledge about the classifier.



Discovery of an accurate causal Bayesian network structure from observational
data can be useful in many areas of science. Often the discoveries are made
under uncertainty, which can be expressed as probabilities. To guide the use of
such discoveries, including directing further investigation, it is important
that those probabilities be well-calibrated. In this paper, we introduce a
novel framework to derive calibrated probabilities of causal relationships from
observational data. The framework consists of three components: (1) an
approximate method for generating initial probability estimates of the edge
types for each pair of variables, (2) the availability of a relatively small
number of the causal relationships in the network for which the truth status is
known, which we call a calibration training set, and (3) a calibration method
for using the approximate probability estimates and the calibration training
set to generate calibrated probabilities for the many remaining pairs of
variables. We also introduce a new calibration method based on a shallow neural
network. Our experiments on simulated data support that the proposed approach
improves the calibration of causal edge predictions. The results also support
that the approach often improves the precision and recall of predictions.



Questions that require counting a variety of objects in images remain a major
challenge in visual question answering (VQA). The most common approaches to VQA
involve either classifying answers based on fixed length representations of
both the image and question or summing fractional counts estimated from each
section of the image. In contrast, we treat counting as a sequential decision
process and force our model to make discrete choices of what to count.
Specifically, the model sequentially selects from detected objects and learns
interactions between objects that influence subsequent selections. A
distinction of our approach is its intuitive and interpretable output, as
discrete counts are automatically grounded in the image. Furthermore, our
method outperforms the state of the art architecture for VQA on multiple
metrics that evaluate counting.



The hard problem in artificial intelligence asks how the shuffling of
syntactical symbols in a program can lead to systems which experience semantics
and qualia. We address this question in three stages. First, we introduce a new
class of human semantic symbols which appears when unexpected and drastic
environmental change causes humans to become surprised, confused, uncertain,
and in extreme cases, unresponsive, passive and dysfunctional. For this class
of symbols, pre-learned programs become inoperative so these syntactical
programs cannot be the source of experienced qualia. Second, we model the
dysfunctional human response to a radically changed environment as being the
natural response of any learning machine facing novel inputs from well outside
its previous training set. In this situation, learning machines are unable to
extract information from their input and will typically enter a dynamical state
characterized by null outputs and a lack of response. This state immediately
predicts and explains the characteristics of the semantic experiences of humans
in similar circumstances. In the third stage, we consider learning machines
trained to implement multiple functions in simple sequential programs using
environmental data to specify subroutine names, control flow instructions,
memory calls, and so on. Drastic change in any of these environmental inputs
can again lead to inoperative programs. By examining changes specific to people
or locations we can model human cognitive symbols featuring these dependencies,
such as attachment and grief. Our approach links known dynamical machines
states with human qualia and thus offers new insight into the hard problem of
artificial intelligence.



Deep Neural Networks are built to generalize outside of training set in mind
by using techniques such as regularization, early stopping and dropout. But
considerations to make them more resilient to adversarial examples are rarely
taken. As deep neural networks become more prevalent in mission-critical and
real-time systems, miscreants start to attack them by intentionally making deep
neural networks to misclassify an object of one type to be seen as another
type. This can be catastrophic in some scenarios where the classification of a
deep neural network can lead to a fatal decision by a machine. In this work, we
used GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method
and Jacobian Saliency Method, used those crafted adversarial samples to attack
another Deep Convolutional Neural Network and built the attacked network to be
more resilient against adversarial attacks by making it more robust by
Defensive Distillation and Adversarial Training



Reinforcement learning (RL) algorithms involve the deep nesting of distinct
components, where each component typically exhibits opportunities for
distributed computation. Current RL libraries offer parallelism at the level of
the entire program, coupling all the components together and making existing
implementations difficult to extend, combine, and reuse. We argue for building
composable RL components by encapsulating parallelism and resource requirements
within individual components, which can be achieved by building on top of a
flexible task-based programming model. We demonstrate this principle by
building Ray RLlib on top of Ray and show that we can implement a wide range of
state-of-the-art algorithms by composing and reusing a handful of standard
components. This composability does not come at the cost of performance --- in
our experiments, RLlib matches or exceeds the performance of highly optimized
reference implementations. Ray RLlib is available as part of Ray at
https://github.com/ray-project/ray/.



Integrated information theory provides a mathematical framework to fully
characterize the cause-effect structure of a physical system. Here, we
introduce PyPhi, a Python software package that implements this framework for
causal analysis and unfolds the full cause-effect structure of discrete
dynamical systems of binary elements. The software allows users to easily study
these structures, serves as an up-to-date reference implementation of the
formalisms of integrated information theory, and has been applied in research
on complexity, emergence, and certain biological questions. We first provide an
overview of the main algorithm and demonstrate PyPhi's functionality in the
course of analyzing an example system, and then describe details of the
algorithm's design and implementation.
  PyPhi can be installed with Python's package manager via the command 'pip
install pyphi' on Linux and macOS systems equipped with Python 3.4 or higher.
PyPhi is open-source and licensed under the GPLv3; the source code is hosted on
GitHub at https://github.com/wmayner/pyphi . Comprehensive and
continually-updated documentation is available at https://pyphi.readthedocs.io/
. The pyphi-users mailing list can be joined at
https://groups.google.com/forum/#!forum/pyphi-users . A web-based graphical
interface to the software is available at
http://integratedinformationtheory.org/calculate.html .



In the research of the impact of gestures using by a lecturer, one
challenging task is to infer the attention of a group of audiences. Two
important measurements that can help infer the level of attention are eye
movement data and Electroencephalography (EEG) data. Under the fundamental
assumption that a group of people would look at the same place if they all pay
attention at the same time, we apply a method, "Time Warp Edit Distance", to
calculate the similarity of their eye movement trajectories. Moreover, we also
cluster eye movement pattern of audiences based on these pair-wised similarity
metrics. Besides, since we don't have a direct metric for the "attention"
ground truth, a visual assessment would be beneficial to evaluate the
gesture-attention relationship. Thus we also implement a visualization tool.



While end-to-end neural conversation models have led to promising advances in
reducing hand-crafted features and errors induced by the traditional complex
system architecture, they typically require an enormous amount of data due to
the lack of modularity. Previous studies adopted a hybrid approach with
knowledge-based components either to abstract out domain-specific information
or to augment data to cover more diverse patterns. On the contrary, we propose
to directly address the problem using recent developments in the space of
continual learning for neural models. Specifically, we adopt a
domain-independent neural conversational model and introduce a novel neural
continual learning algorithm that allows a conversational agent to accumulate
skills across different tasks in a data-efficient way. To the best of our
knowledge, this is the first work that applies continual learning to
conversation systems. We verified the efficacy of our method through a
conversational skill transfer from either synthetic dialogs or human-human
dialogs to human-computer conversations in a customer support domain.



Under covariate shift, training (source) data and testing (target) data
differ in input space distribution, but share the same conditional label
distribution. This poses a challenging machine learning task. Robust Bias-Aware
(RBA) prediction provides the conditional label distribution that is robust to
the worstcase logarithmic loss for the target distribution while matching
feature expectation constraints from the source distribution. However,
employing RBA with insufficient feature constraints may result in high
certainty predictions for much of the source data, while leaving too much
uncertainty for target data predictions. To overcome this issue, we extend the
representer theorem to the RBA setting, enabling minimization of regularized
expected target risk by a reweighted kernel expectation under the source
distribution. By applying kernel methods, we establish consistency guarantees
and demonstrate better performance of the RBA classifier than competing methods
on synthetically biased UCI datasets as well as datasets that have natural
covariate shift.



Interior tomography for the region-of-interest (ROI) imaging has advantages
of using a small detector and reducing X-ray radiation dose. However, standard
analytic reconstruction suffers from severe cupping artifacts due to existence
of null space in the truncated Radon transform. Existing penalized
reconstruction methods may address this problem but they require extensive
computations due to the iterative reconstruction. Inspired by the recent deep
learning approaches to low-dose and sparse view CT, here we propose a deep
learning architecture that removes null space signals from the FBP
reconstruction. Experimental results have shown that the proposed method
provides near-perfect reconstruction with about 7-10 dB improvement in PSNR
over existing methods in spite of significantly reduced run-time complexity.



Hierarchies are of fundamental interest in both stochastic optimal control
and biological control due to their facilitation of a range of desirable
computational traits in a control algorithm and the possibility that they may
form a core principle of sensorimotor and cognitive control systems. However, a
theoretically justified construction of state-space hierarchies over all
spatial resolutions and their evolution through a policy inference process
remains elusive. Here, a formalism for deriving such normative representations
of discrete Markov decision processes is introduced in the context of graphs.
The resulting hierarchies correspond to a hierarchical policy inference
algorithm approximating a discrete gradient flow between state-space trajectory
densities generated by the prior and optimal policies.



To ensure stability of learning, state-of-the-art generalized policy
iteration algorithms augment the policy improvement step with a trust region
constraint bounding the information loss. The size of the trust region is
commonly determined by the Kullback-Leibler (KL) divergence, which not only
captures the notion of distance well but also yields closed-form solutions. In
this paper, we consider a more general class of f-divergences and derive the
corresponding policy update rules. The generic solution is expressed through
the derivative of the convex conjugate function to f and includes the KL
solution as a special case. Within the class of f-divergences, we further focus
on a one-parameter family of {\alpha}-divergences to study effects of the
choice of divergence on policy improvement. Previously known as well as new
policy updates emerge for different values of {\alpha}. We show that every type
of policy update comes with a compatible policy evaluation resulting from the
chosen f-divergence. Interestingly, the mean-squared Bellman error minimization
is closely related to policy evaluation with the Pearson $\chi^2$-divergence
penalty, while the KL divergence results in the soft-max policy update and a
log-sum-exp critic. We carry out asymptotic analysis of the solutions for
different values of {\alpha} and demonstrate the effects of using different
divergence functions on a multi-armed bandit problem and on common standard
reinforcement learning problems.



Learning probability distributions on the weights of neural networks (NNs)
has recently proven beneficial in many applications. Bayesian methods, such as
Stein variational gradient descent (SVGD), offer an elegant framework to reason
about NN model uncertainty. However, by assuming independent Gaussian priors
for the individual NN weights (as often applied), SVGD does not impose prior
knowledge that there is often structural information (dependence) among
weights. We propose efficient posterior learning of structural weight
uncertainty, within an SVGD framework, by employing matrix variate Gaussian
priors on NN parameters. We further investigate the learned structural
uncertainty in sequential decision-making problems, including contextual
bandits and reinforcement learning. Experiments on several synthetic and real
datasets indicate the superiority of our model, compared with state-of-the-art
methods.



Players in the online ad ecosystem are struggling to acquire the user data
required for precise targeting. Audience look-alike modeling has the potential
to alleviate this issue, but models' performance strongly depends on quantity
and quality of available data. In order to maximize the predictive performance
of our look-alike modeling algorithms, we propose two novel hybrid filtering
techniques that utilize the recent neural probabilistic language model
algorithm doc2vec. We apply these methods to data from a large mobile ad
exchange and additional app metadata acquired from the Apple App store and
Google Play store. First, we model mobile app users through their app usage
histories and app descriptions (user2vec). Second, we introduce context
awareness to that model by incorporating additional user and app-related
metadata in model training (context2vec). Our findings are threefold: (1) the
quality of recommendations provided by user2vec is notably higher than current
state-of-the-art techniques. (2) User representations generated through hybrid
filtering using doc2vec prove to be highly valuable features in supervised
machine learning models for look-alike modeling. This represents the first
application of hybrid filtering user models using neural probabilistic language
models, specifically doc2vec, in look-alike modeling. (3) Incorporating context
metadata in the doc2vec model training process to introduce context awareness
has positive effects on performance and is superior to directly including the
data as features in the downstream supervised models.



Text representation using neural word embeddings has proven efficacy in many
NLP applications. Recently, a lot of research interest goes beyond word
embeddings by adapting the traditional word embedding models to learn vectors
of multiword expressions (concepts/entities). However, current methods are
limited to textual knowledge bases only (e.g., Wikipedia). In this paper, we
propose a novel approach for learning concept vectors from two large scale
knowledge bases (Wikipedia, and Probase). We adapt the skip-gram model to
seamlessly learn from the knowledge in Wikipedia text and Probase concept
graph. We evaluate our concept embedding models intrinsically on two tasks: 1)
analogical reasoning where we achieve a state-of-the-art performance of 91% on
semantic analogies, 2) concept categorization where we achieve a
state-of-the-art performance on two benchmark datasets achieving categorization
accuracy of 100% on one and 98% on the other. Additionally, we present a case
study to extrinsically evaluate our model on unsupervised argument type
identification for neural semantic parsing. We demonstrate the competitive
accuracy of our unsupervised method and its ability to better generalize to out
of vocabulary entity mentions compared to the tedious and error prone methods
which depend on gazetteers and regular expressions.



Restricted Boltzmann machines (RBMs) and their extensions, called
'deep-belief networks', are powerful neural networks that have found
applications in the fields of machine learning and big data. The standard way
to training these models resorts to an iterative unsupervised procedure based
on Gibbs sampling, called 'contrastive divergence' (CD), and additional
supervised tuning via back-propagation. However, this procedure has been shown
not to follow any gradient and can lead to suboptimal solutions. In this paper,
we show an efficient alternative to CD by means of simulations of digital
memcomputing machines (DMMs). We test our approach on pattern recognition using
a modified version of the MNIST data set. DMMs sample effectively the vast
phase space given by the model distribution of the RBM, and provide a very good
approximation close to the optimum. This efficient search significantly reduces
the number of pretraining iterations necessary to achieve a given level of
accuracy, as well as a total performance gain over CD. In fact, the
acceleration of pretraining achieved by simulating DMMs is comparable to, in
number of iterations, the recently reported hardware application of the quantum
annealing method on the same network and data set. Notably, however, DMMs
perform far better than the reported quantum annealing results in terms of
quality of the training. We also compare our method to advances in supervised
training, like batch-normalization and rectifiers, that work to reduce the
advantage of pretraining. We find that the memcomputing method still maintains
a quality advantage ($>1\%$ in accuracy, and a $20\%$ reduction in error rate)
over these approaches. Furthermore, our method is agnostic about the
connectivity of the network. Therefore, it can be extended to train full
Boltzmann machines, and even deep networks at once.



In neuroscience, all kinds of computation models were designed to answer the
open question of how sensory stimuli are encoded by neurons and conversely, how
sensory stimuli can be decoded from neuronal activities. Especially, functional
Magnetic Resonance Imaging (fMRI) studies have made many great achievements
with the rapid development of the deep network computation. However, comparing
with the goal of decoding orientation, position and object category from
activities in visual cortex, accurate reconstruction of image stimuli from
human fMRI is a still challenging work. In this paper, the capsule network
(CapsNet) architecture based visual reconstruction (CNAVR) method is developed
to reconstruct image stimuli. The capsule means containing a group of neurons
to perform the better organization of feature structure and representation,
inspired by the structure of cortical mini column including several hundred
neurons in primates. The high-level capsule features in the CapsNet includes
diverse features of image stimuli such as semantic class, orientation, location
and so on. We used these features to bridge between human fMRI and image
stimuli. We firstly employed the CapsNet to train the nonlinear mapping from
image stimuli to high-level capsule features, and from high-level capsule
features to image stimuli again in an end-to-end manner. After estimating the
serviceability of each voxel by encoding performance to accomplish the
selecting of voxels, we secondly trained the nonlinear mapping from
dimension-decreasing fMRI data to high-level capsule features. Finally, we can
predict the high-level capsule features with fMRI data, and reconstruct image
stimuli with the CapsNet. We evaluated the proposed CNAVR method on the dataset
of handwritten digital images, and exceeded about 10% than the accuracy of all
existing state-of-the-art methods on the structural similarity index (SSIM).



Although deep learning has historical roots going back decades, neither the
term "deep learning" nor the approach was popular just over five years ago,
when the field was reignited by papers such as Krizhevsky, Sutskever and
Hinton's now classic (2012) deep network model of Imagenet. What has the field
discovered in the five subsequent years? Against a background of considerable
progress in areas such as speech recognition, image recognition, and game
playing, and considerable enthusiasm in the popular press, I present ten
concerns for deep learning, and suggest that deep learning must be supplemented
by other techniques if we are to reach artificial general intelligence.



We present a system for identifying conceptual shifts between visual
categories, which will form the basis for a co-creative drawing system to help
users draw more creative sketches. The system recognizes human sketches and
matches them to structurally similar sketches from categories to which they do
not belong. This would allow a co-creative drawing system to produce an
ambiguous sketch that blends features from both categories.



What is "intelligent" information retrieval? Essentially this is asking what
is intelligence, in this article I will attempt to show some of the aspects of
human intelligence, as related to information retrieval. I will do this by the
device of a semi-imaginary Oracle. Every Observatory has an oracle, someone who
is a distinguished scientist, has great administrative responsibilities, acts
as mentor to a number of less senior people, and as trusted advisor to even the
most accomplished scientists, and knows essentially everyone in the field. In
an appendix I will present a brief summary of the Statistical Factor Space
method for text indexing and retrieval, and indicate how it will be used in the
Astrophysics Data System Abstract Service. 2018 Keywords: Personal Digital
Assistant; Supervised Topic Models



We establish fun parallels between coin-weighing puzzles and
knights-and-knaves puzzles.



For homeland and transportation security applications, 2D X-ray explosive
detection system (EDS) have been widely used, but they have limitations in
recognizing 3D shape of the hidden objects. Among various types of 3D computed
tomography (CT) systems to address this issue, this paper is interested in a
stationary CT using fixed X-ray sources and detectors. However, due to the
limited number of projection views, analytic reconstruction algorithms produce
severe streaking artifacts. Inspired by recent success of deep learning
approach for sparse view CT reconstruction, here we propose a novel image and
sinogram domain deep learning architecture for 3D reconstruction from very
sparse view measurement. The algorithm has been tested with the real data from
a prototype 9-view dual energy stationary CT EDS carry-on baggage scanner
developed by GEMSS Medical Systems, Korea, which confirms the superior
reconstruction performance over the existing approaches.



Model-free deep reinforcement learning (RL) algorithms have been demonstrated
on a range of challenging decision making and control tasks. However, these
methods typically suffer from two major challenges: very high sample complexity
and brittle convergence properties, which necessitate meticulous hyperparameter
tuning. Both of these challenges severely limit the applicability of such
methods to complex, real-world domains. In this paper, we propose soft
actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum
entropy reinforcement learning framework. In this framework, the actor aims to
maximize expected reward while also maximizing entropy - that is, succeed at
the task while acting as randomly as possible. Prior deep RL methods based on
this framework have been formulated as Q-learning methods. By combining
off-policy updates with a stable stochastic actor-critic formulation, our
method achieves state-of-the-art performance on a range of continuous control
benchmark tasks, outperforming prior on-policy and off-policy methods.
Furthermore, we demonstrate that, in contrast to other off-policy algorithms,
our approach is very stable, achieving very similar performance across
different random seeds.



Catastrophic forgetting occurs when a neural network loses the information
learned with the first task, after training on a second task. This problem
remains a hurdle for general artificial intelligence systems with sequential
learning capabilities. In this paper, we propose a task-based hard attention
mechanism that preserves previous tasks' information without substantially
affecting the current task's learning. An attention mask is learned
concurrently to every task through stochastic gradient descent, and previous
masks are exploited to constrain such learning. We show that the proposed
mechanism is effective for reducing catastrophic forgetting, cutting current
rates by 33 to 84%. We also show that it is robust to different hyperparameter
choices and that it offers a number of monitoring capabilities. The approach
features the possibility to control both the stability and compactness of the
learned knowledge, which we believe makes it also attractive for online
learning and network compression applications.



Deep learning has achieved impressive results on many problems. However, it
requires high degree of expertise or a lot of experience to tune well the
hyperparameters, and such manual tuning process is likely to be biased.
Moreover, it is not practical to try out as many different hyperparameter
configurations in deep learning as in other machine learning scenarios, because
evaluating each single hyperparameter configuration in deep learning would mean
training a deep neural network, which usually takes quite long time. Hyperband
algorithm achieves state-of-the-art performance on various hyperparameter
optimization problems in the field of deep learning. However, Hyperband
algorithm does not utilize history information of previous explored
hyperparameter configurations, thus the solution found is suboptimal. We
propose to combine Hyperband algorithm with Bayesian optimization (which does
not ignore history when sampling next trial configuration). Experimental
results show that our combination approach is superior to other hyperparameter
optimization approaches including Hyperband algorithm.



The automatic detection of software vulnerabilities is an important research
problem. However, existing solutions to this problem rely on human experts to
define features and often miss many vulnerabilities (i.e., incurring high false
negative rate). In this paper, we initiate the study of using deep
learning-based vulnerability detection to relieve human experts from the
tedious and subjective task of manually defining features. Since deep learning
is motivated to deal with problems that are very different from the problem of
vulnerability detection, we need some guiding principles for applying deep
learning to vulnerability detection. In particular, we need to find
representations of software programs that are suitable for deep learning. For
this purpose, we propose using code gadgets to represent programs and then
transform them into vectors, where a code gadget is a number of (not
necessarily consecutive) lines of code that are semantically related to each
other. This leads to the design and implementation of a deep learning-based
vulnerability detection system, called Vulnerability Deep Pecker
(VulDeePecker). In order to evaluate VulDeePecker, we present the first
vulnerability dataset for deep learning approaches. Experimental results show
that VulDeePecker can achieve much fewer false negatives (with reasonable false
positives) than other approaches. We further apply VulDeePecker to 3 software
products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which
are not reported in the National Vulnerability Database but were "silently"
patched by the vendors when releasing later versions of these products; in
contrast, these vulnerabilities are almost entirely missed by the other
vulnerability detection systems we experimented with.



We construct targeted audio adversarial examples on automatic speech
recognition. Given any audio waveform, we can produce another that is over
99.9% similar, but transcribes as any phrase we choose (at a rate of up to 50
characters per second). We apply our iterative optimization-based attack to
Mozilla's implementation DeepSpeech end-to-end, and show it has a 100% success
rate. The feasibility of this attack introduce a new domain to study
adversarial examples.



Inductive inference is the process of extracting general rules from specific
observations. This problem also arises in the analysis of biological networks,
such as genetic regulatory networks, where the interactions are complex and the
observations are incomplete. A typical task in these problems is to extract
general interaction rules as combinations of Boolean covariates, that explain a
measured response variable. The inductive inference process can be considered
as an incompletely specified Boolean function synthesis problem. This
incompleteness of the problem will also generate spurious inferences, which are
a serious threat to valid inductive inference rules. Using random Boolean data
as a null model, here we attempt to measure the competition between valid and
spurious inductive inference rules from a given data set. We formulate two
greedy search algorithms, which synthesize a given Boolean response variable in
a sparse disjunct normal form, and respectively a sparse generalized algebraic
normal form of the variables from the observation data, and we evaluate
numerically their performance.



This paper is to explore the possibility to use alternative data and
artificial intelligence techniques to trade stocks. The efficacy of the daily
Twitter sentiment on predicting the stock return is examined using machine
learning methods. Reinforcement learning(Q-learning) is applied to generate the
optimal trading policy based on the sentiment signal. The predicting power of
the sentiment signal is more significant if the stock price is driven by the
expectation of the company growth and when the company has a major event that
draws the public attention. The optimal trading strategy based on reinforcement
learning outperforms the trading strategy based on the machine learning
prediction.



Recurrent Neural Networks (RNNs) with sophisticated units that implement a
gating mechanism have emerged as powerful technique for modeling sequential
signals such as speech or electroencephalography (EEG). The latter is the focus
on this paper. A significant big data resource, known as the TUH EEG Corpus
(TUEEG), has recently become available for EEG research, creating a unique
opportunity to evaluate these recurrent units on the task of seizure detection.
In this study, we compare two types of recurrent units: long short-term memory
units (LSTM) and gated recurrent units (GRU). These are evaluated using a state
of the art hybrid architecture that integrates Convolutional Neural Networks
(CNNs) with RNNs. We also investigate a variety of initialization methods and
show that initialization is crucial since poorly initialized networks cannot be
trained. Furthermore, we explore regularization of these convolutional gated
recurrent networks to address the problem of overfitting. Our experiments
revealed that convolutional LSTM networks can achieve significantly better
performance than convolutional GRU networks. The convolutional LSTM
architecture with proper initialization and regularization delivers 30%
sensitivity at 6 false alarms per 24 hours.



Crowd-powered conversational assistants have been shown to be more robust
than automated systems, but do so at the cost of higher response latency and
monetary costs. A promising direction is to combine the two approaches for high
quality, low latency, and low cost solutions. In this paper, we introduce
Evorus, a crowd-powered conversational assistant built to automate itself over
time by (i) allowing new chatbots to be easily integrated to automate more
scenarios, (ii) reusing prior crowd answers, and (iii) learning to
automatically approve response candidates. Our 5-month-long deployment with 80
participants and 281 conversations shows that Evorus can automate itself
without compromising conversation quality. Crowd-AI architectures have long
been proposed as a way to reduce cost and latency for crowd-powered systems;
Evorus demonstrates how automation can be introduced successfully in a deployed
system. Its architecture allows future researchers to make further innovation
on the underlying automated components in the context of a deployed open domain
dialog system.



We present a micro-traffic simulation (named "DeepTraffic") where the
perception, control, and planning systems for one of the cars are all handled
by a single neural network as part of a model-free, off-policy reinforcement
learning process. The primary goal of DeepTraffic is to make the hands-on study
of deep reinforcement learning accessible to thousands of students, educators,
and researchers in order to inspire and fuel the exploration and evaluation of
DQN variants and hyperparameter configurations through large-scale, open
competition. This paper investigates the crowd-sourced hyperparameter tuning of
the policy network that resulted from the first iteration of the DeepTraffic
competition where thousands of participants actively searched through the
hyperparameter space with the objective of their neural network submission to
make it onto the top-10 leaderboard.



The amount of publicly available biomedical literature has been growing
rapidly in recent years, yet question answering systems still struggle to
exploit the full potential of this source of data. In a preliminary processing
step, many question answering systems rely on retrieval models for identifying
relevant documents and passages. This paper proposes a weighted cosine distance
retrieval scheme based on neural network word embeddings. Our experiments are
based on publicly available data and tasks from the BioASQ biomedical question
answering challenge and demonstrate significant performance gains over a wide
range of state-of-the-art models.



In this paper we present a new form of access to knowledge through what we
call "hypermediator websites". These hypermediator sites are intermediate
between information devices that just scan the book culture and a "real"
hypertext writing format.



In biostatistics, propensity score is a common approach to analyze the
imbalance of covariate and process confounding covariates to eliminate
differences between groups. While there are an abundant amount of methods to
compute propensity score, a common issue of them is the corrupted labels in the
dataset. For example, the data collected from the patients could contain
samples that are treated mistakenly, and the computing methods could
incorporate them as a misleading information. In this paper, we propose a
Machine Learning-based method to handle the problem. Specifically, we utilize
the fact that the majority of sample should be labeled with the correct
instance and design an approach to first cluster the data with spectral
clustering and then sample a new dataset with a distribution processed from the
clustering results. The propensity score is computed by Xgboost, and a
mathematical justification of our method is provided in this paper. The
experimental results illustrate that xgboost propensity scores computing with
the data processed by our method could outperform the same method with original
data, and the advantages of our method increases as we add some artificial
corruptions to the dataset. Meanwhile, the implementation of xgboost to compute
propensity score for multiple treatments is also a pioneering work in the area.



The rise of deep learning in recent years has brought with it increasingly
clever optimization methods to deal with complex, non-linear loss functions.
These methods are often designed with convex optimization in mind, but have
been shown to work well in practice even for the highly non-convex optimization
associated with neural networks. However, one significant drawback of these
methods when they are applied to deep learning is that the magnitude of the
update step is sometimes disproportionate to the magnitude of the weights (much
smaller or larger), leading to training instabilities such as vanishing and
exploding gradients. An idea to combat this issue is gradient descent with
proportional updates. Gradient descent with proportional updates was introduced
in 2017. It was independently developed by You et al (Layer-wise Adaptive Rate
Scaling (LARS) algorithm) and by Abu-El-Haija (PercentDelta algorithm). The
basic idea of both of these algorithms is to make each step of the gradient
descent proportional to the current weight norm and independent of the gradient
magnitude. It is common in the context of new optimization methods to prove
convergence or derive regret bounds under the assumption of Lipschitz
continuity and convexity. However, even though LARS and PercentDelta were shown
to work well in practice, there is no theoretical analysis of the convergence
properties of these algorithms. Thus it is not clear if the idea of gradient
descent with proportional updates is used in the optimal way, or if it could be
improved by using a different norm or specific learning rate schedule, for
example. Moreover, it is not clear if these algorithms can be extended to other
problems, besides neural networks. We attempt to answer these questions by
establishing the theoretical analysis of gradient descent with proportional
updates, and verifying this analysis with empirical examples.



In this paper, we present Paranom, a parallel anomaly dataset generator. We
discuss its design and provide brief experimental results demonstrating its
usefulness in improving the classification correctness of LSTM-AD, a
state-of-the-art anomaly detection model.



In an effort to understand the meaning of the intermediate representations
captured by deep networks, recent papers have tried to associate specific
semantic concepts to individual neural network filter responses, where
interesting correlations are often found, largely by focusing on extremal
filter responses. In this paper, we show that this approach can favor
easy-to-interpret cases that are not necessarily representative of the average
behavior of a representation.
  A more realistic but harder-to-study hypothesis is that semantic
representations are distributed, and thus filters must be studied in
conjunction. In order to investigate this idea while enabling systematic
visualization and quantification of multiple filter responses, we introduce the
Net2Vec framework, in which semantic concepts are mapped to vectorial
embeddings based on corresponding filter responses. By studying such
embeddings, we are able to show that 1., in most cases, multiple filters are
required to code for a concept, that 2., often filters are not concept specific
and help encode multiple concepts, and that 3., compared to single filter
activations, filter embeddings are able to better characterize the meaning of a
representation and its relationship to other concepts.



Rapid popularity of Internet of Things (IoT) and cloud computing permits
neuroscientists to collect multilevel and multichannel brain data to better
understand brain functions, diagnose diseases, and devise treatments. To ensure
secure and reliable data communication between end-to-end (E2E) devices
supported by current IoT and cloud infrastructure, trust management is needed
at the IoT and user ends. This paper introduces a Neuro-Fuzzy based
Brain-inspired trust management model (TMM) to secure IoT devices and relay
nodes, and to ensure data reliability. The proposed TMM utilizes node
behavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference
System and weighted-additive methods respectively to assess the nodes
trustworthiness. In contrast to the existing fuzzy based TMMs, the NS2
simulation results confirm the robustness and accuracy of the proposed TMM in
identifying malicious nodes in the communication network. With the growing
usage of cloud based IoT frameworks in Neuroscience research, integrating the
proposed TMM into the existing infrastructure will assure secure and reliable
data communication among the E2E devices.



Current machine learning systems operate, almost exclusively, in a
statistical, or model-free mode, which entails severe theoretical limits on
their power and performance. Such systems cannot reason about interventions and
retrospection and, therefore, cannot serve as the basis for strong AI. To
achieve human level intelligence, learning machines need the guidance of a
model of reality, similar to the ones used in causal inference tasks. To
demonstrate the essential role of such models, I will present a summary of
seven tasks which are beyond reach of current machine learning systems and
which have been accomplished using the tools of causal modeling.



We present a novel deep neural network architecture for representing robot
experiences in an episodic-like memory which facilitates encoding, recalling,
and predicting action experiences. Our proposed unsupervised deep episodic
memory model 1) encodes observed actions in a latent vector space and, based on
this latent encoding, 2) infers action categories, 3) reconstructs original
frames, and 4) predicts future frames. We evaluate the proposed model on two
different large-scale action datasets. Results show that conceptually similar
actions are mapped into the same region of the latent vector space. Results
show that conceptual similarity of videos is reflected by the proximity of
their vector representations in the latent space.Based on this contribution, we
introduce an action matching and retrieval mechanism and evaluate its
performance and generalization capability on a real humanoid robot in an action
execution scenario.



Neural programming involves training neural networks to learn programs from
data. Previous works have failed to achieve good generalization performance,
especially on programs with high complexity or on large domains. This is
because they mostly rely either on black-box function evaluations that do not
capture the structure of the program, or on detailed execution traces that are
expensive to obtain, and hence the training data has poor coverage of the
domain under consideration. We present a novel framework that utilizes
black-box function evaluations, in conjunction with symbolic expressions that
integrate relationships between the given functions. We employ tree LSTMs to
incorporate the structure of the symbolic expression trees. We use tree
encoding for numbers present in function evaluation data, based on their
decimal representation. We present an evaluation benchmark for this task to
demonstrate our proposed model combines symbolic reasoning and function
evaluation in a fruitful manner, obtaining high accuracies in our experiments.
Our framework generalizes significantly better to expressions of higher depth
and is able to fill partial equations with valid completions.



Recent work has shown local convergence of GAN training for absolutely
continuous data and generator distributions. In this note we show that the
requirement of absolute continuity is necessary: we describe a simple yet
prototypical counterexample showing that in the more realistic case of
distributions that are not absolutely continuous, unregularized GAN training is
generally not convergent. Furthermore, we discuss recent regularization
strategies that were proposed to stabilize GAN training. Our analysis shows
that while GAN training with instance noise or gradient penalties converges,
Wasserstein-GANs and Wasserstein-GANs-GP with a finite number of discriminator
updates per generator update do in general not converge to the equilibrium
point. We explain these results and show that both instance noise and gradient
penalties constitute solutions to the problem of purely imaginary eigenvalues
of the Jacobian of the gradient vector field. Based on our analysis, we also
propose a simplified gradient penalty with the same effects on local
convergence as more complicated penalties.



This paper discusses whether computers, using Artifical Intelligence (AI),
could create art. The first part concerns AI-based tools for assisting with art
making. The history of technologies that automated aspects of art is covered,
including photography and animation. In each case, we see initial fears and
denial of the technology, followed by a blossoming of new creative and
professional opportunities for artists. The hype and reality of Artificial
Intelligence (AI) tools for art making is discussed, together with predictions
about how AI tools will be used. The second part speculates about whether it
could ever happen that AI systems could conceive of artwork, and be credited
with authorship of an artwork.



ConvNets have been very effective in many applications where it is required
to learn invariances to within-class nuisance transformations. However, through
their architecture, ConvNets only enforce invariance to translation. In this
paper, we introduce a new class of convolutional architectures called
Non-Parametric Transformation Networks (NPTNs) which can learn general
invariances and symmetries directly from data. NPTNs are a direct and natural
generalization of ConvNets and can be optimized directly using gradient
descent. They make no assumption regarding structure of the invariances present
in the data and in that aspect are very flexible and powerful. We also model
ConvNets and NPTNs under a unified framework called Transformation Networks
which establishes the natural connection between the two. We demonstrate the
efficacy of NPTNs on natural data such as MNIST and CIFAR 10 where it
outperforms ConvNet baselines with the same number of parameters. We show it is
effective in learning invariances unknown apriori directly from data from
scratch. Finally, we apply NPTNs to Capsule Networks and show that they enable
them to perform even better.



Recent advances in video super-resolution have shown that convolutional
neural networks combined with motion compensation are able to merge information
from multiple low-resolution (LR) frames to generate high-quality images.
Current state-of-the-art methods process a batch of LR frames to generate a
single high-resolution (HR) frame and run this scheme in a sliding window
fashion over the entire video, effectively treating the problem as a large
number of separate multi-frame super-resolution tasks. This approach has two
main weaknesses: 1) Each input frame is processed and warped multiple times,
increasing the computational cost, and 2) each output frame is estimated
independently conditioned on the input frames, limiting the system's ability to
produce temporally consistent results.
  In this work, we propose an end-to-end trainable frame-recurrent video
super-resolution framework that uses the previously inferred HR estimate to
super-resolve the subsequent frame. This naturally encourages temporally
consistent results and reduces the computational cost by warping only one image
in each step. Furthermore, due to its recurrent nature, the proposed method has
the ability to assimilate a large number of previous frames without increased
computational demands. Extensive evaluations and comparisons with previous
methods validate the strengths of our approach and demonstrate that the
proposed framework is able to significantly outperform the current state of the
art.



Learning a classifier with control on the false-positive rate plays a
critical role in many machine learning applications. Existing approaches either
introduce prior knowledge dependent label cost or tune parameters based on
traditional classifiers, which lack consistency in methodology because they do
not strictly adhere to the false-positive rate constraint. In this paper, we
propose a novel scoring-thresholding approach, tau-False Positive Learning
(tau-FPL) to address this problem. We show the scoring problem which takes the
false-positive rate tolerance into accounts can be efficiently solved in linear
time, also an out-of-bootstrap thresholding method can transform the learned
ranking function into a low false-positive classifier. Both theoretical
analysis and experimental results show superior performance of the proposed
tau-FPL over existing approaches.



A large body of compelling evidence has been accumulated demonstrating that
embodiment - the agent's physical setup, including its shape, materials,
sensors and actuators - is constitutive for any form of cognition and as a
consequence, models of cognition need to be embodied. In contrast to methods
from empirical sciences to study cognition, robots can be freely manipulated
and virtually all key variables of their embodiment and control programs can be
systematically varied. As such, they provide an extremely powerful tool of
investigation. We present a robotic bottom-up or developmental approach,
focusing on three stages: (a) low-level behaviors like walking and reflexes,
(b) learning regularities in sensorimotor spaces, and (c) human-like cognition.
We also show that robotic based research is not only a productive path to
deepening our understanding of cognition, but that robots can strongly benefit
from human-like cognition in order to become more autonomous, robust,
resilient, and safe.



In recent years, research on decoding brain activity based on functional
magnetic resonance imaging (fMRI) has made remarkable achievements. However,
constraint-free natural image reconstruction from brain activity is still a
challenge. The existing methods simplified the problem by using semantic prior
information or just reconstructing simple images such as letters and digitals.
Without semantic prior information, we present a novel method to reconstruct
nature images from fMRI signals of human visual cortex based on the computation
model of convolutional neural network (CNN). Firstly, we extracted the units
output of viewed natural images in each layer of a pre-trained CNN as CNN
features. Secondly, we transformed image reconstruction from fMRI signals into
the problem of CNN feature visualizations by training a sparse linear
regression to map from the fMRI patterns to CNN features. By iteratively
optimization to find the matched image, whose CNN unit features become most
similar to those predicted from the brain activity, we finally achieved the
promising results for the challenging constraint-free natural image
reconstruction. As there was no use of semantic prior information of the
stimuli when training decoding model, any category of images (not constraint by
the training set) could be reconstructed theoretically. We found that the
reconstructed images resembled the natural stimuli, especially in position and
shape. The experimental results suggest that hierarchical visual features can
effectively express the visual perception process of human brain.



We present extensive experiments training and testing hidden units in deep
networks that emit only a predefined, static, number of discretized values.
These units provide benefits in real-world deployment in systems in which
memory and/or computation may be limited. Additionally, they are particularly
well suited for use in large recurrent network models that require the
maintenance of large amounts of internal state in memory. Surprisingly, we find
that despite reducing the number of values that can be represented in the
output activations from $2^{32}-2^{64}$ to between 64 and 256, there is little
to no degradation in network performance across a variety of different
settings. We investigate simple classification and regression tasks, as well as
memorization and compression problems. We compare the results with more
standard activations, such as tanh and relu. Unlike previous discretization
studies which often concentrate only on binary units, we examine the effects of
varying the number of allowed activation levels. Compared to existing
approaches for discretization, the approach presented here is both conceptually
and programatically simple, has no stochastic component, and allows the
training, testing, and usage phases to be treated in exactly the same manner.



This paper proposes a novel adaptive algorithm for the automated short-term
trading of financial instrument. The algorithm adopts a semantic sentiment
analysis technique to inspect the Twitter posts and to use them to predict the
behaviour of the stock market. Indeed, the algorithm is specifically developed
to take advantage of both the sentiment and the past values of a certain
financial instrument in order to choose the best investment decision. This
allows the algorithm to ensure the maximization of the obtainable profits by
trading on the stock market. We have conducted an investment simulation and
compared the performance of our proposed with a well-known benchmark (DJTATO
index) and the optimal results, in which an investor knows in advance the
future price of a product. The result shows that our approach outperforms the
benchmark and achieves the performance score close to the optimal result.



Internet of things (IoT) applications have become increasingly popular in
recent years, with applications ranging from building energy monitoring to
personal health tracking and activity recognition. In order to leverage these
data, automatic knowledge extraction - whereby we map from observations to
interpretable states and transitions - must be done at scale. As such, we have
seen many recent IoT data sets include annotations with a human expert
specifying states, recorded as a set of boundaries and associated labels in a
data sequence. These data can be used to build automatic labeling algorithms
that produce labels as an expert would. Here, we refer to human-specified
boundaries as breakpoints. Traditional changepoint detection methods only look
for statistically-detectable boundaries that are defined as abrupt variations
in the generative parameters of a data sequence. However, we observe that
breakpoints occur on more subtle boundaries that are non-trivial to detect with
these statistical methods. In this work, we propose a new unsupervised
approach, based on deep learning, that outperforms existing techniques and
learns the more subtle, breakpoint boundaries with a high accuracy. Through
extensive experiments on various real-world data sets - including
human-activity sensing data, speech signals, and electroencephalogram (EEG)
activity traces - we demonstrate the effectiveness of our algorithm for
practical applications. Furthermore, we show that our approach achieves
significantly better performance than previous methods.



In this paper, an interference-aware path planning scheme for a network of
cellular-connected unmanned aerial vehicles (UAVs) is proposed. In particular,
each UAV aims at achieving a tradeoff between maximizing energy efficiency and
minimizing both wireless latency and the interference level caused on the
ground network along its path. The problem is cast as a dynamic game among
UAVs. To solve this game, a deep reinforcement learning algorithm, based on
echo state network (ESN) cells, is proposed. The introduced deep ESN
architecture is trained to allow each UAV to map each observation of the
network state to an action, with the goal of minimizing a sequence of
time-dependent utility functions. Each UAV uses ESN to learn its optimal path,
transmission power level, and cell association vector at different locations
along its path. The proposed algorithm is shown to reach a subgame perfect Nash
equilibrium (SPNE) upon convergence. Moreover, an upper and lower bound for the
altitude of the UAVs is derived thus reducing the computational complexity of
the proposed algorithm. Simulation results show that the proposed scheme
achieves better wireless latency per UAV and rate per ground user (UE) while
requiring a number of steps that is comparable to a heuristic baseline that
considers moving via the shortest distance towards the corresponding
destinations. The results also show that the optimal altitude of the UAVs
varies based on the ground network density and the UE data rate requirements
and plays a vital role in minimizing the interference level on the ground UEs
as well as the wireless transmission delay of the UAV.



In this technical report, we consider an approach that combines the PPO
objective and K-FAC natural gradient optimization, for which we call PPOKFAC.
We perform a range of empirical analysis on various aspects of the algorithm,
such as sample complexity, training speed, and sensitivity to batch size and
training epochs. We observe that PPOKFAC is able to outperform PPO in terms of
sample complexity and speed in a range of MuJoCo environments, while being
scalable in terms of batch size. In spite of this, it seems that adding more
epochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to
be worse than its A2C counterpart, ACKTR.



Dempster-Shafer evidence theory has been widely used in various fields of
applications. Besides, it has been proven that the quantum theory has powerful
capabilities of solving the decision making problems. However, due to the
inconsistency of the expression, the classical Dempster-Shafer evidence theory
modelled by real numbers can not be integrated directly with the quantum theory
modelled by complex numbers. The main contribution in this study is that,
unlike the existing evidence theory, a mass function in the generalized
Dempster-Shafer evidence theory is modelled by a complex number, called as a
complex mass function. When the complex mass function is degenerated from
complex numbers to real numbers, the generalized Dempster's combination rule
degenerates to the classical evidence theory. This generalized Dempster-Shafer
evidence theory provides a promising way to model and handle more uncertain
information. Numerical examples are illustrated to show the efficiency of the
generalized Dempster-Shafer evidence theory.



Modern communication networks have become very complicated and highly
dynamic, which makes them hard to model, predict and control. In this paper, we
develop a novel experience-driven approach that can learn to well control a
communication network from its own experience rather than an accurate
mathematical model, just as a human learns a new skill (such as driving,
swimming, etc). Specifically, we, for the first time, propose to leverage
emerging Deep Reinforcement Learning (DRL) for enabling model-free control in
communication networks; and present a novel and highly effective DRL-based
control framework, DRL-TE, for a fundamental networking problem: Traffic
Engineering (TE). The proposed framework maximizes a widely-used utility
function by jointly learning network environment and its dynamics, and making
decisions under the guidance of powerful Deep Neural Networks (DNNs). We
propose two new techniques, TE-aware exploration and actor-critic-based
prioritized experience replay, to optimize the general DRL framework
particularly for TE. To validate and evaluate the proposed framework, we
implemented it in ns-3, and tested it comprehensively with both representative
and randomly generated network topologies. Extensive packet-level simulation
results show that 1) compared to several widely-used baseline methods, DRL-TE
significantly reduces end-to-end delay and consistently improves the network
utility, while offering better or comparable throughput; 2) DRL-TE is robust to
network changes; and 3) DRL-TE consistently outperforms a state-ofthe-art DRL
method (for continuous control), Deep Deterministic Policy Gradient (DDPG),
which, however, does not offer satisfying performance.



The evolution of social media popularity exhibits rich temporality, i.e.,
popularities change over time at various levels of temporal granularity. This
is influenced by temporal variations of public attentions or user activities.
For example, popularity patterns of street snap on Flickr are observed to
depict distinctive fashion styles at specific time scales, such as season-based
periodic fluctuations for Trench Coat or one-off peak in days for Evening
Dress. However, this fact is often overlooked by existing research of
popularity modeling. We present the first study to incorporate multiple
time-scale dynamics into predicting online popularity. We propose a novel
computational framework in the paper, named Multi-scale Temporalization, for
estimating popularity based on multi-scale decomposition and structural
reconstruction in a tensor space of user, post, and time by joint low-rank
constraints. By considering the noise caused by context inconsistency, we
design a data rearrangement step based on context aggregation as preprocessing
to enhance contextual relevance of neighboring data in the tensor space. As a
result, our approach can leverage multiple levels of temporal characteristics
and reduce the noise of data decomposition to improve modeling effectiveness.
We evaluate our approach on two large-scale Flickr image datasets with over 1.8
million photos in total, for the task of popularity prediction. The results
show that our approach significantly outperforms state-of-the-art popularity
prediction techniques, with a relative improvement of 10.9%-47.5% in terms of
prediction accuracy.



Now a days, the major challenge in machine learning is the `Big~Data'
challenge. The big data problems due to large number of data points or large
number of features in each data point, or both, the training of models have
become very slow. The training time has two major components: Time to access
the data and time to process (learn from) the data. In this paper, we have
proposed one possible solution to handle the big data problems in machine
learning. The idea is to reduce the training time through reducing data access
time by proposing systematic sampling and cyclic/sequential sampling to select
mini-batches from the dataset. To prove the effectiveness of proposed sampling
techniques, we have used Empirical Risk Minimization, which is commonly used
machine learning problem, for strongly convex and smooth case. The problem has
been solved using SAG, SAGA, SVRG, SAAG-II and MBSGD (Mini-batched SGD), each
using two step determination techniques, namely, constant step size and
backtracking line search method. Theoretical results prove the same convergence
for systematic sampling, cyclic sampling and the widely used random sampling
technique, in expectation. Experimental results with bench marked datasets
prove the efficacy of the proposed sampling techniques.



We train multi-task autoencoders on linguistic tasks and analyze the learned
hidden sentence representations. The representations change significantly when
translation and part-of-speech decoders are added. The more decoders a model
employs, the better it clusters sentences according to their syntactic
similarity, as the representation space becomes less entangled. We explore the
structure of the representation space by interpolating between sentences, which
yields interesting pseudo-English sentences, many of which have recognizable
syntactic structure. Lastly, we point out an interesting property of our
models: The difference-vector between two sentences can be added to change a
third sentence with similar features in a meaningful way.



Training a task-completion dialogue agent with real users via reinforcement
learning (RL) could be prohibitively expensive, because it requires many
interactions with users. One alternative is to resort to a user simulator,
while the discrepancy of between simulated and real users makes the learned
policy unreliable in practice. This paper addresses these challenges by
integrating planning into the dialogue policy learning based on Dyna-Q
framework, and provides a more sample-efficient approach to learn the dialogue
polices. The proposed agent consists of a planner trained on-line with limited
real user experience that can generate large amounts of simulated experience to
supplement with limited real user experience, and a policy model trained on
these hybrid experiences. The effectiveness of our approach is validated on a
movie-booking task in both a simulation setting and a human-in-the-loop
setting.



Social media has grown to be a crucial information source for
pharmacovigilance studies where an increasing number of people post adverse
reactions to medical drugs that are previously unreported. Aiming to
effectively monitor various aspects of Adverse Drug Reactions (ADRs) from
diversely expressed social medical posts, we propose a multi-task neural
network framework that learns several tasks associated with ADR monitoring with
different levels of supervisions collectively. Besides being able to correctly
classify ADR posts and accurately extract ADR mentions from online posts, the
proposed framework is also able to further understand reasons for which the
drug is being taken, known as 'indication', from the given social media post. A
coverage-based attention mechanism is adopted in our framework to help the
model properly identify 'phrasal' ADRs and Indications that are attentive to
multiple words in a post. Our framework is applicable in situations where
limited parallel data for different pharmacovigilance tasks are available.We
evaluate the proposed framework on real-world Twitter datasets, where the
proposed model outperforms the state-of-the-art alternatives of each individual
task consistently.



The 11th Summer Workshop on Multimodal Interfaces eNTERFACE 2015 was hosted
by the Numediart Institute of Creative Technologies of the University of Mons
from August 10th to September 2015. During the four weeks, students and
researchers from all over the world came together in the Numediart Institute of
the University of Mons to work on eight selected projects structured around
intelligent interfaces. Eight projects were selected and their reports are
shown here.



Strict partial order is a mathematical structure commonly seen in relational
data. One obstacle to extracting such type of relations at scale is the lack of
large-scale labels for building effective data-driven solutions. We develop an
active learning framework for mining such relations subject to a strict order.
Our approach incorporates relational reasoning not only in finding new
unlabeled pairs whose labels can be deduced from an existing label set, but
also in devising new query strategies that consider the relational structure of
labels. Our experiments on concept prerequisite relations show our proposed
framework can substantially improve the classification performance with the
same query budget compared to other baseline approaches.



Multi-view networks are ubiquitous in real-world applications. In order to
extract knowledge or business value, it is of interest to transform such
networks into representations that are easily machine-actionable. Meanwhile,
network embedding has emerged as an effective approach to generate distributed
network representations. Therefore, we are motivated to study the problem of
multi-view network embedding, with a focus on the characteristics that are
specific and important in embedding this type of networks. In our practice of
embedding real-world multi-view networks, we identify two such characteristics,
which we refer to as preservation and collaboration. We then explore the
feasibility of achieving better embedding quality by simultaneously modeling
preservation and collaboration, and propose the mvn2vec algorithms. With
experiments on a series of synthetic datasets, an internal Snapchat dataset,
and two public datasets, we further confirm the presence and importance of
preservation and collaboration. These experiments also demonstrate that better
embedding can be obtained by simultaneously modeling the two characteristics,
while not over-complicating the model or requiring additional supervision.



This paper introduces the factorial marked temporal point process model and
presents efficient learning methods. In conventional (multi-dimensional) marked
temporal point process models, event is often encoded by a single discrete
variable i.e. a marker. In this paper, we describe the factorial marked point
processes whereby time-stamped event is factored into multiple markers.
Accordingly the size of the infectivity matrix modeling the effect between
pairwise markers is in power order w.r.t. the number of the discrete marker
space. We propose a decoupled learning method with two learning procedures: i)
directly solving the model based on two techniques: Alternating Direction
Method of Multipliers and Fast Iterative Shrinkage-Thresholding Algorithm; ii)
involving a reformulation that transforms the original problem into a Logistic
Regression model for more efficient learning. Moreover, a sparse group
regularizer is added to identify the key profile features and event labels.
Empirical results on real world datasets demonstrate the efficiency of our
decoupled and reformulated method. The source code is available online.



Deep learning has recently seen rapid development and significant attention
due to its state-of-the-art performance on previously-thought hard problems.
However, because of the innate complexity and nonlinear structure of deep
neural networks, the underlying decision making processes for why these models
are achieving such high performance are challenging and sometimes mystifying to
interpret. As deep learning spreads across domains, it is of paramount
importance that we equip users of deep learning with tools for understanding
when a model works correctly, when it fails, and ultimately how to improve its
performance. Standardized toolkits for building neural networks have helped
democratize deep learning; visual analytics systems have now been developed to
support model explanation, interpretation, debugging, and improvement. We
present a survey of the role of visual analytics in deep learning research,
noting its short yet impactful history and summarize the state-of-the-art using
a human-centered interrogative framework, focusing on the Five W's and How
(Why, Who, What, How, When, and Where), to thoroughly summarize deep learning
visual analytics research. We conclude by highlighting research directions and
open research problems. This survey helps new researchers and practitioners in
both visual analytics and deep learning to quickly learn key aspects of this
young and rapidly growing body of research, whose impact spans a diverse range
of domains.



In this paper, we present a new approach to Transfer Learning (TL) in
Reinforcement Learning (RL) for cross-domain tasks. Many of the available
techniques approach the transfer architecture as a method of speeding up the
target task learning. We propose to adapt and reuse the mapped source task
optimal-policy directly in related domains. We show the optimal policy from a
related source task can be near optimal in target domain provided an adaptive
policy accounts for the model error between target and source. The main benefit
of this policy augmentation is generalizing policies across multiple related
domains without having to re-learn the new tasks. Our results show that this
architecture leads to better sample efficiency in the transfer, reducing sample
complexity of target task learning to target apprentice learning.



This paper is concerned with the sparsification of the input-hidden weights
of ELM (Extreme Learning Machine). For ordinary feedforward neural networks,
the sparsification is usually done by introducing certain regularization
technique into the learning process of the network. But this strategy can not
be applied for ELM, since the input-hidden weights of ELM are supposed to be
randomly chosen rather than to be learned. To this end, we propose a modified
ELM, called ELM-LC (ELM with local connections), which is designed for the
sparsification of the input-hidden weights as follows: The hidden nodes and the
input nodes are divided respectively into several corresponding groups, and an
input node group is fully connected with its corresponding hidden node group,
but is not connected with any other hidden node group. As in the usual ELM, the
hidden-input weights are randomly given, and the hidden-output weights are
obtained through a least square learning. In the numerical simulations on some
benchmark problems, the new ELM-CL behaves better than the traditional ELM.



Due to exponential growth of complex data, graph structure has become
increasingly important to model various entities and their interactions, with
many interesting applications including, bioinformatics, social network
analysis, etc. Depending on the complexity of the data, the underlying graph
model can be a simple directed/undirected and/or weighted/un-weighted graph to
a complex graph (aka multi-attributed graph) where vertices and edges are
labelled with multi-dimensional vectors. In this paper, we present a novel
weighted distance measure based on weighted Euclidean norm which is defined as
a function of both vertex and edge attributes, and it can be used for various
graph analysis tasks including classification and cluster analysis. The
proposed distance measure has flexibility to increase/decrease the weightage of
edge labels while calculating the distance between vertex-pairs. We have also
proposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV
files containing the list of vertex vectors and edge vectors, and calculates
the distance between each vertex-pair using the proposed weighted distance
measure. Finally, we have proposed a multi-attributed similarity graph
generation algorithm, MAGSim, which reads the output of MAGDist algorithm and
generates a similarity graph that can be analysed using classification and
clustering algorithms. The significance and accuracy of the proposed distance
measure and algorithms is evaluated on Iris and Twitter data sets, and it is
found that the similarity graph generated by our proposed method yields better
clustering results than the existing similarity graph generation methods.



We study generalization properties of distributed algorithms in the setting
of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We
first investigate distributed stochastic gradient methods (SGM), with
mini-batches and multi-passes over the data. We show that optimal
generalization error bounds can be retained for distributed SGM provided that
the partition level is not too large. We then extend our results to
spectral-regularization algorithms (SRA), including kernel ridge regression
(KRR), kernel principal component analysis, and gradient methods. Our results
are superior to the state-of-the-art theory. Particularly, our results show
that distributed SGM has a smaller theoretical computational complexity,
compared with distributed KRR and classic SGM. Moreover, even for
non-distributed SRA, they provide the first optimal, capacity-dependent
convergence rates, considering the case that the regression function may not be
in the RKHS.



Time series data constitutes a distinct and growing problem in machine
learning. As the corpus of time series data grows larger, deep models that
simultaneously learn features and classify with these features can be
intractable or suboptimal. In this paper, we present feature learning via long
short term memory (LSTM) networks and prediction via gradient boosting trees
(XGB). Focusing on the consequential setting of electronic health record data,
we predict the occurrence of hypoxemia five minutes into the future based on
past features. We make two observations: 1) long short term memory networks are
effective at capturing long term dependencies based on a single feature and 2)
gradient boosting trees are capable of tractably combining a large number of
features including static features like height and weight. With these
observations in mind, we generate features by performing "supervised"
representation learning with LSTM networks. Augmenting the original XGB model
with these features gives significantly better performance than either
individual method.



Machine learning is a tool for building models that accurately represent
input training data. When undesired biases concerning demographic groups are in
the training data, well-trained models will reflect those biases. We present a
framework for mitigating such biases by including a variable for the group of
interest and simultaneously learning a predictor and an adversary. The input to
the network X, here text or census data, produces a prediction Y, such as an
analogy completion or income bracket, while the adversary tries to model a
protected variable Z, here gender or zip code.
  The objective is to maximize the predictor's ability to predict Y while
minimizing the adversary's ability to predict Z. Applied to analogy completion,
this method results in accurate predictions that exhibit less evidence of
stereotyping Z. When applied to a classification task using the UCI Adult
(Census) Dataset, it results in a predictive model that does not lose much
accuracy while achieving very close to equality of odds (Hardt, et al., 2016).
The method is flexible and applicable to multiple definitions of fairness as
well as a wide range of gradient-based learning models, including both
regression and classification tasks.



Based on the observation that semantic segmentation errors are partially
predictable, we propose a compact formulation using confusion statistics of the
trained classifier to refine (re-estimate) the initial pixel label hypotheses.
The proposed strategy is contingent upon computing the classifier confusion
probabilities for a given dataset and estimating a relevant prior on the object
classes present in the image to be classified. We provide a procedure to
robustly estimate the confusion probabilities and explore multiple prior
definitions. Experiments are shown comparing performances on multiple
challenging datasets using different priors to improve a state-of-the-art
semantic segmentation classifier. This study demonstrates the potential to
significantly improve semantic labeling and motivates future work for reliable
label prior estimation from images.



This paper describes and discusses Bayesian Neural Network (BNN). The paper
showcases a few different applications of them for classification and
regression problems. BNNs are comprised of a Probabilistic Model and a Neural
Network. The intent of such a design is to combine the strengths of Neural
Networks and Stochastic modeling. Neural Networks exhibit continuous function
approximator capabilities. Stochastic models allow direct specification of a
model with known interaction between parameters to generate data. During the
prediction phase, stochastic models generate a complete posterior distribution
and produce probabilistic guarantees on the predictions. Thus BNNs are a unique
combination of neural network and stochastic models with the stochastic model
forming the core of this integration. BNNs can then produce probabilistic
guarantees on it's predictions and also generate the distribution of parameters
that it has learnt from the observations. That means, in the parameter space,
one can deduce the nature and shape of the neural network's learnt parameters.
These two characteristics makes them highly attractive to theoreticians as well
as practitioners. Recently there has been a lot of activity in this area, with
the advent of numerous probabilistic programming libraries such as: PyMC3,
Edward, Stan etc. Further this area is rapidly gaining ground as a standard
machine learning approach for numerous problems



Neural text generation models are often autoregressive language models or
seq2seq models. These models generate text by sampling words sequentially, with
each word conditioned on the previous word, and are state-of-the-art for
several machine translation and summarization benchmarks. These benchmarks are
often defined by validation perplexity even though this is not a direct measure
of the quality of the generated text. Additionally, these models are typically
trained via maxi- mum likelihood and teacher forcing. These methods are
well-suited to optimizing perplexity but can result in poor sample quality
since generating text requires conditioning on sequences of words that may have
never been observed at training time. We propose to improve sample quality
using Generative Adversarial Networks (GANs), which explicitly train the
generator to produce high quality samples and have shown a lot of success in
image generation. GANs were originally designed to output differentiable
values, so discrete language generation is challenging for them. We claim that
validation perplexity alone is not indicative of the quality of text generated
by a model. We introduce an actor-critic conditional GAN that fills in missing
text conditioned on the surrounding context. We show qualitatively and
quantitatively, evidence that this produces more realistic conditional and
unconditional text samples compared to a maximum likelihood trained model.



Inspired by the matching of supply to demand in logistical problems, the
optimal transportation (or Monge-Kantorovich) problem involves the matching of
probability distributions defined over a geometric domain such as a surface or
manifold. After discretization, optimal transportation becomes a large-scale
linear program, which typically is infeasible to solve efficiently on triangle
meshes, graphs, point clouds, and other domains encountered in graphics and
machine learning. Recent breakthroughs in numerical optimal transportation
enable scalability to orders-of-magnitude larger problems, solvable in a
fraction of a second. In these lecture notes, we discuss advances in numerical
optimal transport that leverage understanding of both discrete and smooth
aspects of the problem. State-of-the-art techniques in discrete optimal
transportation combine insight from partial differential equations (PDE) with
convex analysis to reformulate, discretize, and optimize transportation
problems. The end result is a set of theoretically-justified models suitable
for domains with thousands or millions of vertices. Since numerical optimal
transport is a relatively new discipline, special emphasis is placed on
identifying and explaining open problems in need of mathematical insight and
additional research.



We present a simple and general framework for feature learning from point
cloud. The key to the success of CNNs is the convolution operator that is
capable of leveraging spatially-local correlation in data represented densely
in grids (e.g. images). However, point cloud are irregular and unordered, thus
a direct convolving of kernels against the features associated with the points
will result in deserting the shape information while being variant to the
orders. To address these problems, we propose to learn a X-transformation from
the input points, and then use it to simultaneously weight the input features
associated with the points and permute them into latent potentially canonical
order, before the element-wise product and sum operations are applied. The
proposed method is a generalization of typical CNNs into learning features from
point cloud, thus we call it PointCNN. Experiments show that PointCNN achieves
on par or better performance than state-of-the-art methods on multiple
challenging benchmark datasets and tasks.



This paper analyzes consumer choices over lunchtime restaurants using data
from a sample of several thousand anonymous mobile phone users in the San
Francisco Bay Area. The data is used to identify users' approximate typical
morning location, as well as their choices of lunchtime restaurants. We build a
model where restaurants have latent characteristics (whose distribution may
depend on restaurant observables, such as star ratings, food category, and
price range), each user has preferences for these latent characteristics, and
these preferences are heterogeneous across users. Similarly, each item has
latent characteristics that describe users' willingness to travel to the
restaurant, and each user has individual-specific preferences for those latent
characteristics. Thus, both users' willingness to travel and their base utility
for each restaurant vary across user-restaurant pairs. We use a Bayesian
approach to estimation. To make the estimation computationally feasible, we
rely on variational inference to approximate the posterior distribution, as
well as stochastic gradient descent as a computational approach. Our model
performs better than more standard competing models such as multinomial logit
and nested logit models, in part due to the personalization of the estimates.
We analyze how consumers re-allocate their demand after a restaurant closes to
nearby restaurants versus more distant restaurants with similar
characteristics, and we compare our predictions to actual outcomes. Finally, we
show how the model can be used to analyze counterfactual questions such as what
type of restaurant would attract the most consumers in a given location.



A long line of work in social psychology has studied variations in people's
susceptibility to persuasion -- the extent to which they are willing to modify
their opinions on a topic. This body of literature suggests an interesting
perspective on theoretical models of opinion formation by interacting parties
in a network: in addition to considering interventions that directly modify
people's intrinsic opinions, it is also natural to consider interventions that
modify people's susceptibility to persuasion. In this work, we adopt a popular
model for social opinion dynamics, and we formalize the opinion maximization
and minimization problems where interventions happen at the level of
susceptibility.
  We show that modeling interventions at the level of susceptibility lead to an
interesting family of new questions in network opinion dynamics. We find that
the questions are quite different depending on whether there is an overall
budget constraining the number of agents we can target or not. We give a
polynomial-time algorithm for finding the optimal target-set to optimize the
sum of opinions when there are no budget constraints on the size of the
target-set. We show that this problem is NP-hard when there is a budget, and
that the objective function is neither submodular nor supermodular. Finally, we
propose a heuristic for the budgeted opinion optimization and show its efficacy
at finding target-sets that optimize the sum of opinions compared on real world
networks, including a Twitter network with real opinion estimates.



Machine learning models are susceptible to adversarial perturbations: small
changes to input that can cause large changes in output. It is also
demonstrated that there exist input-agnostic perturbations, called universal
adversarial perturbations, which can change the inference of target model on
most of the data samples. However, existing methods to craft universal
perturbations are (i) task specific, (ii) require samples from the training
data distribution, and (iii) perform complex optimizations. Also, because of
the data dependence, fooling ability of the crafted perturbations is
proportional to the available training data. In this paper, we present a novel,
generalizable and data-free objective for crafting universal adversarial
perturbations. Independent of the underlying task, our objective achieves
fooling via corrupting the extracted features at multiple layers. Therefore,
the proposed objective is generalizable to craft image-agnostic perturbations
across multiple vision tasks such as object recognition, semantic segmentation
and depth estimation. In the practical setting of black-box attacking scenario,
we show that our objective outperforms the data dependent objectives to fool
the learned models. Further, via exploiting simple priors related to the data
distribution, our objective remarkably boosts the fooling ability of the
crafted perturbations. Significant fooling rates achieved by our objective
emphasize that the current deep learning models are now at an increased risk,
since our objective generalizes across multiple tasks without the requirement
of training data for crafting the perturbations.



Although Recurrent Neural Network (RNN) has been a powerful tool for modeling
sequential data, its performance is inadequate when processing sequences with
multiple patterns. In this paper, we address this challenge by introducing an
external memory and constructing a novel persistent memory augmented RNN
(termed as PRNN). The PRNN captures the principle patterns in training
sequences and stores them in an external memory. By leveraging the persistent
memory, the proposed method can adaptively update states according to the
similarities between encoded inputs and memory slots, leading to a stronger
capacity in assimilating sequences with multiple patterns. Content-based
addressing is suggested in memory accessing, and gradient descent is utilized
for implicitly updating the memory. Our approach can be further extended by
combining the prior knowledge of data. Experiments on several datasets
demonstrate the effectiveness of the proposed method.



Psychlab is a simulated psychology laboratory inside the first-person 3D game
world of DeepMind Lab (Beattie et al. 2016). Psychlab enables implementations
of classical laboratory psychological experiments so that they work with both
human and artificial agents. Psychlab has a simple and flexible API that
enables users to easily create their own tasks. As examples, we are releasing
Psychlab implementations of several classical experimental paradigms including
visual search, change detection, random dot motion discrimination, and multiple
object tracking. We also contribute a study of the visual psychophysics of a
specific state-of-the-art deep reinforcement learning agent: UNREAL (Jaderberg
et al. 2016). This study leads to the surprising conclusion that UNREAL learns
more quickly about larger target stimuli than it does about smaller stimuli. In
turn, this insight motivates a specific improvement in the form of a simple
model of foveal vision that turns out to significantly boost UNREAL's
performance, both on Psychlab tasks, and on standard DeepMind Lab tasks. By
open-sourcing Psychlab we hope to facilitate a range of future such studies
that simultaneously advance deep reinforcement learning and improve its links
with cognitive science.



In this paper, we address referring expression comprehension: localizing an
image region described by a natural language expression. While most recent work
treats expressions as a single unit, we propose to decompose them into three
modular components related to subject appearance, location, and relationship to
other objects. This allows us to flexibly adapt to expressions containing
different types of information in an end-to-end framework. In our model, which
we call the Modular Attention Network (MAttNet), two types of attention are
utilized: language-based attention that learns the module weights as well as
the word/phrase attention that each module should focus on; and visual
attention that allows the subject and relationship modules to focus on relevant
image components. Module weights combine scores from all three modules
dynamically to output an overall score. Experiments show that MAttNet
outperforms previous state-of-art methods by a large margin on both
bounding-box-level and pixel-level comprehension tasks.



Localization is the problem of estimating the location of an autonomous agent
from an observation and a map of the environment. Traditional methods of
localization, which filter the belief based on the observations, are
sub-optimal in the number of steps required, as they do not decide the actions
taken by the agent. We propose "Active Neural Localizer", a fully
differentiable neural network that learns to localize accurately and
efficiently. The proposed model incorporates ideas of traditional
filtering-based localization methods, by using a structured belief of the state
with multiplicative interactions to propagate belief, and combines it with a
policy model to localize accurately while minimizing the number of steps
required for localization. Active Neural Localizer is trained end-to-end with
reinforcement learning. We use a variety of simulation environments for our
experiments which include random 2D mazes, random mazes in the Doom game engine
and a photo-realistic environment in the Unreal game engine. The results on the
2D environments show the effectiveness of the learned policy in an idealistic
setting while results on the 3D environments demonstrate the model's capability
of learning the policy and perceptual model jointly from raw-pixel based RGB
observations. We also show that a model trained on random textures in the Doom
environment generalizes well to a photo-realistic office space environment in
the Unreal engine.



We present Etymo (https://etymo.io), a discovery engine to facilitate
artificial intelligence (AI) research and development. It aims to help readers
navigate a large number of AI-related papers published every week by using a
novel form of search that finds relevant papers and displays related papers in
a graphical interface. Etymo constructs and maintains an adaptive
similarity-based network of research papers as an all-purpose knowledge graph
for ranking, recommendation, and visualisation. The network is constantly
evolving and can learn from user feedback to adjust itself.



Deep learning has shown promising results on many machine learning tasks but
DL models are often complex networks with large number of neurons and layers,
and recently, complex layer structures known as building blocks. Finding the
best deep model requires a combination of finding both the right architecture
and the correct set of parameters appropriate for that architecture. In
addition, this complexity (in terms of layer types, number of neurons, and
number of layers) also present problems with generalization since larger
networks are easier to overfit to the data. In this paper, we propose a search
framework for finding effective architectural building blocks for convolutional
neural networks (CNN). Our approach is much faster at finding models that are
close to state-of-the-art in performance. In addition, the models discovered by
our approach are also smaller than models discovered by similar techniques. We
achieve these twin advantages by designing our search space in such a way that
it searches over a reduced set of state-of-the-art building blocks for CNNs
including residual block, inception block, inception-residual block, ResNeXt
block and many others. We apply this technique to generate models for multiple
image datasets and show that these models achieve performance comparable to
state-of-the-art (and even surpassing the state-of-the-art in one case). We
also show that learned models are transferable between datasets.



Deep neural networks are among the most influential architectures of deep
learning algorithms, being deployed in many mobile intelligent applications.
End-side services, such as intelligent personal assistants (IPAs), autonomous
cars, and smart home services often employ either simple local models or
complex remote models on the cloud. Mobile-only and cloud-only computations are
currently the status quo approaches. In this paper, we propose an efficient,
adaptive, and practical engine, JointDNN, for collaborative computation between
a mobile device and cloud for DNNs in both inference and training phase.
JointDNN not only provides an energy and performance efficient method of
querying DNNs for the mobile side, but also benefits the cloud server by
reducing the amount of its workload and communications compared to the
cloud-only approach. Given the DNN architecture, we investigate the efficiency
of processing some layers on the mobile device and some layers on the cloud
server. We provide optimization formulations at layer granularity for forward
and backward propagation in DNNs, which can adapt to mobile battery limitations
and cloud server load constraints and quality of service. JointDNN achieves up
to 18X and 32X reductions on the latency and mobile energy consumption of
querying DNNs, respectively.



Many recent state-of-the-art recommender systems such as D-ATT, TransNet and
DeepCoNN exploit reviews for representation learning. This paper proposes a new
neural architecture for recommendation with reviews. Our model operates on a
multi-hierarchical paradigm and is based on the intuition that not all reviews
are created equal, i.e., only a select few are important. The importance,
however, should be dynamically inferred depending on the current target. To
this end, we propose a review-by-review pointer-based learning scheme that
extracts important reviews, subsequently matching them in a word-by-word
fashion. This enables not only the most informative reviews to be utilized for
prediction but also a deeper word-level interaction. Our pointer-based method
operates with a novel gumbel-softmax based pointer mechanism that enables the
incorporation of discrete vectors within differentiable neural architectures.
Our pointer mechanism is co-attentive in nature, learning pointers which are
co-dependent on user-item relationships. Finally, we propose a multi-pointer
learning scheme that learns to combine multiple views of interactions between
user and item. Overall, we demonstrate the effectiveness of our proposed model
via extensive experiments on \textbf{24} benchmark datasets from Amazon and
Yelp. Empirical results show that our approach significantly outperforms
existing state-of-the-art, with up to 19% and 71% relative improvement when
compared to TransNet and DeepCoNN respectively. We study the behavior of our
multi-pointer learning mechanism, shedding light on evidence aggregation
patterns in review-based recommender systems.



This paper describes a general framework for learning Higher-Order Network
Embeddings (HONE) from graph data based on network motifs. The HONE framework
is highly expressive and flexible with many interchangeable components. The
experimental results demonstrate the effectiveness of learning higher-order
network representations. In all cases, HONE outperforms recent embedding
methods that are unable to capture higher-order structures with a mean relative
gain in AUC of $19\%$ (and up to $75\%$ gain) across a wide variety of networks
and embedding methods.



Machine Learning (ML) has revamped every domain of life as it provides
powerful tools to build complex systems that learn and improve from experience
and data. Our key insight is that to solve a machine learning problem, data
scientists do not invent a new algorithm each time, but evaluate a range of
existing models with different configurations and select the best one. This
task is laborious, error-prone, and drains a large chunk of project budget and
time. In this paper we present a novel framework inspired by programming by
Sketching and Partial Evaluation to minimize human intervention in developing
ML solutions. We templatize machine learning algorithms to expose configuration
choices as holes to be searched. We share code and computation between
different algorithms, and only partially evaluate configuration space of
algorithms based on information gained from initial algorithm evaluations. We
also employ hierarchical and heuristic based pruning to reduce the search
space. Our initial findings indicate that our approach can generate highly
accurate ML models. Interviews with data scientists show that they feel our
framework can eliminate sources of common errors and significantly reduce
development time.



Systematic reviews are essential to summarizing the results of different
clinical and social science studies. The first step in a systematic review task
is to identify all the studies relevant to the review. The task of identifying
relevant studies for a given systematic review is usually performed manually,
and as a result, involves substantial amounts of expensive human resource.
Lately, there have been some attempts to reduce this manual effort using active
learning. In this work, we build upon some such existing techniques, and
validate by experimenting on a larger and comprehensive dataset than has been
attempted until now. Our experiments provide insights on the use of different
feature extraction models for different disciplines. More importantly, we
identify that a naive active learning based screening process is biased in
favour of selecting similar documents. We aimed to improve the performance of
the screening process using a novel active learning algorithm with success.
Additionally, we propose a mechanism to choose the best feature extraction
method for a given review.



The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combines
complementary cognitive strengths of humans and machines in an intelligent
manner to tackle various inference tasks and achieves higher performance than
either humans or machines by themselves. While inference performance
optimization techniques for human-only or sensor-only networks are quite
mature, HuMaINs require novel signal processing and machine learning solutions.
In this paper, we present an overview of the HuMaINs architecture with a focus
on three main issues that include architecture design, inference algorithms
including security/privacy challenges, and application areas/use cases.



Relational data sources are still one of the most popular ways to store
enterprise or Web data, however, the issue with relational schema is the lack
of a well-defined semantic description. A common ontology provides a way to
represent the meaning of a relational schema and can facilitate the integration
of heterogeneous data sources within a domain. Semantic labeling is achieved by
mapping attributes from the data sources to the classes and properties in the
ontology. We formulate this problem as a multi-class classification problem
where previously labeled data sources are used to learn rules for labeling new
data sources. The majority of existing approaches for semantic labeling have
focused on data integration challenges such as naming conflicts and semantic
heterogeneity. In addition, machine learning approaches typically have issues
around class imbalance, lack of labeled instances and relative importance of
attributes. To address these issues, we develop a new machine learning model
with engineered features as well as two deep learning models which do not
require extensive feature engineering. We evaluate our new approaches with the
state-of-the-art.



A flashover occurs when a fire spreads very rapidly through crevices due to
intense heat. Flashovers present one of the most frightening and challenging
fire phenomena to those who regularly encounter them: firefighters.
Firefighters' safety and lives often depend on their ability to predict
flashovers before they occur. Typical pre-flashover fire characteristics
include dark smoke, high heat, and rollover ("angel fingers") and can be
quantified by color, size, and shape. Using a color video stream from a
firefighter's body camera, we applied generative adversarial neural networks
for image enhancement. The neural networks were trained to enhance very dark
fire and smoke patterns in videos and monitor dynamic changes in smoke and fire
areas. Preliminary tests with limited flashover training videos showed that we
predicted a flashover as early as 55 seconds before it occurred.



The Availability bias, manifested in the over-representation of extreme
eventualities in decision-making, is a well-known cognitive bias, and is
generally taken as evidence of human irrationality. In this work, we present
the first rational, metacognitive account of the Availability bias, formally
articulated at Marr's algorithmic level of analysis. Concretely, we present a
normative, metacognitive model of how a cognitive system should over-represent
extreme eventualities, depending on the amount of time available at its
disposal for decision-making. Our model also accounts for two well-known
framing effects in human decision-making under risk---the fourfold pattern of
risk preferences in outcome probability (Tversky & Kahneman, 1992) and in
outcome magnitude (Markovitz, 1952)---thereby providing the first
metacognitively-rational basis for those effects. Empirical evidence,
furthermore, confirms an important prediction of our model. Surprisingly, our
model is unimaginably robust with respect to its focal parameter. We discuss
the implications of our work for studies on human decision-making, and conclude
by presenting a counterintuitive prediction of our model, which, if confirmed,
would have intriguing implications for human decision-making under risk. To our
knowledge, our model is the first metacognitive, resource-rational process
model of cognitive biases in decision-making.



Clustering is inherently ill-posed: there often exist multiple valid
clusterings of a single dataset, and without any additional information a
clustering system has no way of knowing which clustering it should produce.
This motivates the use of constraints in clustering, as they allow users to
communicate their interests to the clustering system. Active constraint-based
clustering algorithms select the most useful constraints to query, aiming to
produce a good clustering using as few constraints as possible. We propose
COBRA, an active method that first over-clusters the data by running K-means
with a $K$ that is intended to be too large, and subsequently merges the
resulting small clusters into larger ones based on pairwise constraints. In its
merging step, COBRA is able to keep the number of pairwise queries low by
maximally exploiting constraint transitivity and entailment. We experimentally
show that COBRA outperforms the state of the art in terms of clustering quality
and runtime, without requiring the number of clusters in advance.



It is inconceivable how chaotic the world would look to humans, faced with
innumerable decisions a day to be made under uncertainty, had they been lacking
the capacity to distinguish the relevant from the irrelevant---a capacity which
computationally amounts to handling probabilistic independence relations. The
highly parallel and distributed computational machinery of the brain suggests
that a satisfying process-level account of human independence judgment should
also mimic these features. In this work, we present the first rational,
distributed, message-passing, process-level account of independence judgment,
called $\mathcal{D}^\ast$. Interestingly, $\mathcal{D}^\ast$ shows a curious,
but normatively-justified tendency for quick detection of dependencies,
whenever they hold. Furthermore, $\mathcal{D}^\ast$ outperforms all the
previously proposed algorithms in the AI literature in terms of worst-case
running time, and a salient aspect of it is supported by recent work in
neuroscience investigating possible implementations of Bayes nets at the neural
level. $\mathcal{D}^\ast$ nicely exemplifies how the pursuit of cognitive
plausibility can lead to the discovery of state-of-the-art algorithms with
appealing properties, and its simplicity makes $\mathcal{D}^\ast$ potentially a
good candidate for pedagogical purposes.



Pretraining with expert demonstrations have been found useful in speeding up
the training process of deep reinforcement learning algorithms since less
online simulation data is required. Some people use supervised learning to
speed up the process of feature learning, others pretrain the policies by
imitating expert demonstrations. However, these methods are unstable and not
suitable for actor-critic reinforcement learning algorithms. Also, some
existing methods rely on the global optimum assumption, which is not true in
most scenarios. In this paper, we employ expert demonstrations in a
actor-critic reinforcement learning framework, and meanwhile ensure that the
performance is not affected by the fact that expert demonstrations are not
global optimal. We theoretically derive a method for computing policy gradients
and value estimators with only expert demonstrations. Our method is
theoretically plausible for actor-critic reinforcement learning algorithms that
pretrains both policy and value functions. We apply our method to two of the
typical actor-critic reinforcement learning algorithms, DDPG and ACER, and
demonstrate with experiments that our method not only outperforms the RL
algorithms without pretraining process, but also is more simulation efficient.



Novice programmers often struggle with the formal syntax of programming
languages. To assist them, we design a novel programming language correction
framework amenable to reinforcement learning. The framework allows an agent to
mimic human actions for text navigation and editing. We demonstrate that the
agent can be trained through self-exploration directly from the raw input, that
is, program text itself, without any knowledge of the formal syntax of the
programming language. We leverage expert demonstrations for one tenth of the
training data to accelerate training. The proposed technique is evaluated on
6975 erroneous C programs with typographic errors, written by students during
an introductory programming course. Our technique fixes 14% more programs and
29% more compiler error messages relative to those fixed by a state-of-the-art
tool, DeepFix, which uses a fully supervised neural machine translation
approach.



In this paper, we propose a novel approach, 3D-RecGAN++, which reconstructs
the complete 3D structure of a given object from a single arbitrary depth view
using generative adversarial networks. Unlike existing work which typically
requires multiple views of the same object or class labels to recover the full
3D geometry, the proposed 3D-RecGAN++ only takes the voxel grid representation
of a depth view of the object as input, and is able to generate the complete 3D
occupancy grid with a high resolution of 256^3 by recovering the
occluded/missing regions. The key idea is to combine the generative
capabilities of autoencoders and the conditional Generative Adversarial
Networks (GAN) framework, to infer accurate and fine-grained 3D structures of
objects in high-dimensional voxel space. Extensive experiments on large
synthetic datasets and real-world Kinect datasets show that the proposed
3D-RecGAN++ significantly outperforms the state of the art in single view 3D
object reconstruction, and is able to reconstruct unseen types of objects.



We identify obfuscated gradients as a phenomenon that leads to a false sense
of security in defenses against adversarial examples. While defenses that cause
obfuscated gradients appear to defeat optimization-based attacks, we find
defenses relying on this effect can be circumvented.
  For each of the three types of obfuscated gradients we discover, we describe
indicators of defenses exhibiting this effect and develop attack techniques to
overcome it. In a case study, examining all defenses accepted to ICLR 2018, we
find obfuscated gradients are a common occurrence, with 7 of 8 defenses relying
on obfuscated gradients. Using our new attack techniques, we successfully
circumvent all 7 of them.



Gaussian processes are rich distributions over functions, with generalization
properties determined by a kernel function. When used for long-range
extrapolation, predictions are particularly sensitive to the choice of kernel
parameters. It is therefore critical to account for kernel uncertainty in our
predictive distributions. We propose a distribution over kernels formed by
modelling a spectral mixture density with a L\'evy process. The resulting
distribution has support for all stationary covariances--including the popular
RBF, periodic, and Mat\'ern kernels--combined with inductive biases which
enable automatic and data efficient learning, long-range extrapolation, and
state of the art predictive performance. The proposed model also presents an
approach to spectral regularization, as the L\'evy process introduces a
sparsity-inducing prior over mixture components, allowing automatic selection
over model order and pruning of extraneous components. We exploit the algebraic
structure of the proposed process for $\mathcal{O}(n)$ training and
$\mathcal{O}(1)$ predictions. We perform extrapolations having reasonable
uncertainty estimates on several benchmarks, show that the proposed model can
recover flexible ground truth covariances and that it is robust to errors in
initialization.



Deep neural networks are complex and opaque. As they enter application in a
variety of important and safety critical domains, users seek methods to explain
their output predictions. We develop an approach to explaining deep neural
networks by constructing causal models on salient concepts contained in a CNN.
We develop methods to extract salient concepts throughout a target network by
using autoencoders trained to extract human-understandable representations of
network activations. We then build a bayesian causal model using these
extracted concepts as variables in order to explain image classification.
Finally, we use this causal model to identify and visualize features with
significant causal influence on final classification.



Model interpretability is a requirement in many applications in which crucial
decisions are made by users relying on a model's outputs. The recent movement
for "algorithmic fairness" also stipulates explainability, and therefore
interpretability of learning models. And yet the most successful contemporary
Machine Learning approaches, the Deep Neural Networks, produce models that are
highly non-interpretable. We attempt to address this challenge by proposing a
technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN)
via meta-learning. In this work, we interpret a specific hidden layer of the
deep CNN model on the MNIST image dataset. We use a clustering algorithm in a
two-level structure to find the meta-level training data and Random Forest as
base learning algorithms to generate the meta-level test data. The
interpretation results are displayed visually via diagrams, which clearly
indicates how a specific test instance is classified. Our method achieves
global interpretation for all the test instances without sacrificing the
accuracy obtained by the original deep CNN model. This means our model is
faithful to the deep CNN model, which leads to reliable interpretations.



The extension of deep learning towards temporal data processing is gaining an
increasing research interest. In this paper we investigate the properties of
state dynamics developed in successive levels of deep recurrent neural networks
(RNNs) in terms of short-term memory abilities. Our results reveal interesting
insights that shed light on the nature of layering as a factor of RNN design.
Noticeably, higher layers in a hierarchically organized RNN architecture
results to be inherently biased towards longer memory spans even prior to
training of the recurrent connections. Moreover, in the context of Reservoir
Computing framework, our analysis also points out the benefit of a layered
recurrent organization as an efficient approach to improve the memory skills of
reservoir models.



Training deep neural networks results in strong learned representations that
show good generalization capabilities. In most cases, training involves
iterative modification of all weights inside the network via back-propagation.
In Extreme Learning Machines, it has been suggested to set the first layer of a
network to fixed random values instead of learning it. In this paper, we
propose to take this approach a step further and fix almost all layers of a
deep convolutional neural network, allowing only a small portion of the weights
to be learned. As our experiments show, fixing even the majority of the
parameters of the network often results in performance which is on par with the
performance of learning all of them. The implications of this intriguing
property of deep neural networks are discussed and we suggest ways to harness
it to create more robust representations.



Human face-to-face communication is a complex multimodal signal. We use words
(language modality), gestures (vision modality) and changes in tone (acoustic
modality) to convey our intentions. Humans easily process and understand
face-to-face communication, however, comprehending this form of communication
remains a significant challenge for Artificial Intelligence (AI). AI must
understand each modality and the interactions between them that shape human
communication. In this paper, we present a novel neural architecture for
understanding human communication called the Multi-attention Recurrent Network
(MARN). The main strength of our model comes from discovering interactions
between modalities through time using a neural component called the
Multi-attention Block (MAB) and storing them in the hybrid memory of a
recurrent component called the Long-short Term Hybrid Memory (LSTHM). We
perform extensive comparisons on six publicly available datasets for multimodal
sentiment analysis, speaker trait recognition and emotion recognition. MARN
shows state-of-the-art performance on all the datasets.



With the increasing popularity of video sharing websites such as YouTube and
Facebook, multimodal sentiment analysis has received increasing attention from
the scientific community. Contrary to previous works in multimodal sentiment
analysis which focus on holistic information in speech segments such as bag of
words representations and average facial expression intensity, we develop a
novel deep architecture for multimodal sentiment analysis that performs
modality fusion at the word level. In this paper, we propose the Gated
Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is
composed of 2 modules. The Gated Multimodal Embedding alleviates the
difficulties of fusion when there are noisy modalities. The LSTM with Temporal
Attention performs word level fusion at a finer fusion resolution between input
modalities and attends to the most important time steps. As a result, the
GME-LSTM(A) is able to better model the multimodal structure of speech through
time and perform better sentiment comprehension. We demonstrate the
effectiveness of this approach on the publicly-available Multimodal Corpus of
Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving
state-of-the-art sentiment classification and regression results. Qualitative
analysis on our model emphasizes the importance of the Temporal Attention Layer
in sentiment prediction because the additional acoustic and visual modalities
are noisy. We also demonstrate the effectiveness of the Gated Multimodal
Embedding in selectively filtering these noisy modalities out. Our results and
analysis open new areas in the study of sentiment analysis in human
communication and provide new models for multimodal fusion.



We consider an extension of the contextual bandit setting, motivated by
several practical applications, where an unlabeled history of contexts can
become available for pre-training before the online decision-making begins. We
propose an approach for improving the performance of contextual bandit in such
setting, via adaptive, dynamic representation learning, which combines offline
pre-training on unlabeled history of contexts with online selection and
modification of embedding functions. Our experiments on a variety of datasets
and in different nonstationary environments demonstrate clear advantages of our
approach over the standard contextual bandit.



We describe an approach to learn, in a term-rewriting setting, function
definitions from input/output equations. By confining ourselves to structurally
recursive definitions we obtain a fairly fast learning algorithm that often
yields definitions close to intuitive expectations. We provide a Prolog
prototype implementation of our approach, and indicate open issues of further
investigation.



Robots have great potential to facilitate future therapies for children on
the autism spectrum. However, existing robots lack the ability to automatically
perceive and respond to human affect, which is necessary for establishing and
maintaining engaging interactions. Moreover, their inference challenge is made
harder by the fact that many individuals with autism have atypical and
unusually diverse styles of expressing their affective-cognitive states. To
tackle the heterogeneity in behavioral cues of children with autism, we use the
latest advances in deep learning to formulate a personalized machine learning
(ML) framework for automatic perception of the childrens affective states and
engagement during robot-assisted autism therapy. The key to our approach is a
novel shift from the traditional ML paradigm - instead of using
'one-size-fits-all' ML models, our personalized ML framework is optimized for
each child by leveraging relevant contextual information (demographics and
behavioral assessment scores) and individual characteristics of each child. We
designed and evaluated this framework using a dataset of multi-modal audio,
video and autonomic physiology data of 35 children with autism (age 3-13) and
from 2 cultures (Asia and Europe), participating in a 25-minute child-robot
interaction (~500k datapoints). Our experiments confirm the feasibility of the
robot perception of affect and engagement, showing clear improvements due to
the model personalization. The proposed approach has potential to improve
existing therapies for autism by offering more efficient monitoring and
summarization of the therapy progress.



This paper addresses the interpretability of deep learning-enabled image
recognition processes in computer vision science in relation to theories in art
history and cognitive psychology on the vision-related perceptual capabilities
of humans. Examination of what is determinable about the machine-learned image
in comparison to humanistic theories of visual perception, particularly in
regard to art historian Erwin Panofsky's methodology for image analysis and
psychologist Eleanor Rosch's theory of graded categorization according to
prototypes, finds that there are surprising similarities between the two that
suggest that researchers in the arts and the sciences would have much to
benefit from closer collaborations. Utilizing the examples of Google's
DeepDream and the Machine Learning and Perception Lab at Georgia Tech's
Grad-CAM: Gradient-weighted Class Activation Mapping programs, this study
suggests that a revival of art historical research in iconography and formalism
in the age of AI is essential for shaping the future navigation and
interpretation of all machine-learned images, given the rapid developments in
image recognition technologies.



We build a virtual agent for learning language in a 2D maze-like world. The
agent sees images of the surrounding environment, listens to a virtual teacher,
and takes actions to receive rewards. It interactively learns the teacher's
language from scratch based on two language use cases: sentence-directed
navigation and question answering. It learns simultaneously the visual
representations of the world, the language, and the action control. By
disentangling language grounding from other computational routines and sharing
a concept detection function between language grounding and prediction, the
agent reliably interpolates and extrapolates to interpret sentences that
contain new word combinations or new words missing from training sentences. The
new words are transferred from the answers of language prediction. Such a
language ability is trained and evaluated on a population of over 1.6 million
distinct sentences consisting of 119 object words, 8 color words, 9
spatial-relation words, and 50 grammatical words. The proposed model
significantly outperforms five comparison methods for interpreting zero-shot
sentences. In addition, we demonstrate human-interpretable intermediate outputs
of the model in the appendix.



Convolutional Neural Networks are a well-known staple of modern image
classification. However, it can be difficult to assess the quality and
robustness of such models. Deep models are known to perform well on a given
training and estimation set, but can easily be fooled by data that is
specifically generated for the purpose. It has been shown that one can produce
an artificial example that does not represent the desired class, but activates
the network in the desired way. This paper describes a new way of
reconstructing a sample from the training set distribution of an image
classifier without deep knowledge about the underlying distribution. This
enables access to the elements of images that most influence the decision of a
convolutional network and to extract meaningful information about the training
distribution.



The effort devoted to hand-crafting image classifiers has motivated the use
of architecture search to discover them automatically. Reinforcement learning
and evolution have both shown promise for this purpose. This study employs a
regularized version of a popular asynchronous evolutionary algorithm. We
rigorously compare it to the non-regularized form and to a highly-successful
reinforcement learning baseline. Using the same hardware, compute effort and
neural network training code, we conduct repeated experiments side-by-side,
exploring different datasets, search spaces and scales. We show regularized
evolution consistently produces models with similar or higher accuracy, across
a variety of contexts without need for re-tuning parameters. In addition,
evolution exhibits considerably better performance than reinforcement learning
at early search stages, suggesting it may be the better choice when fewer
compute resources are available. This constitutes the first controlled
comparison of the two search algorithms in this context. Finally, we present
new architectures discovered with evolution that we nickname AmoebaNets. These
models set a new state of the art for CIFAR-10 (mean test error = 2.13%) and
mobile-size ImageNet (top-5 accuracy = 92.1% with 5.06M parameters), and reach
the current state of the art for ImageNet (top-5 accuracy = 96.2%).



Deep learning algorithms and networks are vulnerable to perturbed inputs
which are known as the adversarial attack. Many defense methodologies have been
investigated to defend such adversarial attack. In this work, we propose a
novel methodology to defend the existing powerful attack model. Such attack
models have achieved record success against MNIST dataset to force it to
miss-classify all of its inputs. Whereas Our proposed defense method robust
pre-processing achieves the best accuracy among the current state of the art
defenses. It consists of Tanh (hyperbolic tangent) function, smoothing and
batch normalization to process the input data which will make it more robust
over the adversarial attack. robust pre-processing improves the white box
attack accuracy of MNIST from 94.3% to 98.7%. Even with increasing defense when
others defenses completely fail, robust pre-processing remains one of the
strongest ever reported. Another strength of our defense is that it eliminates
the need for adversarial training as it can significantly increase the MNIST
accuracy without adversarial training as well. This makes it a more generalized
defense method with almost half training overhead and much-improved accuracy.
robust pre-processing can also increase the inference accuracy in the face of
the powerful attack on CIFAR-10 and SVHN data set as well without much
sacrificing clean data accuracy.



Humans and animals are capable of learning a new behavior by observing others
perform the skill just once. We consider the problem of allowing a robot to do
the same -- learning from a raw video pixels of a human, even when there is
substantial domain shift in the perspective, environment, and embodiment
between the robot and the observed human. Prior approaches to this problem have
hand-specified how human and robot actions correspond and often relied on
explicit human pose detection systems. In this work, we present an approach for
one-shot learning from a video of a human by using human and robot
demonstration data from a variety of previous tasks to build up prior knowledge
through meta-learning. Then, combining this prior knowledge and only a single
video demonstration from a human, the robot can perform the task that the human
demonstrated. We show experiments on both a PR2 arm and a Sawyer arm,
demonstrating that after meta-learning, the robot can learn to place, push, and
pick-and-place new objects using just one video of a human performing the
manipulation.



Humans and most animals can learn new tasks without forgetting old ones.
However, training artificial neural networks (ANNs) on new tasks typically
cause it to forget previously learned tasks. This phenomenon is the result of
"catastrophic forgetting", in which training an ANN disrupts connection weights
that were important for solving previous tasks, degrading task performance.
Several recent studies have proposed methods to stabilize connection weights of
ANNs that are deemed most important for solving a task, which helps alleviate
catastrophic forgetting. Here, drawing inspiration from algorithms that are
believed to be implemented in vivo, we propose a complementary method: adding a
context-dependent gating signal, such that only sparse, mostly non-overlapping
patterns of units are active for any one task. This method is easy to
implement, requires little computational overhead, and allows ANNs to maintain
high performance across large numbers of sequentially presented tasks when
combined with weight stabilization. This work provides another example of how
neuroscience-inspired algorithms can benefit ANN design and capability.



Designing mechanisms that leverage cooperation between agents has been a
long-lasting goal in Multiagent Systems. The task is especially challenging
when agents are selfish, lack common goals and face social dilemmas, i.e.,
situations in which individual interest conflicts with social welfare. Past
works explored mechanisms that explain cooperation in biological and social
systems, providing important clues for the aim of designing cooperative
artificial societies. In particular, several works show that cooperation is
able to emerge when specific network structures underlie agents' interactions.
Notwithstanding, social dilemmas in which defection is highly tempting still
pose challenges concerning the effective sustainability of cooperation. Here we
propose a new redistribution mechanism that can be applied in structured
populations of agents. Importantly, we show that, when implemented locally
(i.e., agents share a fraction of their wealth surplus with their nearest
neighbors), redistribution excels in promoting cooperation under regimes where,
before, only defection prevailed.



The study of human-robot interaction is fundamental to the design and use of
robotics in real-world applications. Robots will need to predict and adapt to
the actions of human collaborators in order to achieve good performance and
improve safety and end-user adoption. This paper evaluates a human-robot
collaboration scheme that combines the task allocation and motion levels of
reasoning: the robotic agent uses Bayesian inference to predict the next goal
of its human partner from his or her ongoing motion, and re-plans its own
actions in real time. This anticipative adaptation is desirable in many
practical scenarios, where humans are unable or unwilling to take on the
cognitive overhead required to explicitly communicate their intent to the
robot. A behavioral experiment indicates that the combination of goal inference
and dynamic task planning significantly improves both objective and perceived
performance of the human-robot team. Participants were highly sensitive to the
differences between robot behaviors, preferring to work with a robot that
adapted to their actions over one that did not.



Attention-based sequence-to-sequence model has proved successful in Neural
Machine Translation (NMT). However, the attention without consideration of
decoding history, which includes the past information in the decoder and the
attention mechanism, often causes much repetition. To address this problem, we
propose the decoding-history-based Adaptive Control of Attention (ACA) for the
NMT model. ACA learns to control the attention by keeping track of the decoding
history and the current information with a memory vector, so that the model can
take the translated contents and the current information into consideration.
Experiments on Chinese-English translation and the English-Vietnamese
translation have demonstrated that our model significantly outperforms the
strong baselines. The analysis shows that our model is capable of generating
translation with less repetition and higher accuracy. The code will be
available at https://github.com/lancopku



In the last years many accurate decision support systems have been
constructed as black boxes, that is as systems that hide their internal logic
to the user. This lack of explanation constitutes both a practical and an
ethical issue. The literature reports many approaches aimed at overcoming this
crucial weakness sometimes at the cost of scarifying accuracy for
interpretability. The applications in which black box decision systems can be
used are various, and each approach is typically developed to provide a
solution for a specific problem and, as a consequence, delineating explicitly
or implicitly its own definition of interpretability and explanation. The aim
of this paper is to provide a classification of the main problems addressed in
the literature with respect to the notion of explanation and the type of black
box system. Given a problem definition, a black box type, and a desired
explanation this survey should help the researcher to find the proposals more
useful for his own work. The proposed classification of approaches to open
black box models should also be useful for putting the many research open
questions in perspective.



Variational encoder-decoders (VEDs) have shown promising results in dialogue
generation. However, the latent variable distributions are usually approximated
by a much simpler model than the powerful RNN structure used for encoding and
decoding, yielding the KL-vanishing problem and inconsistent training
objective. In this paper, we separate the training step into two phases: The
first phase learns to autoencode discrete texts into continuous embeddings,
from which the second phase learns to generalize latent representations by
reconstructing the encoded embedding. In this case, latent variables are
sampled by transforming Gaussian noise through multi-layer perceptrons and are
trained with a separate VED model, which has the potential of realizing a much
more flexible distribution. We compare our model with current popular models
and the experiment demonstrates substantial improvement in both metric-based
and human evaluations.



We present a multi-document summarizer, called MEAD, which generates
summaries using cluster centroids produced by a topic detection and tracking
system. We also describe two new techniques, based on sentence utility and
subsumption, which we have applied to the evaluation of both single and
multiple document summaries. Finally, we describe two user studies that test
our models of multi-document summarization.



Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown distribution. We unify both theories and give
strong arguments that the resulting universal AIXI model behaves optimal in any
computable environment. The major drawback of the AIXI model is that it is
uncomputable. To overcome this problem, we construct a modified algorithm
AIXI^tl, which is still superior to any other time t and space l bounded agent.
The computation time of AIXI^tl is of the order t x 2^l.



The mutual information of two random variables i and j with joint
probabilities t_ij is commonly used in learning Bayesian nets as well as in
many other fields. The chances t_ij are usually estimated by the empirical
sampling frequency n_ij/n leading to a point estimate I(n_ij/n) for the mutual
information. To answer questions like "is I(n_ij/n) consistent with zero?" or
"what is the probability that the true mutual information is much larger than
the point estimate?" one has to go beyond the point estimate. In the Bayesian
framework one can answer these questions by utilizing a (second order) prior
distribution p(t) comprising prior information about t. From the prior p(t) one
can compute the posterior p(t|n), from which the distribution p(I|n) of the
mutual information can be calculated. We derive reliable and quickly computable
approximations for p(I|n). We concentrate on the mean, variance, skewness, and
kurtosis, and non-informative priors. For the mean we also give an exact
expression. Numerical issues and the range of validity are discussed.



It has been shown that a neural network model recently proposed to describe
basic memory performance is based on a ternary/binary coding/decoding algorithm
which leads to a new neural network assembly memory model (NNAMM) providing
maximum-likelihood recall/recognition properties and implying a new memory unit
architecture with Hopfield two-layer network, N-channel time gate, auxiliary
reference memory, and two nested feedback loops. For the data coding used,
conditions are found under which a version of Hopfied network implements
maximum-likelihood convolutional decoding algorithm and, simultaneously, linear
statistical classifier of arbitrary binary vectors with respect to Hamming
distance between vector analyzed and reference vector given. In addition to
basic memory performance and etc, the model explicitly describes the dependence
on time of memory trace retrieval, gives a possibility of one-trial learning,
metamemory simulation, generalized knowledge representation, and distinct
description of conscious and unconscious mental processes. It has been shown
that an assembly memory unit may be viewed as a model of a smallest inseparable
part or an 'atom' of consciousness. Some nontraditional neurobiological
backgrounds (dynamic spatiotemporal synchrony, properties of time dependent and
error detector neurons, early precise spike firing, etc) and the model's
application to solve some interdisciplinary problems from different scientific
fields are discussed.



Solomonoff unified Occam's razor and Epicurus' principle of multiple
explanations to one elegant, formal, universal theory of inductive inference,
which initiated the field of algorithmic information theory. His central result
is that the posterior of his universal semimeasure M converges rapidly to the
true sequence generating posterior mu, if the latter is computable. Hence, M is
eligible as a universal predictor in case of unknown mu. We investigate the
existence and convergence of computable universal (semi)measures for a
hierarchy of computability classes: finitely computable, estimable, enumerable,
and approximable. For instance, M is known to be enumerable, but not finitely
computable, and to dominate all enumerable semimeasures. We define seven
classes of (semi)measures based on these four computability concepts. Each
class may or may not contain a (semi)measure which dominates all elements of
another class. The analysis of these 49 cases can be reduced to four basic
cases, two of them being new. The results hold for discrete and continuous
semimeasures. We also investigate more closely the types of convergence,
possibly implied by universality: in difference and in ratio, with probability
1, in mean sum, and for Martin-Loef random sequences. We introduce a
generalized concept of randomness for individual sequences and use it to
exhibit difficulties regarding these issues.



A ternary/binary data coding algorithm and conditions under which Hopfield
networks implement optimal convolutional or Hamming decoding algorithms has
been described. Using the coding/decoding approach (an optimal Binary Signal
Detection Theory, BSDT) introduced a Neural Network Assembly Memory Model
(NNAMM) is built. The model provides optimal (the best) basic memory
performance and demands the use of a new memory unit architecture with
two-layer Hopfield network, N-channel time gate, auxiliary reference memory,
and two nested feedback loops. NNAMM explicitly describes the dependence on
time of a memory trace retrieval, gives a possibility of metamemory simulation,
generalized knowledge representation, and distinct description of conscious and
unconscious mental processes. A model of smallest inseparable part or an "atom"
of consciousness is also defined. The NNAMM's neurobiological backgrounds and
its applications to solving some interdisciplinary problems are shortly
discussed. BSDT could implement the "best neural code" used in nervous tissues
of animals and humans.



We consider the Minimum Description Length principle for online sequence
prediction. If the underlying model class is discrete, then the total expected
square loss is a particularly interesting performance measure: (a) this
quantity is bounded, implying convergence with probability one, and (b) it
additionally specifies a `rate of convergence'. Generally, for MDL only
exponential loss bounds hold, as opposed to the linear bounds for a Bayes
mixture. We show that this is even the case if the model class contains only
Bernoulli distributions. We derive a new upper bound on the prediction error
for countable Bernoulli classes. This implies a small bound (comparable to the
one for Bayes mixtures) for certain important model classes. The results apply
to many Machine Learning tasks including classification and hypothesis testing.
We provide arguments that our theorems generalize to countable classes of
i.i.d. models.



We tackle the problem of robust dialogue processing from the perspective of
language engineering. We propose an agent-oriented architecture that allows us
a flexible way of composing robust processors. Our approach is based on
Shoham's Agent Oriented Programming (AOP) paradigm. We will show how the AOP
agent model can be enriched with special features and components that allow us
to deal with classical problems of dialogue understanding.



Words and phrases acquire meaning from the way they are used in society, from
their relative semantics to other words and phrases. For computers the
equivalent of `society' is `database,' and the equivalent of `use' is `way to
search the database.' We present a new theory of similarity between words and
phrases based on information distance and Kolmogorov complexity. To fix
thoughts we use the world-wide-web as database, and Google as search engine.
The method is also applicable to other search engines and databases. This
theory is then applied to construct a method to automatically extract
similarity, the Google similarity distance, of words and phrases from the
world-wide-web using Google page counts. The world-wide-web is the largest
database on earth, and the context information entered by millions of
independent users averages out to provide automatic semantics of useful
quality. We give applications in hierarchical clustering, classification, and
language translation. We give examples to distinguish between colors and
numbers, cluster names of paintings by 17th century Dutch masters and names of
books by English novelists, the ability to understand emergencies, and primes,
and we demonstrate the ability to do a simple automatic English-Spanish
translation. Finally, we use the WordNet database as an objective baseline
against which to judge the performance of our method. We conduct a massive
randomized trial in binary classification using support vector machines to
learn categories based on our Google distance, resulting in an a mean agreement
of 87% with the expert crafted WordNet categories.



We address the practical problems of estimating the information relations
that characterize large networks. Building on methods developed for analysis of
the neural code, we show that reliable estimates of mutual information can be
obtained with manageable computational effort. The same methods allow
estimation of higher order, multi--information terms. These ideas are
illustrated by analyses of gene expression, financial markets, and consumer
preferences. In each case, information theoretic measures correlate with
independent, intuitive measures of the underlying structures in the system.



A major challenge for the realization of intelligent robots is to supply them
with cognitive abilities in order to allow ordinary users to program them
easily and intuitively. One way of such programming is teaching work tasks by
interactive demonstration. To make this effective and convenient for the user,
the machine must be capable to establish a common focus of attention and be
able to use and integrate spoken instructions, visual perceptions, and
non-verbal clues like gestural commands. We report progress in building a
hybrid architecture that combines statistical methods, neural networks, and
finite state machines into an integrated system for instructing grasping tasks
by man-machine interaction. The system combines the GRAVIS-robot for visual
attention and gestural instruction with an intelligent interface for speech
recognition and linguistic interpretation, and an modality fusion module to
allow multi-modal task-oriented man-machine communication with respect to
dextrous robot manipulation of objects.



This paper addresses the problem of distributed learning under communication
constraints, motivated by distributed signal processing in wireless sensor
networks and data mining with distributed databases. After formalizing a
general model for distributed learning, an algorithm for collaboratively
training regularized kernel least-squares regression estimators is derived.
Noting that the algorithm can be viewed as an application of successive
orthogonal projection algorithms, its convergence properties are investigated
and the statistical behavior of the estimator is discussed in a simplified
theoretical setting.



In this paper after short review of the CSIEC project initialized by us in
2003 we present the continuing development and improvement of the CSIEC project
in details, including the design of five new Microsoft agent characters
representing different virtual chatting partners and the limitation of
simulated dialogs in specific practical scenarios like graduate job application
interview, then briefly analyze the actual conditions and features of its
application field: web-based English education in China. Finally we introduce
our efforts to adapt this system to the requirements of English teaching and
learning in China and point out the work next to do.



This paper discusses the systematic use of product feedback information to
support life-cycle design approaches and provides guidelines for developing a
design at both the product and the system levels. Design activities are
surveyed in the light of the product life cycle, and the design information
flow is interpreted from a semiotic perspective. The natural evolution of a
design is considered, the notion of design expectations is introduced, and the
importance of evaluation of these expectations in dynamic environments is
argued. Possible strategies for reconciliation of the expectations and
environmental factors are described. An Internet-enabled technology is proposed
to monitor product functionality, usage, and operational environment and supply
the designer with relevant information. A pilot study of assessing design
expectations of a refrigerator is outlined, and conclusions are drawn.



Automated management and monitoring of service contracts like Service Level
Agreements (SLAs) or higher-level policies is vital for efficient and reliable
distributed service-oriented architectures (SOA) with high quality of ser-vice
(QoS) levels. IT service provider need to manage, execute and maintain
thousands of SLAs for different customers and different types of services,
which needs new levels of flexibility and automation not available with the
current technol-ogy. I propose a novel rule-based knowledge representation (KR)
for SLA rules and a respective rule-based service level management (RBSLM)
framework. My rule-based approach based on logic programming provides several
advantages including automated rule chaining allowing for compact knowledge
representation and high levels of automation as well as flexibility to adapt to
rapidly changing business requirements. Therewith, I address an urgent need
service-oriented busi-nesses do have nowadays which is to dynamically change
their business and contractual logic in order to adapt to rapidly changing
business environments and to overcome the restricting nature of slow change
cycles.



In this paper we analyze mathematically how human factors can be effectively
incorporated into the analysis and control of complex systems. As an example,
we focus our discussion around one of the key problems in the Intelligent
Transportation Systems (ITS) theory and practice, the problem of speed control,
considered here as a decision making process with limited information
available. The problem is cast mathematically in the general framework of
control problems and is treated in the context of dynamically changing
environments where control is coupled to human-centered automation. Since in
this case control might not be limited to a small number of control settings,
as it is often assumed in the control literature, serious difficulties arise in
the solution of this problem. We demonstrate that the problem can be reduced to
a set of Hamilton-Jacobi-Bellman equations where human factors are incorporated
via estimations of the system Hamiltonian. In the ITS context, these
estimations can be obtained with the use of on-board equipment like
sensors/receivers/actuators, in-vehicle communication devices, etc. The
proposed methodology provides a way to integrate human factor into the solving
process of the models for other complex dynamic systems.



We have previously shown in an abstract simulation (Gershenson, 2005) that
self-organizing traffic lights can improve greatly traffic flow for any
density. In this paper, we extend these results to a realistic setting,
implementing self-organizing traffic lights in an advanced traffic simulator
using real data from a Brussels avenue. On average, for different traffic
densities, travel waiting times are reduced by 50% compared to the current
green wave method.



The social media site Flickr allows users to upload their photos, annotate
them with tags, submit them to groups, and also to form social networks by
adding other users as contacts. Flickr offers multiple ways of browsing or
searching it. One option is tag search, which returns all images tagged with a
specific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an
insect or a car, tag search results will include many images that are not
relevant to the sense the user had in mind when executing the query. We claim
that users express their photography interests through the metadata they add in
the form of contacts and image annotations. We show how to exploit this
metadata to personalize search results for the user, thereby improving search
performance. First, we show that we can significantly improve search precision
by filtering tag search results by user's contacts or a larger social network
that includes those contact's contacts. Secondly, we describe a probabilistic
model that takes advantage of tag information to discover latent topics
contained in the search results. The users' interests can similarly be
described by the tags they used for annotating their images. The latent topics
found by the model are then used to personalize search results by finding
images on topics that are of interest to the user.



Max-product belief propagation is a local, iterative algorithm to find the
mode/MAP estimate of a probability distribution. While it has been successfully
employed in a wide variety of applications, there are relatively few
theoretical guarantees of convergence and correctness for general loopy graphs
that may have many short cycles. Of these, even fewer provide exact ``necessary
and sufficient'' characterizations.
  In this paper we investigate the problem of using max-product to find the
maximum weight matching in an arbitrary graph with edge weights. This is done
by first constructing a probability distribution whose mode corresponds to the
optimal matching, and then running max-product. Weighted matching can also be
posed as an integer program, for which there is an LP relaxation. This
relaxation is not always tight. In this paper we show that \begin{enumerate}
\item If the LP relaxation is tight, then max-product always converges, and
that too to the correct answer. \item If the LP relaxation is loose, then
max-product does not converge. \end{enumerate} This provides an exact,
data-dependent characterization of max-product performance, and a precise
connection to LP relaxation, which is a well-studied optimization technique.
Also, since LP relaxation is known to be tight for bipartite graphs, our
results generalize other recent results on using max-product to find weighted
matchings in bipartite graphs.



We examine an evolutionary naming-game model where communicating agents are
equipped with an evolutionarily selected learning ability. Such a coupling of
biological and linguistic ingredients results in an abrupt transition: upon a
small change of a model control parameter a poorly communicating group of
linguistically unskilled agents transforms into almost perfectly communicating
group with large learning abilities. When learning ability is kept fixed, the
transition appears to be continuous. Genetic imprinting of the learning
abilities proceeds via Baldwin effect: initially unskilled communicating agents
learn a language and that creates a niche in which there is an evolutionary
pressure for the increase of learning ability.Our model suggests that when
linguistic (or cultural) processes became intensive enough, a transition took
place where both linguistic performance and biological endowment of our species
experienced an abrupt change that perhaps triggered the rapid expansion of
human civilization.



Many problems that arise in machine learning domain deal with nonlinearity
and quite often demand users to obtain global optimal solutions rather than
local optimal ones. Optimization problems are inherent in machine learning
algorithms and hence many methods in machine learning were inherited from the
optimization literature. Popularly known as the initialization problem, the
ideal set of parameters required will significantly depend on the given
initialization values. The recently developed TRUST-TECH (TRansformation Under
STability-reTaining Equilibria CHaracterization) methodology systematically
explores the subspace of the parameters to obtain a complete set of local
optimal solutions. In this thesis work, we propose TRUST-TECH based methods for
solving several optimization and machine learning problems. Two stages namely,
the local stage and the neighborhood-search stage, are repeated alternatively
in the solution space to achieve improvements in the quality of the solutions.
Our methods were tested on both synthetic and real datasets and the advantages
of using this novel framework are clearly manifested. This framework not only
reduces the sensitivity to initialization, but also allows the flexibility for
the practitioners to use various global and local methods that work well for a
particular problem of interest. Other hierarchical stochastic algorithms like
evolutionary algorithms and smoothing algorithms are also studied and
frameworks for combining these methods with TRUST-TECH have been proposed and
evaluated on several test systems.



It is well known that an arbitrary graphical model of statistical inference
defined on a tree, i.e. on a graph without loops, is solved exactly and
efficiently by an iterative Belief Propagation (BP) algorithm convergent to
unique minimum of the so-called Bethe free energy functional. For a general
graphical model on a loopy graph the functional may show multiple minima, the
iterative BP algorithm may converge to one of the minima or may not converge at
all, and the global minimum of the Bethe free energy functional is not
guaranteed to correspond to the optimal Maximum-Likelihood (ML) solution in the
zero-temperature limit. However, there are exceptions to this general rule,
discussed in \cite{05KW} and \cite{08BSS} in two different contexts, where
zero-temperature version of the BP algorithm finds ML solution for special
models on graphs with loops. These two models share a key feature: their ML
solutions can be found by an efficient Linear Programming (LP) algorithm with a
Totally-Uni-Modular (TUM) matrix of constraints. Generalizing the two models we
consider a class of graphical models reducible in the zero temperature limit to
LP with TUM constraints. Assuming that a gedanken algorithm, g-BP, funding the
global minimum of the Bethe free energy is available we show that in the limit
of zero temperature g-BP outputs the ML solution. Our consideration is based on
equivalence established between gapless Linear Programming (LP) relaxation of
the graphical model in the $T\to 0$ limit and respective LP version of the
Bethe-Free energy minimization.



It is generally accepted that human vision is an extremely powerful
information processing system that facilitates our interaction with the
surrounding world. However, despite extended and extensive research efforts,
which encompass many exploration fields, the underlying fundamentals and
operational principles of visual information processing in human brain remain
unknown. We still are unable to figure out where and how along the path from
eyes to the cortex the sensory input perceived by the retina is converted into
a meaningful object representation, which can be consciously manipulated by the
brain. Studying the vast literature considering the various aspects of brain
information processing, I was surprised to learn that the respected scholarly
discussion is totally indifferent to the basic keynote question: "What is
information?" in general or "What is visual information?" in particular. In the
old days, it was assumed that any scientific research approach has first to
define its basic departure points. Why was it overlooked in brain information
processing research remains a conundrum. In this paper, I am trying to find a
remedy for this bizarre situation. I propose an uncommon definition of
"information", which can be derived from Kolmogorov's Complexity Theory and
Chaitin's notion of Algorithmic Information. Embracing this new definition
leads to an inevitable revision of traditional dogmas that shape the state of
the art of brain information processing research. I hope this revision would
better serve the challenging goal of human visual information processing
modeling.



The ongoing discussion whether modern vision systems have to be viewed as
visually-enabled cognitive systems or cognitively-enabled vision systems is
groundless, because perceptual and cognitive faculties of vision are separate
components of human (and consequently, artificial) information processing
system modeling.



We show how universal codes can be used for solving some of the most
important statistical problems for time series. By definition, a universal code
(or a universal lossless data compressor) can compress any sequence generated
by a stationary and ergodic source asymptotically to the Shannon entropy,
which, in turn, is the best achievable ratio for lossless data compressors.
  We consider finite-alphabet and real-valued time series and the following
problems: estimation of the limiting probabilities for finite-alphabet time
series and estimation of the density for real-valued time series, the on-line
prediction, regression, classification (or problems with side information) for
both types of the time series and the following problems of hypothesis testing:
goodness-of-fit testing, or identity testing, and testing of serial
independence. It is important to note that all problems are considered in the
framework of classical mathematical statistics and, on the other hand, everyday
methods of data compression (or archivers) can be used as a tool for the
estimation and testing. It turns out, that quite often the suggested methods
and tests are more powerful than known ones when they are applied in practice.



In this paper we analyse Belief Propagation over a Gaussian model in a
dynamic environment. Recently, this has been proposed as a method to average
local measurement values by a distributed protocol ("Consensus Propagation",
Moallemi & Van Roy, 2006), where the average is available for read-out at every
single node. In the case that the underlying network is constant but the values
to be averaged fluctuate ("dynamic data"), convergence and accuracy are
determined by the spectral properties of an associated Ruelle-Perron-Frobenius
operator. For Gaussian models on Erdos-Renyi graphs, numerical computation
points to a spectral gap remaining in the large-size limit, implying
exceptionally good scalability. In a model where the underlying network also
fluctuates ("dynamic network"), averaging is more effective than in the dynamic
data case. Altogether, this implies very good performance of these methods in
very large systems, and opens a new field of statistical physics of large (and
dynamic) information systems.



The aim of this work is to address the question of whether we can in
principle design rational decision-making agents or artificial intelligences
embedded in computable physics such that their decisions are optimal in
reasonable mathematical senses. Recent developments in rare event probability
estimation, recursive bayesian inference, neural networks, and probabilistic
planning are sufficient to explicitly approximate reinforcement learners of the
AIXI style with non-trivial model classes (here, the class of resource-bounded
Turing machines). Consideration of the effects of resource limitations in a
concrete implementation leads to insights about possible architectures for
learning systems using optimal decision makers as components.



In density estimation task, maximum entropy model (Maxent) can effectively
use reliable prior information via certain constraints, i.e., linear
constraints without empirical parameters. However, reliable prior information
is often insufficient, and the selection of uncertain constraints becomes
necessary but poses considerable implementation complexity. Improper setting of
uncertain constraints can result in overfitting or underfitting. To solve this
problem, a generalization of Maxent, under Tsallis entropy framework, is
proposed. The proposed method introduces a convex quadratic constraint for the
correction of (expected) Tsallis entropy bias (TEB). Specifically, we
demonstrate that the expected Tsallis entropy of sampling distributions is
smaller than the Tsallis entropy of the underlying real distribution. This
expected entropy reduction is exactly the (expected) TEB, which can be
expressed by a closed-form formula and act as a consistent and unbiased
correction. TEB indicates that the entropy of a specific sampling distribution
should be increased accordingly. This entails a quantitative re-interpretation
of the Maxent principle. By compensating TEB and meanwhile forcing the
resulting distribution to be close to the sampling distribution, our
generalized TEBC Maxent can be expected to alleviate the overfitting and
underfitting. We also present a connection between TEB and Lidstone estimator.
As a result, TEB-Lidstone estimator is developed by analytically identifying
the rate of probability correction in Lidstone. Extensive empirical evaluation
shows promising performance of both TEBC Maxent and TEB-Lidstone in comparison
with various state-of-the-art density estimation methods.



In this paper, we propose a novel policy iteration method, called dynamic
policy programming (DPP), to estimate the optimal policy in the
infinite-horizon Markov decision processes. We prove the finite-iteration and
asymptotic l\infty-norm performance-loss bounds for DPP in the presence of
approximation/estimation error. The bounds are expressed in terms of the
l\infty-norm of the average accumulated error as opposed to the l\infty-norm of
the error in the case of the standard approximate value iteration (AVI) and the
approximate policy iteration (API). This suggests that DPP can achieve a better
performance than AVI and API since it averages out the simulation noise caused
by Monte-Carlo sampling throughout the learning process. We examine this
theoretical results numerically by com- paring the performance of the
approximate variants of DPP with existing reinforcement learning (RL) methods
on different problem domains. Our results show that, in all cases, DPP-based
algorithms outperform other RL methods by a wide margin.



We study the convergence of Markov Decision Processes made of a large number
of objects to optimization problems on ordinary differential equations (ODE).
We show that the optimal reward of such a Markov Decision Process, satisfying a
Bellman equation, converges to the solution of a continuous
Hamilton-Jacobi-Bellman (HJB) equation based on the mean field approximation of
the Markov Decision Process. We give bounds on the difference of the rewards,
and a constructive algorithm for deriving an approximating solution to the
Markov Decision Process from a solution of the HJB equations. We illustrate the
method on three examples pertaining respectively to investment strategies,
population dynamics control and scheduling in queues are developed. They are
used to illustrate and justify the construction of the controlled ODE and to
show the gain obtained by solving a continuous HJB equation rather than a large
discrete Bellman equation.



The theoretical limits of 'lossy' data compression algorithms are considered.
The complexity of an object as seen by a macroscopic observer is the size of
the perceptual code which discards all information that can be lost without
altering the perception of the specified observer. The complexity of this
macroscopically observed state is the simplest description of any microstate
comprising that macrostate. Inference and pattern recognition based on
macrostate rather than microstate complexities will take advantage of the
complexity of the macroscopic observer to ignore irrelevant noise.



Computational Intelligence (CI) is a sub-branch of Artificial Intelligence
paradigm focusing on the study of adaptive mechanisms to enable or facilitate
intelligent behavior in complex and changing environments. There are several
paradigms of CI [like artificial neural networks, evolutionary computations,
swarm intelligence, artificial immune systems, fuzzy systems and many others],
each of these has its origins in biological systems [biological neural systems,
natural Darwinian evolution, social behavior, immune system, interactions of
organisms with their environment]. Most of those paradigms evolved into
separate machine learning (ML) techniques, where probabilistic methods are used
complementary with CI techniques in order to effectively combine elements of
learning, adaptation, evolution and Fuzzy logic to create heuristic algorithms
that are, in some sense, intelligent. The current trend is to develop consensus
techniques, since no single machine learning algorithms is superior to others
in all possible situations. In order to overcome this problem several
meta-approaches were proposed in ML focusing on the integration of results from
different methods into single prediction. We discuss here the Landau theory for
the nonlinear equation that can describe the adaptive integration of
information acquired from an ensemble of independent learning agents. The
influence of each individual agent on other learners is described similarly to
the social impact theory. The final decision outcome for the consensus system
is calculated using majority rule in the stationary limit, yet the minority
solutions can survive inside the majority population as the complex
intermittent clusters of opposite opinion.



This study presents a method to discover an outbreak of an infectious disease
in a region for which data are missing, but which is at work as a disease
spreader. Node discovery for the spread of an infectious disease is defined as
discriminating between the nodes which are neighboring to a missing disease
spreader node, and the rest, given a dataset on the number of cases. The spread
is described by stochastic differential equations. A perturbation theory
quantifies the impact of the missing spreader on the moments of the number of
cases. Statistical discriminators examine the mid-body or tail-ends of the
probability density function, and search for the disturbance from the missing
spreader. They are tested with computationally synthesized datasets, and
applied to the SARS outbreak and flu pandemic.



Transactional network data can be thought of as a list of one-to-many
communications(e.g., email) between nodes in a social network. Most social
network models convert this type of data into binary relations between pairs of
nodes. We develop a latent mixed membership model capable of modeling richer
forms of transactional network data, including relations between more than two
nodes. The model can cluster nodes and predict transactions. The block-model
nature of the model implies that groups can be characterized in very general
ways. This flexible notion of group structure enables discovery of rich
structure in transactional networks. Estimation and inference are accomplished
via a variational EM algorithm. Simulations indicate that the learning
algorithm can recover the correct generative model. Interesting structure is
discovered in the Enron email dataset and another dataset extracted from the
Reddit website. Analysis of the Reddit data is facilitated by a novel
performance measure for comparing two soft clusterings. The new model is
superior at discovering mixed membership in groups and in predicting
transactions.



The Monty Hall problem is the TV game scenario where you, the contestant, are
presented with three doors, with a car hidden behind one and goats hidden
behind the other two. After you select a door, the host (Monty Hall) opens a
second door to reveal a goat. You are then invited to stay with your original
choice of door, or to switch to the remaining unopened door, and claim whatever
you find behind it. Assuming your objective is to win the car, is your best
strategy to stay or switch, or does it not matter? Jason Rosenhouse has
provided the definitive analysis of this game, along with several intriguing
variations, and discusses some of its psychological and philosophical
implications. This extended review examines several themes from the book in
some detail from a Bayesian perspective, and points out one apparently
inadvertent error.



Predicting the occurrence of links is a fundamental problem in networks. In
the link prediction problem we are given a snapshot of a network and would like
to infer which interactions among existing members are likely to occur in the
near future or which existing interactions are we missing. Although this
problem has been extensively studied, the challenge of how to effectively
combine the information from the network structure with rich node and edge
attribute data remains largely open.
  We develop an algorithm based on Supervised Random Walks that naturally
combines the information from the network structure with node and edge level
attributes. We achieve this by using these attributes to guide a random walk on
the graph. We formulate a supervised learning task where the goal is to learn a
function that assigns strengths to edges in the network such that a random
walker is more likely to visit the nodes to which new links will be created in
the future. We develop an efficient training algorithm to directly learn the
edge strength estimation function.
  Our experiments on the Facebook social graph and large collaboration networks
show that our approach outperforms state-of-the-art unsupervised approaches as
well as approaches that are based on feature extraction.



We study decision making in environments where the reward is only partially
observed, but can be modeled as a function of an action and an observed
context. This setting, known as contextual bandits, encompasses a wide variety
of applications including health-care policy and Internet advertising. A
central task is evaluation of a new policy given historic data consisting of
contexts, actions and received rewards. The key challenge is that the past data
typically does not faithfully represent proportions of actions taken by a new
policy. Previous approaches rely either on models of rewards or models of the
past policy. The former are plagued by a large bias whereas the latter have a
large variance.
  In this work, we leverage the strength and overcome the weaknesses of the two
approaches by applying the doubly robust technique to the problems of policy
evaluation and optimization. We prove that this approach yields accurate value
estimates when we have either a good (but not necessarily consistent) model of
rewards or a good (but not necessarily consistent) model of past policy.
Extensive empirical comparison demonstrates that the doubly robust approach
uniformly improves over existing techniques, achieving both lower variance in
value estimation and better policies. As such, we expect the doubly robust
approach to become common practice.



Many widely studied graphical models with latent variables lead to nontrivial
constraints on the distribution of the observed variables. Inspired by the Bell
inequalities in quantum mechanics, we refer to any linear inequality whose
violation rules out some latent variable model as a "hidden variable test" for
that model. Our main contribution is to introduce a sequence of relaxations
which provides progressively tighter hidden variable tests. We demonstrate
applicability to mixtures of sequences of i.i.d. variables, Bell inequalities,
and homophily models in social networks. For the last, we demonstrate that our
method provides a test that is able to rule out latent homophily as the sole
explanation for correlations on a real social network that are known to be due
to influence.



This work gives a simultaneous analysis of both the ordinary least squares
estimator and the ridge regression estimator in the random design setting under
mild assumptions on the covariate/response distributions. In particular, the
analysis provides sharp results on the ``out-of-sample'' prediction error, as
opposed to the ``in-sample'' (fixed design) error. The analysis also reveals
the effect of errors in the estimated covariance structure, as well as the
effect of modeling errors, neither of which effects are present in the fixed
design setting. The proofs of the main results are based on a simple
decomposition lemma combined with concentration inequalities for random vectors
and matrices.



Probabilistic principal component analysis (PPCA) seeks a low dimensional
representation of a data set in the presence of independent spherical Gaussian
noise, Sigma = (sigma^2)*I. The maximum likelihood solution for the model is an
eigenvalue problem on the sample covariance matrix. In this paper we consider
the situation where the data variance is already partially explained by other
factors, e.g. covariates of interest, or temporal correlations leaving some
residual variance. We decompose the residual variance into its components
through a generalized eigenvalue problem, which we call residual component
analysis (RCA). We show that canonical covariates analysis (CCA) is a special
case of our algorithm and explore a range of new algorithms that arise from the
framework. We illustrate the ideas on a gene expression time series data set
and the recovery of human pose from silhouette.



Prediction markets show considerable promise for developing flexible
mechanisms for machine learning. Here, machine learning markets for
multivariate systems are defined, and a utility-based framework is established
for their analysis. This differs from the usual approach of defining static
betting functions. It is shown that such markets can implement model
combination methods used in machine learning, such as product of expert and
mixture of expert approaches as equilibrium pricing models, by varying agent
utility functions. They can also implement models composed of local potentials,
and message passing methods. Prediction markets also allow for more flexible
combinations, by combining multiple different utility functions. Conversely,
the market mechanisms implement inference in the relevant probabilistic models.
This means that market mechanism can be utilized for implementing parallelized
model building and inference for probabilistic modelling.



An important problem in bioinformatics is the inference of gene regulatory
networks (GRN) from temporal expression profiles. In general, the main
limitations faced by GRN inference methods is the small number of samples with
huge dimensionalities and the noisy nature of the expression measurements. In
face of these limitations, alternatives are needed to get better accuracy on
the GRNs inference problem. This work addresses this problem by presenting an
alternative feature selection method that applies prior knowledge on its search
strategy, called SFFS-BA. The proposed search strategy is based on the
Sequential Floating Forward Selection (SFFS) algorithm, with the inclusion of a
scale-free (Barab\'asi-Albert) topology information in order to guide the
search process to improve inference. The proposed algorithm explores the
scale-free property by pruning the search space and using a power law as a
weight for reducing it. In this way, the search space traversed by the SFFS-BA
method combines a breadth-first search when the number of combinations is small
(<k> <= 2) with a depth-first search when the number of combinations becomes
explosive (<k> >= 3), being guided by the scale-free prior information.
Experimental results show that the SFFS-BA provides a better inference
similarities than SFS and SFFS, keeping the robustness of the SFS and SFFS
methods, thus presenting very good results.



PAQ8 is an open source lossless data compression algorithm that currently
achieves the best compression rates on many benchmarks. This report presents a
detailed description of PAQ8 from a statistical machine learning perspective.
It shows that it is possible to understand some of the modules of PAQ8 and use
this understanding to improve the method. However, intuitive statistical
explanations of the behavior of other modules remain elusive. We hope the
description in this report will be a starting point for discussions that will
increase our understanding, lead to improvements to PAQ8, and facilitate a
transfer of knowledge from PAQ8 to other machine learning methods, such a
recurrent neural networks and stochastic memoizers. Finally, the report
presents a broad range of new applications of PAQ to machine learning tasks
including language modeling and adaptive text prediction, adaptive game
playing, classification, and compression using features from the field of deep
learning.



Markov Random Field models are powerful tools for the study of complex
systems. However, little is known about how the interactions between the
elements of such systems are encoded, especially from an information-theoretic
perspective. In this paper, our goal is to enlight the connection between
Fisher information, Shannon entropy, information geometry and the behavior of
complex systems modeled by isotropic pairwise Gaussian Markov random fields. We
propose analytical expressions to compute local and global versions of these
measures using Besag's pseudo-likelihood function, characterizing the system's
behavior through its \emph{Fisher curve}, a parametric trajectory accross the
information space that provides a geometric representation for the study of
complex systems. Computational experiments show how the proposed tools can be
useful in extrating relevant information from complex patterns. The obtained
results quantify and support our main conclusion, which is: in terms of
information, moving towards higher entropy states (A --> B) is different from
moving towards lower entropy states (B --> A), since the \emph{Fisher curves}
are not the same given a natural orientation (the direction of time).



We construct a class of real-valued nonnegative binary functions on a set of
jointly distributed random variables, which satisfy the triangle inequality and
vanish at identical arguments (pseudo-quasi-metrics). These functions are
useful in dealing with the problem of selective probabilistic causality
encountered in behavioral sciences and in quantum physics. The problem reduces
to that of ascertaining the existence of a joint distribution for a set of
variables with known distributions of certain subsets of this set. Any
violation of the triangle inequality or its consequences by one of our
functions when applied to such a set rules out the existence of this joint
distribution. We focus on an especially versatile and widely applicable
pseudo-quasi-metric called an order-distance and its special case called a
classification distance.



We study the performance of different message passing algorithms in the two
dimensional Edwards Anderson model. We show that the standard Belief
Propagation (BP) algorithm converges only at high temperature to a paramagnetic
solution. Then, we test a Generalized Belief Propagation (GBP) algorithm,
derived from a Cluster Variational Method (CVM) at the plaquette level. We
compare its performance with BP and with other algorithms derived under the
same approximation: Double Loop (DL) and a two-ways message passing algorithm
(HAK). The plaquette-CVM approximation improves BP in at least three ways: the
quality of the paramagnetic solution at high temperatures, a better estimate
(lower) for the critical temperature, and the fact that the GBP message passing
algorithm converges also to non paramagnetic solutions. The lack of convergence
of the standard GBP message passing algorithm at low temperatures seems to be
related to the implementation details and not to the appearance of long range
order. In fact, we prove that a gauge invariance of the constrained CVM free
energy can be exploited to derive a new message passing algorithm which
converges at even lower temperatures. In all its region of convergence this new
algorithm is faster than HAK and DL by some orders of magnitude.



In this work we introduce a novel approach, based on sampling, for finding
assignments that are likely to be solutions to stochastic constraint
satisfaction problems and constraint optimisation problems. Our approach
reduces the size of the original problem being analysed; by solving this
reduced problem, with a given confidence probability, we obtain assignments
that satisfy the chance constraints in the original model within prescribed
error tolerance thresholds. To achieve this, we blend concepts from stochastic
constraint programming and statistics. We discuss both exact and approximate
variants of our method. The framework we introduce can be immediately employed
in concert with existing approaches for solving stochastic constraint programs.
A thorough computational study on a number of stochastic combinatorial
optimisation problems demonstrates the effectiveness of our approach.



The success of an agent mediated e-market system lies in the underlying
reputation management system to improve the quality of services in an
information asymmetric e-market. Reputation provides an operatable metric for
establishing trustworthiness between mutually unknown online entities.
Reputation systems encourage honest behaviour and discourage malicious
behaviour of participating agents in the e-market. A dynamic reputation model
would provide virtually instantaneous knowledge about the changing e-market
environment and would utilise Internets' capacity for continuous interactivity
for reputation computation. This paper proposes a dynamic reputation framework
using reinforcement learning and fuzzy set theory that ensures judicious use of
information sharing for inter-agent cooperation. This framework is sensitive to
the changing parameters of e-market like the value of transaction and the
varying experience of agents with the purpose of improving inbuilt defense
mechanism of the reputation system against various attacks so that e-market
reaches an equilibrium state and dishonest agents are weeded out of the market.



Undirected graphical models are widely used in statistics, physics and
machine vision. However Bayesian parameter estimation for undirected models is
extremely challenging, since evaluation of the posterior typically involves the
calculation of an intractable normalising constant. This problem has received
much attention, but very little of this has focussed on the important practical
case where the data consists of noisy or incomplete observations of the
underlying hidden structure. This paper specifically addresses this problem,
comparing two alternative methodologies. In the first of these approaches
particle Markov chain Monte Carlo (Andrieu et al., 2010) is used to efficiently
explore the parameter space, combined with the exchange algorithm (Murray et
al., 2006) for avoiding the calculation of the intractable normalising constant
(a proof showing that this combination targets the correct distribution in
found in a supplementary appendix online). This approach is compared with
approximate Bayesian computation (Pritchard et al., 1999). Applications to
estimating the parameters of Ising models and exponential random graphs from
noisy data are presented. Each algorithm used in the paper targets an
approximation to the true posterior due to the use of MCMC to simulate from the
latent graphical model, in lieu of being able to do this exactly in general.
The supplementary appendix also describes the nature of the resulting
approximation.



The problem of structure estimation in graphical models with latent variables
is considered. We characterize conditions for tractable graph estimation and
develop efficient methods with provable guarantees. We consider models where
the underlying Markov graph is locally tree-like, and the model is in the
regime of correlation decay. For the special case of the Ising model, the
number of samples $n$ required for structural consistency of our method scales
as $n=\Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where p is the
number of variables, $\theta_{\min}$ is the minimum edge potential, $\delta$ is
the depth (i.e., distance from a hidden node to the nearest observed nodes),
and $\eta$ is a parameter which depends on the bounds on node and edge
potentials in the Ising model. Necessary conditions for structural consistency
under any algorithm are derived and our method nearly matches the lower bound
on sample requirements. Further, the proposed method is practical to implement
and provides flexibility to control the number of latent variables and the
cycle lengths in the output graph.



The problem of modeling and predicting spatiotemporal traffic phenomena over
an urban road network is important to many traffic applications such as
detecting and forecasting congestion hotspots. This paper presents a
decentralized data fusion and active sensing (D2FAS) algorithm for mobile
sensors to actively explore the road network to gather and assimilate the most
informative data for predicting the traffic phenomenon. We analyze the time and
communication complexity of D2FAS and demonstrate that it can scale well with a
large number of observations and sensors. We provide a theoretical guarantee on
its predictive performance to be equivalent to that of a sophisticated
centralized sparse approximation for the Gaussian process (GP) model: The
computation of such a sparse approximate GP model can thus be parallelized and
distributed among the mobile sensors (in a Google-like MapReduce paradigm),
thereby achieving efficient and scalable prediction. We also theoretically
guarantee its active sensing performance that improves under various practical
environmental conditions. Empirical evaluation on real-world urban road network
data shows that our D2FAS algorithm is significantly more time-efficient and
scalable than state-of-the-art centralized algorithms while achieving
comparable predictive performance.



Using the theory of group action, we first introduce the concept of the
automorphism group of an exponential family or a graphical model, thus
formalizing the general notion of symmetry of a probabilistic model. This
automorphism group provides a precise mathematical framework for lifted
inference in the general exponential family. Its group action partitions the
set of random variables and feature functions into equivalent classes (called
orbits) having identical marginals and expectations. Then the inference problem
is effectively reduced to that of computing marginals or expectations for each
class, thus avoiding the need to deal with each individual variable or feature.
We demonstrate the usefulness of this general framework in lifting two classes
of variational approximation for MAP inference: local LP relaxation and local
LP relaxation with cycle constraints; the latter yields the first lifted
inference that operate on a bound tighter than local constraints. Initial
experimental results demonstrate that lifted MAP inference with cycle
constraints achieved the state of the art performance, obtaining much better
objective function values than local approximation while remaining relatively
efficient.



The aim of a Content-Based Image Retrieval (CBIR) system, also known as Query
by Image Content (QBIC), is to help users to retrieve relevant images based on
their contents. CBIR technologies provide a method to find images in large
databases by using unique descriptors from a trained image. The image
descriptors include texture, color, intensity and shape of the object inside an
image. Several feature-extraction techniques viz., Average RGB, Color Moments,
Co-occurrence, Local Color Histogram, Global Color Histogram and Geometric
Moment have been critically compared in this paper. However, individually these
techniques result in poor performance. So, combinations of these techniques
have also been evaluated and results for the most efficient combination of
techniques have been presented and optimized for each class of image query. We
also propose an improvement in image retrieval performance by introducing the
idea of Query modification through image cropping. It enables the user to
identify a region of interest and modify the initial query to refine and
personalize the image retrieval results.



This work shows how to leverage causal inference to understand the behavior
of complex learning systems interacting with their environment and predict the
consequences of changes to the system. Such predictions allow both humans and
algorithms to select changes that improve both the short-term and long-term
performance of such systems. This work is illustrated by experiments carried
out on the ad placement system associated with the Bing search engine.



Automated reasoning about uncertain knowledge has many applications. One
difficulty when developing such systems is the lack of a completely
satisfactory integration of logic and probability. We address this problem
directly. Expressive languages like higher-order logic are ideally suited for
representing and reasoning about structured knowledge. Uncertain knowledge can
be modeled by using graded probabilities rather than binary truth-values. The
main technical problem studied in this paper is the following: Given a set of
sentences, each having some probability of being true, what probability should
be ascribed to other (query) sentences? A natural wish-list, among others, is
that the probability distribution (i) is consistent with the knowledge base,
(ii) allows for a consistent inference procedure and in particular (iii)
reduces to deductive logic in the limit of probabilities being 0 and 1, (iv)
allows (Bayesian) inductive reasoning and (v) learning in the limit and in
particular (vi) allows confirmation of universally quantified
hypotheses/sentences. We translate this wish-list into technical requirements
for a prior probability and show that probabilities satisfying all our criteria
exist. We also give explicit constructions and several general
characterizations of probabilities that satisfy some or all of the criteria and
various (counter) examples. We also derive necessary and sufficient conditions
for extending beliefs about finitely many sentences to suitable probabilities
over all sentences, and in particular least dogmatic or least biased ones. We
conclude with a brief outlook on how the developed theory might be used and
approximated in autonomous reasoning agents. Our theory is a step towards a
globally consistent and empirically satisfactory unification of probability and
logic.



We investigate cortical learning from the perspective of mechanism design.
First, we show that discretizing standard models of neurons and synaptic
plasticity leads to rational agents maximizing simple scoring rules. Second,
our main result is that the scoring rules are proper, implying that neurons
faithfully encode expected utilities in their synaptic weights and encode
high-scoring outcomes in their spikes. Third, with this foundation in hand, we
propose a biologically plausible mechanism whereby neurons backpropagate
incentives which allows them to optimize their usefulness to the rest of
cortex. Finally, experiments show that networks that backpropagate incentives
can learn simple tasks.



A highly comparative, feature-based approach to time series classification is
introduced that uses an extensive database of algorithms to extract thousands
of interpretable features from time series. These features are derived from
across the scientific time-series analysis literature, and include summaries of
time series in terms of their correlation structure, distribution, entropy,
stationarity, scaling properties, and fits to a range of time-series models.
After computing thousands of features for each time series in a training set,
those that are most informative of the class structure are selected using
greedy forward feature selection with a linear classifier. The resulting
feature-based classifiers automatically learn the differences between classes
using a reduced number of time-series properties, and circumvent the need to
calculate distances between time series. Representing time series in this way
results in orders of magnitude of dimensionality reduction, allowing the method
to perform well on very large datasets containing long time series or time
series of different lengths. For many of the datasets studied, classification
performance exceeded that of conventional instance-based classifiers, including
one nearest neighbor classifiers using Euclidean distances and dynamic time
warping and, most importantly, the features selected provide an understanding
of the properties of the dataset, insight that can guide further scientific
investigation.



We marry ideas from deep neural networks and approximate Bayesian inference
to derive a generalised class of deep, directed generative models, endowed with
a new algorithm for scalable inference and learning. Our algorithm introduces a
recognition model to represent approximate posterior distributions, and that
acts as a stochastic encoder of the data. We develop stochastic
back-propagation -- rules for back-propagation through stochastic variables --
and use this to develop an algorithm that allows for joint optimisation of the
parameters of both the generative and recognition model. We demonstrate on
several real-world data sets that the model generates realistic samples,
provides accurate imputations of missing data and is a useful tool for
high-dimensional data visualisation.



Given a matrix $A$, a linear feasibility problem (of which linear
classification is a special case) aims to find a solution to a primal problem
$w: A^Tw > \textbf{0}$ or a certificate for the dual problem which is a
probability distribution $p: Ap = \textbf{0}$. Inspired by the continued
importance of "large-margin classifiers" in machine learning, this paper
studies a condition measure of $A$ called its \textit{margin} that determines
the difficulty of both the above problems. To aid geometrical intuition, we
first establish new characterizations of the margin in terms of relevant balls,
cones and hulls. Our second contribution is analytical, where we present
generalizations of Gordan's theorem, and variants of Hoffman's theorems, both
using margins. We end by proving some new results on a classical iterative
scheme, the Perceptron, whose convergence rates famously depends on the margin.
Our results are relevant for a deeper understanding of margin-based learning
and proving convergence rates of iterative schemes, apart from providing a
unifying perspective on this vast topic.



Controlling sensori-motor systems in higher animals or complex robots is a
challenging combinatorial problem, because many sensory signals need to be
simultaneously coordinated into a broad behavioural spectrum. To rapidly
interact with the environment, this control needs to be fast and adaptive.
Current robotic solutions operate with limited autonomy and are mostly
restricted to few behavioural patterns. Here we introduce chaos control as a
new strategy to generate complex behaviour of an autonomous robot. In the
presented system, 18 sensors drive 18 motors via a simple neural control
circuit, thereby generating 11 basic behavioural patterns (e.g., orienting,
taxis, self-protection, various gaits) and their combinations. The control
signal quickly and reversibly adapts to new situations and additionally enables
learning and synaptic long-term storage of behaviourally useful motor
responses. Thus, such neural control provides a powerful yet simple way to
self-organize versatile behaviours in autonomous agents with many degrees of
freedom.



Recently, there has been a renewed interest in the machine learning community
for variants of a sparse greedy approximation procedure for concave
optimization known as {the Frank-Wolfe (FW) method}. In particular, this
procedure has been successfully applied to train large-scale instances of
non-linear Support Vector Machines (SVMs). Specializing FW to SVM training has
allowed to obtain efficient algorithms but also important theoretical results,
including convergence analysis of training algorithms and new characterizations
of model sparsity.
  In this paper, we present and analyze a novel variant of the FW method based
on a new way to perform away steps, a classic strategy used to accelerate the
convergence of the basic FW procedure. Our formulation and analysis is focused
on a general concave maximization problem on the simplex. However, the
specialization of our algorithm to quadratic forms is strongly related to some
classic methods in computational geometry, namely the Gilbert and MDM
algorithms.
  On the theoretical side, we demonstrate that the method matches the
guarantees in terms of convergence rate and number of iterations obtained by
using classic away steps. In particular, the method enjoys a linear rate of
convergence, a result that has been recently proved for MDM on quadratic forms.
  On the practical side, we provide experiments on several classification
datasets, and evaluate the results using statistical tests. Experiments show
that our method is faster than the FW method with classic away steps, and works
well even in the cases in which classic away steps slow down the algorithm.
Furthermore, these improvements are obtained without sacrificing the predictive
accuracy of the obtained SVM model.



A constraint satisfaction problem (CSP) is a computational problem where the
input consists of a finite set of variables and a finite set of constraints,
and where the task is to decide whether there exists a satisfying assignment of
values to the variables. Depending on the type of constraints that we allow in
the input, a CSP might be tractable, or computationally hard. In recent years,
general criteria have been discovered that imply that a CSP is polynomial-time
tractable, or that it is NP-hard. Finite-domain CSPs have become a major common
research focus of graph theory, artificial intelligence, and finite model
theory. It turned out that the key questions for complexity classification of
CSPs are closely linked to central questions in universal algebra.
  This thesis studies CSPs where the variables can take values from an infinite
domain. This generalization enhances dramatically the range of computational
problems that can be modeled as a CSP. Many problems from areas that have so
far seen no interaction with constraint satisfaction theory can be formulated
using infinite domains, e.g. problems from temporal and spatial reasoning,
phylogenetic reconstruction, and operations research.
  It turns out that the universal-algebraic approach can also be applied to
study large classes of infinite-domain CSPs, yielding elegant complexity
classification results. A new tool in this thesis that becomes relevant
particularly for infinite domains is Ramsey theory. We demonstrate the
feasibility of our approach with two complete complexity classification
results: one on CSPs in temporal reasoning, the other on a generalization of
Schaefer's theorem for propositional logic to logic over graphs. We also study
the limits of complexity classification, and present classes of computational
problems provably do not exhibit a complexity dichotomy into hard and easy
problems.



A number of representation schemes have been presented for use within
learning classifier systems, ranging from binary encodings to neural networks.
This paper presents results from an investigation into using discrete and fuzzy
dynamical system representations within the XCSF learning classifier system. In
particular, asynchronous random Boolean networks are used to represent the
traditional condition-action production system rules in the discrete case and
asynchronous fuzzy logic networks in the continuous-valued case. It is shown
possible to use self-adaptive, open-ended evolution to design an ensemble of
such dynamical systems within XCSF to solve a number of well-known test
problems.



Marginal MAP problems are notoriously difficult tasks for graphical models.
We derive a general variational framework for solving marginal MAP problems, in
which we apply analogues of the Bethe, tree-reweighted, and mean field
approximations. We then derive a "mixed" message passing algorithm and a
convergent alternative using CCCP to solve the BP-type approximations.
Theoretically, we give conditions under which the decoded solution is a global
or local optimum, and obtain novel upper bounds on solutions. Experimentally we
demonstrate that our algorithms outperform related approaches. We also show
that EM and variational EM comprise a special case of our framework.



There are various approaches to exploiting "hidden structure" in instances of
hard combinatorial problems to allow faster algorithms than for general
unstructured or random instances. For SAT and its counting version #SAT, hidden
structure has been exploited in terms of decomposability and strong backdoor
sets. Decomposability can be considered in terms of the treewidth of a graph
that is associated with the given CNF formula, for instance by considering
clauses and variables as vertices of the graph, and making a variable adjacent
with all the clauses it appears in. On the other hand, a strong backdoor set of
a CNF formula is a set of variables such that each possible partial assignment
to this set moves the formula into a fixed class for which (#)SAT can be solved
in polynomial time.
  In this paper we combine the two above approaches. In particular, we study
the algorithmic question of finding a small strong backdoor set into the class
W_t of CNF formulas whose associated graphs have treewidth at most t. The main
results are positive:
  (1) There is a cubic-time algorithm that, given a CNF formula F and two
constants k,t\ge 0, either finds a strong W_t-backdoor set of size at most 2^k,
or concludes that F has no strong W_t-backdoor set of size at most k.
  (2) There is a cubic-time algorithm that, given a CNF formula F, computes the
number of satisfying assignments of F or concludes that sb_t(F)>k, for any pair
of constants k,t\ge 0. Here, sb_t(F) denotes the size of a smallest strong
W_t-backdoor set of F.
  The significance of our results lies in the fact that they allow us to
exploit algorithmically a hidden structure in formulas that is not accessible
by any one of the two approaches (decomposability, backdoors) alone. Already a
backdoor size 1 on top of treewidth 1 (i.e., sb_1(F)=1) entails formulas of
arbitrarily large treewidth and arbitrarily large cycle cutsets.



Recent works have validated the possibility of improving energy efficiency in
radio access networks (RANs), achieved by dynamically turning on/off some base
stations (BSs). In this paper, we extend the research over BS switching
operations, which should match up with traffic load variations. Instead of
depending on the dynamic traffic loads which are still quite challenging to
precisely forecast, we firstly formulate the traffic variations as a Markov
decision process. Afterwards, in order to foresightedly minimize the energy
consumption of RANs, we design a reinforcement learning framework based BS
switching operation scheme. Furthermore, to avoid the underlying curse of
dimensionality in reinforcement learning, a transfer actor-critic algorithm
(TACT), which utilizes the transferred learning expertise in historical periods
or neighboring regions, is proposed and provably converges. In the end, we
evaluate our proposed scheme by extensive simulations under various practical
configurations and show that the proposed TACT algorithm contributes to a
performance jumpstart and demonstrates the feasibility of significant energy
efficiency improvement at the expense of tolerable delay performance.



Submodular functions have many applications. Matchings have many
applications. The bitext word alignment problem can be modeled as the problem
of maximizing a nonnegative, monotone, submodular function constrained to
matchings in a complete bipartite graph where each vertex corresponds to a word
in the two input sentences and each edge represents a potential word-to-word
translation. We propose a more general problem of maximizing a nonnegative,
monotone, submodular function defined on the edge set of a complete graph
constrained to matchings; we call this problem the CSM-Matching problem.
CSM-Matching also generalizes the maximum-weight matching problem, which has a
polynomial-time algorithm; however, we show that it is NP-hard to approximate
CSM-Matching within a factor of e/(e-1) by reducing the max k-cover problem to
it. Our main result is a simple, greedy, 3-approximation algorithm for
CSM-Matching. Then we reduce CSM-Matching to maximizing a nonnegative,
monotone, submodular function over two matroids, i.e., CSM-2-Matroids.
CSM-2-Matroids has a (2+epsilon)-approximation algorithm - called LSV2. We show
that we can find a (4+epsilon)-approximate solution to CSM-Matching using LSV2.
We extend this approach to similar problems.



The modeling of cascade processes in multi-agent systems in the form of
complex networks has in recent years become an important topic of study due to
its many applications: the adoption of commercial products, spread of disease,
the diffusion of an idea, etc. In this paper, we begin by identifying a
desiderata of seven properties that a framework for modeling such processes
should satisfy: the ability to represent attributes of both nodes and edges, an
explicit representation of time, the ability to represent non-Markovian
temporal relationships, representation of uncertain information, the ability to
represent competing cascades, allowance of non-monotonic diffusion, and
computational tractability. We then present the MANCaLog language, a formalism
based on logic programming that satisfies all these desiderata, and focus on
algorithms for finding minimal models (from which the outcome of cascades can
be obtained) as well as how this formalism can be applied in real world
scenarios. We are not aware of any other formalism in the literature that meets
all of the above requirements.



We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive
Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal
exploration mechanism which allows active learning of inverse models in
high-dimensional redundant robots. This allows a robot to efficiently and
actively learn distributions of parameterized motor skills/policies that solve
a corresponding distribution of parameterized tasks/goals. The architecture
makes the robot sample actively novel parameterized tasks in the task space,
based on a measure of competence progress, each of which triggers low-level
goal-directed learning of the motor policy pa- rameters that allow to solve it.
For both learning and generalization, the system leverages regression
techniques which allow to infer the motor policy parameters corresponding to a
given novel parameterized task, and based on the previously learnt
correspondences between policy and task parameters. We present experiments with
high-dimensional continuous sensorimotor spaces in three different robotic
setups: 1) learning the inverse kinematics in a highly-redundant robotic arm,
2) learning omnidirectional locomotion with motor primitives in a quadruped
robot, 3) an arm learning to control a fishing rod with a flexible wire. We
show that 1) exploration in the task space can be a lot faster than exploration
in the actuator space for learning inverse models in redundant robots; 2)
selecting goals maximizing competence progress creates developmental
trajectories driving the robot to progressively focus on tasks of increasing
complexity and is statistically significantly more efficient than selecting
tasks randomly, as well as more efficient than different standard active motor
babbling methods; 3) this architecture allows the robot to actively discover
which parts of its task space it can learn to reach and which part it cannot.



The marginal maximum a posteriori probability (MAP) estimation problem, which
calculates the mode of the marginal posterior distribution of a subset of
variables with the remaining variables marginalized, is an important inference
problem in many models, such as those with hidden variables or uncertain
parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has
attracted less attention in the literature compared to the joint MAP
(maximization) and marginalization problems. We derive a general dual
representation for marginal MAP that naturally integrates the marginalization
and maximization operations into a joint variational optimization problem,
making it possible to easily extend most or all variational-based algorithms to
marginal MAP. In particular, we derive a set of "mixed-product" message passing
algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product
and a novel "argmax-product" message updates. We also derive a class of
convergent algorithms based on proximal point methods, including one that
transforms the marginal MAP problem into a sequence of standard marginalization
problems. Theoretically, we provide guarantees under which our algorithms give
globally or locally optimal solutions, and provide novel upper bounds on the
optimal objectives. Empirically, we demonstrate that our algorithms
significantly outperform the existing approaches, including a state-of-the-art
algorithm based on local search methods.



Adversarial SAT (AdSAT) is a generalization of the satisfiability (SAT)
problem in which two players try to make a boolean formula true (resp. false)
by controlling their respective sets of variables. AdSAT belongs to a higher
complexity class in the polynomial hierarchy than SAT and therefore the nature
of the critical region and the transition are not easily paralleled to those of
SAT and worth of independent study. AdSAT also provides an upper bound for the
transition threshold of the quantum satisfiability problem (QSAT). We present a
complete algorithm for AdSAT, show that 2-AdSAT is in $\mathbf{P}$, and then
study two stochastic algorithms (simulated annealing and its improved variant)
and compare their performances in detail for 3-AdSAT. Varying the density of
clauses $\alpha$ we find a sharp SAT-UNSAT transition at a critical value whose
upper bound is $\alpha_c \lesssim 1.5$, thus providing a much stricter upper
bound for the QSAT transition than those previously found.



As a present to Mizar on its 40th anniversary, we develop an AI/ATP system
that in 30 seconds of real time on a 14-CPU machine automatically proves 40% of
the theorems in the latest official version of the Mizar Mathematical Library
(MML). This is a considerable improvement over previous performance of large-
theory AI/ATP methods measured on the whole MML. To achieve that, a large suite
of AI/ATP methods is employed and further developed. We implement the most
useful methods efficiently, to scale them to the 150000 formulas in MML. This
reduces the training times over the corpus to 1-3 seconds, allowing a simple
practical deployment of the methods in the online automated reasoning service
for the Mizar users (MizAR).



Approximate dynamic programming (ADP) has proven itself in a wide range of
applications spanning large-scale transportation problems, health care, revenue
management, and energy systems. The design of effective ADP algorithms has many
dimensions, but one crucial factor is the stepsize rule used to update a value
function approximation. Many operations research applications are
computationally intensive, and it is important to obtain good results quickly.
Furthermore, the most popular stepsize formulas use tunable parameters and can
produce very poor results if tuned improperly. We derive a new stepsize rule
that optimizes the prediction error in order to improve the short-term
performance of an ADP algorithm. With only one, relatively insensitive tunable
parameter, the new rule adapts to the level of noise in the problem and
produces faster convergence in numerical experiments.



As robots leave the controlled environments of factories to autonomously
function in more complex, natural environments, they will have to respond to
the inevitable fact that they will become damaged. However, while animals can
quickly adapt to a wide variety of injuries, current robots cannot "think
outside the box" to find a compensatory behavior when damaged: they are limited
to their pre-specified self-sensing abilities, can diagnose only anticipated
failure modes, and require a pre-programmed contingency plan for every type of
potential damage, an impracticality for complex robots. Here we introduce an
intelligent trial and error algorithm that allows robots to adapt to damage in
less than two minutes, without requiring self-diagnosis or pre-specified
contingency plans. Before deployment, a robot exploits a novel algorithm to
create a detailed map of the space of high-performing behaviors: This map
represents the robot's intuitions about what behaviors it can perform and their
value. If the robot is damaged, it uses these intuitions to guide a
trial-and-error learning algorithm that conducts intelligent experiments to
rapidly discover a compensatory behavior that works in spite of the damage.
Experiments reveal successful adaptations for a legged robot injured in five
different ways, including damaged, broken, and missing legs, and for a robotic
arm with joints broken in 14 different ways. This new technique will enable
more robust, effective, autonomous robots, and suggests principles that animals
may use to adapt to injury.



To cope with the high level of ambiguity faced in domains such as Computer
Vision or Natural Language processing, robust prediction methods often search
for a diverse set of high-quality candidate solutions or proposals. In
structured prediction problems, this becomes a daunting task, as the solution
space (image labelings, sentence parses, etc.) is exponentially large. We study
greedy algorithms for finding a diverse subset of solutions in
structured-output spaces by drawing new connections between submodular
functions over combinatorial item sets and High-Order Potentials (HOPs) studied
for graphical models. Specifically, we show via examples that when marginal
gains of submodular diversity functions allow structured representations, this
enables efficient (sub-linear time) approximate maximization by reducing the
greedy augmentation step to inference in a factor graph with appropriately
constructed HOPs. We discuss benefits, tradeoffs, and show that our
constructions lead to significantly better proposals.



By drawing on ideas from optimisation theory, artificial neural networks
(ANN), graph embeddings and sparse representations, I develop a novel
technique, termed SENNS (Sparse Extraction Neural NetworkS), aimed at
addressing the feature extraction problem. The proposed method uses (preferably
deep) ANNs for projecting input attribute vectors to an output space wherein
pairwise distances are maximized for vectors belonging to different classes,
but minimized for those belonging to the same class, while simultaneously
enforcing sparsity on the ANN outputs. The vectors that result from the
projection can then be used as features in any classifier of choice.
Mathematically, I formulate the proposed method as the minimisation of an
objective function which can be interpreted, in the ANN output space, as a
negative factor of the sum of the squares of the pair-wise distances between
output vectors belonging to different classes, added to a positive factor of
the sum of squares of the pair-wise distances between output vectors belonging
to the same classes, plus sparsity and weight decay terms. To derive an
algorithm for minimizing the objective function via gradient descent, I use the
multi-variate version of the chain rule to obtain the partial derivatives of
the function with respect to ANN weights and biases, and find that each of the
required partial derivatives can be expressed as a sum of six terms. As it
turns out, four of those six terms can be computed using the standard back
propagation algorithm; the fifth can be computed via a slight modification of
the standard backpropagation algorithm; while the sixth one can be computed via
simple arithmetic. Finally, I propose experiments on the ARABASE Arabic corpora
of digits and letters, the CMU PIE database of faces, the MNIST digits
database, and other standard machine learning databases.



In this paper an alternative approach to solve uncertain Stochastic
Differential Equation (SDE) is proposed. This uncertainty occurs due to the
involved parameters in system and these are considered as Triangular Fuzzy
Numbers (TFN). Here the proposed fuzzy arithmetic in [2] is used as a tool to
handle Fuzzy Stochastic Differential Equation (FSDE). In particular, a system
of Ito stochastic differential equations is analysed with fuzzy parameters.
Further exact and Euler Maruyama approximation methods with fuzzy values are
demonstrated and solved some standard SDE.



Recent progress in using recurrent neural networks (RNNs) for image
description has motivated the exploration of their application for video
description. However, while images are static, working with videos requires
modeling their dynamic temporal structure and then properly integrating that
information into a natural language description. In this context, we propose an
approach that successfully takes into account both the local and global
temporal structure of videos to produce descriptions. First, our approach
incorporates a spatial temporal 3-D convolutional neural network (3-D CNN)
representation of the short temporal dynamics. The 3-D CNN representation is
trained on video action recognition tasks, so as to produce a representation
that is tuned to human motion and behavior. Second we propose a temporal
attention mechanism that allows to go beyond local temporal modeling and learns
to automatically select the most relevant temporal segments given the
text-generating RNN. Our approach exceeds the current state-of-art for both
BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on
a new, larger and more challenging dataset of paired video and natural language
descriptions.



We consider the problem of learning causal networks with interventions, when
each intervention is limited in size under Pearl's Structural Equation Model
with independent errors (SEM-IE). The objective is to minimize the number of
experiments to discover the causal directions of all the edges in a causal
graph. Previous work has focused on the use of separating systems for complete
graphs for this task. We prove that any deterministic adaptive algorithm needs
to be a separating system in order to learn complete graphs in the worst case.
In addition, we present a novel separating system construction, whose size is
close to optimal and is arguably simpler than previous work in combinatorics.
We also develop a novel information theoretic lower bound on the number of
interventions that applies in full generality, including for randomized
adaptive learning algorithms.
  For general chordal graphs, we derive worst case lower bounds on the number
of interventions. Building on observations about induced trees, we give a new
deterministic adaptive algorithm to learn directions on any chordal skeleton
completely. In the worst case, our achievable scheme is an
$\alpha$-approximation algorithm where $\alpha$ is the independence number of
the graph. We also show that there exist graph classes for which the sufficient
number of experiments is close to the lower bound. In the other extreme, there
are graph classes for which the required number of experiments is
multiplicatively $\alpha$ away from our lower bound.
  In simulations, our algorithm almost always performs very close to the lower
bound, while the approach based on separating systems for complete graphs is
significantly worse for random chordal graphs.



Marginal MAP inference involves making MAP predictions in systems defined
with latent variables or missing information. It is significantly more
difficult than pure marginalization and MAP tasks, for which a large class of
efficient and convergent variational algorithms, such as dual decomposition,
exist. In this work, we generalize dual decomposition to a generic power sum
inference task, which includes marginal MAP, along with pure marginalization
and MAP, as special cases. Our method is based on a block coordinate descent
algorithm on a new convex decomposition bound, that is guaranteed to converge
monotonically, and can be parallelized efficiently. We demonstrate our approach
on marginal MAP queries defined on real-world problems from the UAI approximate
inference challenge, showing that our framework is faster and more reliable
than previous methods.



We study the problem of off-policy value evaluation in reinforcement learning
(RL), where one aims to estimate the value of a new policy based on data
collected by a different policy. This problem is often a critical step when
applying RL in real-world problems. Despite its importance, existing general
methods either have uncontrolled bias or suffer high variance. In this work, we
extend the doubly robust estimator for bandits to sequential decision-making
problems, which gets the best of both worlds: it is guaranteed to be unbiased
and can have a much lower variance than the popular importance sampling
estimators. We demonstrate the estimator's accuracy in several benchmark
problems, and illustrate its use as a subroutine in safe policy improvement. We
also provide theoretical results on the hardness of the problem, and show that
our estimator can match the lower bound in certain scenarios.



We study the computational complexity of committee selection problem for
several approval-based voting rules in the presence of outliers. Our first
result shows that outlier consideration makes committee selection problem
intractable for approval, net approval, and minisum approval voting rules. We
then study parameterized complexity of this problem with five natural
parameters, namely the target score, the size of the committee (and its dual
parameter, the number of candidates outside the committee), the number of
outliers (and its dual parameter, the number of non-outliers). For net approval
and minisum approval voting rules, we provide a dichotomous result, resolving
the parameterized complexity of this problem for all subsets of five natural
parameters considered (by showing either FPT or W[1]-hardness for all subsets
of parameters). For the approval voting rule, we resolve the parameterized
complexity of this problem for all subsets of parameters except one.
  We also study approximation algorithms for this problem. We show that there
does not exist any alpha(.) factor approximation algorithm for approval and net
approval voting rules, for any computable function alpha(.), unless P=NP. For
the minisum voting rule, we provide a pseudopolynomial (1+eps) factor
approximation algorithm.



Modern applications and progress in deep learning research have created
renewed interest for generative models of text and of images. However, even
today it is unclear what objective functions one should use to train and
evaluate these models. In this paper we present two contributions.
  Firstly, we present a critique of scheduled sampling, a state-of-the-art
training method that contributed to the winning entry to the MSCOCO image
captioning benchmark in 2015. Here we show that despite this impressive
empirical performance, the objective function underlying scheduled sampling is
improper and leads to an inconsistent learning algorithm.
  Secondly, we revisit the problems that scheduled sampling was meant to
address, and present an alternative interpretation. We argue that maximum
likelihood is an inappropriate training objective when the end-goal is to
generate natural-looking samples. We go on to derive an ideal objective
function to use in this situation instead. We introduce a generalisation of
adversarial training, and show how such method can interpolate between maximum
likelihood training and our ideal training objective. To our knowledge this is
the first theoretical analysis that explains why adversarial training tends to
produce samples with higher perceived quality.



Popular online enrichment analysis tools from the field of molecular systems
biology provide users with the ability to submit their experimental results as
gene sets for individual analysis. Such queries are kept private, and have
never before been considered as a resource for integrative analysis. By
harnessing gene set query submissions from thousands of users, we aim to
discover biological knowledge beyond the scope of an individual study. In this
work, we investigated a large collection of gene sets submitted to the tool
Enrichr by thousands of users. Based on co-occurrence, we constructed a global
gene-gene association network. We interpret this inferred network as providing
a summary of the structure present in this crowdsourced gene set library, and
show that this network recapitulates known protein-protein interactions and
functional associations between genes. This finding implies that this network
also offers predictive value. Furthermore, we visualize this gene-gene
association network using a new edge-pruning algorithm that retains both the
local and global structures of large-scale networks. Our ability to make
predictions for currently unknown gene associations, that may not be captured
by individual researchers and data sources, is a demonstration of the potential
of harnessing collective knowledge from users of popular tools in the field of
molecular systems biology.



We consider the problem of generating motion plans for a robot that are
guaranteed to succeed despite uncertainty in the environment, parametric model
uncertainty, and disturbances. Furthermore, we consider scenarios where these
plans must be generated in real-time, because constraints such as obstacles in
the environment may not be known until they are perceived (with a noisy sensor)
at runtime. Our approach is to pre-compute a library of "funnels" along
different maneuvers of the system that the state is guaranteed to remain within
(despite bounded disturbances) when the feedback controller corresponding to
the maneuver is executed. We leverage powerful computational machinery from
convex optimization (sums-of-squares programming in particular) to compute
these funnels. The resulting funnel library is then used to sequentially
compose motion plans at runtime while ensuring the safety of the robot. A major
advantage of the work presented here is that by explicitly taking into account
the effect of uncertainty, the robot can evaluate motion plans based on how
vulnerable they are to disturbances.
  We demonstrate and validate our method using extensive hardware experiments
on a small fixed-wing airplane avoiding obstacles at high speed (~12 mph),
along with thorough simulation experiments of ground vehicle and quadrotor
models navigating through cluttered environments. To our knowledge, these
demonstrations constitute one of the first examples of provably safe and robust
control for robotic systems with complex nonlinear dynamics that need to plan
in real-time in environments with complex geometric constraints.



The categorical compositional distributional model of natural language
provides a conceptually motivated procedure to compute the meaning of
sentences, given grammatical structure and the meanings of its words. This
approach has outperformed other models in mainstream empirical language
processing tasks. However, until recently it has lacked the crucial feature of
lexical entailment -- as do other distributional models of meaning.
  In this paper we solve the problem of entailment for categorical
compositional distributional semantics. Taking advantage of the abstract
categorical framework allows us to vary our choice of model. This enables the
introduction of a notion of entailment, exploiting ideas from the categorical
semantics of partial knowledge in quantum computation.
  The new model of language uses density matrices, on which we introduce a
novel robust graded order capturing the entailment strength between concepts.
This graded measure emerges from a general framework for approximate
entailment, induced by any commutative monoid. Quantum logic embeds in our
graded order.
  Our main theorem shows that entailment strength lifts compositionally to the
sentence level, giving a lower bound on sentence entailment. We describe the
essential properties of graded entailment such as continuity, and provide a
procedure for calculating entailment strength.



A number of organizations ranging from terrorist groups such as ISIS to
politicians and nation states reportedly conduct explicit campaigns to
influence opinion on social media, posing a risk to democratic processes. There
is thus a growing need to identify and eliminate "influence bots" - realistic,
automated identities that illicitly shape discussion on sites like Twitter and
Facebook - before they get too influential. Spurred by such events, DARPA held
a 4-week competition in February/March 2015 in which multiple teams supported
by the DARPA Social Media in Strategic Communications program competed to
identify a set of previously identified "influence bots" serving as ground
truth on a specific topic within Twitter. Past work regarding influence bots
often has difficulty supporting claims about accuracy, since there is limited
ground truth (though some exceptions do exist [3,7]). However, with the
exception of [3], no past work has looked specifically at identifying influence
bots on a specific topic. This paper describes the DARPA Challenge and
describes the methods used by the three top-ranked teams.



Identifying the type of font (e.g., Roman, Blackletter) used in historical
documents can help optical character recognition (OCR) systems produce more
accurate text transcriptions. Towards this end, we present an active-learning
strategy that can significantly reduce the number of labeled samples needed to
train a font classifier. Our approach extracts image-based features that
exploit geometric differences between fonts at the word level, and combines
them into a bag-of-word representation for each page in a document. We evaluate
six sampling strategies based on uncertainty, dissimilarity and diversity
criteria, and test them on a database containing over 3,000 historical
documents with Blackletter, Roman and Mixed fonts. Our results show that a
combination of uncertainty and diversity achieves the highest predictive
accuracy (89% of test cases correctly classified) while requiring only a small
fraction of the data (17%) to be labeled. We discuss the implications of this
result for mass digitization projects of historical documents.



We present a combinatorial characterization of the Bethe entropy function of
a factor graph, such a characterization being in contrast to the original,
analytical, definition of this function. We achieve this combinatorial
characterization by counting valid configurations in finite graph covers of the
factor graph. Analogously, we give a combinatorial characterization of the
Bethe partition function, whose original definition was also of an analytical
nature. As we point out, our approach has similarities to the replica method,
but also stark differences. The above findings are a natural backdrop for
introducing a decoder for graph-based codes that we will call symbolwise
graph-cover decoding, a decoder that extends our earlier work on blockwise
graph-cover decoding. Both graph-cover decoders are theoretical tools that help
towards a better understanding of message-passing iterative decoding, namely
blockwise graph-cover decoding links max-product (min-sum) algorithm decoding
with linear programming decoding, and symbolwise graph-cover decoding links
sum-product algorithm decoding with Bethe free energy function minimization at
temperature one. In contrast to the Gibbs entropy function, which is a concave
function, the Bethe entropy function is in general not concave everywhere. In
particular, we show that every code picked from an ensemble of regular
low-density parity-check codes with minimum Hamming distance growing (with high
probability) linearly with the block length has a Bethe entropy function that
is convex in certain regions of its domain.



The ability to predict the intentions of people based solely on their visual
actions is a skill only performed by humans and animals. The intelligence of
current computer algorithms has not reached this level of complexity, but there
are several research efforts that are working towards it. With the number of
classification algorithms available, it is hard to determine which algorithm
works best for a particular situation. In classification of visual human intent
data, Hidden Markov Models (HMM), and their variants, are leading candidates.
  The inability of HMMs to provide a probability in the observation to
observation linkages is a big downfall in this classification technique. If a
person is visually identifying an action of another person, they monitor
patterns in the observations. By estimating the next observation, people have
the ability to summarize the actions, and thus determine, with pretty good
accuracy, the intention of the person performing the action. These visual cues
and linkages are important in creating intelligent algorithms for determining
human actions based on visual observations.
  The Evidence Feed Forward Hidden Markov Model is a newly developed algorithm
which provides observation to observation linkages. The following research
addresses the theory behind Evidence Feed Forward HMMs, provides mathematical
proofs of their learning of these parameters to optimize the likelihood of
observations with a Evidence Feed Forwards HMM, which is important in all
computational intelligence algorithm, and gives comparative examples with
standard HMMs in classification of both visual action data and measurement
data; thus providing a strong base for Evidence Feed Forward HMMs in
classification of many types of problems.



We present and study an agent-based model of T-Cell cross-regulation in the
adaptive immune system, which we apply to binary classification. Our method
expands an existing analytical model of T-cell cross-regulation (Carneiro et
al. in Immunol Rev 216(1):48-68, 2007) that was used to study the
self-organizing dynamics of a single population of T-Cells in interaction with
an idealized antigen presenting cell capable of presenting a single antigen.
With agent-based modeling we are able to study the self-organizing dynamics of
multiple populations of distinct T-cells which interact via antigen presenting
cells that present hundreds of distinct antigens. Moreover, we show that such
self-organizing dynamics can be guided to produce an effective binary
classification of antigens, which is competitive with existing machine learning
methods when applied to biomedical text classification. More specifically, here
we test our model on a dataset of publicly available full-text biomedical
articles provided by the BioCreative challenge (Krallinger in The biocreative
ii. 5 challenge overview, p 19, 2009). We study the robustness of our model's
parameter configurations, and show that it leads to encouraging results
comparable to state-of-the-art classifiers. Our results help us understand both
T-cell cross-regulation as a general principle of guided self-organization, as
well as its applicability to document classification. Therefore, we show that
our bio-inspired algorithm is a promising novel method for biomedical article
classification and for binary document classification in general.



We present a method to eliminate redundancy in the transition tables of
Boolean automata: schema redescription with two symbols. One symbol is used to
capture redundancy of individual input variables, and another to capture
permutability in sets of input variables: fully characterizing the canalization
present in Boolean functions. Two-symbol schemata explain aspects of the
behaviour of automata networks that the characterization of their emergent
patterns does not capture. We use our method to compare two well-known cellular
automata for the density classification task: the human engineered CA GKL, and
another obtained via genetic programming (GP). We show that despite having very
different collective behaviour, these rules are very similar. Indeed, GKL is a
special case of GP. Therefore, we demonstrate that it is more feasible to
compare cellular automata via schema redescriptions of their rules, than by
looking at their emergent behaviour, leading us to question the tendency in
complexity research to pay much more attention to emergent patterns than to
local interactions.



We give the first analysis of the computational complexity of {\it coalition
structure generation over graphs}. Given an undirected graph $G=(N,E)$ and a
valuation function $v:2^N\rightarrow\RR$ over the subsets of nodes, the problem
is to find a partition of $N$ into connected subsets, that maximises the sum of
the components' values. This problem is generally NP--complete; in particular,
it is hard for a defined class of valuation functions which are {\it
independent of disconnected members}---that is, two nodes have no effect on
each other's marginal contribution to their vertex separator. Nonetheless, for
all such functions we provide bounds on the complexity of coalition structure
generation over general and minor free graphs. Our proof is constructive and
yields algorithms for solving corresponding instances of the problem.
Furthermore, we derive polynomial time bounds for acyclic, $K_{2,3}$ and $K_4$
minor free graphs. However, as we show, the problem remains NP--complete for
planar graphs, and hence, for any $K_k$ minor free graphs where $k\geq 5$.
Moreover, our hardness result holds for a particular subclass of valuation
functions, termed {\it edge sum}, where the value of each subset of nodes is
simply determined by the sum of given weights of the edges in the induced
subgraph.



We present a novel variant of decision making based on the mathematical
theory of separable Hilbert spaces. This mathematical structure captures the
effect of superposition of composite prospects, including many incorporated
intentions, which allows us to describe a variety of interesting fallacies and
anomalies that have been reported to particularize the decision making of real
human beings. The theory characterizes entangled decision making,
non-commutativity of subsequent decisions, and intention interference. We
demonstrate how the violation of the Savage's sure-thing principle, known as
the disjunction effect, can be explained quantitatively as a result of the
interference of intentions, when making decisions under uncertainty. The
disjunction effects, observed in experiments, are accurately predicted using a
theorem on interference alternation that we derive, which connects
aversion-to-uncertainty to the appearance of negative interference terms
suppressing the probability of actions. The conjunction fallacy is also
explained by the presence of the interference terms. A series of experiments
are analysed and shown to be in excellent agreement with a priori evaluation of
interference effects. The conjunction fallacy is also shown to be a sufficient
condition for the disjunction effect and novel experiments testing the combined
interplay between the two effects are suggested.



In massive open online courses (MOOCs), peer grading serves as a critical
tool for scaling the grading of complex, open-ended assignments to courses with
tens or hundreds of thousands of students. But despite promising initial
trials, it does not always deliver accurate results compared to human experts.
In this paper, we develop algorithms for estimating and correcting for grader
biases and reliabilities, showing significant improvement in peer grading
accuracy on real data with 63,199 peer grades from Coursera's HCI course
offerings --- the largest peer grading networks analysed to date. We relate
grader biases and reliabilities to other student factors such as student
engagement, performance as well as commenting style. We also show that our
model can lead to more intelligent assignment of graders to gradees.



What lies between `\emph{sensing}' and `\emph{sensibility}'? In other words,
what kind of cognitive processes mediate sensing capability, and the formation
of sensible impressions ---e.g., abstractions, analogies, hypotheses and theory
formation, beliefs and their revision, argument formation--- in domain-specific
problem solving, or in regular activities of everyday living, working and
simply going around in the environment? How can knowledge and reasoning about
such capabilities, as exhibited by humans in particular problem contexts, be
used as a model and benchmark for the development of collaborative cognitive
(interaction) systems concerned with human assistance, assurance, and
empowerment?
  We pose these questions in the context of a range of assistive technologies
concerned with \emph{visuo-spatial perception and cognition} tasks encompassing
aspects such as commonsense, creativity, and the application of specialist
domain knowledge and problem-solving thought processes. Assistive technologies
being considered include: (a) human activity interpretation; (b) high-level
cognitive rovotics; (c) people-centred creative design in domains such as
architecture & digital media creation, and (d) qualitative analyses geographic
information systems. Computational narratives not only provide a rich cognitive
basis, but they also serve as a benchmark of functional performance in our
development of computational cognitive assistance systems. We posit that
computational narrativisation pertaining to space, actions, and change provides
a useful model of \emph{visual} and \emph{spatio-temporal thinking} within a
wide-range of problem-solving tasks and application areas where collaborative
cognitive systems could serve an assistive and empowering function.



This paper establishes theoretical bonafides for implicit concurrent
multivariate effect evaluation--implicit concurrency for short---a broad and
versatile computational learning efficiency thought to underlie
general-purpose, non-local, noise-tolerant optimization in genetic algorithms
with uniform crossover (UGAs). We demonstrate that implicit concurrency is
indeed a form of efficient learning by showing that it can be used to obtain
close-to-optimal bounds on the time and queries required to approximately
correctly solve a constrained version (k=7, \eta=1/5) of a recognizable
computational learning problem: learning parities with noisy membership
queries. We argue that a UGA that treats the noisy membership query oracle as a
fitness function can be straightforwardly used to approximately correctly learn
the essential attributes in O(log^1.585 n) queries and O(n log^1.585 n) time,
where n is the total number of attributes. Our proof relies on an accessible
symmetry argument and the use of statistical hypothesis testing to reject a
global null hypothesis at the 10^-100 level of significance. It is, to the best
of our knowledge, the first relatively rigorous identification of efficient
computational learning in an evolutionary algorithm on a non-trivial learning
problem.



In this paper we provide a simple random-variable example of inconsistent
information, and analyze it using three different approaches: Bayesian,
quantum-like, and negative probabilities. We then show that, at least for this
particular example, both the Bayesian and the quantum-like approaches have less
normative power than the negative probabilities one.



When eliciting opinions from a group of experts, traditional devices used to
promote honest reporting assume that there is an observable future outcome. In
practice, however, this assumption is not always reasonable. In this paper, we
propose a scoring method built on strictly proper scoring rules to induce
honest reporting without assuming observable outcomes. Our method provides
scores based on pairwise comparisons between the reports made by each pair of
experts in the group. For ease of exposition, we introduce our scoring method
by illustrating its application to the peer-review process. In order to do so,
we start by modeling the peer-review process using a Bayesian model where the
uncertainty regarding the quality of the manuscript is taken into account.
Thereafter, we introduce our scoring method to evaluate the reported reviews.
Under the assumptions that reviewers are Bayesian decision-makers and that they
cannot influence the reviews of other reviewers, we show that risk-neutral
reviewers strictly maximize their expected scores by honestly disclosing their
reviews. We also show how the group's scores can be used to find a consensual
review. Experimental results show that encouraging honest reporting through the
proposed scoring method creates more accurate reviews than the traditional
peer-review process.



HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable)
mathematics encoded in the HOL Light system. The service allows its users to
upload and automatically process an arbitrary formal development (project)
based on HOL Light, and to attack arbitrary conjectures that use the concepts
defined in some of the uploaded projects. For that, the service uses several
automated reasoning systems combined with several premise selection methods
trained on all the project proofs. The projects that are readily available on
the server for such query answering include the recent versions of the
Flyspeck, Multivariate Analysis and Complex Analysis libraries. The service
runs on a 48-CPU server, currently employing in parallel for each task 7 AI/ATP
combinations and 4 decision procedures that contribute to its overall
performance. The system is also available for local installation by interested
users, who can customize it for their own proof development. An Emacs interface
allowing parallel asynchronous queries to the service is also provided. The
overall structure of the service is outlined, problems that arise and their
solutions are discussed, and an initial account of using the system is given.



Self-assembly is a phenomenon observed in nature at all scales where
autonomous entities build complex structures, without external influences nor
centralised master plan. Modelling such entities and programming correct
interactions among them is crucial for controlling the manufacture of desired
complex structures at the molecular and supramolecular scale. This work focuses
on a programmability model for non DNA-based molecules and complex behaviour
analysis of their self-assembled conformations. In particular, we look into
modelling, programming and simulation of porphyrin molecules self-assembly and
apply Kolgomorov complexity-based techniques to classify and assess simulation
results in terms of information content. The analysis focuses on phase
transition, clustering, variability and parameter discovery which as a whole
pave the way to the notion of complex systems programmability.



Sophisticated multilayer neural networks have achieved state of the art
results on multiple supervised tasks. However, successful applications of such
multilayer networks to control have so far been limited largely to the
perception portion of the control pipeline. In this paper, we explore the
application of deep and recurrent neural networks to a continuous,
high-dimensional locomotion task, where the network is used to represent a
control policy that maps the state of the system (represented by joint angles)
directly to the torques at each joint. By using a recent reinforcement learning
algorithm called guided policy search, we can successfully train neural network
controllers with thousands of parameters, allowing us to compare a variety of
architectures. We discuss the differences between the locomotion control task
and previous supervised perception tasks, present experimental results
comparing various architectures, and discuss future directions in the
application of techniques from deep learning to the problem of optimal control.



Due to recent advances in synthetic biology and artificial life, the origin
of life is currently a hot topic of research. We review the literature and
argue that the two traditionally competing "replicator-first" and
"metabolism-first" approaches are merging into one integrated theory of
individuation and evolution. We contribute to the maturation of this more
inclusive approach by highlighting some problematic assumptions that still lead
to an impoverished conception of the phenomenon of life. In particular, we
argue that the new consensus has so far failed to consider the relevance of
intermediate timescales. We propose that an adequate theory of life must
account for the fact that all living beings are situated in at least four
distinct timescales, which are typically associated with metabolism, motility,
development, and evolution. On this view, self-movement, adaptive behavior and
morphological changes could have already been present at the origin of life. In
order to illustrate this possibility we analyze a minimal model of life-like
phenomena, namely of precarious, individuated, dissipative structures that can
be found in simple reaction-diffusion systems. Based on our analysis we suggest
that processes in intermediate timescales could have already been operative in
prebiotic systems. They may have facilitated and constrained changes occurring
in the faster- and slower-paced timescales of chemical self-individuation and
evolution by natural selection, respectively.



Writing documentation about software internals is rarely considered a
rewarding activity. It is highly time-consuming and the resulting documentation
is fragile when the software is continuously evolving in a multi-developer
setting. Unfortunately, traditional programming environments poorly support the
writing and maintenance of documentation. Consequences are severe as the lack
of documentation on software structure negatively impacts the overall quality
of the software product. We show that using a controlled natural language with
a reasoner and a query engine is a viable technique for verifying the
consistency and accuracy of documentation and source code. Using ACE, a
state-of-the-art controlled natural language, we present positive results on
the comprehensibility and the general feasibility of creating and verifying
documentation. As a case study, we used automatic documentation verification to
identify and fix severe flaws in the architecture of a non-trivial piece of
software. Moreover, a user experiment shows that our language is faster and
easier to learn and understand than other formal languages for software
documentation.



We describe a novel approach for computing collision-free \emph{global}
trajectories for $p$ agents with specified initial and final configurations,
based on an improved version of the alternating direction method of multipliers
(ADMM). Compared with existing methods, our approach is naturally
parallelizable and allows for incorporating different cost functionals with
only minor adjustments. We apply our method to classical challenging instances
and observe that its computational requirements scale well with $p$ for several
cost functionals. We also show that a specialization of our algorithm can be
used for {\em local} motion planning by solving the problem of joint
optimization in velocity space.



We consider LSTD($\lambda$), the least-squares temporal-difference algorithm
with eligibility traces algorithm proposed by Boyan (2002). It computes a
linear approximation of the value function of a fixed policy in a large Markov
Decision Process. Under a $\beta$-mixing assumption, we derive, for any value
of $\lambda \in (0,1)$, a high-probability estimate of the rate of convergence
of this algorithm to its limit. We deduce a high-probability bound on the error
of this algorithm, that extends (and slightly improves) that derived by Lazaric
et al. (2012) in the specific case where $\lambda=0$. In particular, our
analysis sheds some light on the choice of $\lambda$ with respect to the
quality of the chosen linear space and the number of samples, that complies
with simulations.



We consider the problem of learning the canonical parameters specifying an
undirected graphical model (Markov random field) from the mean parameters. For
graphical models representing a minimal exponential family, the canonical
parameters are uniquely determined by the mean parameters, so the problem is
feasible in principle. The goal of this paper is to investigate the
computational feasibility of this statistical task. Our main result shows that
parameter estimation is in general intractable: no algorithm can learn the
canonical parameters of a generic pair-wise binary graphical model from the
mean parameters in time bounded by a polynomial in the number of variables
(unless RP = NP). Indeed, such a result has been believed to be true (see the
monograph by Wainwright and Jordan (2008)) but no proof was known.
  Our proof gives a polynomial time reduction from approximating the partition
function of the hard-core model, known to be hard, to learning approximate
parameters. Our reduction entails showing that the marginal polytope boundary
has an inherent repulsive property, which validates an optimization procedure
over the polytope that does not use any knowledge of its structure (as required
by the ellipsoid method and others).



We propose a technique to detect and generate patterns in a network of
locally interacting dynamical systems. Central to our approach is a novel
spatial superposition logic, whose semantics is defined over the quad-tree of a
partitioned image. We show that formulas in this logic can be efficiently
learned from positive and negative examples of several types of patterns. We
also demonstrate that pattern detection, which is implemented as a model
checking algorithm, performs very well for test data sets different from the
learning sets. We define a quantitative semantics for the logic and integrate
the model checking algorithm with particle swarm optimization in a
computational framework for synthesis of parameters leading to desired patterns
in reaction-diffusion systems.



We propose a simple neural network model to deal with the domain adaptation
problem in object recognition. Our model incorporates the Maximum Mean
Discrepancy (MMD) measure as a regularization in the supervised learning to
reduce the distribution mismatch between the source and target domains in the
latent space. From experiments, we demonstrate that the MMD regularization is
an effective tool to provide good domain adaptation models on both SURF
features and raw image pixels of a particular image data set. We also show that
our proposed model, preceded by the denoising auto-encoder pretraining,
achieves better performance than recent benchmark models on the same data sets.
This work represents the first study of MMD measure in the context of neural
networks.



Due to the huge availability of documents in digital form, and the deception
possibility raise bound to the essence of digital documents and the way they
are spread, the authorship attribution problem has constantly increased its
relevance. Nowadays, authorship attribution,for both information retrieval and
analysis, has gained great importance in the context of security, trust and
copyright preservation. This work proposes an innovative multi-agent driven
machine learning technique that has been developed for authorship attribution.
By means of a preprocessing for word-grouping and time-period related analysis
of the common lexicon, we determine a bias reference level for the recurrence
frequency of the words within analysed texts, and then train a Radial Basis
Neural Networks (RBPNN)-based classifier to identify the correct author. The
main advantage of the proposed approach lies in the generality of the semantic
analysis, which can be applied to different contexts and lexical domains,
without requiring any modification. Moreover, the proposed system is able to
incorporate an external input, meant to tune the classifier, and then
self-adjust by means of continuous learning reinforcement.



We present a new similarity measure based on information theoretic measures
which is superior than Normalized Compression Distance for clustering problems
and inherits the useful properties of conditional Kolmogorov complexity. We
show that Normalized Compression Dictionary Size and Normalized Compression
Dictionary Entropy are computationally more efficient, as the need to perform
the compression itself is eliminated. Also they scale linearly with exponential
vector size growth and are content independent. We show that normalized
compression dictionary distance is compressor independent, if limited to
lossless compressors, which gives space for optimizations and implementation
speed improvement for real-time and big data applications. The introduced
measure is applicable for machine learning tasks of parameter-free unsupervised
clustering, supervised learning such as classification and regression, feature
selection, and is applicable for big data problems with order of magnitude
speed increase.



As language and visual understanding by machines progresses rapidly, we are
observing an increasing interest in holistic architectures that tightly
interlink both modalities in a joint learning and inference process. This trend
has allowed the community to progress towards more challenging and open tasks
and refueled the hope at achieving the old AI dream of building machines that
could pass a turing test in open domains. In order to steadily make progress
towards this goal, we realize that quantifying performance becomes increasingly
difficult. Therefore we ask how we can precisely define such challenges and how
we can evaluate different algorithms on this open tasks? In this paper, we
summarize and discuss such challenges as well as try to give answers where
appropriate options are available in the literature. We exemplify some of the
solutions on a recently presented dataset of question-answering task based on
real-world indoor images that establishes a visual turing challenge. Finally,
we argue despite the success of unique ground-truth annotation, we likely have
to step away from carefully curated dataset and rather rely on 'social
consensus' as the main driving force to create suitable benchmarks. Providing
coverage in this inherently ambiguous output space is an emerging challenge
that we face in order to make quantifiable progress in this area.



In this paper, we present a statistical-mechanical analysis of deep learning.
We elucidate some of the essential components of deep learning---pre-training
by unsupervised learning and fine tuning by supervised learning. We formulate
the extraction of features from the training data as a margin criterion in a
high-dimensional feature-vector space. The self-organized classifier is then
supplied with small amounts of labelled data, as in deep learning. Although we
employ a simple single-layer perceptron model, rather than directly analyzing a
multi-layer neural network, we find a nontrivial phase transition that is
dependent on the number of unlabelled data in the generalization error of the
resultant classifier. In this sense, we evaluate the efficacy of the
unsupervised learning component of deep learning. The analysis is performed by
the replica method, which is a sophisticated tool in statistical mechanics. We
validate our result in the manner of deep learning, using a simple iterative
algorithm to learn the weight vector on the basis of belief propagation.



We introduce a near-linear complexity (geometric and meshless/algebraic)
multigrid/multiresolution method for PDEs with rough ($L^\infty$) coefficients
with rigorous a-priori accuracy and performance estimates. The method is
discovered through a decision/game theory formulation of the problems of (1)
identifying restriction and interpolation operators (2) recovering a signal
from incomplete measurements based on norm constraints on its image under a
linear operator (3) gambling on the value of the solution of the PDE based on a
hierarchy of nested measurements of its solution or source term. The resulting
elementary gambles form a hierarchy of (deterministic) basis functions of
$H^1_0(\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands
with respect to the scalar product induced by the energy norm of the PDE (2)
enable sparse compression of the solution space in $H^1_0(\Omega)$ (3) induce
an orthogonal multiresolution operator decomposition. The operating diagram of
the multigrid method is that of an inverted pyramid in which gamblets are
computed locally (by virtue of their exponential decay), hierarchically (from
fine to coarse scales) and the PDE is decomposed into a hierarchy of
independent linear systems with uniformly bounded condition numbers. The
resulting algorithm is parallelizable both in space (via localization) and in
bandwith/subscale (subscales can be computed independently from each other).
Although the method is deterministic it has a natural Bayesian interpretation
under the measure of probability emerging (as a mixed strategy) from the
information game formulation and multiresolution approximations form a
martingale with respect to the filtration induced by the hierarchy of nested
measurements.



We present a novel framework, called Private Disclosure of Information (PDI),
which is aimed to prevent an adversary from inferring certain sensitive
information about subjects using the data that they disclosed during
communication with an intended recipient. We show cases where it is possible to
achieve perfect privacy regardless of the adversary's auxiliary knowledge while
preserving full utility of the information to the intended recipient and
provide sufficient conditions for such cases. We also demonstrate the
applicability of PDI on a real-world data set that simulates a health
tele-monitoring scenario.



This thesis contributes to ongoing research related to the categorical
compositional model for natural language of Coecke, Sadrzadeh and Clark in
three ways: Firstly, I propose a concrete instantiation of the abstract
framework based on Frobenius algebras (joint work with Sadrzadeh). The theory
improves shortcomings of previous proposals, extends the coverage of the
language, and is supported by experimental work that improves existing results.
The proposed framework describes a new class of compositional models that find
intuitive interpretations for a number of linguistic phenomena. Secondly, I
propose and evaluate in practice a new compositional methodology which
explicitly deals with the different levels of lexical ambiguity (joint work
with Pulman). A concrete algorithm is presented, based on the separation of
vector disambiguation from composition in an explicit prior step. Extensive
experimental work shows that the proposed methodology indeed results in more
accurate composite representations for the framework of Coecke et al. in
particular and every other class of compositional models in general. As a last
contribution, I formalize the explicit treatment of lexical ambiguity in the
context of the categorical framework by resorting to categorical quantum
mechanics (joint work with Coecke). In the proposed extension, the concept of a
distributional vector is replaced with that of a density matrix, which
compactly represents a probability distribution over the potential different
meanings of the specific word. Composition takes the form of quantum
measurements, leading to interesting analogies between quantum physics and
linguistics.



Evaluation often aims to reduce the correctness or error characteristics of a
system down to a single number, but that always involves trade-offs. Another
way of dealing with this is to quote two numbers, such as Recall and Precision,
or Sensitivity and Specificity. But it can also be useful to see more than
this, and a graphical approach can explore sensitivity to cost, prevalence,
bias, noise, parameters and hyper-parameters.
  Moreover, most techniques are implicitly based on two balanced classes, and
our ability to visualize graphically is intrinsically two dimensional, but we
often want to visualize in a multiclass context. We review the dichotomous
approaches relating to Precision, Recall, and ROC as well as the related LIFT
chart, exploring how they handle unbalanced and multiclass data, and deriving
new probabilistic and information theoretic variants of LIFT that help deal
with the issues associated with the handling of multiple and unbalanced
classes.



In active learning, the user sequentially chooses values for feature $X$ and
an oracle returns the corresponding label $Y$. In this paper, we consider the
effect of feature noise in active learning, which could arise either because
$X$ itself is being measured, or it is corrupted in transmission to the oracle,
or the oracle returns the label of a noisy version of the query point. In
statistics, feature noise is known as "errors in variables" and has been
studied extensively in non-active settings. However, the effect of feature
noise in active learning has not been studied before. We consider the
well-known Berkson errors-in-variables model with additive uniform noise of
width $\sigma$.
  Our simple but revealing setting is that of one-dimensional binary
classification setting where the goal is to learn a threshold (point where the
probability of a $+$ label crosses half). We deal with regression functions
that are antisymmetric in a region of size $\sigma$ around the threshold and
also satisfy Tsybakov's margin condition around the threshold. We prove minimax
lower and upper bounds which demonstrate that when $\sigma$ is smaller than the
minimiax active/passive noiseless error derived in \cite{CN07}, then noise has
no effect on the rates and one achieves the same noiseless rates. For larger
$\sigma$, the \textit{unflattening} of the regression function on convolution
with uniform noise, along with its local antisymmetry around the threshold,
together yield a behaviour where noise \textit{appears} to be beneficial. Our
key result is that active learning can buy significant improvement over a
passive strategy even in the presence of feature noise.



The choice of approximate posterior distribution is one of the core problems
in variational inference. Most applications of variational inference employ
simple families of posterior approximations in order to allow for efficient
inference, focusing on mean-field or other simple structured approximations.
This restriction has a significant impact on the quality of inferences made
using variational methods. We introduce a new approach for specifying flexible,
arbitrarily complex and scalable approximate posterior distributions. Our
approximations are distributions constructed through a normalizing flow,
whereby a simple initial density is transformed into a more complex one by
applying a sequence of invertible transformations until a desired level of
complexity is attained. We use this view of normalizing flows to develop
categories of finite and infinitesimal flows and provide a unified view of
approaches for constructing rich posterior approximations. We demonstrate that
the theoretical advantages of having posteriors that better match the true
posterior, combined with the scalability of amortized variational approaches,
provides a clear improvement in performance and applicability of variational
inference.



Can we develop a computer algorithm that assesses the creativity of a
painting given its context within art history? This paper proposes a novel
computational framework for assessing the creativity of creative products, such
as paintings, sculptures, poetry, etc. We use the most common definition of
creativity, which emphasizes the originality of the product and its influential
value. The proposed computational framework is based on constructing a network
between creative products and using this network to infer about the originality
and influence of its nodes. Through a series of transformations, we construct a
Creativity Implication Network. We show that inference about creativity in this
network reduces to a variant of network centrality problems which can be solved
efficiently. We apply the proposed framework to the task of quantifying
creativity of paintings (and sculptures). We experimented on two datasets with
over 62K paintings to illustrate the behavior of the proposed framework. We
also propose a methodology for quantitatively validating the results of the
proposed algorithm, which we call the "time machine experiment".



We deliver a call to arms for probabilistic numerical methods: algorithms for
numerical tasks, including linear algebra, integration, optimization and
solving differential equations, that return uncertainties in their
calculations. Such uncertainties, arising from the loss of precision induced by
numerical calculation with limited time or hardware, are important for much
contemporary science and industry. Within applications such as climate science
and astrophysics, the need to make decisions on the basis of computations with
large and complex data has led to a renewed focus on the management of
numerical uncertainty. We describe how several seminal classic numerical
methods can be interpreted naturally as probabilistic inference. We then show
that the probabilistic view suggests new algorithms that can flexibly be
adapted to suit application specifics, while delivering improved empirical
performance. We provide concrete illustrations of the benefits of probabilistic
numeric algorithms on real scientific problems from astrometry and astronomical
imaging, while highlighting open problems with these new algorithms. Finally,
we describe how probabilistic numerical methods provide a coherent framework
for identifying the uncertainty in calculations performed with a combination of
numerical algorithms (e.g. both numerical optimisers and differential equation
solvers), potentially allowing the diagnosis (and control) of error sources in
computations.



Recent studies have demonstrated the power of recurrent neural networks for
machine translation, image captioning and speech recognition. For the task of
capturing temporal structure in video, however, there still remain numerous
open research questions. Current research suggests using a simple temporal
feature pooling strategy to take into account the temporal aspect of video. We
demonstrate that this method is not sufficient for gesture recognition, where
temporal information is more discriminative compared to general video
classification tasks. We explore deep architectures for gesture recognition in
video and propose a new end-to-end trainable neural network architecture
incorporating temporal convolutions and bidirectional recurrence. Our main
contributions are twofold; first, we show that recurrence is crucial for this
task; second, we show that adding temporal convolutions leads to significant
improvements. We evaluate the different approaches on the Montalbano gesture
recognition dataset, where we achieve state-of-the-art results.



We present a Bayesian tensor factorization model for inferring latent group
structures from dynamic pairwise interaction patterns. For decades, political
scientists have collected and analyzed records of the form "country $i$ took
action $a$ toward country $j$ at time $t$"---known as dyadic events---in order
to form and test theories of international relations. We represent these event
data as a tensor of counts and develop Bayesian Poisson tensor factorization to
infer a low-dimensional, interpretable representation of their salient
patterns. We demonstrate that our model's predictive performance is better than
that of standard non-negative tensor factorization methods. We also provide a
comparison of our variational updates to their maximum likelihood counterparts.
In doing so, we identify a better way to form point estimates of the latent
factors than that typically used in Bayesian Poisson matrix factorization.
Finally, we showcase our model as an exploratory analysis tool for political
scientists. We show that the inferred latent factor matrices capture
interpretable multilateral relations that both conform to and inform our
knowledge of international affairs.



We propose a neural sequence-to-sequence model for direction following, a
task that is essential to realizing effective autonomous agents. Our
alignment-based encoder-decoder model with long short-term memory recurrent
neural networks (LSTM-RNN) translates natural language instructions to action
sequences based upon a representation of the observable world state. We
introduce a multi-level aligner that empowers our model to focus on sentence
"regions" salient to the current world state by using multiple abstractions of
the input sentence. In contrast to existing methods, our model uses no
specialized linguistic resources (e.g., parsers) or task-specific annotations
(e.g., seed lexicons). It is therefore generalizable, yet still achieves the
best results reported to-date on a benchmark single-sentence dataset and
competitive results for the limited-training multi-sentence setting. We analyze
our model through a series of ablations that elucidate the contributions of the
primary components of our model.



Interpersonal relations are fickle, with close friendships often dissolving
into enmity. In this work, we explore linguistic cues that presage such
transitions by studying dyadic interactions in an online strategy game where
players form alliances and break those alliances through betrayal. We
characterize friendships that are unlikely to last and examine temporal
patterns that foretell betrayal.
  We reveal that subtle signs of imminent betrayal are encoded in the
conversational patterns of the dyad, even if the victim is not aware of the
relationship's fate. In particular, we find that lasting friendships exhibit a
form of balance that manifests itself through language. In contrast, sudden
changes in the balance of certain conversational attributes---such as positive
sentiment, politeness, or focus on future planning---signal impending betrayal.



Many online companies sell advertisement space in second-price auctions with
reserve. In this paper, we develop a probabilistic method to learn a profitable
strategy to set the reserve price. We use historical auction data with features
to fit a predictor of the best reserve price. This problem is delicate - the
structure of the auction is such that a reserve price set too high is much
worse than a reserve price set too low. To address this we develop objective
variables, a new framework for combining probabilistic modeling with optimal
decision-making. Objective variables are "hallucinated observations" that
transform the revenue maximization task into a regularized maximum likelihood
estimation problem, which we solve with an EM algorithm. This framework enables
a variety of prediction mechanisms to set the reserve price. As examples, we
study objective variable methods with regression, kernelized regression, and
neural networks on simulated and real data. Our methods outperform previous
approaches both in terms of scalability and profit.



Cloud controllers aim at responding to application demands by automatically
scaling the compute resources at runtime to meet performance guarantees and
minimize resource costs. Existing cloud controllers often resort to scaling
strategies that are codified as a set of adaptation rules. However, for a cloud
provider, applications running on top of the cloud infrastructure are more or
less black-boxes, making it difficult at design time to define optimal or
pre-emptive adaptation rules. Thus, the burden of taking adaptation decisions
often is delegated to the cloud application. Yet, in most cases, application
developers in turn have limited knowledge of the cloud infrastructure. In this
paper, we propose learning adaptation rules during runtime. To this end, we
introduce FQL4KE, a self-learning fuzzy cloud controller. In particular, FQL4KE
learns and modifies fuzzy rules at runtime. The benefit is that for designing
cloud controllers, we do not have to rely solely on precise design-time
knowledge, which may be difficult to acquire. FQL4KE empowers users to specify
cloud controllers by simply adjusting weights representing priorities in system
goals instead of specifying complex adaptation rules. The applicability of
FQL4KE has been experimentally assessed as part of the cloud application
framework ElasticBench. The experimental results indicate that FQL4KE
outperforms our previously developed fuzzy controller without learning
mechanisms and the native Azure auto-scaling.



News is a pertinent source of information on financial risks and stress
factors, which nevertheless is challenging to harness due to the sparse and
unstructured nature of natural text. We propose an approach based on
distributional semantics and deep learning with neural networks to model and
link text to a scarce set of bank distress events. Through unsupervised
training, we learn semantic vector representations of news articles as
predictors of distress events. The predictive model that we learn can signal
coinciding stress with an aggregated index at bank or European level, while
crucially allowing for automatic extraction of text descriptions of the events,
based on passages with high stress levels. The method offers insight that
models based on other types of data cannot provide, while offering a general
means for interpreting this type of semantic-predictive model. We model bank
distress with data on 243 events and 6.6M news articles for 101 large European
banks.



We investigate the problem of winner determination from computational social
choice theory in the data stream model. Specifically, we consider the task of
summarizing an arbitrarily ordered stream of $n$ votes on $m$ candidates into a
small space data structure so as to be able to obtain the winner determined by
popular voting rules. As we show, finding the exact winner requires storing
essentially all the votes. So, we focus on the problem of finding an {\em
$\eps$-winner}, a candidate who could win by a change of at most $\eps$
fraction of the votes. We show non-trivial upper and lower bounds on the space
complexity of $\eps$-winner determination for several voting rules, including
$k$-approval, $k$-veto, scoring rules, approval, maximin, Bucklin, Copeland,
and plurality with run off.



Graphical models use the intuitive and well-studied methods of graph theory
to implicitly represent dependencies between variables in large systems. They
can model the global behaviour of a complex system by specifying only local
factors. This thesis studies inference in discrete graphical models from an
algebraic perspective and the ways inference can be used to express and
approximate NP-hard combinatorial problems.
  We investigate the complexity and reducibility of various inference problems,
in part by organizing them in an inference hierarchy. We then investigate
tractable approximations for a subset of these problems using distributive law
in the form of message passing. The quality of the resulting message passing
procedure, called Belief Propagation (BP), depends on the influence of loops in
the graphical model. We contribute to three classes of approximations that
improve BP for loopy graphs A) loop correction techniques; B) survey
propagation, another message passing technique that surpasses BP in some
settings; and C) hybrid methods that interpolate between deterministic message
passing and Markov Chain Monte Carlo inference.
  We then review the existing message passing solutions and provide novel
graphical models and inference techniques for combinatorial problems under
three broad classes: A) constraint satisfaction problems such as
satisfiability, coloring, packing, set / clique-cover and dominating /
independent set and their optimization counterparts; B) clustering problems
such as hierarchical clustering, K-median, K-clustering, K-center and
modularity optimization; C) problems over permutations including assignment,
graph morphisms and alignment, finding symmetries and traveling salesman
problem. In many cases we show that message passing is able to find solutions
that are either near optimal or favourably compare with today's
state-of-the-art approaches.



We present a unified framework which supports grounding natural-language
semantics in robotic driving. This framework supports acquisition (learning
grounded meanings of nouns and prepositions from human annotation of robotic
driving paths), generation (using such acquired meanings to generate sentential
description of new robotic driving paths), and comprehension (using such
acquired meanings to support automated driving to accomplish navigational goals
specified in natural language). We evaluate the performance of these three
tasks by having independent human judges rate the semantic fidelity of the
sentences associated with paths, achieving overall average correctness of 94.6%
and overall average completeness of 85.6%.



In this paper, we address the task of Optical Character Recognition(OCR) for
the Telugu script. We present an end-to-end framework that segments the text
image, classifies the characters and extracts lines using a language model. The
segmentation is based on mathematical morphology. The classification module,
which is the most challenging task of the three, is a deep convolutional neural
network. The language is modelled as a third degree markov chain at the glyph
level. Telugu script is a complex alphasyllabary and the language is
agglutinative, making the problem hard. In this paper we apply the latest
advances in neural networks to achieve state-of-the-art error rates. We also
review convolutional neural networks in great detail and expound the
statistical justification behind the many tricks needed to make Deep Learning
work.



Search in an environment with an uncertain distribution of resources involves
a trade-off between exploitation of past discoveries and further exploration.
This extends to information foraging, where a knowledge-seeker shifts between
reading in depth and studying new domains. To study this decision-making
process, we examine the reading choices made by one of the most celebrated
scientists of the modern era: Charles Darwin. From the full-text of books
listed in his chronologically-organized reading journals, we generate topic
models to quantify his local (text-to-text) and global (text-to-past) reading
decisions using Kullback-Liebler Divergence, a cognitively-validated,
information-theoretic measure of relative surprise. Rather than a pattern of
surprise-minimization, corresponding to a pure exploitation strategy, Darwin's
behavior shifts from early exploitation to later exploration, seeking unusually
high levels of cognitive surprise relative to previous eras. These shifts,
detected by an unsupervised Bayesian model, correlate with major intellectual
epochs of his career as identified both by qualitative scholarship and Darwin's
own self-commentary. Our methods allow us to compare his consumption of texts
with their publication order. We find Darwin's consumption more exploratory
than the culture's production, suggesting that underneath gradual societal
changes are the explorations of individual synthesis and discovery. Our
quantitative methods advance the study of cognitive search through a framework
for testing interactions between individual and collective behavior and between
short- and long-term consumption choices. This novel application of topic
modeling to characterize individual reading complements widespread studies of
collective scientific behavior.



Boolean matrix factorization and Boolean matrix completion from noisy
observations are desirable unsupervised data-analysis methods due to their
interpretability, but hard to perform due to their NP-hardness. We treat these
problems as maximum a posteriori inference problems in a graphical model and
present a message passing approach that scales linearly with the number of
observations and factors. Our empirical study demonstrates that message passing
is able to recover low-rank Boolean matrices, in the boundaries of
theoretically possible recovery and compares favorably with state-of-the-art in
real-world applications, such collaborative filtering with large-scale Boolean
data.



We develop a parallel variational inference (VI) procedure for use in
data-distributed settings, where each machine only has access to a subset of
data and runs VI independently, without communicating with other machines. This
type of "embarrassingly parallel" procedure has recently been developed for
MCMC inference algorithms; however, in many cases it is not possible to
directly extend this procedure to VI methods without requiring certain
restrictive exponential family conditions on the form of the model.
Furthermore, most existing (nonparallel) VI methods are restricted to use on
conditionally conjugate models, which limits their applicability. To combat
these issues, we make use of the recently proposed nonparametric VI to
facilitate an embarrassingly parallel VI procedure that can be applied to a
wider scope of models, including to nonconjugate models. We derive our
embarrassingly parallel VI algorithm, analyze our method theoretically, and
demonstrate our method empirically on a few nonconjugate models.



The only rigorous approaches for achieving a numerical proof of optimality in
global optimization are interval-based methods that interleave branching of the
search-space and pruning of the subdomains that cannot contain an optimal
solution. State-of-the-art solvers generally integrate local optimization
algorithms to compute a good upper bound of the global minimum over each
subspace. In this document, we propose a cooperative framework in which
interval methods cooperate with evolutionary algorithms. The latter are
stochastic algorithms in which a population of candidate solutions iteratively
evolves in the search-space to reach satisfactory solutions.
  Within our cooperative solver Charibde, the evolutionary algorithm and the
interval-based algorithm run in parallel and exchange bounds, solutions and
search-space in an advanced manner via message passing. A comparison of
Charibde with state-of-the-art interval-based solvers (GlobSol, IBBA, Ibex) and
NLP solvers (Couenne, BARON) on a benchmark of difficult COCONUT problems shows
that Charibde is highly competitive against non-rigorous solvers and converges
faster than rigorous solvers by an order of magnitude.



Existing methods for retrieving k-nearest neighbours suffer from the curse of
dimensionality. We argue this is caused in part by inherent deficiencies of
space partitioning, which is the underlying strategy used by most existing
methods. We devise a new strategy that avoids partitioning the vector space and
present a novel randomized algorithm that runs in time linear in dimensionality
of the space and sub-linear in the intrinsic dimensionality and the size of the
dataset and takes space constant in dimensionality of the space and linear in
the size of the dataset. The proposed algorithm allows fine-grained control
over accuracy and speed on a per-query basis, automatically adapts to
variations in data density, supports dynamic updates to the dataset and is
easy-to-implement. We show appealing theoretical properties and demonstrate
empirically that the proposed algorithm outperforms locality-sensitivity
hashing (LSH) in terms of approximation quality, speed and space efficiency.



We present a novel framework for performing statistical sampling, expectation
estimation, and partition function approximation using \emph{arbitrary}
heuristic stochastic processes defined over discrete state spaces. Using a
highly parallel construction we call the \emph{sequential constraining
process}, we are able to simultaneously generate states with the heuristic
process and accurately estimate their probabilities, even when they are far too
small to be realistically inferred by direct counting. After showing that both
theoretically correct importance sampling and Markov chain Monte Carlo are
possible using the sequential constraining process, we integrate it into a
methodology called \emph{state space sampling}, extending the ideas of state
space search from computer science to the sampling context. The methodology
comprises a dynamic data structure that constructs a robust Bayesian model of
the statistics generated by the heuristic process subject to an accuracy
constraint, the posterior Kullback-Leibler divergence. Sampling from the
dynamic structure will generally yield partial states, which are completed by
recursively calling the heuristic to refine the structure and resuming the
sampling. Our experiments on various Ising models suggest that state space
sampling enables heuristic state generation with accurate probability
estimates, demonstrated by illustrating the convergence of a simulated
annealing process to the Boltzmann distribution with increasing run length.
Consequently, heretofore unprecedented direct importance sampling using the
\emph{final} (marginal) distribution of a generic stochastic process is
allowed, potentially augmenting the range of algorithms at the Monte Carlo
practitioner's disposal.



We present ShapeNet: a richly-annotated, large-scale repository of shapes
represented by 3D CAD models of objects. ShapeNet contains 3D models from a
multitude of semantic categories and organizes them under the WordNet taxonomy.
It is a collection of datasets providing many semantic annotations for each 3D
model such as consistent rigid alignments, parts and bilateral symmetry planes,
physical sizes, keywords, as well as other planned annotations. Annotations are
made available through a public web-based interface to enable data
visualization of object attributes, promote data-driven geometric analysis, and
provide a large-scale quantitative benchmark for research in computer graphics
and vision. At the time of this technical report, ShapeNet has indexed more
than 3,000,000 models, 220,000 models out of which are classified into 3,135
categories (WordNet synsets). In this report we describe the ShapeNet effort as
a whole, provide details for all currently available datasets, and summarize
future plans.



During the past decade, several areas of speech and language understanding
have witnessed substantial breakthroughs from the use of data-driven models. In
the area of dialogue systems, the trend is less obvious, and most practical
systems are still built through significant engineering and expert knowledge.
Nevertheless, several recent results suggest that data-driven approaches are
feasible and quite promising. To facilitate research in this area, we have
carried out a wide survey of publicly available datasets suitable for
data-driven learning of dialogue systems. We discuss important characteristics
of these datasets, how they can be used to learn diverse dialogue strategies,
and their other potential uses. We also examine methods for transfer learning
between datasets and the use of external knowledge. Finally, we discuss
appropriate choice of evaluation metrics for the learning objective.



We consider data in the form of pairwise comparisons of n items, with the
goal of precisely identifying the top k items for some value of k < n, or
alternatively, recovering a ranking of all the items. We analyze the Copeland
counting algorithm that ranks the items in order of the number of pairwise
comparisons won, and show it has three attractive features: (a) its
computational efficiency leads to speed-ups of several orders of magnitude in
computation time as compared to prior work; (b) it is robust in that
theoretical guarantees impose no conditions on the underlying matrix of
pairwise-comparison probabilities, in contrast to some prior work that applies
only to the BTL parametric model; and (c) it is an optimal method up to
constant factors, meaning that it achieves the information-theoretic limits for
recovering the top k-subset. We extend our results to obtain sharp guarantees
for approximate recovery under the Hamming distortion metric, and more
generally, to any arbitrary error requirement that satisfies a simple and
natural monotonicity condition.



This paper presents to the best of our knowledge the first end-to-end object
tracking approach which directly maps from raw sensor input to object tracks in
sensor space without requiring any feature engineering or system identification
in the form of plant or sensor models. Specifically, our system accepts a
stream of raw sensor data at one end and, in real-time, produces an estimate of
the entire environment state at the output including even occluded objects. We
achieve this by framing the problem as a deep learning task and exploit
sequence models in the form of recurrent neural networks to learn a mapping
from sensor measurements to object tracks. In particular, we propose a learning
method based on a form of input dropout which allows learning in an
unsupervised manner, only based on raw, occluded sensor data without access to
ground-truth annotations. We demonstrate our approach using a synthetic dataset
designed to mimic the task of tracking objects in 2D laser data -- as commonly
encountered in robotics applications -- and show that it learns to track many
dynamic objects despite occlusions and the presence of sensor noise.



When data analysts train a classifier and check if its accuracy is
significantly different from random guessing, they are implicitly and
indirectly performing a hypothesis test (two sample testing) and it is of
importance to ask whether this indirect method for testing is statistically
optimal or not. Given that hypothesis tests attempt to maximize statistical
power subject to a bound on the allowable false positive rate, while prediction
attempts to minimize statistical risk on future predictions on unseen data, we
wish to study whether a predictive approach for an ultimate aim of testing is
prudent. We formalize this problem by considering the two-sample mean-testing
setting where one must determine if the means of two Gaussians (with known and
equal covariance) are the same or not, but the analyst indirectly does so by
checking whether the accuracy achieved by Fisher's LDA classifier is
significantly different from chance or not. Unexpectedly, we find that the
asymptotic power of LDA's sample-splitting classification accuracy is actually
minimax rate-optimal in terms of problem-dependent parameters. Since prediction
is commonly thought to be harder than testing, it might come as a surprise to
some that solving a harder problem does not create a information-theoretic
bottleneck for the easier one. On the flip side, even though the power is
rate-optimal, our derivation suggests that it may be worse by a small constant
factor; hence practitioners must be wary of using (admittedly flexible)
prediction methods on disguised testing problems.



We study an ancient problem that in a static or dynamical system, sought an
optimal path, which the context always means within an extremal condition. In
fact, through those discussions about this theme, we established a universal
essential calculated model to serve for these complex systems. Meanwhile we
utilize the sample space to character the system. These contents in this paper
would involve in several major areas including the geometry, probability, graph
algorithms and some prior approaches, which stands the ultimately subtle linear
algorithm to solve this class problem. Along with our progress, our discussion
would demonstrate more general meaning and robust character, which provides
clear ideas or notion to support our concrete applications, who work in a more
popular complex system.



The widespread integration of cameras in hand-held and head-worn devices as
well as the ability to share content online enables a large and diverse visual
capture of the world that millions of users build up collectively every day. We
envision these images as well as associated meta information, such as GPS
coordinates and timestamps, to form a collective visual memory that can be
queried while automatically taking the ever-changing context of mobile users
into account. As a first step towards this vision, in this work we present
Xplore-M-Ego: a novel media retrieval system that allows users to query a
dynamic database of images and videos using spatio-temporal natural language
queries. We evaluate our system using a new dataset of real user queries as
well as through a usability study. One key finding is that there is a
considerable amount of inter-user variability, for example in the resolution of
spatial relations in natural language utterances. We show that our retrieval
system can cope with this variability using personalisation through an online
learning-based retrieval formulation.



We develop a general duality between neural networks and compositional
kernels, striving towards a better understanding of deep learning. We show that
initial representations generated by common random initializations are
sufficiently rich to express all functions in the dual kernel space. Hence,
though the training objective is hard to optimize in the worst case, the
initial weights form a good starting point for optimization. Our dual view also
reveals a pragmatic and aesthetic perspective of neural networks and
underscores their expressive power.



We propose a new reinforcement learning algorithm for partially observable
Markov decision processes (POMDP) based on spectral decomposition methods.
While spectral methods have been previously employed for consistent learning of
(passive) latent variable models such as hidden Markov models, POMDPs are more
challenging since the learner interacts with the environment and possibly
changes the future observations in the process. We devise a learning algorithm
running through episodes, in each episode we employ spectral techniques to
learn the POMDP parameters from a trajectory generated by a fixed policy. At
the end of the episode, an optimization oracle returns the optimal memoryless
planning policy which maximizes the expected reward based on the estimated
POMDP model. We prove an order-optimal regret bound with respect to the optimal
memoryless policy and efficient scaling with respect to the dimensionality of
observation and action spaces.



Quantum computer has an amazing potential of fast information processing.
However, realisation of a digital quantum computer is still a challenging
problem requiring highly accurate controls and key application strategies. Here
we propose a novel platform, quantum reservoir computing, to solve these issues
successfully by exploiting natural quantum dynamics, which is ubiquitous in
laboratories nowadays, for machine learning. In this framework, nonlinear
dynamics including classical chaos can be universally emulated in quantum
systems. A number of numerical experiments show that quantum systems consisting
of at most seven qubits possess computational capabilities comparable to
conventional recurrent neural networks of 500 nodes. This discovery opens up a
new paradigm for information processing with artificial intelligence powered by
quantum physics.



Two fundamental problems in computational game theory are computing a Nash
equilibrium and learning to exploit opponents given observations of their play
(opponent exploitation). The latter is perhaps even more important than the
former: Nash equilibrium does not have a compelling theoretical justification
in game classes other than two-player zero-sum, and for all games one can
potentially do better by exploiting perceived weaknesses of the opponent than
by following a static equilibrium strategy throughout the match. The natural
setting for opponent exploitation is the Bayesian setting where we have a prior
model that is integrated with observations to create a posterior opponent model
that we respond to. The most natural, and a well-studied prior distribution is
the Dirichlet distribution. An exact polynomial-time algorithm is known for
best-responding to the posterior distribution for an opponent assuming a
Dirichlet prior with multinomial sampling in normal-form games; however, for
imperfect-information games the best known algorithm is based on approximating
an infinite integral without theoretical guarantees. We present the first exact
algorithm for a natural class of imperfect-information games. We demonstrate
that our algorithm runs quickly in practice and outperforms the best prior
approaches. We also present an algorithm for the uniform prior setting.



Recent approaches based on artificial neural networks (ANNs) have shown
promising results for short-text classification. However, many short texts
occur in sequences (e.g., sentences in a document or utterances in a dialog),
and most existing ANN-based systems do not leverage the preceding short texts
when classifying a subsequent one. In this work, we present a model based on
recurrent neural networks and convolutional neural networks that incorporates
the preceding short texts. Our model achieves state-of-the-art results on three
different datasets for dialog act prediction.



One goal of online social recommendation systems is to harness the wisdom of
crowds in order to identify high quality content. Yet the sequential voting
mechanisms that are commonly used by these systems are at odds with existing
theoretical and empirical literature on optimal aggregation. This literature
suggests that sequential voting will promote herding---the tendency for
individuals to copy the decisions of others around them---and hence lead to
suboptimal content recommendation. Is there a problem with our practice, or a
problem with our theory? Previous attempts at answering this question have been
limited by a lack of objective measurements of content quality. Quality is
typically defined endogenously as the popularity of content in absence of
social influence. The flaw of this metric is its presupposition that the
preferences of the crowd are aligned with underlying quality. Domains in which
content quality can be defined exogenously and measured objectively are thus
needed in order to better assess the design choices of social recommendation
systems. In this work, we look to the domain of education, where content
quality can be measured via how well students are able to learn from the
material presented to them. Through a behavioral experiment involving a
simulated massive open online course (MOOC) run on Amazon Mechanical Turk, we
show that sequential voting systems can surface better content than systems
that elicit independent votes.



While many models are purposed for detecting the occurrence of significant
events in financial systems, the task of providing qualitative detail on the
developments is not usually as well automated. We present a deep learning
approach for detecting relevant discussion in text and extracting natural
language descriptions of events. Supervised by only a small set of event
information, comprising entity names and dates, the model is leveraged by
unsupervised learning of semantic vector representations on extensive text
data. We demonstrate applicability to the study of financial risk based on news
(6.6M articles), particularly bank distress and government interventions (243
events), where indices can signal the level of bank-stress-related reporting at
the entity level, or aggregated at national or European level, while being
coupled with explanations. Thus, we exemplify how text, as timely, widely
available and descriptive data, can serve as a useful complementary source of
information for financial and systemic risk analytics.



We study methods for aggregating pairwise comparison data in order to
estimate outcome probabilities for future comparisons among a collection of n
items. Working within a flexible framework that imposes only a form of strong
stochastic transitivity (SST), we introduce an adaptivity index defined by the
indifference sets of the pairwise comparison probabilities. In addition to
measuring the usual worst-case risk of an estimator, this adaptivity index also
captures the extent to which the estimator adapts to instance-specific
difficulty relative to an oracle estimator. We prove three main results that
involve this adaptivity index and different algorithms. First, we propose a
three-step estimator termed Count-Randomize-Least squares (CRL), and show that
it has adaptivity index upper bounded as $\sqrt{n}$ up to logarithmic factors.
We then show that that conditional on the hardness of planted clique, no
computationally efficient estimator can achieve an adaptivity index smaller
than $\sqrt{n}$. Second, we show that a regularized least squares estimator can
achieve a poly-logarithmic adaptivity index, thereby demonstrating a
$\sqrt{n}$-gap between optimal and computationally achievable adaptivity.
Finally, we prove that the standard least squares estimator, which is known to
be optimally adaptive in several closely related problems, fails to adapt in
the context of estimating pairwise probabilities.



Recent progress in artificial intelligence (AI) has renewed interest in
building systems that learn and think like people. Many advances have come from
using deep neural networks trained end-to-end in tasks such as object
recognition, video games, and board games, achieving performance that equals or
even beats humans in some respects. Despite their biological inspiration and
performance achievements, these systems differ from human intelligence in
crucial ways. We review progress in cognitive science suggesting that truly
human-like learning and thinking machines will have to reach beyond current
engineering trends in both what they learn, and how they learn it.
Specifically, we argue that these machines should (a) build causal models of
the world that support explanation and understanding, rather than merely
solving pattern recognition problems; (b) ground learning in intuitive theories
of physics and psychology, to support and enrich the knowledge that is learned;
and (c) harness compositionality and learning-to-learn to rapidly acquire and
generalize knowledge to new tasks and situations. We suggest concrete
challenges and promising routes towards these goals that can combine the
strengths of recent neural network advances with more structured cognitive
models.



Researchers often summarize their work in the form of posters. Posters
provide a coherent and efficient way to convey core ideas from scientific
papers. Generating a good scientific poster, however, is a complex and time
consuming cognitive task, since such posters need to be readable, informative,
and visually aesthetic. In this paper, for the first time, we study the
challenging problem of learning to generate posters from scientific papers. To
this end, a data-driven framework, that utilizes graphical models, is proposed.
Specifically, given content to display, the key elements of a good poster,
including panel layout and attributes of each panel, are learned and inferred
from data. Then, given inferred layout and attributes, composition of graphical
elements within each panel is synthesized. To learn and validate our model, we
collect and make public a Poster-Paper dataset, which consists of scientific
papers and corresponding posters with exhaustively labelled panels and
attributes. Qualitative and quantitative results indicate the effectiveness of
our approach.



Feature extraction has gained increasing attention in the field of machine
learning, as in order to detect patterns, extract information, or predict
future observations from big data, the urge of informative features is crucial.
The process of extracting features is highly linked to dimensionality reduction
as it implies the transformation of the data from a sparse high-dimensional
space, to higher level meaningful abstractions. This dissertation employs
Neural Networks for distributed paragraph representations, and Latent Dirichlet
Allocation to capture higher level features of paragraph vectors. Although
Neural Networks for distributed paragraph representations are considered the
state of the art for extracting paragraph vectors, we show that a quick topic
analysis model such as Latent Dirichlet Allocation can provide meaningful
features too. We evaluate the two methods on the CMU Movie Summary Corpus, a
collection of 25,203 movie plot summaries extracted from Wikipedia. Finally,
for both approaches, we use K-Nearest Neighbors to discover similar movies, and
plot the projected representations using T-Distributed Stochastic Neighbor
Embedding to depict the context similarities. These similarities, expressed as
movie distances, can be used for movies recommendation. The recommended movies
of this approach are compared with the recommended movies from IMDB, which use
a collaborative filtering recommendation approach, to show that our two models
could constitute either an alternative or a supplementary recommendation
approach.



While perception tasks such as visual object recognition and text
understanding play an important role in human intelligence, the subsequent
tasks that involve inference, reasoning and planning require an even higher
level of intelligence. The past few years have seen major advances in many
perception tasks using deep learning models. For higher-level inference,
however, probabilistic graphical models with their Bayesian nature are still
more powerful and flexible. To achieve integrated intelligence that involves
both perception and inference, it is naturally desirable to tightly integrate
deep learning and Bayesian models within a principled probabilistic framework,
which we call Bayesian deep learning. In this unified framework, the perception
of text or images using deep learning can boost the performance of higher-level
inference and in return, the feedback from the inference process is able to
enhance the perception of text or images. This survey provides a general
introduction to Bayesian deep learning and reviews its recent applications on
recommender systems, topic models, and control. In this survey, we also discuss
the relationship and differences between Bayesian deep learning and other
related topics like Bayesian treatment of neural networks.



We formalize the idea of probability distributions that lead to reliable
predictions about some, but not all aspects of a domain. The resulting notion
of `safety' provides a fresh perspective on foundational issues in statistics,
providing a middle ground between imprecise probability and multiple-prior
models on the one hand and strictly Bayesian approaches on the other. It also
allows us to formalize fiducial distributions in terms of the set of random
variables that they can safely predict, thus taking some of the sting out of
the fiducial idea. By restricting probabilistic inference to safe uses, one
also automatically avoids paradoxes such as the Monty Hall problem. Safety
comes in a variety of degrees, such as "validity" (the strongest notion),
"calibration", "confidence safety" and "unbiasedness" (almost the weakest
notion).



Public debates are a common platform for presenting and juxtaposing diverging
views on important issues. In this work we propose a methodology for tracking
how ideas flow between participants throughout a debate. We use this approach
in a case study of Oxford-style debates---a competitive format where the winner
is determined by audience votes---and show how the outcome of a debate depends
on aspects of conversational flow. In particular, we find that winners tend to
make better use of a debate's interactive component than losers, by actively
pursuing their opponents' points rather than promoting their own ideas over the
course of the conversation.



The Coalitional Manipulation problem has been studied extensively in the
literature for many voting rules. However, most studies have focused on the
complete information setting, wherein the manipulators know the votes of the
non-manipulators. While this assumption is reasonable for purposes of showing
intractability, it is unrealistic for algorithmic considerations. In most
real-world scenarios, it is impractical for the manipulators to have accurate
knowledge of all the other votes. In this paper, we investigate manipulation
with incomplete information. In our framework, the manipulators know a partial
order for each voter that is consistent with the true preference of that voter.
In this setting, we formulate three natural computational notions of
manipulation, namely weak, opportunistic, and strong manipulation. We say that
an extension of a partial order is if there exists a manipulative vote for that
extension.
  1. Weak Manipulation (WM): the manipulators seek to vote in a way that makes
their preferred candidate win in at least one extension of the partial votes of
the non-manipulators.
  2. Opportunistic Manipulation (OM): the manipulators seek to vote in a way
that makes their preferred candidate win in every viable extension of the
partial votes of the non-manipulators.
  3. Strong Manipulation (SM): the manipulators seek to vote in a way that
makes their preferred candidate win in every extension of the partial votes of
the non-manipulators.
  We consider several scenarios for which the traditional manipulation problems
are easy (for instance, Borda with a single manipulator). For many of them, the
corresponding manipulative questions that we propose turn out to be
computationally intractable. Our hardness results often hold even when very
little information is missing, or in other words, even when the instances are
quite close to the complete information setting.



In multiagent systems, we often have a set of agents each of which have a
preference ordering over a set of items and one would like to know these
preference orderings for various tasks, for example, data analysis, preference
aggregation, voting etc. However, we often have a large number of items which
makes it impractical to ask the agents for their complete preference ordering.
In such scenarios, we usually elicit these agents' preferences by asking (a
hopefully small number of) comparison queries --- asking an agent to compare
two items. Prior works on preference elicitation focus on unrestricted domain
and the domain of single peaked preferences and show that the preferences in
single peaked domain can be elicited by much less number of queries compared to
unrestricted domain. We extend this line of research and study preference
elicitation for single peaked preferences on trees which is a strict superset
of the domain of single peaked preferences. We show that the query complexity
crucially depends on the number of leaves, the path cover number, and the
distance from path of the underlying single peaked tree, whereas the other
natural parameters like maximum degree, diameter, pathwidth do not play any
direct role in determining query complexity. We then investigate the query
complexity for finding a weak Condorcet winner for preferences single peaked on
a tree and show that this task has much less query complexity than preference
elicitation. Here again we observe that the number of leaves in the underlying
single peaked tree and the path cover number of the tree influence the query
complexity of the problem.



In this work we present a novel end-to-end framework for tracking and
classifying a robot's surroundings in complex, dynamic and only partially
observable real-world environments. The approach deploys a recurrent neural
network to filter an input stream of raw laser measurements in order to
directly infer object locations, along with their identity in both visible and
occluded areas. To achieve this we first train the network using unsupervised
Deep Tracking, a recently proposed theoretical framework for end-to-end space
occupancy prediction. We show that by learning to track on a large amount of
unsupervised data, the network creates a rich internal representation of its
environment which we in turn exploit through the principle of inductive
transfer of knowledge to perform the task of it's semantic classification. As a
result, we show that only a small amount of labelled data suffices to steer the
network towards mastering this additional task. Furthermore we propose a novel
recurrent neural network architecture specifically tailored to tracking and
semantic classification in real-world robotics applications. We demonstrate the
tracking and classification performance of the method on real-world data
collected at a busy road junction. Our evaluation shows that the proposed
end-to-end framework compares favourably to a state-of-the-art, model-free
tracking solution and that it outperforms a conventional one-shot training
scheme for semantic classification.



Learning goal-directed behavior in environments with sparse feedback is a
major challenge for reinforcement learning algorithms. The primary difficulty
arises due to insufficient exploration, resulting in an agent being unable to
learn robust value functions. Intrinsically motivated agents can explore new
behavior for its own sake rather than to directly solve problems. Such
intrinsic behaviors could eventually help the agent solve tasks posed by the
environment. We present hierarchical-DQN (h-DQN), a framework to integrate
hierarchical value functions, operating at different temporal scales, with
intrinsically motivated deep reinforcement learning. A top-level value function
learns a policy over intrinsic goals, and a lower-level function learns a
policy over atomic actions to satisfy the given goals. h-DQN allows for
flexible goal specifications, such as functions over entities and relations.
This provides an efficient space for exploration in complicated environments.
We demonstrate the strength of our approach on two problems with very sparse,
delayed feedback: (1) a complex discrete stochastic decision process, and (2)
the classic ATARI game `Montezuma's Revenge'.



Despite significant progress in object categorization, in recent years, a
number of important challenges remain, mainly, ability to learn from limited
labeled data and ability to recognize object classes within large, potentially
open, set of labels. Zero-shot learning is one way of addressing these
challenges, but it has only been shown to work with limited sized class
vocabularies and typically requires separation between supervised and
unsupervised classes, allowing former to inform the latter but not vice versa.
We propose the notion of semi-supervised vocabulary-informed learning to
alleviate the above mentioned challenges and address problems of supervised,
zero-shot and open set recognition using a unified framework. Specifically, we
propose a maximum margin framework for semantic manifold-based recognition that
incorporates distance constraints from (both supervised and unsupervised)
vocabulary atoms, ensuring that labeled samples are projected closest to their
correct prototypes, in the embedding space, than to others. We show that
resulting model shows improvements in supervised, zero-shot, and large open set
recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.



Protein secondary structure prediction is an important problem in
bioinformatics. Inspired by the recent successes of deep neural networks, in
this paper, we propose an end-to-end deep network that predicts protein
secondary structures from integrated local and global contextual features. Our
deep architecture leverages convolutional neural networks with different kernel
sizes to extract multiscale local contextual features. In addition, considering
long-range dependencies existing in amino acid sequences, we set up a
bidirectional neural network consisting of gated recurrent unit to capture
global contextual features. Furthermore, multi-task learning is utilized to
predict secondary structure labels and amino-acid solvent accessibility
simultaneously. Our proposed deep network demonstrates its effectiveness by
achieving state-of-the-art performance, i.e., 69.7% Q8 accuracy on the public
benchmark CB513, 76.9% Q8 accuracy on CASP10 and 73.1% Q8 accuracy on CASP11.
Our model and results are publicly available.



Group discussions are essential for organizing every aspect of modern life,
from faculty meetings to senate debates, from grant review panels to papal
conclaves. While costly in terms of time and organization effort, group
discussions are commonly seen as a way of reaching better decisions compared to
solutions that do not require coordination between the individuals (e.g.
voting)---through discussion, the sum becomes greater than the parts. However,
this assumption is not irrefutable: anecdotal evidence of wasteful discussions
abounds, and in our own experiments we find that over 30% of discussions are
unproductive.
  We propose a framework for analyzing conversational dynamics in order to
determine whether a given task-oriented discussion is worth having or not. We
exploit conversational patterns reflecting the flow of ideas and the balance
between the participants, as well as their linguistic choices. We apply this
framework to conversations naturally occurring in an online collaborative world
exploration game developed and deployed to support this research. Using this
setting, we show that linguistic cues and conversational patterns extracted
from the first 20 seconds of a team discussion are predictive of whether it
will be a wasteful or a productive one.



The inverse problem of general rough sets, considered by the present author
in some of her earlier papers, in one of its manifestations is essentially the
question of when an agent's view about crisp and non crisp objects over a set
of objects has a rough evolution. In this research the nature of the problem is
examined from number-theoretic and combinatorial perspectives under very few
assumptions about the nature of data and some necessary conditions are proved.



We study the stochastic online problem of learning to influence in a social
network with semi-bandit feedback, where we observe how users influence each
other. The problem combines challenges of limited feedback, because the
learning agent only observes the influenced portion of the network, and
combinatorial number of actions, because the cardinality of the feasible set is
exponential in the maximum number of influencers. We propose a computationally
efficient UCB-like algorithm, IMLinUCB, and analyze it. Our regret bounds are
polynomial in all quantities of interest; reflect the structure of the network
and the probabilities of influence. Moreover, they do not depend on inherently
large quantities, such as the cardinality of the action set. To the best of our
knowledge, these are the first such results. IMLinUCB permits linear
generalization and therefore is suitable for large-scale problems. Our
experiments show that the regret of IMLinUCB scales as suggested by our upper
bounds in several representative graph topologies; and based on linear
generalization, IMLinUCB can significantly reduce regret of real-world
influence maximization semi-bandits.



The iterative nature of the expectation maximization (EM) algorithm presents
a challenge for privacy-preserving estimation, as each iteration increases the
amount of noise needed. We propose a practical private EM algorithm that
overcomes this challenge using two innovations: (1) a novel moment perturbation
formulation for differentially private EM (DP-EM), and (2) the use of two
recently developed composition methods to bound the privacy "cost" of multiple
EM iterations: the moments accountant (MA) and zero-mean concentrated
differential privacy (zCDP). Both MA and zCDP bound the moment generating
function of the privacy loss random variable and achieve a refined tail bound,
which effectively decrease the amount of additive noise. We present empirical
results showing the benefits of our approach, as well as similar performance
between these two composition methods in the DP-EM setting for Gaussian mixture
models. Our approach can be readily extended to many iterative learning
algorithms, opening up various exciting future directions.



While great strides have been made in using deep learning algorithms to solve
supervised learning tasks, the problem of unsupervised learning - leveraging
unlabeled examples to learn about the structure of a domain - remains a
difficult unsolved challenge. Here, we explore prediction of future frames in a
video sequence as an unsupervised learning rule for learning about the
structure of the visual world. We describe a predictive neural network
("PredNet") architecture that is inspired by the concept of "predictive coding"
from the neuroscience literature. These networks learn to predict future frames
in a video sequence, with each layer in the network making local predictions
and only forwarding deviations from those predictions to subsequent network
layers. We show that these networks are able to robustly learn to predict the
movement of synthetic (rendered) objects, and that in doing so, the networks
learn internal representations that are useful for decoding latent object
parameters (e.g. pose) that support object recognition with fewer training
views. We also show that these networks can scale to complex natural image
streams (car-mounted camera videos), capturing key aspects of both egocentric
movement and the movement of objects in the visual scene, and the
representation learned in this setting is useful for estimating the steering
angle. Altogether, these results suggest that prediction represents a powerful
framework for unsupervised learning, allowing for implicit learning of object
and scene structure.



The ability of the Generative Adversarial Networks (GANs) framework to learn
generative models mapping from simple latent distributions to arbitrarily
complex data distributions has been demonstrated empirically, with compelling
results showing that the latent space of such generators captures semantic
variation in the data distribution. Intuitively, models trained to predict
these semantic latent representations given data may serve as useful feature
representations for auxiliary problems where semantics are relevant. However,
in their existing form, GANs have no means of learning the inverse mapping --
projecting data back into the latent space. We propose Bidirectional Generative
Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and
demonstrate that the resulting learned feature representation is useful for
auxiliary supervised discrimination tasks, competitive with contemporary
approaches to unsupervised and self-supervised feature learning.



We introduce the multiresolution recurrent neural network, which extends the
sequence-to-sequence framework to model natural language generation as two
parallel discrete stochastic processes: a sequence of high-level coarse tokens,
and a sequence of natural language tokens. There are many ways to estimate or
learn the high-level coarse tokens, but we argue that a simple extraction
procedure is sufficient to capture a wealth of high-level discourse semantics.
Such procedure allows training the multiresolution recurrent neural network by
maximizing the exact joint log-likelihood over both sequences. In contrast to
the standard log- likelihood objective w.r.t. natural language tokens (word
perplexity), optimizing the joint log-likelihood biases the model towards
modeling high-level abstractions. We apply the proposed model to the task of
dialogue response generation in two challenging domains: the Ubuntu technical
support domain, and Twitter conversations. On Ubuntu, the model outperforms
competing approaches by a substantial margin, achieving state-of-the-art
results according to both automatic evaluation metrics and a human evaluation
study. On Twitter, the model appears to generate more relevant and on-topic
responses according to automatic evaluation metrics. Finally, our experiments
demonstrate that the proposed model is more adept at overcoming the sparsity of
natural language and is better able to capture long-term structure.



While Bayesian methods are praised for their ability to incorporate useful
prior knowledge, in practice, convenient priors that allow for computationally
cheap or tractable inference are commonly used. In this paper, we investigate
the following question: for a given model, is it possible to compute an
inference result with any convenient false prior, and afterwards, given any
target prior of interest, quickly transform this result into the target
posterior? A potential solution is to use importance sampling (IS). However, we
demonstrate that IS will fail for many choices of the target prior, depending
on its parametric form and similarity to the false prior. Instead, we propose
prior swapping, a method that leverages the pre-inferred false posterior to
efficiently generate accurate posterior samples under arbitrary target priors.
Prior swapping lets us apply less-costly inference algorithms to certain
models, and incorporate new or updated prior information "post-inference". We
give theoretical guarantees about our method, and demonstrate it empirically on
a number of models and priors.



We introduce Bayesian Poisson Tucker decomposition (BPTD) for modeling
country--country interaction event data. These data consist of interaction
events of the form "country $i$ took action $a$ toward country $j$ at time
$t$." BPTD discovers overlapping country--community memberships, including the
number of latent communities. In addition, it discovers directed
community--community interaction networks that are specific to "topics" of
action types and temporal "regimes." We show that BPTD yields an efficient MCMC
inference algorithm and achieves better predictive performance than related
models. We also demonstrate that it discovers interpretable latent structure
that agrees with our knowledge of international relations.



Word embedding maps words into a low-dimensional continuous embedding space
by exploiting the local word collocation patterns in a small context window. On
the other hand, topic modeling maps documents onto a low-dimensional topic
space, by utilizing the global word collocation patterns in the same document.
These two types of patterns are complementary. In this paper, we propose a
generative topic embedding model to combine the two types of patterns. In our
model, topics are represented by embedding vectors, and are shared across
documents. The probability of each word is influenced by both its local context
and its topic. A variational inference method yields the topic embeddings as
well as the topic mixing proportions for each document. Jointly they represent
the document in a low-dimensional continuous space. In two document
classification tasks, our method performs better than eight existing methods,
with fewer features. In addition, we illustrate with an example that our method
can generate coherent topics even based on only one document.



Supervised machine learning models boast remarkable predictive capabilities.
But can you trust your model? Will it work in deployment? What else can it tell
you about the world? We want models to be not only good, but interpretable. And
yet the task of interpretation appears underspecified. Papers provide diverse
and sometimes non-overlapping motivations for interpretability, and offer
myriad notions of what attributes render models interpretable. Despite this
ambiguity, many papers proclaim interpretability axiomatically, absent further
explanation. In this paper, we seek to refine the discourse on
interpretability. First, we examine the motivations underlying interest in
interpretability, finding them to be diverse and occasionally discordant. Then,
we address model properties and techniques thought to confer interpretability,
identifying transparency to humans and post-hoc explanations as competing
notions. Throughout, we discuss the feasibility and desirability of different
notions, and question the oft-made assertions that linear models are
interpretable and that deep neural networks are not.



Optimal power flow (OPF) is the central optimization problem in electric
power grids. Although solved routinely in the course of power grid operations,
it is known to be strongly NP-hard in general, and weakly NP-hard over tree
networks. In this paper, we formulate the optimal power flow problem over tree
networks as an inference problem over a tree-structured graphical model where
the nodal variables are low-dimensional vectors. We adapt the standard dynamic
programming algorithm for inference over a tree-structured graphical model to
the OPF problem. Combining this with an interval discretization of the nodal
variables, we develop an approximation algorithm for the OPF problem. Further,
we use techniques from constraint programming (CP) to perform interval
computations and adaptive bound propagation to obtain practically efficient
algorithms. Compared to previous algorithms that solve OPF with optimality
guarantees using convex relaxations, our approach is able to work for arbitrary
distribution networks and handle mixed-integer optimization problems. Further,
it can be implemented in a distributed message-passing fashion that is scalable
and is suitable for "smart grid" applications like control of distributed
energy resources. We evaluate our technique numerically on several benchmark
networks and show that practical OPF problems can be solved effectively using
this approach.



Stability evaluation of a weight-update system of higher-order neural units
(HONUs) with polynomial aggregation of neural inputs (also known as classes of
polynomial neural networks) for adaptation of both feedforward and recurrent
HONUs by a gradient descent method is introduced. An essential core of the
approach is based on spectral radius of a weight-update system, and it allows
stability monitoring and its maintenance at every adaptation step individually.
Assuring stability of the weight-update system (at every single adaptation
step) naturally results in adaptation stability of the whole neural
architecture that adapts to target data. As an aside, the used approach
highlights the fact that the weight optimization of HONU is a linear problem,
so the proposed approach can be generally extended to any neural architecture
that is linear in its adaptable parameters.



We investigate the problem of learning Bayesian networks in an agnostic model
where an $\epsilon$-fraction of the samples are adversarially corrupted. Our
agnostic learning model is similar to -- in fact, stronger than -- Huber's
contamination model in robust statistics. In this work, we study the fully
observable Bernoulli case where the structure of the network is given. Even in
this basic setting, previous learning algorithms either run in exponential time
or lose dimension-dependent factors in their error guarantees. We provide the
first computationally efficient agnostic learning algorithm for this problem
with dimension-independent error guarantees. Our algorithm has polynomial
sample complexity, runs in polynomial time, and achieves error that scales
nearly-linearly with the fraction of adversarially corrupted samples.



We consider sequential or active ranking of a set of n items based on noisy
pairwise comparisons. Items are ranked according to the probability that a
given item beats a randomly chosen item, and ranking refers to partitioning the
items into sets of pre-specified sizes according to their scores. This notion
of ranking includes as special cases the identification of the top-k items and
the total ordering of the items. We first analyze a sequential ranking
algorithm that counts the number of comparisons won, and uses these counts to
decide whether to stop, or to compare another pair of items, chosen based on
confidence intervals specified by the data collected up to that point. We prove
that this algorithm succeeds in recovering the ranking using a number of
comparisons that is optimal up to logarithmic factors. This guarantee does not
require any structural properties of the underlying pairwise probability
matrix, unlike a significant body of past work on pairwise ranking based on
parametric models such as the Thurstone or Bradley-Terry-Luce models. It has
been a long-standing open question as to whether or not imposing these
parametric assumptions allows for improved ranking algorithms. For stochastic
comparison models, in which the pairwise probabilities are bounded away from
zero, our second contribution is to resolve this issue by proving a lower bound
for parametric models. This shows, perhaps surprisingly, that these popular
parametric modeling choices offer at most logarithmic gains for stochastic
comparisons.



The aggregation and denoising of crowd labeled data is a task that has gained
increased significance with the advent of crowdsourcing platforms and massive
datasets. In this paper, we propose a permutation-based model for crowd labeled
data that is a significant generalization of the common Dawid-Skene model, and
introduce a new error metric by which to compare different estimators. Working
in a high-dimensional non-asymptotic framework that allows both the number of
workers and tasks to scale, we derive optimal rates of convergence for the
permutation-based model. We show that the permutation-based model offers
significant robustness in estimation due to its richness, while surprisingly
incurring only a small additional statistical penalty as compared to the
Dawid-Skene model. Finally, we propose a computationally-efficient method,
called the OBI-WAN estimator, that is uniformly optimal over a class
intermediate between the permutation-based and the Dawid-Skene models, and is
uniformly consistent over the entire permutation-based model class. In
contrast, the guarantees for estimators available in prior literature are
sub-optimal over the original Dawid-Skene model.



A natural language interface exploits the conceptual simplicity and
naturalness of the language to create a high-level user-friendly communication
channel between humans and machines. One of the promising applications of such
interfaces is generating visual interpretations of semantic content of a given
natural language that can be then visualized either as a static scene or a
dynamic animation. This survey discusses requirements and challenges of
developing such systems and reports 26 graphical systems that exploit natural
language interfaces and addresses both artificial intelligence and
visualization aspects. This work serves as a frame of reference to researchers
and to enable further advances in the field.



This paper addresses the task of zero-shot image classification. The key
contribution of the proposed approach is to control the semantic embedding of
images -- one of the main ingredients of zero-shot learning -- by formulating
it as a metric learning problem. The optimized empirical criterion associates
two types of sub-task constraints: metric discriminating capacity and accurate
attribute prediction. This results in a novel expression of zero-shot learning
not requiring the notion of class in the training phase: only pairs of
image/attributes, augmented with a consistency indicator, are given as ground
truth. At test time, the learned model can predict the consistency of a test
image with a given set of attributes , allowing flexible ways to produce
recognition inferences. Despite its simplicity, the proposed approach gives
state-of-the-art results on four challenging datasets used for zero-shot
recognition evaluation.



Characterizing human values is a topic deeply interwoven with the sciences,
humanities, art, and many other human endeavors. In recent years, a number of
thinkers have argued that accelerating trends in computer science, cognitive
science, and related disciplines foreshadow the creation of intelligent
machines which meet and ultimately surpass the cognitive abilities of human
beings, thereby entangling an understanding of human values with future
technological development. Contemporary research accomplishments suggest
sophisticated AI systems becoming widespread and responsible for managing many
aspects of the modern world, from preemptively planning users' travel schedules
and logistics, to fully autonomous vehicles, to domestic robots assisting in
daily living. The extrapolation of these trends has been most forcefully
described in the context of a hypothetical "intelligence explosion," in which
the capabilities of an intelligent software agent would rapidly increase due to
the presence of feedback loops unavailable to biological organisms. The
possibility of superintelligent agents, or simply the widespread deployment of
sophisticated, autonomous AI systems, highlights an important theoretical
problem: the need to separate the cognitive and rational capacities of an agent
from the fundamental goal structure, or value system, which constrains and
guides the agent's actions. The "value alignment problem" is to specify a goal
structure for autonomous agents compatible with human values. In this brief
article, we suggest that recent ideas from affective neuroscience and related
disciplines aimed at characterizing neurological and behavioral universals in
the mammalian kingdom provide important conceptual foundations relevant to
describing human values. We argue that the notion of "mammalian value systems"
points to a potential avenue for fundamental research in AI safety and AI
ethics.



Collective intelligence is believed to underly the remarkable success of
human society. The formation of accurate shared beliefs is one of the key
components of human collective intelligence. How are accurate shared beliefs
formed in groups of fallible individuals? Answering this question requires a
multiscale analysis. We must understand both the individual decision mechanisms
people use, and the properties and dynamics of those mechanisms in the
aggregate. As of yet, mathematical tools for such an approach have been
lacking. To address this gap, we introduce a new analytical framework: We
propose that groups arrive at accurate shared beliefs via distributed Bayesian
inference. Distributed inference occurs through information processing at the
individual level, and yields rational belief formation at the group level. We
instantiate this framework in a new model of human social decision-making,
which we validate using a dataset we collected of over 50,000 users of an
online social trading platform where investors mimic each others' trades using
real money in foreign exchange and other asset markets. We find that in this
setting people use a decision mechanism in which popularity is treated as a
prior distribution for which decisions are best to make. This mechanism is
boundedly rational at the individual level, but we prove that in the aggregate
implements a type of approximate "Thompson sampling"---a well-known and highly
effective single-agent Bayesian machine learning algorithm for sequential
decision-making. The perspective of distributed Bayesian inference therefore
reveals how collective rationality emerges from the boundedly rational decision
mechanisms people use.



The Smallest Grammar Problem -- the problem of finding the smallest
context-free grammar that generates exactly one given sequence -- has never
been successfully applied to grammatical inference. We investigate the reasons
and propose an extended formulation that seeks to minimize non-recursive
grammars, instead of straight-line programs. In addition, we provide very
efficient algorithms that approximate the minimization problem of this class of
grammars. Our empirical evaluation shows that we are able to find smaller
models than the current best approximations to the Smallest Grammar Problem on
standard benchmarks, and that the inferred rules capture much better the
syntactic structure of natural language.



Observable operator models (OOMs) and related models are one of the most
important and powerful tools for modeling and analyzing stochastic systems.
They exactly describe dynamics of finite-rank systems and can be efficiently
and consistently estimated through spectral learning under the assumption of
identically distributed data. In this paper, we investigate the properties of
spectral learning without this assumption due to the requirements of analyzing
large-time scale systems, and show that the equilibrium dynamics of a system
can be extracted from nonequilibrium observation data by imposing an
equilibrium constraint. In addition, we propose a binless extension of spectral
learning for continuous data. In comparison with the other continuous-valued
spectral algorithms, the binless algorithm can achieve consistent estimation of
equilibrium dynamics with only linear complexity.



Computation is classically studied in terms of automata, formal languages and
algorithms; yet, the relation between neural dynamics and symbolic
representations and operations is still unclear in traditional eliminative
connectionism. Therefore, we suggest a unique perspective on this central
issue, to which we would like to refer as to transparent connectionism, by
proposing accounts of how symbolic computation can be implemented in neural
substrates. In this study we first introduce a new model of dynamics on a
symbolic space, the versatile shift, showing that it supports the real-time
simulation of a range of automata. We then show that the Goedelization of
versatile shifts defines nonlinear dynamical automata, dynamical systems
evolving on a vectorial space. Finally, we present a mapping between nonlinear
dynamical automata and recurrent artificial neural networks. The mapping
defines an architecture characterized by its granular modularity, where data,
symbolic operations and their control are not only distinguishable in
activation space, but also spatially localizable in the network itself, while
maintaining a distributed encoding of symbolic representations. The resulting
networks simulate automata in real-time and are programmed directly, in absence
of network training. To discuss the unique characteristics of the architecture
and their consequences, we present two examples: i) the design of a Central
Pattern Generator from a finite-state locomotive controller, and ii) the
creation of a network simulating a system of interactive automata that supports
the parsing of garden-path sentences as investigated in psycholinguistics
experiments.



Preference orderings are orderings of a set of items according to the
preferences (of judges). Such orderings arise in a variety of domains,
including group decision making, consumer marketing, voting and machine
learning. Measuring the mutual information and extracting the common patterns
in a set of preference orderings are key to these areas. In this paper we deal
with the representation of sets of preference orderings, the quantification of
the degree to which judges agree on their ordering of the items (i.e. the
concordance), and the efficient, meaningful description of such sets.
  We propose to represent the orderings in a subsequence-based feature space
and present a new algorithm to calculate the size of the set of all common
subsequences - the basis of a quantification of concordance, not only for pairs
of orderings but also for sets of orderings. The new algorithm is fast and
storage efficient with a time complexity of only $O(Nn^2)$ for the orderings of
$n$ items by $N$ judges and a space complexity of only $O(\min\{Nn,n^2\})$.
  Also, we propose to represent the set of all $N$ orderings through a smallest
set of covering preferences and present an algorithm to construct this smallest
covering set.
  The source code for the algorithms is available at
https://github.com/zhiweiuu/secs



We investigated the possibility of using a machine-learning scheme in
conjunction with commercial wearable EEG-devices for translating listener's
subjective experience of music into scores that can be used for the automated
annotation of music in popular on-demand streaming services. Based on the
established -neuroscientifically sound- concepts of brainwave frequency bands,
activation asymmetry index and cross-frequency-coupling (CFC), we introduce a
Brain Computer Interface (BCI) system that automatically assigns a rating score
to the listened song. Our research operated in two distinct stages: i) a
generic feature engineering stage, in which features from signal-analytics were
ranked and selected based on their ability to associate music induced
perturbations in brainwaves with listener's appraisal of music. ii) a
personalization stage, during which the efficiency of ex- treme learning
machines (ELMs) is exploited so as to translate the derived pat- terns into a
listener's score. Encouraging experimental results, from a pragmatic use of the
system, are presented.



The paper introduces a new method for discrimination of documents given in
different scripts. The document is mapped into a uniformly coded text of
numerical values. It is derived from the position of the letters in the text
line, based on their typographical characteristics. Each code is considered as
a gray level. Accordingly, the coded text determines a 1-D image, on which
texture analysis by run-length statistics and local binary pattern is
performed. It defines feature vectors representing the script content of the
document. A modified clustering approach employed on document feature vector
groups documents written in the same script. Experimentation performed on two
custom oriented databases of historical documents in old Cyrillic, angular and
round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the
superiority of the proposed method with respect to well-known methods in the
state-of-the-art.



This paper proposes a computationally efficient approach to detecting objects
natively in 3D point clouds using convolutional neural networks (CNNs). In
particular, this is achieved by leveraging a feature-centric voting scheme to
implement novel convolutional layers which explicitly exploit the sparsity
encountered in the input. To this end, we examine the trade-off between
accuracy and speed for different architectures and additionally propose to use
an L1 penalty on the filter activations to further encourage sparsity in the
intermediate representations. To the best of our knowledge, this is the first
work to propose sparse convolutional layers and L1 regularisation for efficient
large-scale processing of 3D data. We demonstrate the efficacy of our approach
on the KITTI object detection benchmark and show that Vote3Deep models with as
few as three layers outperform the previous state of the art in both laser and
laser-vision based approaches by margins of up to 40% while remaining highly
competitive in terms of processing time.



Together with the development of more accurate methods in Computer Vision and
Natural Language Understanding, holistic architectures that answer on questions
about the content of real-world images have emerged. In this tutorial, we build
a neural-based approach to answer questions about images. We base our tutorial
on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the
models that we present here can achieve a competitive performance on both
datasets, in fact, they are among the best methods that use a combination of
LSTM with a global, full frame CNN representation of an image. We hope that
after reading this tutorial, the reader will be able to use Deep Learning
frameworks, such as Keras and introduced Kraino, to build various architectures
that will lead to a further performance improvement on this challenging task.



While deep learning has had significant successes in computer vision thanks
to the abundance of visual data, collecting sufficiently large real-world
datasets for robot learning can be costly. To increase the practicality of
these techniques on real robots, we propose a modular deep reinforcement
learning method capable of transferring models trained in simulation to a
real-world robotic task. We introduce a bottleneck between perception and
control, enabling the networks to be trained independently, but then merged and
fine-tuned in an end-to-end manner to further improve hand-eye coordination. On
a canonical, planar visually-guided robot reaching task a fine-tuned accuracy
of 1.6 pixels is achieved, a significant improvement over naive transfer (17.5
pixels), showing the potential for more complicated and broader applications.
Our method provides a technique for more efficient learning and transfer of
visuo-motor policies for real robotic systems without relying entirely on large
real-world robot datasets.



This work presents a parametrized family of divergences, namely Alpha-Beta
Log- Determinant (Log-Det) divergences, between positive definite unitized
trace class operators on a Hilbert space. This is a generalization of the
Alpha-Beta Log-Determinant divergences between symmetric, positive definite
matrices to the infinite-dimensional setting. The family of Alpha-Beta Log-Det
divergences is highly general and contains many divergences as special cases,
including the recently formulated infinite dimensional affine-invariant
Riemannian distance and the infinite-dimensional Alpha Log-Det divergences
between positive definite unitized trace class operators. In particular, it
includes a parametrized family of metrics between positive definite trace class
operators, with the affine-invariant Riemannian distance and the square root of
the symmetric Stein divergence being special cases. For the Alpha-Beta Log-Det
divergences between covariance operators on a Reproducing Kernel Hilbert Space
(RKHS), we obtain closed form formulas via the corresponding Gram matrices.



Probabilistic modeling is a powerful approach for analyzing empirical
information. We describe Edward, a library for probabilistic modeling. Edward's
design reflects an iterative process pioneered by George Box: build a model of
a phenomenon, make inferences about the model given data, and criticize the
model's fit to the data. Edward supports a broad class of probabilistic models,
efficient algorithms for inference, and many techniques for model criticism.
The library builds on top of TensorFlow to support distributed training and
hardware such as GPUs. Edward enables the development of complex probabilistic
models and their algorithms at a massive scale.



The use of bots as virtual confederates in online field experiments holds
extreme promise as a new methodological tool in computational social science.
However, this potential tool comes with inherent ethical challenges. Informed
consent can be difficult to obtain in many cases, and the use of confederates
necessarily implies the use of deception. In this work we outline a design
space for bots as virtual confederates, and we propose a set of guidelines for
meeting the status quo for ethical experimentation. We draw upon examples from
prior work in the CSCW community and the broader social science literature for
illustration. While a handful of prior researchers have used bots in online
experimentation, our work is meant to inspire future work in this area and
raise awareness of the associated ethical issues.



Neural networks (NN) have achieved state-of-the-art performance in various
applications. Unfortunately in applications where training data is
insufficient, they are often prone to overfitting. One effective way to
alleviate this problem is to exploit the Bayesian approach by using Bayesian
neural networks (BNN). Another shortcoming of NN is the lack of flexibility to
customize different distributions for the weights and neurons according to the
data, as is often done in probabilistic graphical models. To address these
problems, we propose a class of probabilistic neural networks, dubbed
natural-parameter networks (NPN), as a novel and lightweight Bayesian treatment
of NN. NPN allows the usage of arbitrary exponential-family distributions to
model the weights and neurons. Different from traditional NN and BNN, NPN takes
distributions as input and goes through layers of transformation before
producing distributions to match the target output distributions. As a Bayesian
treatment, efficient backpropagation (BP) is performed to learn the natural
parameters for the distributions over both the weights and neurons. The output
distributions of each layer, as byproducts, may be used as second-order
representations for the associated tasks such as link prediction. Experiments
on real-world datasets show that NPN can achieve state-of-the-art performance.



Hybrid methods that utilize both content and rating information are commonly
used in many recommender systems. However, most of them use either handcrafted
features or the bag-of-words representation as a surrogate for the content
information but they are neither effective nor natural enough. To address this
problem, we develop a collaborative recurrent autoencoder (CRAE) which is a
denoising recurrent autoencoder (DRAE) that models the generation of content
sequences in the collaborative filtering (CF) setting. The model generalizes
recent advances in recurrent deep learning from i.i.d. input to non-i.i.d.
(CF-based) input and provides a new denoising scheme along with a novel
learnable pooling scheme for the recurrent autoencoder. To do this, we first
develop a hierarchical Bayesian model for the DRAE and then generalize it to
the CF setting. The synergy between denoising and CF enables CRAE to make
accurate recommendations while learning to fill in the blanks in sequences.
Experiments on real-world datasets from different domains (CiteULike and
Netflix) show that, by jointly modeling the order-aware generation of sequences
for the content information and performing CF for the ratings, CRAE is able to
significantly outperform the state of the art on both the recommendation task
based on ratings and the sequence generation task based on content information.



In this paper we investigate the family of functions representable by deep
neural networks (DNN) with rectified linear units (ReLU). We give the
first-ever polynomial time (in the size of data) algorithm to train to global
optimality a ReLU DNN with one hidden layer, assuming the input dimension and
number of nodes of the network as fixed constants.
  We also improve on the known lower bounds on size (from exponential to super
exponential) for approximating a ReLU deep net function by a shallower ReLU
net. Our gap theorems hold for smoothly parametrized families of "hard"
functions, contrary to countable, discrete families known in the literature. An
example consequence of our gap theorems is the following: for every natural
number $k$ there exists a function representable by a ReLU DNN with $k^2$
hidden layers and total size $k^3$, such that any ReLU DNN with at most $k$
hidden layers will require at least $\frac{1}{2}k^{k+1}-1$ total nodes.
  Finally, we construct a family of $\mathbb{R}^n\to \mathbb{R}$ piecewise
linear functions for $n\geq 2$ (also smoothly parameterized), whose number of
affine pieces scales exponentially with the dimension $n$ at any fixed size and
depth. To the best of our knowledge, such a construction with exponential
dependence on $n$ has not been achieved by previous families of "hard"
functions in the neural nets literature. This construction utilizes the theory
of zonotopes from polyhedral theory.



Detecting and classifying targets in video streams from surveillance cameras
is a cumbersome, error-prone and expensive task. Often, the incurred costs are
prohibitive for real-time monitoring. This leads to data being stored locally
or transmitted to a central storage site for post-incident examination. The
required communication links and archiving of the video data are still
expensive and this setup excludes preemptive actions to respond to imminent
threats. An effective way to overcome these limitations is to build a smart
camera that transmits alerts when relevant video sequences are detected. Deep
neural networks (DNNs) have come to outperform humans in visual classifications
tasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be
extended to make use of higher-dimensional input data such as multispectral
data. We explore this opportunity in terms of achievable accuracy and required
computational effort. To analyze the precision of DNNs for scene labeling in an
urban surveillance scenario we have created a dataset with 8 classes obtained
in a field experiment. We combine an RGB camera with a 25-channel VIS-NIR
snapshot sensor to assess the potential of multispectral image data for target
classification. We evaluate several new DNNs, showing that the spectral
information fused together with the RGB frames can be used to improve the
accuracy of the system or to achieve similar accuracy with a 3x smaller
computation effort. We achieve a very high per-pixel accuracy of 99.1%. Even
for scarcely occurring, but particularly interesting classes, such as cars, 75%
of the pixels are labeled correctly with errors occurring only around the
border of the objects. This high accuracy was obtained with a training set of
only 30 labeled images, paving the way for fast adaptation to various
application scenarios.



We propose a method to optimize the representation and distinguishability of
samples from two probability distributions, by maximizing the estimated power
of a statistical test based on the maximum mean discrepancy (MMD). This
optimized MMD is applied to the setting of unsupervised learning by generative
adversarial networks (GAN), in which a model attempts to generate realistic
samples, and a discriminator attempts to tell these apart from data samples. In
this context, the MMD may be used in two roles: first, as a discriminator,
either directly on the samples, or on features of the samples. Second, the MMD
can be used to evaluate the performance of a generative model, by testing the
model's samples against a reference data set. In the latter role, the optimized
MMD is particularly helpful, as it gives an interpretable indication of how the
model and data distributions differ, even in cases where individual model
samples are not easily distinguished either by eye or by classifier.



The artistic style of a painting is a subtle aesthetic judgment used by art
historians for grouping and classifying artwork. The recently introduced
`neural-style' algorithm substantially succeeds in merging the perceived
artistic style of one image or set of images with the perceived content of
another. In light of this and other recent developments in image analysis via
convolutional neural networks, we investigate the effectiveness of a
`neural-style' representation for classifying the artistic style of paintings.



Lossy image compression algorithms are pervasively used to reduce the size of
images transmitted over the web and recorded on data storage media. However, we
pay for their high compression rate with visual artifacts degrading the user
experience. Deep convolutional neural networks have become a widespread tool to
address high-level computer vision tasks very successfully. Recently, they have
found their way into the areas of low-level computer vision and image
processing to solve regression problems mostly with relatively shallow
networks.
  We present a novel 12-layer deep convolutional network for image compression
artifact suppression with hierarchical skip connections and a multi-scale loss
function. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an
improvement of up to 0.36 dB over the best previous ConvNet result. We show
that a network trained for a specific quality factor (QF) is resilient to the
QF used to compress the input image - a single network trained for QF 60
provides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76.



Many real world stochastic control problems suffer from the "curse of
dimensionality". To overcome this difficulty, we develop a deep learning
approach that directly solves high-dimensional stochastic control problems
based on Monte-Carlo sampling. We approximate the time-dependent controls as
feedforward neural networks and stack these networks together through model
dynamics. The objective function for the control problem plays the role of the
loss function for the deep neural network. We test this approach using examples
from the areas of optimal trading and energy storage. Our results suggest that
the algorithm presented here achieves satisfactory accuracy and at the same
time, can handle rather high dimensional problems.



Building neural networks to query a knowledge base (a table) with natural
language is an emerging research topic in deep learning. An executor for table
querying typically requires multiple steps of execution because queries may
have complicated structures. In previous studies, researchers have developed
either fully distributed executors or symbolic executors for table querying. A
distributed executor can be trained in an end-to-end fashion, but is weak in
terms of execution efficiency and explicit interpretability. A symbolic
executor is efficient in execution, but is very difficult to train especially
at initial stages. In this paper, we propose to couple distributed and symbolic
execution for natural language queries, where the symbolic executor is
pretrained with the distributed executor's intermediate execution results in a
step-by-step fashion. Experiments show that our approach significantly
outperforms both distributed and symbolic executors, exhibiting high accuracy,
high learning efficiency, high execution efficiency, and high interpretability.



We propose a quantum machine learning algorithm for efficiently solving a
class of problems encoded in quantum controlled unitary operations. The central
physical mechanism of the protocol is the iteration of a quantum time-delayed
equation that introduces feedback in the dynamics and eliminates the necessity
of intermediate measurements. The performance of the quantum algorithm is
analyzed by comparing the results obtained in numerical simulations with the
outcome of classical machine learning methods for the same problem. The use of
time-delayed equations enhances the toolbox of the field of quantum machine
learning, which may enable unprecedented applications in quantum technologies.



We investigate whether quantum annealers with select chip layouts can
outperform classical computers in reinforcement learning tasks. We associate a
transverse field Ising spin Hamiltonian with a layout of qubits similar to that
of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to
numerically simulate quantum sampling from this system. We design a
reinforcement learning algorithm in which the set of visible nodes representing
the states and actions of an optimal policy are the first and last layers of
the deep network. In absence of a transverse field, our simulations show that
DBMs train more effectively than restricted Boltzmann machines (RBM) with the
same number of weights. Since sampling from Boltzmann distributions of a DBM is
not classically feasible, this is evidence of advantage of a non-Turing
sampling oracle. We then develop a framework for training the network as a
quantum Boltzmann machine (QBM) in the presence of a significant transverse
field for reinforcement learning. This further improves the reinforcement
learning method using DBMs.



This paper presents a novel yet intuitive approach to unsupervised feature
learning. Inspired by the human visual system, we explore whether low-level
motion-based grouping cues can be used to learn an effective visual
representation. Specifically, we use unsupervised motion-based segmentation on
videos to obtain segments, which we use as 'pseudo ground truth' to train a
convolutional network to segment objects from a single frame. Given the
extensive evidence that motion plays a key role in the development of the human
visual system, we hope that this straightforward approach to unsupervised
learning will be more effective than cleverly designed 'pretext' tasks studied
in the literature. Indeed, our extensive experiments show that this is the
case. When used for transfer learning on object detection, our representation
significantly outperforms previous unsupervised approaches across multiple
settings, especially when training data for the target task is scarce.



Techniques known as Nonlinear Set Membership prediction, Lipschitz
Interpolation or Kinky Inference are approaches to machine learning that
utilise presupposed Lipschitz properties to compute inferences over unobserved
function values. Provided a bound on the true best Lipschitz constant of the
target function is known a priori they offer convergence guarantees as well as
bounds around the predictions. Considering a more general setting that builds
on Hoelder continuity relative to pseudo-metrics, we propose an online method
for estimating the Hoelder constant online from function value observations
that possibly are corrupted by bounded observational errors. Utilising this to
compute adaptive parameters within a kinky inference rule gives rise to a
nonparametric machine learning method, for which we establish strong universal
approximation guarantees. That is, we show that our prediction rule can learn
any continuous function in the limit of increasingly dense data to within a
worst-case error bound that depends on the level of observational uncertainty.
We apply our method in the context of nonparametric model-reference adaptive
control (MRAC). Across a range of simulated aircraft roll-dynamics and
performance metrics our approach outperforms recently proposed alternatives
that were based on Gaussian processes and RBF-neural networks. For
discrete-time systems, we provide stability guarantees for our learning-based
controllers both for the batch and the online learning setting.



In de novo drug design, computational strategies are used to generate novel
molecules with good affinity to the desired biological target. In this work, we
show that recurrent neural networks can be trained as generative models for
molecular structures, similar to statistical language models in natural
language processing. We demonstrate that the properties of the generated
molecules correlate very well with the properties of the molecules used to
train the model. In order to enrich libraries with molecules active towards a
given biological target, we propose to fine-tune the model with small sets of
molecules, which are known to be active against that target.
  Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test
molecules that medicinal chemists designed, whereas against Plasmodium
falciparum (Malaria) it reproduced 28% of 1240 test molecules. When coupled
with a scoring function, our model can perform the complete de novo drug design
cycle to generate large sets of novel molecules for drug discovery.



Higher-order probabilistic programming languages allow programmers to write
sophisticated models in machine learning and statistics in a succinct and
structured way, but step outside the standard measure-theoretic formalization
of probability theory. Programs may use both higher-order functions and
continuous distributions, or even define a probability distribution on
functions. But standard probability theory does not handle higher-order
functions well: the category of measurable spaces is not cartesian closed.
  Here we introduce quasi-Borel spaces. We show that these spaces: form a new
formalization of probability theory replacing measurable spaces; form a
cartesian closed category and so support higher-order functions; form a
well-pointed category and so support good proof principles for equational
reasoning; and support continuous probability distributions. We demonstrate the
use of quasi-Borel spaces for higher-order functions and probability by:
showing that a well-known construction of probability theory involving random
functions gains a cleaner expression; and generalizing de Finetti's theorem,
that is a crucial theorem in probability theory, to quasi-Borel spaces.



We propose Edward, a Turing-complete probabilistic programming language.
Edward defines two compositional representations---random variables and
inference. By treating inference as a first class citizen, on a par with
modeling, we show that probabilistic programming can be as flexible and
computationally efficient as traditional deep learning. For flexibility, Edward
makes it easy to fit the same model using a variety of composable inference
methods, ranging from point estimation to variational inference to MCMC. In
addition, Edward can reuse the modeling representation as part of inference,
facilitating the design of rich variational models and generative adversarial
networks. For efficiency, Edward is integrated into TensorFlow, providing
significant speedups over existing probabilistic systems. For example, we show
on a benchmark logistic regression task that Edward is at least 35x faster than
Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it
is as fast as handwritten TensorFlow.



The promise of compressive sensing (CS) has been offset by two significant
challenges. First, real-world data is not exactly sparse in a fixed basis.
Second, current high-performance recovery algorithms are slow to converge,
which limits CS to either non-real-time applications or scenarios where massive
back-end computing is available. In this paper, we attack both of these
challenges head-on by developing a new signal recovery framework we call {\em
DeepInverse} that learns the inverse transformation from measurement vectors to
signals using a {\em deep convolutional network}. When trained on a set of
representative images, the network learns both a representation for the signals
(addressing challenge one) and an inverse map approximating a greedy or convex
recovery algorithm (addressing challenge two). Our experiments indicate that
the DeepInverse network closely approximates the solution produced by
state-of-the-art CS recovery algorithms yet is hundreds of times faster in run
time. The tradeoff for the ultrafast run time is a computationally intensive,
off-line training procedure typical to deep networks. However, the training
needs to be completed only once, which makes the approach attractive for a host
of sparse recovery problems.



Superconducting circuit technologies have recently achieved quantum protocols
involving closed feedback loops. Quantum artificial intelligence and quantum
machine learning are emerging fields inside quantum technologies which may
enable quantum devices to acquire information from the outer world and improve
themselves via a learning process. Here we propose the implementation of basic
protocols in quantum reinforcement learning, with superconducting circuits
employing feedback-loop control. We introduce diverse scenarios for
proof-of-principle experiments with state-of-the-art superconducting circuit
technologies and analyze their feasibility in presence of imperfections. The
field of quantum artificial intelligence implemented with superconducting
circuits paves the way for enhanced quantum control and quantum computation
protocols.



In this paper, we focus on online representation learning in non-stationary
environments which may require continuous adaptation of model architecture. We
propose a novel online dictionary-learning (sparse-coding) framework which
incorporates the addition and deletion of hidden units (dictionary elements),
and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of
the hippocampus, known to be associated with improved cognitive function and
adaptation to new environments. In the online learning setting, where new input
instances arrive sequentially in batches, the neuronal-birth is implemented by
adding new units with random initial weights (random dictionary elements); the
number of new units is determined by the current performance (representation
error) of the dictionary, higher error causing an increase in the birth rate.
Neuronal-death is implemented by imposing l1/l2-regularization (group sparsity)
on the dictionary within the block-coordinate descent optimization at each
iteration of our online alternating minimization scheme, which iterates between
the code and dictionary updates. Finally, hidden unit connectivity adaptation
is facilitated by introducing sparsity in dictionary elements. Our empirical
evaluation on several real-life datasets (images and language) as well as on
synthetic data demonstrates that the proposed approach can considerably
outperform the state-of-art fixed-size (nonadaptive) online sparse coding of
Mairal et al. (2009) in the presence of nonstationary data. Moreover, we
identify certain properties of the data (e.g., sparse inputs with nearly
non-overlapping supports) and of the model (e.g., dictionary sparsity)
associated with such improvements.



Fuzzing consists of repeatedly testing an application with modified, or
fuzzed, inputs with the goal of finding security vulnerabilities in
input-parsing code. In this paper, we show how to automate the generation of an
input grammar suitable for input fuzzing using sample inputs and
neural-network-based statistical machine-learning techniques. We present a
detailed case study with a complex input format, namely PDF, and a large
complex security-critical parser for this format, namely, the PDF parser
embedded in Microsoft's new Edge browser. We discuss (and measure) the tension
between conflicting learning and fuzzing goals: learning wants to capture the
structure of well-formed inputs, while fuzzing wants to break that structure in
order to cover unexpected code paths and find bugs. We also present a new
algorithm for this learn&fuzz challenge which uses a learnt input probability
distribution to intelligently guide where to fuzz inputs.



The study of mereology (parts and wholes) in the context of formal approaches
to vagueness can be approached in a number of ways. In the context of rough
sets, mereological concepts with a set-theoretic or valuation based ontology
acquire complex and diverse behavior. In this research a general rough set
framework called granular operator spaces is extended and the nature of
parthood in it is explored from a minimally intrusive point of view. This is
used to develop counting strategies that help in classifying the framework. The
developed methodologies would be useful for drawing involved conclusions about
the nature of data (and validity of assumptions about it) from antichains
derived from context. The problem addressed is also about whether counting
procedures help in confirming that the approximations involved in formation of
data are indeed rough approximations?



The beyond worst-case synthesis problem was introduced recently by Bruy\`ere
et al. [BFRR14]: it aims at building system controllers that provide strict
worst-case performance guarantees against an antagonistic environment while
ensuring higher expected performance against a stochastic model of the
environment. Our work extends the framework of [BFRR14] and follow-up papers,
which focused on quantitative objectives, by addressing the case of
$\omega$-regular conditions encoded as parity objectives, a natural way to
represent functional requirements of systems.
  We build strategies that satisfy a main parity objective on all plays, while
ensuring a secondary one with sufficient probability. This setting raises new
challenges in comparison to quantitative objectives, as one cannot easily mix
different strategies without endangering the functional properties of the
system. We establish that, for all variants of this problem, deciding the
existence of a strategy lies in ${\sf NP} \cap {\sf coNP}$, the same complexity
class as classical parity games. Hence, our framework provides additional
modeling power while staying in the same complexity class.
  [BFRR14] V\'eronique Bruy\`ere, Emmanuel Filiot, Mickael Randour, and
Jean-Fran\c{c}ois Raskin. Meet your expectations with guarantees: Beyond
worst-case synthesis in quantitative games. In Ernst W. Mayr and Natacha
Portier, editors, 31st International Symposium on Theoretical Aspects of
Computer Science, STACS 2014, March 5-8, 2014, Lyon, France, volume 25 of
LIPIcs, pages 199-213. Schloss Dagstuhl - Leibniz - Zentrum fuer Informatik,
2014.



Policy evaluation is a crucial step in many reinforcement-learning
procedures, which estimates a value function that predicts states' long-term
value under a given policy. In this paper, we focus on policy evaluation with
linear function approximation over a fixed dataset. We first transform the
empirical policy evaluation problem into a (quadratic) convex-concave saddle
point problem, and then present a primal-dual batch gradient method, as well as
two stochastic variance reduction methods for solving the problem. These
algorithms scale linearly in both sample size and feature dimension. Moreover,
they achieve linear convergence even when the saddle-point problem has only
strong concavity in the dual variables but no strong convexity in the primal
variables. Numerical experiments on benchmark problems demonstrate the
effectiveness of our methods.



The optimal allocation of resources for maximizing influence, spread of
information or coverage, has gained attention in the past years, in particular
in machine learning and data mining. But in applications, the parameters of the
problem are rarely known exactly, and using wrong parameters can lead to
undesirable outcomes. We hence revisit a continuous version of the Budget
Allocation or Bipartite Influence Maximization problem introduced by Alon et
al. (2012) from a robust optimization perspective, where an adversary may
choose the least favorable parameters within a confidence set. The resulting
problem is a nonconvex-concave saddle point problem (or game). We show that
this nonconvex problem can be solved exactly by leveraging connections to
continuous submodular functions, and by solving a constrained submodular
minimization problem. Although constrained submodular minimization is hard in
general, here, we establish conditions under which such a problem can be solved
to arbitrary precision $\epsilon$.



We consider elections where the voters come one at a time, in a streaming
fashion, and devise space-efficient algorithms which identify an approximate
winning committee with respect to common multiwinner proportional
representation voting rules; specifically, we consider the Approval-based and
the Borda-based variants of both the Chamberlin-- ourant rule and the Monroe
rule. We complement our algorithms with lower bounds. Somewhat surprisingly,
our results imply that, using space which does not depend on the number of
voters it is possible to efficiently identify an approximate representative
committee of fixed size over vote streams with huge number of voters.



Most exact methods for k-nearest neighbour search suffer from the curse of
dimensionality; that is, their query times exhibit exponential dependence on
either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing
(DCI) offers a promising way of circumventing the curse and successfully
reduces the dependence of query time on intrinsic dimensionality from
exponential to sublinear. In this paper, we propose a variant of DCI, which we
call Prioritized DCI, and show a remarkable improvement in the dependence of
query time on intrinsic dimensionality. In particular, a linear increase in
intrinsic dimensionality, or equivalently, an exponential increase in the
number of points near a query, can be mostly counteracted with just a linear
increase in space. We also demonstrate empirically that Prioritized DCI
significantly outperforms prior methods. In particular, relative to
Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of
distance evaluations by a factor of 14 to 116 and the memory consumption by a
factor of 21.



We investigate the performance of the standard Greedy algorithm for
cardinality constrained maximization of non-submodular nondecreasing set
functions. While there are strong theoretical guarantees on the performance of
Greedy for maximizing submodular functions, there are few guarantees for
non-submodular ones. However, Greedy enjoys strong empirical performance for
many important non-submodular functions, e.g., the Bayesian A-optimality
objective in experimental design. We prove theoretical guarantees supporting
the empirical performance. Our guarantees are characterized by a combination of
the (generalized) curvature $\alpha$ and the submodularity ratio $\gamma$. In
particular, we prove that Greedy enjoys a tight approximation guarantee of
$\frac{1}{\alpha}(1- e^{-\gamma\alpha})$ for cardinality constrained
maximization. In addition, we bound the submodularity ratio and curvature for
several important real-world objectives, including the Bayesian A-optimality
objective, the determinantal function of a square submatrix and certain linear
programs with combinatorial constraints. We experimentally validate our
theoretical findings for both synthetic and real-world applications.



This paper is a tutorial on Formal Concept Analysis (FCA) and its
applications. FCA is an applied branch of Lattice Theory, a mathematical
discipline which enables formalisation of concepts as basic units of human
thinking and analysing data in the object-attribute form. Originated in early
80s, during the last three decades, it became a popular human-centred tool for
knowledge representation and data analysis with numerous applications. Since
the tutorial was specially prepared for RuSSIR 2014, the covered FCA topics
include Information Retrieval with a focus on visualisation aspects, Machine
Learning, Data Mining and Knowledge Discovery, Text Mining and several others.



We introduce a criterion, resilience, which allows properties of a dataset
(such as its mean or best low rank approximation) to be robustly computed, even
in the presence of a large fraction of arbitrary additional data. Resilience is
a weaker condition than most other properties considered so far in the
literature, and yet enables robust estimation in a broader variety of settings.
We provide new information-theoretic results on robust distribution learning,
robust estimation of stochastic block models, and robust mean estimation under
bounded $k$th moments. We also provide new algorithmic results on robust
distribution learning, as well as robust mean estimation in $\ell_p$-norms.
Among our proof techniques is a method for pruning a high-dimensional
distribution with bounded $1$st moments to a stable "core" with bounded $2$nd
moments, which may be of independent interest.



Many problems in image processing and computer vision (e.g. colorization,
style transfer) can be posed as 'manipulating' an input image into a
corresponding output image given a user-specified guiding signal. A holy-grail
solution towards generic image manipulation should be able to efficiently alter
an input image with any personalized signals (even signals unseen during
training), such as diverse paintings and arbitrary descriptive attributes.
However, existing methods are either inefficient to simultaneously process
multiple signals (let alone generalize to unseen signals), or unable to handle
signals from other modalities. In this paper, we make the first attempt to
address the zero-shot image manipulation task. We cast this problem as
manipulating an input image according to a parametric model whose key
parameters can be conditionally generated from any guiding signal (even unseen
ones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a
fully-differentiable architecture that jointly optimizes an
image-transformation network (TNet) and a parameter network (PNet). The PNet
learns to generate key transformation parameters for the TNet given any guiding
signal while the TNet performs fast zero-shot image manipulation according to
both signal-dependent parameters from the PNet and signal-invariant parameters
from the TNet itself. Extensive experiments show that our ZM-Net can perform
high-quality image manipulation conditioned on different forms of guiding
signals (e.g. style images and attributes) in real-time (tens of milliseconds
per image) even for unseen signals. Moreover, a large-scale style dataset with
over 20,000 style images is also constructed to promote further research.



In one perspective, the central problem pursued in this research is that of
the inverse problem in the context of general rough sets. The problem is about
the existence of rough basis for given approximations in a context. Granular
operator spaces were recently introduced by the present author as an optimal
framework for anti-chain based algebraic semantics of general rough sets and
the inverse problem. In the framework, various subtypes of crisp and non crisp
objects are identifiable that may be missed in more restrictive formalism. This
is also because in the latter cases the concept of complementation and negation
are taken for granted. This opens the door for a general approach to
dialectical rough sets building on previous work of the present author and
figures of opposition. In this paper dialectical rough logics are developed
from a semantic perspective, concept of dialectical predicates is formalized,
connection with dialethias and glutty negation established, parthood analyzed
and studied from the point of view of classical and dialectical figures of
opposition. Potential semantics through dialectical counting based on these
figures are proposed building on earlier work by the present author. Her
methods become more geometrical and encompass parthood as a primary relation
(as opposed to roughly equivalent objects) for algebraic semantics. Dialectical
counting strategies over anti chains (a specific form of dialectical structure)
for semantics are also proposed.



An important goal of computer vision is to build systems that learn visual
representations over time that can be applied to many tasks. In this paper, we
investigate a vision-language embedding as a core representation and show that
it leads to better cross-task transfer than standard multi-task learning. In
particular, the task of visual recognition is aligned to the task of visual
question answering by forcing each to use the same word-region embeddings. We
show this leads to greater inductive transfer from recognition to VQA than
standard multitask learning. Visual recognition also improves, especially for
categories that have relatively few recognition training labels but appear
often in the VQA setting. Thus, our paper takes a small step towards creating
more general vision systems by showing the benefit of interpretable, flexible,
and trainable core representations.



General human action recognition requires understanding of various visual
cues. In this paper, we propose a network architecture that computes and
integrates the most important visual cues for action recognition: pose, motion,
and the raw images. For the integration, we introduce a Markov chain model
which adds cues successively. The resulting approach is efficient and
applicable to action classification as well as to spatial and temporal action
localization. The two contributions clearly improve the performance over
respective baselines. The overall approach achieves state-of-the-art action
classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover,
it yields state-of-the-art spatio-temporal action localization results on
UCF101 and J-HMDB.



This paper describes an intuitive generalization to the Generative
Adversarial Networks (GANs) to generate samples while capturing diverse modes
of the true data distribution. Firstly, we propose a very simple and intuitive
multi-agent GAN architecture that incorporates multiple generators capable of
generating samples from high probability modes. Secondly, in order to enforce
different generators to generate samples from diverse modes, we propose two
extensions to the standard GAN objective function. (1) We augment the generator
specific GAN objective function with a diversity enforcing term that encourage
different generators to generate diverse samples using a user-defined
similarity based function. (2) We modify the discriminator objective function
where along with finding the real and fake samples, the discriminator has to
predict the generator which generated the given fake sample. Intuitively, in
order to succeed in this task, the discriminator must learn to push different
generators towards different identifiable modes. Our framework is generalizable
in the sense that it can be easily combined with other existing variants of
GANs to produce diverse samples. Experimentally we show that our framework is
able to produce high quality diverse samples for the challenging tasks such as
image/face generation and image-to-image translation. We also show that it is
capable of learning a better feature representation in an unsupervised setting.



Extracting per-frame features using convolutional neural networks for
real-time processing of video data is currently mainly performed on powerful
GPU-accelerated workstations and compute clusters. However, there are many
applications such as smart surveillance cameras that require or would benefit
from on-site processing. To this end, we propose and evaluate a novel algorithm
for change-based evaluation of CNNs for video data recorded with a static
camera setting, exploiting the spatio-temporal sparsity of pixel changes. We
achieve an average speed-up of 8.6x over a cuDNN baseline on a realistic
benchmark with a negligible accuracy loss of less than 0.1% and no retraining
of the network. The resulting energy efficiency is 10x higher than that of
per-frame evaluation and reaches an equivalent of 328 GOp/s/W on the Tegra X1
platform.



This work presents a new multi-chemical experimental platform for molecular
communication where the transmitter can release different chemicals. This
platform is designed to be inexpensive and accessible, and it can be expanded
to simulate different environments including the cardiovascular system and
complex network of pipes in industrial complexes and city infrastructures. To
demonstrate the capabilities of the platform, we implement a time-slotted
binary communication system where a bit-0 is represented by an acid pulse, a
bit-1 by a base pulse, and information is carried via pH signals. The channel
model for this system, which is nonlinear and has long memories, is unknown.
Therefore, we devise novel detection algorithms that use techniques from
machine learning and deep learning to train a maximum-likelihood detector.
Using these algorithms the bit error rate improves by an order of magnitude
relative to the approach used in previous works. Moreover, our system achieves
a data rate that is an order of magnitude higher than any of the previous
molecular communication platforms.



Not all approximations arise from information systems. The problem of fitting
approximations, subjected to some rules (and related data), to information
systems in a rough scheme of things is known as the \emph{inverse problem}. The
inverse problem is more general than the duality (or abstract representation)
problems and was introduced by the present author in her earlier papers. From
the practical perspective, a few (as opposed to one) theoretical frameworks may
be suitable for formulating the problem itself. \emph{Granular operator spaces}
have been recently introduced and investigated by the present author in her
recent work in the context of antichain based and dialectical semantics for
general rough sets. The nature of the inverse problem is examined from
number-theoretic and combinatorial perspectives in a higher order variant of
granular operator spaces and some necessary conditions are proved. The results
and the novel approach would be useful in a number of unsupervised and semi
supervised learning contexts and algorithms.



Lattice-theoretic ideals have been used to define and generate non granular
rough approximations over general approximation spaces over the last few years
by few authors. The goal of these studies, in relation based rough sets, have
been to obtain nice properties comparable to those of classical rough
approximations. In this research paper, these ideas are generalized in a severe
way by the present author and associated semantic features are investigated by
her. Granules are used in the construction of approximations in implicit ways
and so a concept of co-granularity is introduced. Knowledge interpretation
associable with the approaches is also investigated. This research will be of
relevance for a number of logico-algebraic approaches to rough sets that
proceed from point-wise definitions of approximations and also for using
alternative approximations in spatial mereological contexts involving actual
contact relations. The antichain based semantics invented in earlier papers by
the present author also applies to the contexts considered.



While deep learning is remarkably successful on perceptual tasks, it was also
shown to be vulnerable to adversarial perturbations of the input. These
perturbations denote noise added to the input that was generated specifically
to fool the system while being quasi-imperceptible for humans. More severely,
there even exist universal perturbations that are input-agnostic but fool the
network on the majority of inputs. While recent work has focused on image
classification, this work proposes attacks against semantic image segmentation:
we present an approach for generating (universal) adversarial perturbations
that make the network yield a desired target segmentation as output. We show
empirically that there exist barely perceptible universal noise patterns which
result in nearly the same predicted segmentation for arbitrary inputs.
Furthermore, we also show the existence of universal noise which removes a
target class (e.g., all pedestrians) from the segmentation while leaving the
segmentation mostly unchanged otherwise.



String Kernel (SK) techniques, especially those using gapped $k$-mers as
features (gk), have obtained great success in classifying sequences like DNA,
protein, and text. However, the state-of-the-art gk-SK runs extremely slow when
we increase the dictionary size ($\Sigma$) or allow more mismatches ($M$). This
is because current gk-SK uses a trie-based algorithm to calculate co-occurrence
of mismatched substrings resulting in a time cost proportional to
$O(\Sigma^{M})$. We propose a \textbf{fast} algorithm for calculating
\underline{Ga}pped $k$-mer \underline{K}ernel using \underline{Co}unting
(GaKCo). GaKCo uses associative arrays to calculate the co-occurrence of
substrings using cumulative counting. This algorithm is fast, scalable to
larger $\Sigma$ and $M$, and naturally parallelizable. We provide a rigorous
asymptotic analysis that compares GaKCo with the state-of-the-art gk-SK.
Theoretically, the time cost of GaKCo is independent of the $\Sigma^{M}$ term
that slows down the trie-based approach. Experimentally, we observe that GaKCo
achieves the same accuracy as the state-of-the-art and outperforms its speed by
factors of 2, 100, and 4, on classifying sequences of DNA (5 datasets), protein
(12 datasets), and character-based English text (2 datasets), respectively.
  GaKCo is shared as an open source tool at
\url{https://github.com/QData/GaKCo-SVM}



As entity type systems become richer and more fine-grained, we expect the
number of types assigned to a given entity to increase. However, most
fine-grained typing work has focused on datasets that exhibit a low degree of
type multiplicity. In this paper, we consider the high-multiplicity regime
inherent in data sources such as Wikipedia that have semi-open type systems. We
introduce a set-prediction approach to this problem and show that our model
outperforms unstructured baselines on a new Wikipedia-based fine-grained typing
corpus.



While the optimization problem behind deep neural networks is highly
non-convex, it is frequently observed in practice that training deep networks
seems possible without getting stuck in suboptimal points. It has been argued
that this is the case as all local minima are close to being globally optimal.
We show that this is (almost) true, in fact almost all local minima are
globally optimal, for a fully connected network with squared loss and analytic
activation function given that the number of hidden units of one layer of the
network is larger than the number of training points and the network structure
from this layer on is pyramidal.



This paper explores the Coevolutionary Optional Prisoner's Dilemma (COPD)
game, which is a simple model to coevolve game strategy and link weights of
agents playing the Optional Prisoner's Dilemma game. We consider a population
of agents placed in a lattice grid with boundary conditions. A number of Monte
Carlo simulations are performed to investigate the impacts of the COPD game on
the emergence of cooperation. Results show that the coevolutionary rules enable
cooperators to survive and even dominate, with the presence of abstainers in
the population playing a key role in the protection of cooperators against
exploitation from defectors. We observe that in adverse conditions such as when
the initial population of abstainers is too scarce/abundant, or when the
temptation to defect is very high, cooperation has no chance of emerging.
However, when the simple coevolutionary rules are applied, cooperators
flourish.



The postulate of independence of cause and mechanism (ICM) has recently led
to several new causal discovery algorithms. The interpretation of independence
and the way it is utilized, however, varies across these methods. Our aim in
this paper is to propose a group theoretic framework for ICM to unify and
generalize these approaches. In our setting, the cause-mechanism relationship
is assessed by comparing it against a null hypothesis through the application
of random generic group transformations. We show that the group theoretic view
provides a very general tool to study the structure of data generating
mechanisms with direct applications to machine learning.



Online reviews provided by consumers are a valuable asset for e-Commerce
platforms, influencing potential consumers in making purchasing decisions.
However, these reviews are of varying quality, with the useful ones buried deep
within a heap of non-informative reviews. In this work, we attempt to
automatically identify review quality in terms of its helpfulness to the end
consumers. In contrast to previous works in this domain exploiting a variety of
syntactic and community-level features, we delve deep into the semantics of
reviews as to what makes them useful, providing interpretable explanation for
the same. We identify a set of consistency and semantic factors, all from the
text, ratings, and timestamps of user-generated reviews, making our approach
generalizable across all communities and domains. We explore review semantics
in terms of several latent factors like the expertise of its author, his
judgment about the fine-grained facets of the underlying product, and his
writing style. These are cast into a Hidden Markov Model -- Latent Dirichlet
Allocation (HMM-LDA) based model to jointly infer: (i) reviewer expertise, (ii)
item facets, and (iii) review helpfulness. Large-scale experiments on five
real-world datasets from Amazon show significant improvement over
state-of-the-art baselines in predicting and ranking useful reviews.



Current recommender systems exploit user and item similarities by
collaborative filtering. Some advanced methods also consider the temporal
evolution of item ratings as a global background process. However, all prior
methods disregard the individual evolution of a user's experience level and how
this is expressed in the user's writing in a review community. In this paper,
we model the joint evolution of user experience, interest in specific item
facets, writing style, and rating behavior. This way we can generate individual
recommendations that take into account the user's maturity level (e.g.,
recommending art movies rather than blockbusters for a cinematography expert).
As only item ratings and review texts are observables, we capture the user's
experience and interests in a latent model learned from her reviews, vocabulary
and writing style. We develop a generative HMM-LDA model to trace user
evolution, where the Hidden Markov Model (HMM) traces her latent experience
progressing over time -- with solely user reviews and ratings as observables
over time. The facets of a user's interest are drawn from a Latent Dirichlet
Allocation (LDA) model derived from her reviews, as a function of her (again
latent) experience level. In experiments with five real-world datasets, we show
that our model improves the rating prediction over state-of-the-art baselines,
by a substantial margin. We also show, in a use-case study, that our model
performs well in the assessment of user experience levels.



Online health communities are a valuable source of information for patients
and physicians. However, such user-generated resources are often plagued by
inaccuracies and misinformation. In this work we propose a method for
automatically establishing the credibility of user-generated medical statements
and the trustworthiness of their authors by exploiting linguistic cues and
distant supervision from expert sources. To this end we introduce a
probabilistic graphical model that jointly learns user trustworthiness,
statement credibility, and language objectivity. We apply this methodology to
the task of extracting rare or unknown side-effects of medical drugs --- this
being one of the problems where large scale non-expert data has the potential
to complement expert medical knowledge. We show that our method can reliably
extract side-effects and filter out false statements, while identifying
trustworthy users that are likely to contribute valuable medical information.



Media seems to have become more partisan, often providing a biased coverage
of news catering to the interest of specific groups. It is therefore essential
to identify credible information content that provides an objective narrative
of an event. News communities such as digg, reddit, or newstrust offer
recommendations, reviews, quality ratings, and further insights on journalistic
works. However, there is a complex interaction between different factors in
such online communities: fairness and style of reporting, language clarity and
objectivity, topical perspectives (like political viewpoint), expertise and
bias of community members, and more. This paper presents a model to
systematically analyze the different interactions in a news community between
users, news, and sources. We develop a probabilistic graphical model that
leverages this joint interaction to identify 1) highly credible news articles,
2) trustworthy news sources, and 3) expert users who perform the role of
"citizen journalists" in the community. Our method extends CRF models to
incorporate real-valued ratings, as some communities have very fine-grained
scales that cannot be easily discretized without losing information. To the
best of our knowledge, this paper is the first full-fledged analysis of
credibility, trust, and expertise in news communities.



Online reviews provide viewpoints on the strengths and shortcomings of
products/services, influencing potential customers' purchasing decisions.
However, the proliferation of non-credible reviews -- either fake (promoting/
demoting an item), incompetent (involving irrelevant aspects), or biased --
entails the problem of identifying credible reviews. Prior works involve
classifiers harnessing rich information about items/users -- which might not be
readily available in several domains -- that provide only limited
interpretability as to why a review is deemed non-credible. This paper presents
a novel approach to address the above issues. We utilize latent topic models
leveraging review texts, item ratings, and timestamps to derive consistency
features without relying on item/user histories, unavailable for "long-tail"
items/users. We develop models, for computing review credibility scores to
provide interpretable evidence for non-credible reviews, that are also
transferable to other domains -- addressing the scarcity of labeled data.
Experiments on real-world datasets demonstrate improvements over
state-of-the-art baselines.



Online review communities are dynamic as users join and leave, adopt new
vocabulary, and adapt to evolving trends. Recent work has shown that
recommender systems benefit from explicit consideration of user experience.
However, prior work assumes a fixed number of discrete experience levels,
whereas in reality users gain experience and mature continuously over time.
This paper presents a new model that captures the continuous evolution of user
experience, and the resulting language model in reviews and other posts. Our
model is unsupervised and combines principles of Geometric Brownian Motion,
Brownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal
progression of user experience and language model respectively. We develop
practical algorithms for estimating the model parameters from data and for
inference with our model (e.g., to recommend items). Extensive experiments with
five real-world datasets show that our model not only fits data better than
discrete-model baselines, but also outperforms state-of-the-art methods for
predicting item ratings.



With pressure to increase graduation rates and reduce time to degree in
higher education, it is important to identify at-risk students early. Automated
early warning systems are therefore highly desirable. In this paper, we use
unsupervised clustering techniques to predict the graduation status of declared
majors in five departments at California State University Northridge (CSUN),
based on a minimal number of lower division courses in each major. In addition,
we use the detected clusters to identify hidden bottleneck courses.



Generative Adversarial Nets (GANs) represent an important milestone for
effective generative models, which has inspired numerous variants seemingly
different from each other. One of the main contributions of this paper is to
reveal a unified geometric structure in GAN and its variants. Specifically, we
show that the adversarial generative model training can be decomposed into
three geometric steps: separating hyperplane search, discriminator parameter
update away from the separating hyperplane, and the generator update along the
normal vector direction of the separating hyperplane. This geometric intuition
reveals the limitations of the existing approaches and leads us to propose a
new formulation called geometric GAN using SVM separating hyperplane that
maximizes the margin. Our theoretical analysis shows that the geometric GAN
converges to a Nash equilibrium between the discriminator and generator. In
addition, extensive numerical results show that the superior performance of
geometric GAN.



This paper introduces an end-to-end fine-tuning method to improve hand-eye
coordination in modular deep visuo-motor policies (modular networks) where each
module is trained independently. Benefiting from weighted losses, the
fine-tuning method significantly improves the performance of the policies for a
robotic planar reaching task.



This article provides the first survey of computational models of emotion in
reinforcement learning (RL) agents. The survey focuses on agent/robot emotions,
and mostly ignores human user emotions. Emotions are recognized as functional
in decision-making by influencing motivation and action selection. Therefore,
computational emotion models are usually grounded in the agent's decision
making architecture, of which RL is an important subclass. Studying emotions in
RL-based agents is useful for three research fields. For machine learning (ML)
researchers, emotion models may improve learning efficiency. For the
interactive ML and human-robot interaction (HRI) community, emotions can
communicate state and enhance user investment. Lastly, it allows affective
modelling (AM) researchers to investigate their emotion theories in a
successful AI agent class. This survey provides background on emotion theory
and RL. It systematically addresses 1) from what underlying dimensions (e.g.,
homeostasis, appraisal) emotions can be derived and how these can be modelled
in RL-agents, 2) what types of emotions have been derived from these
dimensions, and 3) how these emotions may either influence the learning
efficiency of the agent or be useful as social signals. We also systematically
compare evaluation criteria, and draw connections to important RL sub-domains
like (intrinsic) motivation and model-based RL. In short, this survey provides
both a practical overview for engineers wanting to implement emotions in their
RL agents, and identifies challenges and directions for future emotion-RL
research.



In many real-world scenarios, rewards extrinsic to the agent are extremely
sparse, or absent altogether. In such cases, curiosity can serve as an
intrinsic reward signal to enable the agent to explore its environment and
learn skills that might be useful later in its life. We formulate curiosity as
the error in an agent's ability to predict the consequence of its own actions
in a visual feature space learned by a self-supervised inverse dynamics model.
Our formulation scales to high-dimensional continuous state spaces like images,
bypasses the difficulties of directly predicting pixels, and, critically,
ignores the aspects of the environment that cannot affect the agent. The
proposed approach is evaluated in two environments: VizDoom and Super Mario
Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where
curiosity allows for far fewer interactions with the environment to reach the
goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent
to explore more efficiently; and 3) generalization to unseen scenarios (e.g.
new levels of the same game) where the knowledge gained from earlier experience
helps the agent explore new places much faster than starting from scratch. Demo
video and code available at https://pathak22.github.io/noreward-rl/



We propose studying GAN training dynamics as regret minimization, which is in
contrast to the popular view that there is consistent minimization of a
divergence between real and generated distributions. We analyze the convergence
of GAN training from this new point of view to understand why mode collapse
happens. We hypothesize the existence of undesirable local equilibria in this
non-convex game to be responsible for mode collapse. We observe that these
local equilibria often exhibit sharp gradients of the discriminator function
around some real data points. We demonstrate that these degenerate local
equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show
that DRAGAN enables faster training, achieves improved stability with fewer
mode collapses, and leads to generator networks with better modeling
performance across a variety of architectures and objective functions.



We propose a new algorithm for training generative adversarial networks that
jointly learns latent codes for both identities (e.g. individual humans) and
observations (e.g. specific photographs). By fixing the identity portion of the
latent codes, we can generate diverse images of the same subject, and by fixing
the observation portion, we can traverse the manifold of subjects while
maintaining contingent aspects such as lighting and pose. Our algorithm
features a pairwise training scheme in which each sample from the generator
consists of two images with a common identity code. Corresponding samples from
the real dataset consist of two distinct photographs of the same subject. In
order to fool the discriminator, the generator must produce pairs that are
photorealistic, distinct, and appear to depict the same individual. We augment
both the DCGAN and BEGAN approaches with Siamese discriminators to facilitate
pairwise training. Experiments with human judges and an off-the-shelf face
verification system demonstrate our algorithm's ability to generate convincing,
identity-matched photographs.



Transforming a graphical user interface screenshot created by a designer into
computer code is a typical task conducted by a developer in order to build
customized software, websites, and mobile applications. In this paper, we show
that deep learning methods can be leveraged to train a model end-to-end to
automatically generate code from a single input image with over 77% of accuracy
for three different platforms (i.e. iOS, Android and web-based technologies).



Multi-task learning is motivated by the observation that humans bring to bear
what they know about related problems when solving new ones. Similarly, deep
neural networks can profit from related tasks by sharing parameters with other
networks. However, humans do not consciously decide to transfer knowledge
between tasks. In Natural Language Processing (NLP), it is hard to predict if
sharing will lead to improvements, particularly if tasks are only loosely
related. To overcome this, we introduce Sluice Networks, a general framework
for multi-task learning where trainable parameters control the amount of
sharing. Our framework generalizes previous proposals in enabling sharing of
all combinations of subspaces, layers, and skip connections. We perform
experiments on three task pairs, and across seven different domains, using data
from OntoNotes 5.0, and achieve up to 15% average error reductions over common
approaches to multi-task learning. We show that a) label entropy is predictive
of gains in sluice networks, confirming findings for hard parameter sharing and
b) while sluice networks easily fit noise, they are robust across domains in
practice.



We consider the online one-class collaborative filtering (CF) problem that
consists of recommending items to users over time in an online fashion based on
positive ratings only. This problem arises when users respond only occasionally
to a recommendation with a positive rating, and never with a negative one. We
study the impact of the probability of a user responding to a recommendation,
p_f, on the sample complexity, i.e., the number of ratings required to make
`good' recommendations, and ask whether receiving positive and negative
ratings, instead of positive ratings only, improves the sample complexity. Both
questions arise in the design of recommender systems. We introduce a simple
probabilistic user model, and analyze the performance of an online user-based
CF algorithm. We prove that after an initial cold start phase, where
recommendations are invested in exploring the user's preferences, this
algorithm makes---up to a fraction of the recommendations required for updating
the user's preferences---perfect recommendations. The number of ratings
required for the cold start phase is nearly proportional to 1/p_f, and that for
updating the user's preferences is essentially independent of p_f. As a
consequence we find that, receiving positive and negative ratings instead of
only positive ones improves the number of ratings required for initial
exploration by a factor of 1/p_f, which can be significant.



Recent theoretical and experimental results suggest the possibility of using
current and near-future quantum hardware in challenging sampling tasks. In this
paper, we introduce free energy-based reinforcement learning (FERL) as an
application of quantum hardware. We propose a method for processing a quantum
annealer's measured qubit spin configurations in approximating the free energy
of a quantum Boltzmann machine (QBM). We then apply this method to perform
reinforcement learning on the grid-world problem using the D-Wave 2000Q quantum
annealer. The experimental results show that our technique is a promising
method for harnessing the power of quantum sampling in reinforcement learning
tasks.



Deep reinforcement learning (RL) methods generally engage in exploratory
behavior through noise injection in the action space. An alternative is to add
noise directly to the agent's parameters, which can lead to more consistent
exploration and a richer set of behaviors. Methods such as evolutionary
strategies use parameter perturbations, but discard all temporal structure in
the process and require significantly more samples. Combining parameter noise
with traditional RL methods allows to combine the best of both worlds. We
demonstrate that both off- and on-policy methods benefit from this approach
through experimental comparison of DQN, DDPG, and TRPO on high-dimensional
discrete action environments as well as continuous control tasks. Our results
show that RL with parameter noise learns more efficiently than traditional RL
with action space noise and evolutionary strategies individually.



Humans and animals are constantly exposed to a continuous stream of sensory
information from different modalities. At the same time, they form more
compressed representations like concepts or symbols. In species that use
language, this process is further structured by this interaction, where a
mapping between the sensorimotor concepts and linguistic elements needs to be
established. There is evidence that children might be learning language by
simply disambiguating potential meanings based on multiple exposures to
utterances in different contexts (cross-situational learning). In existing
models, the mapping between modalities is usually found in a single step by
directly using frequencies of referent and meaning co-occurrences. In this
paper, we present an extension of this one-step mapping and introduce a newly
proposed sequential mapping algorithm together with a publicly available Matlab
implementation. For demonstration, we have chosen a less typical scenario:
instead of learning to associate objects with their names, we focus on body
representations. A humanoid robot is receiving tactile stimulations on its
body, while at the same time listening to utterances of the body part names
(e.g., hand, forearm and torso). With the goal at arriving at the correct "body
categories", we demonstrate how a sequential mapping algorithm outperforms
one-step mapping. In addition, the effect of data set size and noise in the
linguistic input are studied.



Automatic summarisation is a popular approach to reduce a document to its
main arguments. Recent research in the area has focused on neural approaches to
summarisation, which can be very data-hungry. However, few large datasets exist
and none for the traditionally popular domain of scientific publications, which
opens up challenging research avenues centered on encoding large, complex
documents. In this paper, we introduce a new dataset for summarisation of
computer science publications by exploiting a large resource of author provided
summaries and show straightforward ways of extending it further. We develop
models on the dataset making use of both neural sentence encoding and
traditionally used summarisation features and show that models which encode
sentences as well as their local and global context perform best, significantly
outperforming well-established baseline methods.



A reinforcement algorithm solves a classical optimization problem by
introducing a feedback to the system which slowly changes the energy landscape
and converges the algorithm to an optimal solution in the configuration space.
Here, we use this strategy to concentrate (localize) preferentially the wave
function of a quantum particle, which explores the configuration space of the
problem, on an optimal configuration. We examine the method by solving
numerically the equations governing the evolution of the system, which are
similar to the nonlinear Schr\"odinger equations, for small problem sizes. In
particular, we observe that reinforcement increases the minimal energy gap of
the system in a quantum annealing algorithm. Our numerical simulations and the
latter observation show that such kind of quantum feedbacks might be helpful in
solving a computationally hard optimization problem by a quantum reinforcement
algorithm.



In horizontal collaborations, carriers form coalitions in order to perform
parts of their logistics operations jointly. By exchanging transportation
requests among each other, they can operate more efficiently and in a more
sustainable way. Collaborative vehicle routing has been extensively discussed
in the literature. We identify three major streams of research: (i) centralized
collaborative planning, (ii) decentralized planning without auctions, and (ii)
auction-based decentralized planning. For each of them we give a structured
overview on the state of knowledge and discuss future research directions.



Adaptive gradient methods have become recently very popular, in particular as
they have been shown to be useful in the training of deep neural networks. In
this paper we have analyzed RMSProp, originally proposed for the training of
deep neural networks, in the context of online convex optimization and show
$\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and
SC-RMSProp for which we show logarithmic regret bounds for strongly convex
functions. Finally, we demonstrate in the experiments that these new variants
outperform other adaptive gradient techniques or stochastic gradient descent in
the optimization of strongly convex functions as well as in training of deep
neural networks.



Imitation learning is an effective approach for autonomous systems to acquire
control policies when an explicit reward function is unavailable, using
supervision provided as demonstrations from an expert, typically a human
operator. However, standard imitation learning methods assume that the agent
receives examples of observation-action tuples that could be provided, for
instance, to a supervised learning algorithm. This stands in contrast to how
humans and animals imitate: we observe another person performing some behavior
and then figure out which actions will realize that behavior, compensating for
changes in viewpoint, surroundings, and embodiment. We term this kind of
imitation learning as imitation-from-observation and propose an imitation
learning method based on video prediction with context translation and deep
reinforcement learning. This lifts the assumption in imitation learning that
the demonstration should consist of observations and actions in the same
environment, and enables a variety of interesting applications, including
learning robotic skills that involve tool use simply by observing videos of
human tool use. Our experimental results show that our approach can perform
imitation-from-observation for a variety of real-world robotic tasks modeled on
common household chores, acquiring skills such as sweeping from videos of a
human demonstrator. Videos can be found at
https://sites.google.com/site/imitationfromobservation



Submodular functions are a broad class of set functions, which naturally
arise in diverse areas. Many algorithms have been suggested for the
maximization of these functions. Unfortunately, once the function deviates from
submodularity, the known algorithms may perform arbitrarily poorly. Amending
this issue, by obtaining approximation results for set functions generalizing
submodular functions, has been the focus of recent works.
  One such class, known as weakly submodular functions, has received a lot of
attention. A key result proved by Das and Kempe (2011) showed that the
approximation ratio of the greedy algorithm for weakly submodular maximization
subject to a cardinality constraint degrades smoothly with the distance from
submodularity. However, no results have been obtained for maximization subject
to constraints beyond cardinality. In particular, it is not known whether the
greedy algorithm achieves any non-trivial approximation ratio for such
constraints.
  In this paper, we prove that a randomized version of the greedy algorithm
(previously used by Buchbinder et al. (2014) for a different problem) achieves
an approximation ratio of $(1 + 1/\gamma)^{-2}$ for the maximization of a
weakly submodular function subject to a general matroid constraint, where
$\gamma$ is a parameter measuring the distance of the function from
submodularity. Moreover, we also experimentally compare the performance of this
version of the greedy algorithm on real world problems against natural
benchmarks, and show that the algorithm we study performs well also in
practice. To the best of our knowledge, this is the first algorithm with a
non-trivial approximation guarantee for maximizing a weakly submodular function
subject to a constraint other than the simple cardinality constraint. In
particular, it is the first algorithm with such a guarantee for the important
and broad class of matroid constraints.



Recent works on representation learning for graph structured data
predominantly focus on learning distributed representations of graph
substructures such as nodes and subgraphs. However, many graph analytics tasks
such as graph classification and clustering require representing entire graphs
as fixed length feature vectors. While the aforementioned approaches are
naturally unequipped to learn such representations, graph kernels remain as the
most effective way of obtaining them. However, these graph kernels use
handcrafted features (e.g., shortest paths, graphlets, etc.) and hence are
hampered by problems such as poor generalization. To address this limitation,
in this work, we propose a neural embedding framework named graph2vec to learn
data-driven distributed representations of arbitrary sized graphs. graph2vec's
embeddings are learnt in an unsupervised manner and are task agnostic. Hence,
they could be used for any downstream task such as graph classification,
clustering and even seeding supervised representation learning approaches. Our
experiments on several benchmark and large real-world datasets show that
graph2vec achieves significant improvements in classification and clustering
accuracies over substructure representation learning approaches and are
competitive with state-of-the-art graph kernels.



Generating adversarial examples is a critical step for evaluating and
improving the robustness of learning machines. So far, most existing methods
only work for classification and are not designed to alter the true performance
measure of the problem at hand. We introduce a novel flexible approach named
Houdini for generating adversarial examples specifically tailored for the final
performance measure of the task considered, be it combinatorial and
non-decomposable. We successfully apply Houdini to a range of applications such
as speech recognition, pose estimation and semantic segmentation. In all cases,
the attacks based on Houdini achieve higher success rate than those based on
the traditional surrogates used to train the models while using a less
perceptible adversarial perturbation.



Pairwise comparison data arises in many domains, including tournament
rankings, web search, and preference elicitation. Given noisy comparisons of a
fixed subset of pairs of items, we study the problem of estimating the
underlying comparison probabilities under the assumption of strong stochastic
transitivity (SST). We also consider the noisy sorting subclass of the SST
model. We show that when the assignment of items to the topology is arbitrary,
these permutation-based models, unlike their parametric counterparts, do not
admit consistent estimation for most comparison topologies used in practice. We
then demonstrate that consistent estimation is possible when the assignment of
items to the topology is randomized, thus establishing a dichotomy between
worst-case and average-case designs. We propose two estimators in the
average-case setting and analyze their risk, showing that it depends on the
comparison topology only through the degree sequence of the topology. The rates
achieved by these estimators are shown to be optimal for a large class of
graphs. Our results are corroborated by simulations on multiple comparison
topologies.



This paper studies Bayesian ranking and selection (R&S) problems with
correlated prior beliefs and continuous domains, i.e. Bayesian optimization
(BO). Knowledge gradient methods [Frazier et al., 2008, 2009] have been widely
studied for discrete R&S problems, which sample the one-step Bayes-optimal
point. When used over continuous domains, previous work on the knowledge
gradient [Scott et al., 2011, Wu and Frazier, 2016, Wu et al., 2017] often rely
on a discretized finite approximation. However, the discretization introduces
error and scales poorly as the dimension of domain grows. In this paper, we
develop a fast discretization-free knowledge gradient method for Bayesian
optimization. Our method is not restricted to the fully sequential setting, but
useful in all settings where knowledge gradient can be used over continuous
domains. We show how our method can be generalized to handle (i) batch of
points suggestion (parallel knowledge gradient); (ii) the setting where
derivative information is available in the optimization process
(derivative-enabled knowledge gradient). In numerical experiments, we
demonstrate that the discretization-free knowledge gradient method finds global
optima significantly faster than previous Bayesian optimization algorithms on
both synthetic test functions and real-world applications, especially when
function evaluations are noisy; and derivative-enabled knowledge gradient can
further improve the performances, even outperforming the gradient-based
optimizer such as BFGS when derivative information is available.



As autonomous service robots become more affordable and thus available also
for the general public, there is a growing need for user friendly interfaces to
control the robotic system. Currently available control modalities typically
expect users to be able to express their desire through either touch, speech or
gesture commands. While this requirement is fulfilled for the majority of
users, paralyzed users may not be able to use such systems. In this paper, we
present a novel framework, that allows these users to interact with a robotic
service assistant in a closed-loop fashion, using only thoughts. The
brain-computer interface (BCI) system is composed of several interacting
components, i.e., non-invasive neuronal signal recording and decoding,
high-level task planning, motion and manipulation planning as well as
environment perception. In various experiments, we demonstrate its
applicability and robustness in real world scenarios, considering
fetch-and-carry tasks and tasks involving human-robot interaction. As our
results demonstrate, our system is capable of adapting to frequent changes in
the environment and reliably completing given tasks within a reasonable amount
of time. Combined with high-level planning and autonomous robotic systems,
interesting new perspectives open up for non-invasive BCI-based human-robot
interactions.



The current processes for building machine learning systems require
practitioners with deep knowledge of machine learning. This significantly
limits the number of machine learning systems that can be created and has led
to a mismatch between the demand for machine learning systems and the ability
for organizations to build them. We believe that in order to meet this growing
demand for machine learning systems we must significantly increase the number
of individuals that can teach machines. We postulate that we can achieve this
goal by making the process of teaching machines easy, fast and above all,
universally accessible.
  While machine learning focuses on creating new algorithms and improving the
accuracy of "learners", the machine teaching discipline focuses on the efficacy
of the "teachers". Machine teaching as a discipline is a paradigm shift that
follows and extends principles of software engineering and programming
languages. We put a strong emphasis on the teacher and the teacher's
interaction with data, as well as crucial components such as techniques and
design principles of interaction and visualization.
  In this paper, we present our position regarding the discipline of machine
teaching and articulate fundamental machine teaching principles. We also
describe how, by decoupling knowledge about machine learning algorithms from
the process of teaching, we can accelerate innovation and empower millions of
new uses for machine learning models.



With the development of neural networks based machine learning and their
usage in mission critical applications, voices are rising against the
\textit{black box} aspect of neural networks as it becomes crucial to
understand their limits and capabilities. With the rise of neuromorphic
hardware, it is even more critical to understand how a neural network, as a
distributed system, tolerates the failures of its computing nodes, neurons, and
its communication channels, synapses. Experimentally assessing the robustness
of neural networks involves the quixotic venture of testing all the possible
failures, on all the possible inputs, which ultimately hits a combinatorial
explosion for the first, and the impossibility to gather all the possible
inputs for the second.
  In this paper, we prove an upper bound on the expected error of the output
when a subset of neurons crashes. This bound involves dependencies on the
network parameters that can be seen as being too pessimistic in the average
case. It involves a polynomial dependency on the Lipschitz coefficient of the
neurons activation function, and an exponential dependency on the depth of the
layer where a failure occurs. We back up our theoretical results with
experiments illustrating the extent to which our prediction matches the
dependencies between the network parameters and robustness. Our results show
that the robustness of neural networks to the average crash can be estimated
without the need to neither test the network on all failure configurations, nor
access the training set used to train the network, both of which are
practically impossible requirements.



One of the major hurdles preventing the full exploitation of information from
online communities is the widespread concern regarding the quality and
credibility of user-contributed content. Prior works in this domain operate on
a static snapshot of the community, making strong assumptions about the
structure of the data (e.g., relational tables), or consider only shallow
features for text classification.
  To address the above limitations, we propose probabilistic graphical models
that can leverage the joint interplay between multiple factors in online
communities --- like user interactions, community dynamics, and textual content
--- to automatically assess the credibility of user-contributed online content,
and the expertise of users and their evolution with user-interpretable
explanation. To this end, we devise new models based on Conditional Random
Fields for different settings like incorporating partial expert knowledge for
semi-supervised learning, and handling discrete labels as well as numeric
ratings for fine-grained analysis. This enables applications such as extracting
reliable side-effects of drugs from user-contributed posts in healthforums, and
identifying credible content in news communities.
  Online communities are dynamic, as users join and leave, adapt to evolving
trends, and mature over time. To capture this dynamics, we propose generative
models based on Hidden Markov Model, Latent Dirichlet Allocation, and Brownian
Motion to trace the continuous evolution of user expertise and their language
model over time. This allows us to identify expert users and credible content
jointly over time, improving state-of-the-art recommender systems by explicitly
considering the maturity of users. This also enables applications such as
identifying helpful product reviews, and detecting fake and anomalous reviews
with limited information.



Sports channel video portals offer an exciting domain for research on
multimodal, multilingual analysis. We present methods addressing the problem of
automatic video highlight prediction based on joint visual features and textual
analysis of the real-world audience discourse with complex slang, in both
English and traditional Chinese. We present a novel dataset based on League of
Legends championships recorded from North American and Taiwanese Twitch.tv
channels (will be released for further research), and demonstrate strong
results on these using multimodal, character-level CNN-RNN model architectures.



Humans are going to delegate the rights of driving to the autonomous vehicles
in near future. However, to fulfill this complicated task, there is a need for
a mechanism, which enforces the autonomous vehicles to obey the road and social
rules that have been practiced by well-behaved drivers. This task can be
achieved by introducing social norms compliance mechanism in the autonomous
vehicles. This research paper is proposing an artificial society of autonomous
vehicles as an analogy of human social society. Each AV has been assigned a
social personality having different social influence. Social norms have been
introduced which help the AVs in making the decisions, influenced by emotions,
regarding road collision avoidance. Furthermore, social norms compliance
mechanism, by artificial social AVs, has been proposed using prospect based
emotion i.e. fear, which is conceived from OCC model. Fuzzy logic has been
employed to compute the emotions quantitatively. Then, using SimConnect
approach, fuzzy values of fear has been provided to the Netlogo simulation
environment to simulate artificial society of AVs. Extensive testing has been
performed using the behavior space tool to find out the performance of the
proposed approach in terms of the number of collisions. For comparison, the
random-walk model based artificial society of AVs has been proposed as well. A
comparative study with a random walk, prove that proposed approach provides a
better option to tailor the autopilots of future AVS, Which will be more
socially acceptable and trustworthy by their riders in terms of safe road
travel.



Questions play a prominent role in social interactions, performing rhetorical
functions that go beyond that of simple informational exchange. The surface
form of a question can signal the intention and background of the person asking
it, as well as the nature of their relation with the interlocutor. While the
informational nature of questions has been extensively examined in the context
of question-answering applications, their rhetorical aspects have been largely
understudied.
  In this work we introduce an unsupervised methodology for extracting surface
motifs that recur in questions, and for grouping them according to their latent
rhetorical role. By applying this framework to the setting of question sessions
in the UK parliament, we show that the resulting typology encodes key aspects
of the political discourse---such as the bifurcation in questioning behavior
between government and opposition parties---and reveals new insights into the
effects of a legislator's tenure and political career ambitions.



Literature on the modeling and simulation of complex adaptive systems (cas)
has primarily advanced vertically in different scientific domains with
scientists developing a variety of domain-specific approaches and applications.
However, while cas researchers are inher-ently interested in an
interdisciplinary comparison of models, to the best of our knowledge, there is
currently no single unified framework for facilitating the development,
comparison, communication and validation of models across different scientific
domains. In this thesis, we propose first steps towards such a unified
framework using a combination of agent-based and complex network-based modeling
approaches and guidelines formulated in the form of a set of four levels of
usage, which allow multidisciplinary researchers to adopt a suitable framework
level on the basis of available data types, their research study objectives and
expected outcomes, thus allowing them to better plan and conduct their
respective re-search case studies.



Agent Based Models are very popular in a number of different areas. For
example, they have been used in a range of domains ranging from modeling of
tumor growth, immune systems, molecules to models of social networks, crowds
and computer and mobile self-organizing networks. One reason for their success
is their intuitiveness and similarity to human cognition. However, with this
power of abstraction, in spite of being easily applicable to such a wide number
of domains, it is hard to validate agent-based models. In addition, building
valid and credible simulations is not just a challenging task but also a
crucial exercise to ensure that what we are modeling is, at some level of
abstraction, a model of our conceptual system; the system that we have in mind.
In this paper, we address this important area of validation of agent based
models by presenting a novel technique which has broad applicability and can be
applied to all kinds of agent-based models. We present a framework, where a
virtual overlay multi-agent system can be used to validate simulation models.
In addition, since agent-based models have been typically growing, in parallel,
in multiple domains, to cater for all of these, we present a new single
validation technique applicable to all agent based models. Our technique, which
allows for the validation of agent based simulations uses VOMAS: a Virtual
Overlay Multi-agent System. This overlay multi-agent system can comprise
various types of agents, which form an overlay on top of the agent based
simulation model that needs to be validated. Other than being able to watch and
log, each of these agents contains clearly defined constraints, which, if
violated, can be logged in real time. To demonstrate its effectiveness, we show
its broad applicability in a wide variety of simulation models ranging from
social sciences to computer networks in spatial and non-spatial conceptual
models.



Rubenstein et al. present an interesting system of programmable
self-assembled structure formation using 1000 Kilobot robots. The paper claims
to advance work in artificial swarms similar to capabilities of natural systems
besides being highly robust. However, the system lacks in terms of matching
motility and complex shapes with holes, thereby limiting practical similarity
to self-assembly in living systems.



Training deep networks is expensive and time-consuming with the training
period increasing with data size and growth in model parameters. In this paper,
we provide a framework for distributed training of deep networks over a cluster
of CPUs in Apache Spark. The framework implements both Data Parallelism and
Model Parallelism making it suitable to use for deep networks which require
huge training data and model parameters which are too big to fit into the
memory of a single machine. It can be scaled easily over a cluster of cheap
commodity hardware to attain significant speedup and obtain better results
making it quite economical as compared to farm of GPUs and supercomputers. We
have proposed a new algorithm for training of deep networks for the case when
the network is partitioned across the machines (Model Parallelism) along with
detailed cost analysis and proof of convergence of the same. We have developed
implementations for Fully-Connected Feedforward Networks, Convolutional Neural
Networks, Recurrent Neural Networks and Long Short-Term Memory architectures.
We present the results of extensive simulations demonstrating the speedup and
accuracy obtained by our framework for different sizes of the data and model
parameters with variation in the number of worker cores/partitions; thereby
showing that our proposed framework can achieve significant speedup (upto 11X
for CNN) and is also quite scalable.



Agent-Based Computing is a diverse research domain concerned with the
building of intelligent software based on the concept of "agents". In this
paper, we use Scientometric analysis to analyze all sub-domains of agent-based
computing. Our data consists of 1,064 journal articles indexed in the ISI web
of knowledge published during a twenty year period: 1990-2010. These were
retrieved using a topic search with various keywords commonly used in
sub-domains of agent-based computing. In our proposed approach, we have
employed a combination of two applications for analysis, namely Network
Workbench and CiteSpace - wherein Network Workbench allowed for the analysis of
complex network aspects of the domain, detailed visualization-based analysis of
the bibliographic data was performed using CiteSpace. Our results include the
identification of the largest cluster based on keywords, the timeline of
publication of index terms, the core journals and key subject categories. We
also identify the core authors, top countries of origin of the manuscripts
along with core research institutes. Finally, our results have interestingly
revealed the strong presence of agent-based computing in a number of
non-computing related scientific domains including Life Sciences, Ecological
Sciences and Social Sciences.



In this paper we present a novel Formal Agent-Based Simulation framework
(FABS). FABS uses formal specification as a means of clear description of
wireless sensor networks (WSN) sensing a Complex Adaptive Environment. This
specification model is then used to develop an agent-based model of both the
wireless sensor network as well as the environment. As proof of concept, we
demonstrate the application of FABS to a boids model of self-organized flocking
of animals monitored by a random deployment of proximity sensors.



We model the spread of news as a social learning game on a network. Agents
can either endorse or oppose a claim made in a piece of news, which itself may
be either true or false. Agents base their decision on a private signal and
their neighbors' past actions. Given these inputs, agents follow strategies
derived via multi-agent deep reinforcement learning and receive utility from
acting in accordance with the veracity of claims. Our framework yields
strategies with agent utility close to a theoretical, Bayes optimal benchmark,
while remaining flexible to model re-specification. Optimized strategies allow
agents to correctly identify most false claims, when all agents receive
unbiased private signals. However, an adversary's attempt to spread fake news
by targeting a subset of agents with a biased private signal can be successful.
Even more so when the adversary has information about agents' network position
or private signal. When agents are aware of the presence of an adversary they
re-optimize their strategies in the training stage and the adversary's attack
is less effective. Hence, exposing agents to the possibility of fake news can
be an effective way to curtail the spread of fake news in social networks. Our
results also highlight that information about the users' private beliefs and
their social network structure can be extremely valuable to adversaries and
should be well protected.



Recently, deep neural networks have demonstrated excellent performances in
recognizing the age and gender on human face images. However, these models were
applied in a black-box manner with no information provided about which facial
features are actually used for prediction and how these features depend on
image preprocessing, model initialization and architecture choice. We present a
study investigating these different effects.
  In detail, our work compares four popular neural network architectures,
studies the effect of pretraining, evaluates the robustness of the considered
alignment preprocessings via cross-method test set swapping and intuitively
visualizes the model's prediction strategies in given preprocessing conditions
using the recent Layer-wise Relevance Propagation (LRP) algorithm. Our
evaluations on the challenging Adience benchmark show that suitable parameter
initialization leads to a holistic perception of the input, compensating
artefactual data representations. With a combination of simple preprocessing
steps, we reach state of the art performance in gender recognition.



In this paper, we propose an algorithm for the positive one-in-three
satisfiability problem (Pos1in3SAT). The proposed algorithm can efficiently
decide the existence of a satisfying assignment in all assignments for a given
formula by using a 2-dimensional binary search method without constructing an
exponential number of assignments.



We investigate the non-identifiability issues associated with bidirectional
adversarial training for joint distribution matching. Within a framework of
conditional entropy, we propose both adversarial and non-adversarial approaches
to learn desirable matched joint distributions for unsupervised and supervised
tasks. We unify a broad family of adversarial models as joint distribution
matching problems. Our approach stabilizes learning of unsupervised
bidirectional adversarial learning methods. Further, we introduce an extension
for semi-supervised learning tasks. Theoretical results are validated in
synthetic data and real-world applications.



We propose an adversarial training procedure for learning a causal implicit
generative model for a given causal graph. We show that adversarial training
can be used to learn a generative model with true observational and
interventional distributions if the generator architecture is consistent with
the given causal graph. We consider the application of generating faces based
on given binary labels where the dependency structure between the labels is
preserved with a causal graph. This problem can be seen as learning a causal
implicit generative model for the image and labels. We devise a two-stage
procedure for this problem. First we train a causal implicit generative model
over binary labels using a neural network consistent with a causal graph as the
generator. We empirically show that WassersteinGAN can be used to output
discrete labels. Later, we propose two new conditional GAN architectures, which
we call CausalGAN and CausalBEGAN. We show that the optimal generator of the
CausalGAN, given the labels, samples from the image distributions conditioned
on these labels. The conditional GAN combined with a trained causal implicit
generative model for the labels is then a causal implicit generative model over
the labels and the generated image. We show that the proposed architectures can
be used to sample from observational and interventional image distributions,
even for interventions which do not naturally occur in the dataset.



We present MILABOT: a deep reinforcement learning chatbot developed by the
Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize
competition. MILABOT is capable of conversing with humans on popular small talk
topics through both speech and text. The system consists of an ensemble of
natural language generation and retrieval models, including template-based
models, bag-of-words models, sequence-to-sequence neural network and latent
variable neural network models. By applying reinforcement learning to
crowdsourced data and real-world user interactions, the system has been trained
to select an appropriate response from the models in its ensemble. The system
has been evaluated through A/B testing with real-world users, where it
performed significantly better than many competing systems. Due to its machine
learning architecture, the system is likely to improve with additional data.



This paper defines software fairness and discrimination and develops a
testing-based method for measuring if and how much software discriminates,
focusing on causality in discriminatory behavior. Evidence of software
discrimination has been found in modern software systems that recommend
criminal sentences, grant access to financial products, and determine who is
allowed to participate in promotions. Our approach, Themis, generates efficient
test suites to measure discrimination. Given a schema describing valid system
inputs, Themis generates discrimination tests automatically and does not
require an oracle. We evaluate Themis on 20 software systems, 12 of which come
from prior work with explicit focus on avoiding discrimination. We find that
(1) Themis is effective at discovering software discrimination, (2)
state-of-the-art techniques for removing discrimination from algorithms fail in
many situations, at times discriminating against as much as 98% of an input
subdomain, (3) Themis optimizations are effective at producing efficient test
suites for measuring discrimination, and (4) Themis is more efficient on
systems that exhibit more discrimination. We thus demonstrate that fairness
testing is a critical aspect of the software development cycle in domains with
possible discrimination and provide initial tools for measuring software
discrimination.



Existing neural conversational models process natural language primarily on a
lexico-syntactic level, thereby ignoring one of the most crucial components of
human-to-human dialogue: its affective content. We take a step in this
direction by proposing three novel ways to incorporate affective/emotional
aspects into long short term memory (LSTM) encoder-decoder neural conversation
models: (1) affective word embeddings, which are cognitively engineered, (2)
affect-based objective functions that augment the standard cross-entropy loss,
and (3) affectively diverse beam search for decoding. Experiments show that
these techniques improve the open-domain conversational prowess of
encoder-decoder networks by enabling them to produce emotionally rich responses
that are more interesting and natural.



Information Cascades Model captures dynamical properties of user activity in
a social network. In this work, we develop a novel framework for activity
shaping under the Continuous-Time Information Cascades Model which allows the
administrator for local control actions by allocating targeted resources that
can alter the spread of the process. Our framework employs the optimization of
the spectral radius of the Hazard matrix, a quantity that has been shown to
drive the maximum influence in a network, while enjoying a simple convex
relaxation when used to minimize the influence of the cascade. In addition,
use-cases such as quarantine and node immunization are discussed to highlight
the generality of the proposed activity shaping framework. Finally, we present
the NetShape influence minimization method which is compared favorably to
baseline and state-of-the-art approaches through simulations on real social
networks.



A modular method is proposed to learn and transfer visuo-motor policies from
simulation to the real world in an efficient manner by combining domain
randomization and adaptation. The feasibility of the approach is demonstrated
in a table-top object reaching task where a 7 DoF arm is controlled in velocity
mode to reach a blue cuboid in clutter through visual observations. The learned
visuo-motor policies are robust to novel (not seen in training) objects in
clutter and even a moving target, achieving a 93.3% success rate and 2.2 cm
control accuracy.



In this paper we introduce ZhuSuan, a python probabilistic programming
library for Bayesian deep learning, which conjoins the complimentary advantages
of Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike
existing deep learning libraries, which are mainly designed for deterministic
neural networks and supervised tasks, ZhuSuan is featured for its deep root
into Bayesian inference, thus supporting various kinds of probabilistic models,
including both the traditional hierarchical Bayesian models and recent deep
generative models. We use running examples to illustrate the probabilistic
programming on ZhuSuan, including Bayesian logistic regression, variational
auto-encoders, deep sigmoid belief networks and Bayesian recurrent neural
networks.



In this paper, we investigate how to learn to control a group of cooperative
agents with limited sensing capabilities such as robot swarms. The agents have
only very basic sensor capabilities, yet in a group they can accomplish
sophisticated tasks, such as distributed assembly or search and rescue tasks.
Learning a policy for a group of agents is difficult due to distributed partial
observability of the state. Here, we follow a guided approach where a critic
has central access to the global state during learning, which simplifies the
policy evaluation problem from a reinforcement learning point of view. For
example, we can get the positions of all robots of the swarm using a camera
image of a scene. This camera image is only available to the critic and not to
the control policies of the robots. We follow an actor-critic approach, where
the actors base their decisions only on locally sensed information. In
contrast, the critic is learned based on the true global state. Our algorithm
uses deep reinforcement learning to approximate both the Q-function and the
policy. The performance of the algorithm is evaluated on two tasks with simple
simulated 2D agents: 1) finding and maintaining a certain distance to each
others and 2) locating a target.



We analyze the convergence of (stochastic) gradient descent algorithm for
learning a convolutional filter with Rectified Linear Unit (ReLU) activation
function. Our analysis does not rely on any specific form of the input
distribution and our proofs only use the definition of ReLU, in contrast with
previous works that are restricted to standard Gaussian input. We show that
(stochastic) gradient descent with random initialization can learn the
convolutional filter in polynomial time and the convergence rate depends on the
smoothness of the input distribution and the closeness of patches. To the best
of our knowledge, this is the first recovery guarantee of gradient-based
algorithms for convolutional filter on non-Gaussian input distributions. Our
theory also justifies the two-stage learning rate strategy in deep neural
networks. While our focus is theoretical, we also present experiments that
illustrate our theoretical findings.



We consider the problem of non-parametric Conditional Independence testing
(CI testing) for continuous random variables. Given i.i.d samples from the
joint distribution $f(x,y,z)$ of continuous random vectors $X,Y$ and $Z,$ we
determine whether $X \perp Y | Z$. We approach this by converting the
conditional independence test into a classification problem. This allows us to
harness very powerful classifiers like gradient-boosted trees and deep neural
networks. These models can handle complex probability distributions and allow
us to perform significantly better compared to the prior state of the art, for
high-dimensional CI testing. The main technical challenge in the classification
problem is the need for samples from the conditional product distribution
$f^{CI}(x,y,z) = f(x|z)f(y|z)f(z)$ -- the joint distribution if and only if $X
\perp Y | Z.$ -- when given access only to i.i.d. samples from the true joint
distribution $f(x,y,z)$. To tackle this problem we propose a novel nearest
neighbor bootstrap procedure and theoretically show that our generated samples
are indeed close to $f^{CI}$ in terms of total variational distance. We then
develop theoretical results regarding the generalization bounds for
classification for our problem, which translate into error bounds for CI
testing. We provide a novel analysis of Rademacher type classification bounds
in the presence of non-i.i.d near-independent samples. We empirically validate
the performance of our algorithm on simulated and real datasets and show
performance gains over previous methods.



Inspired by biological swarms, robotic swarms are envisioned to solve
real-world problems that are difficult for individual agents. Biological swarms
can achieve collective intelligence based on local interactions and simple
rules; however, designing effective distributed policies for large-scale
robotic swarms to achieve a global objective can be challenging. Although it is
often possible to design an optimal centralized strategy for smaller numbers of
agents, those methods can fail as the number of agents increases. Motivated by
the growing success of machine learning, we develop a deep learning approach
that learns distributed coordination policies from centralized policies. In
contrast to traditional distributed control approaches, which are usually based
on human-designed policies for relatively simple tasks, this learning-based
approach can be adapted to more difficult tasks. We demonstrate the efficacy of
our proposed approach on two different tasks, the well-known rendezvous problem
and a more difficult particle assignment problem. For the latter, no known
distributed policy exists. From extensive simulations, it is shown that the
performance of the learned coordination policies is comparable to the
centralized policies, surpassing state-of-the-art distributed policies.
Thereby, our proposed approach provides a promising alternative for real-world
coordination problems that would be otherwise computationally expensive to
solve or intangible to explore.



The most data-efficient algorithms for reinforcement learning in robotics are
model-based policy search algorithms, which alternate between learning a
dynamical model of the robot and optimizing a policy to maximize the expected
return given the model and its uncertainties. Among the few proposed
approaches, the recently introduced Black-DROPS algorithm exploits a black-box
optimization algorithm to achieve both high data-efficiency and good
computation times when several cores are used; nevertheless, like all
model-based policy search approaches, Black-DROPS does not scale to high
dimensional state/action spaces. In this paper, we introduce a new model
learning procedure in Black-DROPS that leverages parameterized black-box priors
to (1) scale up to high-dimensional systems, and (2) be robust to large
inaccuracies of the prior information. We demonstrate the effectiveness of our
approach with the "pendubot" swing-up task in simulation and with a physical
hexapod robot (48D state space, 18D action space) that has to walk forward as
fast as possible. The results show that our new algorithm is more
data-efficient than previous model-based policy search algorithms (with and
without priors) and that it can allow a physical 6-legged robot to learn new
gaits in only 16 to 30 seconds of interaction time.



One of the most interesting features of Bayesian optimization for direct
policy search is that it can leverage priors (e.g., from simulation or from
previous tasks) to accelerate learning on a robot. In this paper, we are
interested in situations for which several priors exist but we do not know in
advance which one fits best the current situation. We tackle this problem by
introducing a novel acquisition function, called Most Likely Expected
Improvement (MLEI), that combines the likelihood of the priors and the expected
improvement. We evaluate this new acquisition function on a transfer learning
task for a 5-DOF planar arm and on a possibly damaged, 6-legged robot that has
to learn to walk on flat ground and on stairs, with priors corresponding to
different stairs and different kinds of damages. Our results show that MLEI
effectively identifies and exploits the priors, even when there is no obvious
match between the current situations and the priors.



Swarm systems constitute a challenging problem for reinforcement learning
(RL) as the algorithm needs to learn decentralized control policies that can
cope with limited local sensing and communication abilities of the agents.
Although there have been recent advances of deep RL algorithms applied to
multi-agent systems, learning communication protocols while simultaneously
learning the behavior of the agents is still beyond the reach of deep RL
algorithms. However, while it is often difficult to directly define the
behavior of the agents, simple communication protocols can be defined more
easily using prior knowledge about the given task. In this paper, we propose a
number of simple communication protocols that can be exploited by deep
reinforcement learning to find decentralized control policies in a multi-robot
swarm environment. The protocols are based on histograms that encode the local
neighborhood relations of the agents and can also transmit task-specific
information, such as the shortest distance and direction to a desired target.
In our framework, we use an adaptation of Trust Region Policy Optimization to
learn complex collaborative tasks, such as formation building, building a
communication link, and pushing an intruder. We evaluate our findings in a
simulated 2D-physics environment, and compare the implications of different
communication protocols.



We propose a method to build quantum memristors in quantum photonic
platforms. We firstly design an effective beam splitter, which is tunable in
real-time, by means of a Mach-Zehnder-type array with two equal 50:50 beam
splitters and a tunable retarder, which allows us to control its reflectivity.
Then, we show that this tunable beam splitter, when equipped with weak
measurements and classical feedback, behaves as a quantum memristor. Indeed, in
order to prove its quantumness, we show how to codify quantum information in
the coherent beams. Moreover, we estimate the memory capability of the quantum
memristor. Finally, we show the feasibility of the proposed setup in integrated
quantum photonics.



We study the quantum synchronization between a pair of two-level systems
inside two coupledcavities. Using a digital-analog decomposition of the master
equation that rules the system dynamics, we show that this approach leads to
quantum synchronization between both two-level systems. Moreover, we can
identify in this digital-analog block decomposition the fundamental elements of
a quantum machine learning protocol, in which the agent and the environment
(learning units) interact through a mediating system, namely, the register. If
we can additionally equip this algorithm with a classical feedback mechanism,
which consists of projective measurements in the register, reinitialization of
the register state and local conditional operations on the agent and register
subspace, a powerful and flexible quantum machine learning protocol emerges.
Indeed, numerical simulations show that this protocol enhances the
synchronization process, even when every subsystem experience different
loss/decoherence mechanisms, and give us flexibility to choose the
synchronization state. Finally, we propose an implementation based on current
technologies in superconducting circuits.



We propose a new generative model of sentences that first samples a prototype
sentence from the training corpus and then edits it into a new sentence.
Compared to traditional models that generate from scratch either left-to-right
or by first sampling a latent sentence vector, our prototype-then-edit model
improves perplexity on language modeling and generates higher quality outputs
according to human evaluation. Furthermore, the model gives rise to a latent
edit vector that captures interpretable semantics such as sentence similarity
and sentence-level analogies.



In this work, we present an approach to deep visuomotor control using
structured deep dynamics models. Our deep dynamics model, a variant of
SE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an
encoder-decoder structure. Unlike prior work, our dynamics model is structured:
given an input scene, our network explicitly learns to segment salient parts
and predict their pose-embedding along with their motion modeled as a change in
the pose space due to the applied actions. We train our model using a pair of
point clouds separated by an action and show that given supervision only in the
form of point-wise data associations between the frames our network is able to
learn a meaningful segmentation of the scene along with consistent poses. We
further show that our model can be used for closed-loop control directly in the
learned low-dimensional pose space, where the actions are computed by
minimizing error in the pose space using gradient-based methods, similar to
traditional model-based control. We present results on controlling a Baxter
robot from raw depth data in simulation and in the real world and compare
against two baseline deep networks. Our method runs in real-time, achieves good
prediction of scene dynamics and outperforms the baseline methods on multiple
control runs. Video results can be found at:
https://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/



In this article we show the duality between tensor networks and undirected
graphical models with discrete variables. We study tensor networks on
hypergraphs, which we call tensor hypernetworks. We show that the tensor
hypernetwork on a hypergraph exactly corresponds to the graphical model given
by the dual hypergraph. We translate various notions under duality. For
example, marginalization in a graphical model is dual to contraction in the
tensor network. Algorithms also translate under duality. We show that belief
propagation corresponds to a known algorithm for tensor network contraction.
This article is a reminder that the research areas of graphical models and
tensor networks can benefit from interaction.



Bayesian inference for models that have an intractable partition function is
known as a doubly intractable problem, where standard Monte Carlo methods are
not applicable. The past decade has seen the development of auxiliary variable
Monte Carlo techniques (M{\o}ller et al., 2006; Murray et al., 2006) for
tackling this problem; these approaches being members of the more general class
of pseudo-marginal, or exact-approximate, Monte Carlo algorithms (Andrieu and
Roberts, 2009), which make use of unbiased estimates of intractable posteriors.
Everitt et al. (2017) investigated the use of exact-approximate importance
sampling (IS) and sequential Monte Carlo (SMC) in doubly intractable problems,
but focussed only on SMC algorithms that used data-point tempering. This paper
describes SMC samplers that may use alternative sequences of distributions, and
describes ways in which likelihood estimates may be improved adaptively as the
algorithm progresses, building on ideas from Moores et al. (2015). This
approach is compared with a number of alternative algorithms for doubly
intractable problems, including approximate Bayesian computation (ABC), which
we show is closely related to the method of M{\o}ller et al. (2006).



With the recent renaissance of deep convolution neural networks, encouraging
breakthroughs have been achieved on the supervised recognition tasks, where
each class has sufficient training data and fully annotated training data.
However, to scale the recognition to a large number of classes with few or now
training samples for each class remains an unsolved problem. One approach to
scaling up the recognition is to develop models capable of recognizing unseen
categories without any training instances, or zero-shot recognition/ learning.
This article provides a comprehensive review of existing zero-shot recognition
techniques covering various aspects ranging from representations of models, and
from datasets and evaluation settings. We also overview related recognition
tasks including one-shot and open set recognition which can be used as natural
extensions of zero-shot recognition when limited number of class samples become
available or when zero-shot recognition is implemented in a real-world setting.
Importantly, we highlight the limitations of existing approaches and point out
future research directions in this existing new research area.



We study learning algorithms that are restricted to using a small amount of
information from their input sample. We introduce a category of learning
algorithms we term d-bit information learners, which are algorithms whose
output conveys at most d bits of information on their input. A central theme in
this work is that such algorithms generalize.
  We focus on the learning capacity of these algorithms, and prove sample
complexity bounds with tight dependencies on the confidence and error
parameters. We also observe connections with well studied notions such as
sample compression schemes, Occam's razor, PAC-Bayes and differential privacy.
  We discuss an approach that allows us to prove upper bounds on the amount of
information that algorithms reveal about their inputs, and also provide a lower
bound by showing a simple concept class for which every (possibly randomized)
empirical risk minimizer must reveal a lot of information. On the other hand,
we show that in the distribution-dependent setting every VC class has empirical
risk minimizers that do not reveal a lot of information.



In this study, we systematically investigate the impact of class imbalance on
classification performance of convolutional neural networks (CNNs) and compare
frequently used methods to address the issue. Class imbalance is a common
problem that has been comprehensively studied in classical machine learning,
yet very limited systematic research is available in the context of deep
learning. In our study, we use three benchmark datasets of increasing
complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of
imbalance on classification and perform an extensive comparison of several
methods to address the issue: oversampling, undersampling, two-phase training,
and thresholding that compensates for prior class probabilities. Our main
evaluation metric is area under the receiver operating characteristic curve
(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is
associated with notable difficulties in the context of imbalanced data. Based
on results from our experiments we conclude that (i) the effect of class
imbalance on classification performance is detrimental; (ii) the method of
addressing class imbalance that emerged as dominant in almost all analyzed
scenarios was oversampling; (iii) oversampling should be applied to the level
that totally eliminates the imbalance, whereas undersampling can perform better
when the imbalance is only removed to some extent; (iv) as opposed to some
classical machine learning models, oversampling does not necessarily cause
overfitting of CNNs; (v) thresholding should be applied to compensate for prior
class probabilities when overall number of properly classified cases is of
interest.



We present Deep Voice 3, a fully-convolutional attention-based neural
text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural
speech synthesis systems in naturalness while training ten times faster. We
scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more
than eight hundred hours of audio from over two thousand speakers. In addition,
we identify common error modes of attention-based speech synthesis networks,
demonstrate how to mitigate them, and compare several different waveform
synthesis methods. We also describe how to scale inference to ten million
queries per day on one single-GPU server.



Regularization is one of the crucial ingredients of deep learning, yet the
term regularization has various definitions, and regularization methods are
often studied separately from each other. In our work we present a systematic,
unifying taxonomy to categorize existing methods. We distinguish methods that
affect data, network architectures, error terms, regularization terms, and
optimization procedures. We do not provide all details about the listed
methods; instead, we present an overview of how the methods can be sorted into
meaningful categories and sub-categories. This helps revealing links and
fundamental similarities between them. Finally, we include practical
recommendations both for users and for developers of new regularization
methods.



We analyze the expressiveness and loss surface of practical deep
convolutional neural networks (CNNs) with shared weights and max pooling
layers. We show that such CNNs produce linearly independent features at a
"wide" layer which has more neurons than the number of training samples. This
condition holds e.g. for the VGG network. Furthermore, we provide for such wide
CNNs necessary and sufficient conditions for global minima with zero training
error. For the case where the wide layer is followed by a fully connected
layer, we show that almost every critical point of the empirical loss is a
global minimum with zero training error. Our analysis suggests that both depth
and width are very important in deep learning. While depth brings more
representational power and allows the network to learn high level features,
width smoothes the optimization landscape of the loss function in the sense
that a sufficiently wide network has a well-behaved loss surface with
potentially no bad local minima.



This paper introduces a novel framework for learning data science models by
using the scientific knowledge encoded in physics-based models. This framework,
termed as physics-guided neural network (PGNN), leverages the output of
physics-based model simulations along with observational features to generate
predictions using a neural network architecture. Further, we present a novel
class of learning objective for training neural networks, which ensures that
the model predictions not only show lower errors on the training data but are
also \emph{consistent} with the known physics. We illustrate the effectiveness
of PGNN for the problem of lake temperature modeling, where physical
relationships between the temperature, density, and depth of water are used in
the learning of neural network model parameters. By using scientific knowledge
to guide the construction and learning of neural networks, we are able to show
that the proposed framework ensures better generalizability as well as physical
consistency of results.



User participation in online communities is driven by the intertwinement of
the social network structure with the crowd-generated content that flows along
its links. These aspects are rarely explored jointly and at scale. By looking
at how users generate and access pictures of varying beauty on Flickr, we
investigate how the production of quality impacts the dynamics of online social
systems. We develop a deep learning computer vision model to score images
according to their aesthetic value and we validate its output through
crowdsourcing. By applying it to over 15B Flickr photos, we study for the first
time how image beauty is distributed over a large-scale social system.
Beautiful images are evenly distributed in the network, although only a small
core of people get social recognition for them. To study the impact of exposure
to quality on user engagement, we set up matching experiments aimed at
detecting causality from observational data. Exposure to beauty is
double-edged: following people who produce high-quality content increases one's
probability of uploading better photos; however, an excessive imbalance between
the quality generated by a user and the user's neighbors leads to a decline in
engagement. Our analysis has practical implications for improving link
recommender systems.



Process Discovery is concerned with the automatic generation of a process
model that describes a business process from execution data of that business
process. Real life event logs can contain chaotic activities. These activities
are independent of the state of the process and can, therefore, happen at
rather arbitrary points in time. We show that the presence of such chaotic
activities in an event log heavily impacts the quality of the process models
that can be discovered with process discovery techniques. The current modus
operandi for filtering activities from event logs is to simply filter out
infrequent activities. We show that frequency-based filtering of activities
does not solve the problems that are caused by chaotic activities. Moreover, we
propose a novel technique to filter out chaotic activities from event logs. We
evaluate this technique on a collection of seventeen real-life event logs that
originate from both the business process management domain and the smart home
environment domain. As demonstrated, the developed activity filtering methods
enable the discovery of process models that are more behaviorally specific
compared to process models that are discovered using standard frequency-based
filtering.



In this paper, we study the representational power of deep neural networks
(DNN) that belong to the family of piecewise-linear (PWL) functions, based on
PWL activation units such as rectifier or maxout. We investigate the complexity
of such networks by studying the number of linear regions of the PWL function.
Typically, a PWL function from a DNN can be seen as a large family of linear
functions acting on millions of such regions. We directly build upon the work
of Montufar et al. (2014), Montufar (2017) and Raghu et al. (2017) by refining
the upper and lower bounds on the number of linear regions for rectified and
maxout networks. In addition to achieving tighter bounds, we also develop a
novel method to perform exact enumeration or counting of the number of linear
regions with a mixed-integer linear formulation that maps the input space to
output. We use this new capability to visualize how the number of linear
regions change while training DNNs.



Building effective recommender systems for domains like fashion is
challenging due to the high level of subjectivity and the semantic complexity
of the features involved (i.e., fashion styles). Recent work has shown that
approaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made
more accurate by incorporating visual signals directly into the recommendation
objective, using `off-the-shelf' feature representations derived from deep
networks. Here, we seek to extend this contribution by showing that
recommendation performance can be significantly improved by learning `fashion
aware' image representations directly, i.e., by training the image
representation (from the pixel level) and the recommender system jointly; this
contribution is related to recent work using Siamese CNNs, though we are able
to show improvements over state-of-the-art recommendation techniques such as
BPR and variants that make use of pre-trained visual features. Furthermore, we
show that our model can be used \emph{generatively}, i.e., given a user and a
product category, we can generate new images (i.e., clothing items) that are
most consistent with their personal taste. This represents a first step towards
building systems that go beyond recommending existing items from a product
corpus, but which can be used to suggest styles and aid the design of new
products.



Sparsity inducing regularization is an important part for learning
over-complete visual representations. Despite the popularity of $\ell_1$
regularization, in this paper, we investigate the usage of non-convex
regularizations in this problem. Our contribution consists of three parts.
First, we propose the leaky capped norm regularization (LCNR), which allows
model weights below a certain threshold to be regularized more strongly as
opposed to those above, therefore imposes strong sparsity and only introduces
controllable estimation bias. We propose a majorization-minimization algorithm
to optimize the joint objective function. Second, our study over monocular 3D
shape recovery and neural networks with LCNR outperforms $\ell_1$ and other
non-convex regularizations, achieving state-of-the-art performance and faster
convergence. Third, we prove a theoretical global convergence speed on the 3D
recovery problem. To the best of our knowledge, this is the first convergence
analysis of the 3D recovery problem.



We propose a new splitting criterion for a meta-learning approach to
multiclass classifier design that adaptively merges the classes into a
tree-structured hierarchy of increasingly difficult binary classification
problems. The classification tree is constructed from empirical estimates of
the Henze-Penrose bounds on the pairwise Bayes misclassification rates that
rank the binary subproblems in terms of difficulty of classification. The
proposed empirical estimates of the Bayes error rate are computed from the
minimal spanning tree (MST) of the samples from each pair of classes. Moreover,
a meta-learning technique is presented for quantifying the one-vs-rest Bayes
error rate for each individual class from a single MST on the entire dataset.
Extensive simulations on benchmark datasets show that the proposed hierarchical
method can often be learned much faster than competing methods, while achieving
competitive accuracy.



As technology becomes more advanced, those who design, use and are otherwise
affected by it want to know that it will perform correctly, and understand why
it does what it does, and how to use it appropriately. In essence they want to
be able to trust the systems that are being designed. In this survey we present
assurances that are the method by which users can understand how to trust
autonomous systems. Trust between humans and autonomy is reviewed, and the
implications for the design of assurances are highlighted. A survey of existing
research related to assurances is presented. Much of the surveyed research
originates from fields such as interpretable, comprehensible, transparent, and
explainable machine learning, as well as human-computer interaction,
human-robot interaction, and e-commerce. Several key ideas are extracted from
this work in order to refine the definition of assurances. The design of
assurances is found to be highly dependent not only on the capabilities of the
autonomous system, but on the characteristics of the human user, and the
appropriate trust-related behaviors. Several directions for future research are
identified and discussed.



Spectral clustering has found extensive use in many areas. Most traditional
spectral clustering algorithms work in three separate steps: similarity graph
construction; continuous labels learning; discretizing the learned labels by
k-means clustering. Such common practice has two potential flaws, which may
lead to severe information loss and performance degradation. First, predefined
similarity graph might not be optimal for subsequent clustering. It is
well-accepted that similarity graph highly affects the clustering results. To
this end, we propose to automatically learn similarity information from data
and simultaneously consider the constraint that the similarity matrix has exact
c connected components if there are c clusters. Second, the discrete solution
may deviate from the spectral solution since k-means method is well-known as
sensitive to the initialization of cluster centers. In this work, we transform
the candidate solution into a new one that better approximates the discrete
one. Finally, those three subtasks are integrated into a unified framework,
with each subtask iteratively boosted by using the results of the others
towards an overall optimal solution. It is known that the performance of a
kernel method is largely determined by the choice of kernels. To tackle this
practical problem of how to select the most suitable kernel for a particular
data set, we further extend our model to incorporate multiple kernel learning
ability. Extensive experiments demonstrate the superiority of our proposed
method as compared to existing clustering approaches.



In this article an automation system for human-machine-interfaces (HMI) for
setpoint adjustment using supervised learning is presented. We use HMIs of
multi-modal thermal conditioning systems in passenger cars as example for a
complex setpoint selection system. The goal is the reduction of interaction
complexity up to full automation. The approach is not limited to climate
control applications but can be extended to other setpoint-based HMIs.



Neuromorphic hardware tends to pose limits on the connectivity of deep
networks that one can run on them. But also generic hardware and software
implementations of deep learning run more efficiently for sparse networks.
Several methods exist for pruning connections of a neural network after it was
trained without connectivity constraints. We present an algorithm, DEEP R, that
enables us to train directly a sparsely connected neural network. DEEP R
automatically rewires the network during supervised training so that
connections are there where they are most needed for the task, while its total
number is all the time strictly bounded. We demonstrate that DEEP R can be used
to train very sparse feedforward and recurrent neural networks on standard
benchmark tasks with just a minor loss in performance. DEEP R is based on a
rigorous theoretical foundation that views rewiring as stochastic sampling of
network configurations from a posterior.



Approximate Bayesian computation (ABC) and synthetic likelihood (SL)
techniques have enabled the use of Bayesian inference for models that may be
simulated, but for which the likelihood cannot be evaluated pointwise at values
of an unknown parameter $\theta$. The main idea in ABC and SL is to, for
different values of $\theta$ (usually chosen using a Monte Carlo algorithm),
build estimates of the likelihood based on simulations from the model
conditional on $\theta$. The quality of these estimates determines the
efficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to
improve an estimated likelihood at $\theta$ is to simulate more times from the
model conditional on $\theta$, which is infeasible in cases where the simulator
is computationally expensive. In this paper we describe how to use
bootstrapping as a means for improving SL estimates whilst using fewer
simulations from the model, and also investigate its use in ABC. Further, we
investigate the use of the bag of little bootstraps as a means for applying
this approach to large datasets, yielding Monte Carlo algorithms that
accurately approximate posterior distributions whilst only simulating
subsamples of the full data. Examples of the approach applied to i.i.d.,
temporal and spatial data are given.



A growing field in robotics and Artificial Intelligence (AI) research is
human-robot collaboration, whose target is to enable effective teamwork between
humans and robots. However, in many situations human teams are still superior
to human-robot teams, primarily because human teams can easily agree on a
common goal with language, and the individual members observe each other
effectively, leveraging their shared motor repertoire and sensorimotor
resources. This paper shows that for cognitive robots it is possible, and
indeed fruitful, to combine knowledge acquired from interacting with elements
of the environment (affordance exploration) with the probabilistic observation
of another agent's actions.
  We propose a model that unites (i) learning robot affordances and word
descriptions with (ii) statistical recognition of human gestures with vision
sensors. We discuss theoretical motivations, possible implementations, and we
show initial results which highlight that, after having acquired knowledge of
its surrounding environment, a humanoid robot can generalize this knowledge to
the case when it observes another agent (human partner) performing the same
motor actions previously executed during training.



We introduce physics informed neural networks -- neural networks that are
trained to solve supervised learning tasks while respecting any given law of
physics described by general nonlinear partial differential equations. In this
two part treatise, we present our developments in the context of solving two
main classes of problems: data-driven solution and data-driven discovery of
partial differential equations. Depending on the nature and arrangement of the
available data, we devise two distinct classes of algorithms, namely continuous
time and discrete time models. The resulting neural networks form a new class
of data-efficient universal function approximators that naturally encode any
underlying physical laws as prior information. In this first part, we
demonstrate how these networks can be used to infer solutions to partial
differential equations, and obtain physics-informed surrogate models that are
fully differentiable with respect to all input coordinates and free parameters.



We introduce physics informed neural networks -- neural networks that are
trained to solve supervised learning tasks while respecting any given law of
physics described by general nonlinear partial differential equations. In this
second part of our two-part treatise, we focus on the problem of data-driven
discovery of partial differential equations. Depending on whether the available
data is scattered in space-time or arranged in fixed temporal snapshots, we
introduce two main classes of algorithms, namely continuous time and discrete
time models. The effectiveness of our approach is demonstrated using a wide
range of benchmark problems in mathematical physics, including conservation
laws, incompressible fluid flow, and the propagation of nonlinear shallow-water
waves.



ConvNets and Imagenet have driven the recent success of deep learning for
image classification. However, the marked slowdown in performance improvement,
the recent studies on the lack of robustness of neural networks to adversarial
examples and their tendency to exhibit undesirable biases (e.g racial biases)
questioned the reliability and the sustained development of these methods. This
work investigates these questions from the perspective of the end-user by using
human subject studies and explanations. We experimentally demonstrate that the
accuracy and robustness of ConvNets measured on Imagenet are underestimated. We
show that explanations can mitigate the impact of misclassified adversarial
examples from the perspective of the end-user and we introduce a novel tool for
uncovering the undesirable biases learned by a model. These contributions also
show that explanations are a promising tool for improving our understanding of
ConvNets' predictions and for designing more reliable models



We propose to use neural networks (NNs) for simultaneous detection and
localization of multiple sound sources in Human-Robot Interaction (HRI). Unlike
conventional signal processing techniques, NN-based Sound Source Localization
(SSL) methods are relatively straightforward and require no or fewer
assumptions that hardly hold in real HRI scenarios. Previously, NN-based
methods have been successfully applied to single SSL problems, which do not
extend to multiple sources in terms of detection and localization. In this
paper, we thus propose a likelihood-based encoding of the network output, which
naturally allows the detection of an arbitrary number of sources. In addition,
we investigate the use of sub-band cross-correlation information as features
for better localization in sound mixtures, as well as three different NN
architectures based on different processing motivations. Experiments on real
data recorded from the robot show that our NN-based methods significantly
outperform the popular spatial spectrum-based approaches.



Automatic transcriptions of consumer-generated multi-media content such as
"Youtube" videos still exhibit high word error rates. Such data typically
occupies a very broad domain, has been recorded in challenging conditions, with
cheap hardware and a focus on the visual modality, and may have been
post-processed or edited. In this paper, we extend our earlier work on adapting
the acoustic model of a DNN-based speech recognition system to an RNN language
model and show how both can be adapted to the objects and scenes that can be
automatically detected in the video. We are working on a corpus of "how-to"
videos from the web, and the idea is that an object that can be seen ("car"),
or a scene that is being detected ("kitchen") can be used to condition both
models on the "context" of the recording, thereby reducing perplexity and
improving transcription. We achieve good improvements in both cases and compare
and analyze the respective reductions in word error rate. We expect that our
results can be used for any type of speech processing in which "context"
information is available, for example in robotics, man-machine interaction, or
when indexing large audio-visual archives, and should ultimately help to bring
together the "video-to-text" and "speech-to-text" communities.



Predictable Feature Analysis (PFA) (Richthofer, Wiskott, ICMLA 2015) is an
algorithm that performs dimensionality reduction on high dimensional input
signal. It extracts those subsignals that are most predictable according to a
certain prediction model. We refer to these extracted signals as predictable
features.
  In this work we extend the notion of PFA to take supplementary information
into account for improving its predictions. Such information can be a
multidimensional signal like the main input to PFA, but is regarded external.
That means it won't participate in the feature extraction - no features get
extracted or composed of it. Features will be exclusively extracted from the
main input such that they are most predictable based on themselves and the
supplementary information. We refer to this enhanced PFA as PFAx (PFA
extended).
  Even more important than improving prediction quality is to observe the
effect of supplementary information on feature selection. PFAx transparently
provides insight how the supplementary information adds to prediction quality
and whether it is valuable at all. Finally we show how to invert that relation
and can generate the supplementary information such that it would yield a
certain desired outcome of the main signal.
  We apply this to a setting inspired by reinforcement learning and let the
algorithm learn how to control an agent in an environment. With this method it
is feasible to locally optimize the agent's state, i.e. reach a certain goal
that is near enough. We are preparing a follow-up paper that extends this
method such that also global optimization is feasible.



In this project we analysed how much semantic information images carry, and
how much value image data can add to sentiment analysis of the text associated
with the images. To better understand the contribution from images, we compared
models which only made use of image data, models which only made use of text
data, and models which combined both data types. We also analysed if this
approach could help sentiment classifiers generalize to unknown sentiments.



We consider the problem of learning a one-hidden-layer neural network with
non-overlapping convolutional layer and ReLU activation function, i.e.,
$f(\mathbf{Z}; \mathbf{w}, \mathbf{a}) = \sum_j
a_j\sigma(\mathbf{w}^\top\mathbf{Z}_j)$, in which both the convolutional
weights $\mathbf{w}$ and the output weights $\mathbf{a}$ are parameters to be
learned. We prove that with Gaussian input $\mathbf{Z}$, there is a spurious
local minimum that is not a global mininum. Surprisingly, in the presence of
local minimum, starting from randomly initialized weights, gradient descent
with weight normalization can still be proven to recover the true parameters
with constant probability (which can be boosted to arbitrarily high accuracy
with multiple restarts). We also show that with constant probability, the same
procedure could also converge to the spurious local minimum, showing that the
local minimum plays a non-trivial role in the dynamics of gradient descent.
Furthermore, a quantitative analysis shows that the gradient descent dynamics
has two phases: it starts off slow, but converges much faster after several
iterations.



In this work we propose a blackbox intervention method for visual dialog
models, with the aim of assessing the contribution of individual linguistic or
visual components. Concretely, we conduct structured or randomized
interventions that aim to impair an individual component of the model, and
observe changes in task performance. We reproduce a state-of-the-art visual
dialog model and demonstrate that our methodology yields surprising insights,
namely that both dialog and image information have minimal contributions to
task performance. The intervention method presented here can be applied as a
sanity check for the strength and robustness of each component in visual dialog
systems.



The materialist dialectical method is a philosophical investigative method to
analyze aspects of reality. These aspects are viewed as complex processes
composed by basic units named poles, which interact with each other. Dialectics
has experienced considerable progress in the 19th century, with Hegel's
dialectics and, in the 20th century, with the works of Marx, Engels, and
Gramsci, in Philosophy and Economics. The movement of poles through their
contradictions is viewed as a dynamic process with intertwined phases of
evolution and revolutionary crisis. In order to build a computational process
based on dialectics, the interaction between poles can be modeled using fuzzy
membership functions. Based on this assumption, we introduce the Objective
Dialectical Classifier (ODC), a non-supervised map for classification based on
materialist dialectics and designed as an extension of fuzzy c-means
classifier. As a case study, we used ODC to classify 181 magnetic resonance
synthetic multispectral images composed by proton density, $T_1$- and
$T_2$-weighted synthetic brain images. Comparing ODC to k-means, fuzzy c-means,
and Kohonen's self-organized maps, concerning with image fidelity indexes as
estimatives of quantization distortion, we proved that ODC can reach almost the
same quantization performance as optimal non-supervised classifiers like
Kohonen's self-organized maps.



We present MINOS, a simulator designed to support the development of
multisensory models for goal-directed navigation in complex indoor
environments. The simulator leverages large datasets of complex 3D environments
and supports flexible configuration of multimodal sensor suites. We use MINOS
to benchmark deep-learning-based navigation methods, to analyze the influence
of environmental complexity on navigation performance, and to carry out a
controlled study of multimodality in sensorimotor learning. The experiments
show that current deep reinforcement learning approaches fail in large
realistic environments. The experiments also indicate that multimodality is
beneficial in learning to navigate cluttered scenes. MINOS is released
open-source to the research community at http://minosworld.org . A video that
shows MINOS can be found at https://youtu.be/c0mL9K64q84



The spread of invasive species to new areas threatens the stability of
ecosystems and causes major economic losses in agriculture and forestry. We
propose a novel approach to minimizing the spread of an invasive species given
a limited intervention budget. We first model invasive species propagation
using Hawkes processes, and then derive closed-form expressions for
characterizing the effect of an intervention action on the invasion process. We
use this to obtain an optimal intervention plan based on an integer programming
formulation, and compare the optimal plan against several
ecologically-motivated heuristic strategies used in practice. We present an
empirical study of two variants of the invasive control problem: minimizing the
final rate of invasions, and minimizing the number of invasions at the end of a
given time horizon. Our results show that the optimized intervention achieves
nearly the same level of control that would be attained by completely
eradicating the species, with a 20% cost saving. Additionally, we design a
heuristic intervention strategy based on a combination of the density and life
stage of the invasive individuals, and find that it comes surprisingly close to
the optimized strategy, suggesting that this could serve as a good rule of
thumb in invasive species management.



Learning customer preferences from an observed behaviour is an important
topic in the marketing literature. Structural models typically model
forward-looking customers or firms as utility-maximizing agents whose utility
is estimated using methods of Stochastic Optimal Control. We suggest an
alternative approach to study dynamic consumer demand, based on Inverse
Reinforcement Learning (IRL). We develop a version of the Maximum Entropy IRL
that leads to a highly tractable model formulation that amounts to
low-dimensional convex optimization in the search for optimal model parameters.
Using simulations of consumer demand, we show that observational noise for
identical customers can be easily confused with an apparent consumer
heterogeneity.



The information bottleneck (IB) approach to clustering takes a joint
distribution $P\!\left(X,Y\right)$ and maps the data $X$ to cluster labels $T$
which retain maximal information about $Y$ (Tishby et al., 1999). This
objective results in an algorithm that clusters data points based upon the
similarity of their conditional distributions $P\!\left(Y\mid X\right)$. This
is in contrast to classic "geometric clustering" algorithms such as $k$-means
and gaussian mixture models (GMMs) which take a set of observed data points
$\left\{ \mathbf{x}_{i}\right\}_{i=1:N}$ and cluster them based upon their
geometric (typically Euclidean) distance from one another. Here, we show how to
use the deterministic information bottleneck (DIB) (Strouse and Schwab, 2017),
a variant of IB, to perform geometric clustering, by choosing cluster labels
that preserve information about data point location on a smoothed dataset. We
also introduce a novel intuitive method to choose the number of clusters, via
kinks in the information curve. We apply this approach to a variety of simple
clustering problems, showing that DIB with our model selection procedure
recovers the generative cluster labels. We also show that, for one simple case,
DIB interpolates between the cluster boundaries of GMMs and $k$-means in the
large data limit. Thus, our IB approach to clustering also provides an
information-theoretic perspective on these classic algorithms.



Recent advancements in pattern recognition and signal processing concern the
automatic learning of data representations from labeled training samples.
Typical approaches are based on deep learning and convolutional neural
networks, which require large amount of labeled training samples. In this work,
we propose novel feature extractors that can be used to learn the
representation of single prototype samples in an automatic configuration
process. We employ the proposed feature extractors in applications of audio and
image processing, and show their effectiveness on benchmark data sets.



A common problem in machine learning is to rank a set of n items based on
pairwise comparisons. Here ranking refers to partitioning the items into sets
of pre-specified sizes according to their scores, which includes identification
of the top-k items as the most prominent special case. The score of a given
item is defined as the probability that it beats a randomly chosen other item.
Finding an exact ranking typically requires a prohibitively large number of
comparisons, but in practice, approximate rankings are often adequate.
Accordingly, we study the problem of finding approximate rankings from pairwise
comparisons. We analyze an active ranking algorithm that counts the number of
comparisons won, and decides whether to stop or which pair of items to compare
next, based on confidence intervals computed from the data collected in
previous steps. We show that this algorithm succeeds in recovering approximate
rankings using a number of comparisons that is close to optimal up to
logarithmic factors. We also present numerical results, showing that in
practice, approximation can drastically reduce the number of comparisons
required to estimate a ranking.



According to the World Health Organization, breast cancer is the most common
form of cancer in women. It is the second leading cause of death among women
round the world, becoming the most fatal form of cancer. Mammographic image
segmentation is a fundamental task to support image analysis and diagnosis,
taking into account shape analysis of mammary lesions and their borders.
However, mammogram segmentation is a very hard process, once it is highly
dependent on the types of mammary tissues. In this work we present a new
semi-supervised segmentation algorithm based on the modification of the GrowCut
algorithm to perform automatic mammographic image segmentation once a region of
interest is selected by a specialist. In our proposal, we used fuzzy Gaussian
membership functions to modify the evolution rule of the original GrowCut
algorithm, in order to estimate the uncertainty of a pixel being object or
background. The main impact of the proposed method is the significant reduction
of expert effort in the initialization of seed points of GrowCut to perform
accurate segmentation, once it removes the need of selection of background
seeds. We also constructed an automatic point selection process based on the
simulated annealing optimization method, avoiding the need of human
intervention. The proposed approach was qualitatively compared with other
state-of-the-art segmentation techniques, considering the shape of segmented
regions. In order to validate our proposal, we built an image classifier using
a classical multilayer perceptron. We used Zernike moments to extract segmented
image features. This analysis employed 685 mammograms from IRMA breast cancer
database, using fat and fibroid tissues. Results show that the proposed
technique could achieve a classification rate of 91.28\% for fat tissues,
evidencing the feasibility of our approach.



To compare entities of differing types and structural components, the
artificial neural network paradigm was used to cross-compare structural
components between heterogeneous documents. Trainable weighted structural
components were input into machine-learned activation functions of the neurons.
The model was used for matching news articles and videos, where the inputs and
activation functions respectively consisted of term vectors and cosine
similarity measures between the weighted structural components. The model was
tested with different weights, achieving as high as 59.2% accuracy for matching
videos to news articles. A mobile application user interface for recommending
related videos for news articles was developed to demonstrate consumer value,
including its potential usefulness for cross-selling products from unrelated
categories.



Conversational agents are exploding in popularity. However, much work remains
in the area of social conversation as well as free-form conversation over a
broad range of domains and topics. To advance the state of the art in
conversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar
university competition where sixteen selected university teams were challenged
to build conversational agents, known as socialbots, to converse coherently and
engagingly with humans on popular topics such as Sports, Politics,
Entertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers
the academic community a unique opportunity to perform research with a live
system used by millions of users. The competition provided university teams
with real user conversational data at scale, along with the user-provided
ratings and feedback augmented with annotations by the Alexa team. This enabled
teams to effectively iterate and make improvements throughout the competition
while being evaluated in real-time through live user interactions. To build
their socialbots, university teams combined state-of-the-art techniques with
novel strategies in the areas of Natural Language Understanding, Context
Modeling, Dialog Management, Response Generation, and Knowledge Acquisition. To
support the efforts of participating teams, the Alexa Prize team made
significant scientific and engineering investments to build and improve
Conversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice
User Experience, and tools for traffic management and scalability. This paper
outlines the advances created by the university teams as well as the Alexa
Prize team to achieve the common goal of solving the problem of Conversational
AI.



Dialog evaluation is a challenging problem, especially for non task-oriented
dialogs where conversational success is not well-defined. We propose to
evaluate dialog quality using topic-based metrics that describe the ability of
a conversational bot to sustain coherent and engaging conversations on a topic,
and the diversity of topics that a bot can handle. To detect conversation
topics per utterance, we adopt Deep Average Networks (DAN) and train a topic
classifier on a variety of question and query data categorized into multiple
topics. We propose a novel extension to DAN by adding a topic-word attention
table that allows the system to jointly capture topic keywords in an utterance
and perform topic classification. We compare our proposed topic based metrics
with the ratings provided by users and show that our metrics both correlate
with and complement human judgment. Our analysis is performed on tens of
thousands of real human-bot dialogs from the Alexa Prize competition and
highlights user expectations for conversational bots.



Conversational agents are exploding in popularity. However, much work remains
in the area of non goal-oriented conversations, despite significant growth in
research interest over recent years. To advance the state of the art in
conversational AI, Amazon launched the Alexa Prize, a 2.5-million dollar
university competition where sixteen selected university teams built
conversational agents to deliver the best social conversational experience.
Alexa Prize provided the academic community with the unique opportunity to
perform research with a live system used by millions of users. The subjectivity
associated with evaluating conversations is key element underlying the
challenge of building non-goal oriented dialogue systems. In this paper, we
propose a comprehensive evaluation strategy with multiple metrics designed to
reduce subjectivity by selecting metrics which correlate well with human
judgement. The proposed metrics provide granular analysis of the conversational
agents, which is not captured in human ratings. We show that these metrics can
be used as a reasonable proxy for human judgment. We provide a mechanism to
unify the metrics for selecting the top performing agents, which has also been
applied throughout the Alexa Prize competition. To our knowledge, to date it is
the largest setting for evaluating agents with millions of conversations and
hundreds of thousands of ratings from users. We believe that this work is a
step towards an automatic evaluation process for conversational AIs.



Automated decision making systems are increasingly being used in real-world
applications. In these systems for the most part, the decision rules are
derived by minimizing the training error on the available historical data.
Therefore, if there is a bias related to a sensitive attribute such as gender,
race, religion, etc. in the data, say, due to cultural/historical
discriminatory practices against a certain demographic, the system could
continue discrimination in decisions by including the said bias in its decision
rule. We present an information theoretic framework for designing fair
predictors from data, which aim to prevent discrimination against a specified
sensitive attribute in a supervised learning setting. We use equalized odds as
the criterion for discrimination, which demands that the prediction should be
independent of the protected attribute conditioned on the actual label. To
ensure fairness and generalization simultaneously, we compress the data to an
auxiliary variable, which is used for the prediction task. This auxiliary
variable is chosen such that it is decontaminated from the discriminatory
attribute in the sense of equalized odds. The final predictor is obtained by
applying a Bayesian decision rule to the auxiliary variable.



In current study, a mechanism to extract traffic related information such as
congestion and incidents from textual data from the internet is proposed. The
current source of data is Twitter. As the data being considered is extremely
large in size automated models are developed to stream, download, and mine the
data in real-time. Furthermore, if any tweet has traffic related information
then the models should be able to infer and extract this data.
  Currently, the data is collected only for United States and a total of
120,000 geo-tagged traffic related tweets are extracted, while six million
geo-tagged non-traffic related tweets are retrieved and classification models
are trained. Furthermore, this data is used for various kinds of spatial and
temporal analysis. A mechanism to calculate level of traffic congestion,
safety, and traffic perception for cities in U.S. is proposed. Traffic
congestion and safety rankings for the various urban areas are obtained and
then they are statistically validated with existing widely adopted rankings.
Traffic perception depicts the attitude and perception of people towards the
traffic.
  It is also seen that traffic related data when visualized spatially and
temporally provides the same pattern as the actual traffic flows for various
urban areas. When visualized at the city level, it is clearly visible that the
flow of tweets is similar to flow of vehicles and that the traffic related
tweets are representative of traffic within the cities. With all the findings
in current study, it is shown that significant amount of traffic related
information can be extracted from Twitter and other sources on internet.
Furthermore, Twitter and these data sources are freely available and are not
bound by spatial and temporal limitations. That is, wherever there is a user
there is a potential for data.



We introduce a continuous-time analog solver for MaxSAT, a quintessential
class of NP-hard discrete optimization problems, where the task is to find a
truth assignment for a set of Boolean variables satisfying the maximum number
of given logical constraints. We show that the scaling of an invariant of the
solver's dynamics, the escape rate, as function of the number of unsatisfied
clauses can predict the global optimum value, often well before reaching the
corresponding state. We demonstrate the performance of the solver on hard
MaxSAT competition problems. We then consider the two-color Ramsey number
$R(m,m)$ problem, translate it to SAT, and apply our algorithm to the still
unknown $R(5,5)$. We find edge colorings without monochromatic 5-cliques for
complete graphs up to 42 vertices, while on 43 vertices we find colorings with
only two monochromatic 5-cliques, the best coloring found so far, supporting
the conjecture that $R(5,5) = 43$.



We present MILABOT: a deep reinforcement learning chatbot developed by the
Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize
competition. MILABOT is capable of conversing with humans on popular small talk
topics through both speech and text. The system consists of an ensemble of
natural language generation and retrieval models, including neural network and
template-based models. By applying reinforcement learning to crowdsourced data
and real-world user interactions, the system has been trained to select an
appropriate response from the models in its ensemble. The system has been
evaluated through A/B testing with real-world users, where it performed
significantly better than other systems. The results highlight the potential of
coupling ensemble systems with deep reinforcement learning as a fruitful path
for developing real-world, open-domain conversational agents.



For enhancing the privacy protections of databases, where the increasing
amount of detailed personal data is stored and processed, multiple mechanisms
have been developed, such as audit logging and alert triggers, which notify
administrators about suspicious activities; however, the two main limitations
in common are: 1) the volume of such alerts is often substantially greater than
the capabilities of resource-constrained organizations, and 2) strategic
attackers may disguise their actions or carefully choosing which records they
touch, making incompetent the statistical detection models. For solving them,
we introduce a novel approach to database auditing that explicitly accounts for
adversarial behavior by 1) prioritizing the order in which types of alerts are
investigated and 2) providing an upper bound on how much resource to allocate
for each type. We model the interaction between a database auditor and
potential attackers as a Stackelberg game in which the auditor chooses an
auditing policy and attackers choose which records to target. A corresponding
approach combining linear programming, column generation, and heuristic search
is proposed to derive an auditing policy. For testing the policy-searching
performance, a publicly available credit card application dataset are adopted,
on which it shows that our methods produce high-quality mixed strategies as
database audit policies, and our general approach significantly outperforms
non-game-theoretic baselines.



Clustering is a fundamental machine learning method. The quality of its
results is dependent on the data distribution. For this reason, deep neural
networks can be used for learning better representations of the data. In this
paper, we propose a systematic taxonomy for clustering with deep learning, in
addition to a review of methods from the field. Based on our taxonomy, creating
new methods is more straightforward. We also propose a new approach which is
built on the taxonomy and surpasses some of the limitations of some previous
work. Our experimental evaluation on image datasets shows that the method
approaches state-of-the-art clustering quality, and performs better in some
cases.



The human brain is able to learn, generalize, and predict crossmodal stimuli.
Learning by expectation fine-tunes crossmodal processing at different levels,
thus enhancing our power of generalization and adaptation in highly dynamic
environments. In this paper, we propose a deep neural architecture trained by
using expectation learning accounting for unsupervised learning tasks. Our
learning model exhibits a self-adaptable behavior, setting the first steps
towards the development of deep learning architectures for crossmodal stimuli
association.



Automatic music generation is a compelling task where much recent progress
has been made with deep learning models. In this paper, we ask how these models
can be integrated into interactive music systems; how can they encourage or
enhance the music making of human users? Musical performance requires
prediction to operate instruments, and perform in groups. We argue that
predictive models could help interactive systems to understand their temporal
context, and ensemble behaviour. Deep learning can allow data-driven models
with a long memory of past states.
  We advocate for predictive musical interaction, where a predictive model is
embedded in a musical interface, assisting users by predicting unknown states
of musical processes. We propose a framework for incorporating such predictive
models into the sensing, processing, and result architecture that is often used
in musical interface design. We show that our framework accommodates deep
generative models, as well as models for predicting gestural states, or other
high-level musical information. We motivate the framework with two examples
from our recent work, as well as systems from the literature, and suggest
musical use-cases where prediction is a necessary component.



We propose an architecture for VQA which utilizes recurrent layers to
generate visual and textual attention. The memory characteristic of the
proposed recurrent attention units offers a rich joint embedding of visual and
textual features and enables the model to reason relations between several
parts of the image and question. Our single model outperforms the first place
winner on the VQA 1.0 dataset, performs within margin to the current
state-of-the-art ensemble model. We also experiment with replacing attention
mechanisms in other state-of-the-art models with our implementation and show
increased accuracy. In both cases, our recurrent attention mechanism improves
performance in tasks requiring sequential or relational reasoning on the VQA
dataset.



Several papers have recently contained reports on applying machine learning
(ML) to the automation of software engineering (SE) tasks, such as project
management, modeling and development. However, there appear to be no approaches
comparing how software engineers fare against machine-learning algorithms as
applied to specific software development tasks. Such a comparison is essential
to gain insight into which tasks are better performed by humans and which by
machine learning and how cooperative work or human-in-the-loop processes can be
implemented more effectively. In this paper, we present an empirical study that
compares how software engineers and machine-learning algorithms perform and
reuse tasks. The empirical study involves the synthesis of the control
structure of an autonomous streetlight application. Our approach consists of
four steps. First, we solved the problem using machine learning to determine
specific performance and reuse tasks. Second, we asked software engineers with
different domain knowledge levels to provide a solution to the same tasks.
Third, we compared how software engineers fare against machine-learning
algorithms when accomplishing the performance and reuse tasks based on criteria
such as energy consumption and safety. Finally, we analyzed the results to
understand which tasks are better performed by either humans or algorithms so
that they can work together more effectively. Such an understanding and the
resulting human-in-the-loop approaches, which take into account the strengths
and weaknesses of humans and machine-learning algorithms, are fundamental not
only to provide a basis for cooperative work in support of software
engineering, but also, in other areas.



We propose an exact solution for the problem of finding the size of a Markov
equivalence class (MEC). For the bounded degree graphs, the proposed solution
is capable of computing the size of the MEC in polynomial time. Our proposed
approach is based on a recursive method for counting the number of the elements
of the MEC when a specific vertex is set as the source variable. We will
further use the idea to design a sampler, which is capable of sampling from an
MEC uniformly in polynomial time.



We consider detection based on deep learning, and show it is possible to
train detectors that perform well, without any knowledge of the underlying
channel models. Moreover, when the channel model is known, we demonstrate that
it is possible to train detectors that do not require channel state information
(CSI). In particular, a technique we call sliding bidirectional recurrent
neural network (SBRNN) is proposed for detection where, after training, the
detector estimates the data in real-time as the signal stream arrives at the
receiver. We evaluate this algorithm, as well as other neural network (NN)
architectures, using the Poisson channel model, which is applicable to both
optical and chemical communication systems. In addition, we also evaluate the
performance of this detection method applied to data sent over a chemical
communication platform, where the channel model is difficult to model
analytically. We show that SBRNN is computationally efficient, and can perform
detection under various channel conditions without knowing the underlying
channel model. We also demonstrate that the bit error rate (BER) performance of
the proposed SBRNN detector is better than that of a Viterbi detector with
imperfect CSI as well as that of other NN detectors that have been previously
proposed.



Genetic fitness optimization using small populations or small population
updates across generations generally suffers from randomly diverging
evolutions. We propose a notion of highly probable fitness optimization through
feasible evolutionary computing runs on small size populations. Based on
rapidly mixing Markov chains, the approach pertains to most types of
evolutionary genetic algorithms, genetic programming and the like. We establish
that for systems having associated rapidly mixing Markov chains and appropriate
stationary distributions the new method finds optimal programs (individuals)
with probability almost 1. To make the method useful would require a structured
design methodology where the development of the program and the guarantee of
the rapidly mixing property go hand in hand. We analyze a simple example to
show that the method is implementable. More significant examples require
theoretical advances, for example with respect to the Metropolis filter.



We consider the El Farol bar problem, also known as the minority game (W. B.
Arthur, ``The American Economic Review'', 84(2): 406--411 (1994), D. Challet
and Y.C. Zhang, ``Physica A'', 256:514 (1998)). We view it as an instance of
the general problem of how to configure the nodal elements of a distributed
dynamical system so that they do not ``work at cross purposes'', in that their
collective dynamics avoids frustration and thereby achieves a provided global
goal. We summarize a mathematical theory for such configuration applicable when
(as in the bar problem) the global goal can be expressed as minimizing a global
energy function and the nodes can be expressed as minimizers of local free
energy functions. We show that a system designed with that theory performs
nearly optimally for the bar problem.



This paper describes the system of storage, extract and processing of
information structured similarly to the natural language. For recursive
inference the system uses the rules having the same representation, as the
data. The environment of storage of information is provided with the File
Mapping (SHM) mechanism of operating system. In the paper the main principles
of construction of dynamic data structure and language for record of the
inference rules are stated; the features of available implementation are
considered and the description of the application realizing semantic
information retrieval on the natural language is given.



We provide a new representation-independent formulation of Occam's razor
theorem, based on Kolmogorov complexity. This new formulation allows us to:
  (i) Obtain better sample complexity than both length-based and VC-based
versions of Occam's razor theorem, in many applications.
  (ii) Achieve a sharper reverse of Occam's razor theorem than previous work.
  Specifically, we weaken the assumptions made in an earlier publication, and
extend the reverse to superpolynomial running times.



This paper studies sequence prediction based on the monotone Kolmogorov
complexity Km=-log m, i.e. based on universal deterministic/one-part MDL. m is
extremely close to Solomonoff's prior M, the latter being an excellent
predictor in deterministic as well as probabilistic environments, where
performance is measured in terms of convergence of posteriors or losses.
Despite this closeness to M, it is difficult to assess the prediction quality
of m, since little is known about the closeness of their posteriors, which are
the important quantities for prediction. We show that for deterministic
computable environments, the "posterior" and losses of m converge, but rapid
convergence could only be shown on-sequence; the off-sequence behavior is
unclear. In probabilistic environments, neither the posterior nor the losses
converge, in general.



Recommendation systems for different Document Networks (DN) such as the World
Wide Web (WWW) and Digital Libraries, often use distance functions extracted
from relationships among documents and keywords. For instance, documents in the
WWW are related via a hyperlink network, while documents in bibliographic
databases are related by citation and collaboration networks. Furthermore,
documents are related to keyterms. The distance functions computed from these
relations establish associative networks among items of the DN, referred to as
Distance Graphs, which allow recommendation systems to identify relevant
associations for individual users. However, modern recommendation systems need
to integrate associative data from multiple sources such as different
databases, web sites, and even other users. Thus, we are presented with a
problem of combining evidence (about associations between items) from different
sources characterized by distance functions. In this paper we describe our work
on (1) inferring relevant associations from, as well as characterizing,
semi-metric distance graphs and (2) combining evidence from different distance
graphs in a recommendation system. Regarding (1), we present the idea of
semi-metric distance graphs, and introduce ratios to measure semi-metric
behavior. We compute these ratios for several DN such as digital libraries and
web sites and show that they are useful to identify implicit associations.
Regarding (2), we describe an algorithm to combine evidence from distance
graphs that uses Evidence Sets, a set structure based on Interval Valued Fuzzy
Sets and Dempster-Shafer Theory of Evidence. This algorithm has been developed
for a recommendation system named TalkMine.



We present a new method for clustering based on compression. The method
doesn't use subject-specific features or background knowledge, and works as
follows: First, we determine a universal similarity distance, the normalized
compression distance or NCD, computed from the lengths of compressed data files
(singly and in pairwise concatenation). Second, we apply a hierarchical
clustering method. The NCD is universal in that it is not restricted to a
specific application area, and works across application area boundaries. A
theoretical precursor, the normalized information distance, co-developed by one
of the authors, is provably optimal but uses the non-computable notion of
Kolmogorov complexity. We propose precise notions of similarity metric, normal
compressor, and show that the NCD based on a normal compressor is a similarity
metric that approximates universality. To extract a hierarchy of clusters from
the distance matrix, we determine a dendrogram (binary tree) by a new quartet
method and a fast heuristic to implement it. The method is implemented and
available as public software, and is robust under choice of different
compressors. To substantiate our claims of universality and robustness, we
report evidence of successful application in areas as diverse as genomics,
virology, languages, literature, music, handwritten digits, astronomy, and
combinations of objects from completely different domains, using statistical,
dictionary, and block sorting compressors. In genomics we presented new
evidence for major questions in Mammalian evolution, based on
whole-mitochondrial genomic analysis: the Eutherian orders and the Marsupionta
hypothesis against the Theria hypothesis.



Mutual information is widely used, in a descriptive way, to measure the
stochastic dependence of categorical random variables. In order to address
questions such as the reliability of the descriptive value, one must consider
sample-to-population inferential approaches. This paper deals with the
posterior distribution of mutual information, as obtained in a Bayesian
framework by a second-order Dirichlet prior distribution. The exact analytical
expression for the mean, and analytical approximations for the variance,
skewness and kurtosis are derived. These approximations have a guaranteed
accuracy level of the order O(1/n^3), where n is the sample size. Leading order
approximations for the mean and the variance are derived in the case of
incomplete samples. The derived analytical expressions allow the distribution
of mutual information to be approximated reliably and quickly. In fact, the
derived expressions can be computed with the same order of complexity needed
for descriptive mutual information. This makes the distribution of mutual
information become a concrete alternative to descriptive mutual information in
many applications which would benefit from moving to the inductive side. Some
of these prospective applications are discussed, and one of them, namely
feature selection, is shown to perform significantly better when inductive
mutual information is used.



Solomonoff's central result on induction is that the posterior of a universal
semimeasure M converges rapidly and with probability 1 to the true sequence
generating posterior mu, if the latter is computable. Hence, M is eligible as a
universal sequence predictor in case of unknown mu. Despite some nearby results
and proofs in the literature, the stronger result of convergence for all
(Martin-Loef) random sequences remained open. Such a convergence result would
be particularly interesting and natural, since randomness can be defined in
terms of M itself. We show that there are universal semimeasures M which do not
converge for all random sequences, i.e. we give a partial negative answer to
the open problem. We also provide a positive answer for some non-universal
semimeasures. We define the incomputable measure D as a mixture over all
computable measures and the enumerable semimeasure W as a mixture over all
enumerable nearly-measures. We show that W converges to D and D to mu on all
random sequences. The Hellinger distance measuring closeness of two
distributions plays a central role.



The speech code is a vehicle of language: it defines a set of forms used by a
community to carry information. Such a code is necessary to support the
linguistic interactions that allow humans to communicate. How then may a speech
code be formed prior to the existence of linguistic interactions? Moreover, the
human speech code is discrete and compositional, shared by all the individuals
of a community but different across communities, and phoneme inventories are
characterized by statistical regularities. How can a speech code with these
properties form? We try to approach these questions in the paper, using the
"methodology of the artificial". We build a society of artificial agents, and
detail a mechanism that shows the formation of a discrete speech code without
pre-supposing the existence of linguistic capacities or of coordinated
interactions. The mechanism is based on a low-level model of sensory-motor
interactions. We show that the integration of certain very simple and non
language-specific neural devices leads to the formation of a speech code that
has properties similar to the human speech code. This result relies on the
self-organizing properties of a generic coupling between perception and
production within agents, and on the interactions between agents. The
artificial system helps us to develop better intuitions on how speech might
have appeared, by showing how self-organization might have helped natural
selection to find speech.



We present results from the first geological field tests of the `Cyborg
Astrobiologist', which is a wearable computer and video camcorder system that
we are using to test and train a computer-vision system towards having some of
the autonomous decision-making capabilities of a field-geologist. The Cyborg
Astrobiologist platform has thus far been used for testing and development of
these algorithms and systems: robotic acquisition of quasi-mosaics of images,
real-time image segmentation, and real-time determination of interesting points
in the image mosaics. This work is more of a test of the whole system, rather
than of any one part of the system. However, beyond the concept of the system
itself, the uncommon map (despite its simplicity) is the main innovative part
of the system. The uncommon map helps to determine interest-points in a
context-free manner. Overall, the hardware and software systems function
reliably, and the computer-vision algorithms are adequate for the first field
tests. In addition to the proof-of-concept aspect of these field tests, the
main result of these field tests is the enumeration of those issues that we can
improve in the future, including: dealing with structural shadow and
microtexture, and also, controlling the camera's zoom lens in an intelligent
manner. Nonetheless, despite these and other technical inadequacies, this
Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer
and its computer-vision algorithms, has demonstrated its ability of finding
genuinely interesting points in real-time in the geological scenery, and then
gathering more information about these interest points in an automated manner.
We use these capabilities for autonomous guidance towards geological
points-of-interest.



Wireless sensor networks (WSNs) have attracted considerable attention in
recent years and motivate a host of new challenges for distributed signal
processing. The problem of distributed or decentralized estimation has often
been considered in the context of parametric models. However, the success of
parametric methods is limited by the appropriateness of the strong statistical
assumptions made by the models. In this paper, a more flexible nonparametric
model for distributed regression is considered that is applicable in a variety
of WSN applications including field estimation. Here, starting with the
standard regularized kernel least-squares estimator, a message-passing
algorithm for distributed estimation in WSNs is derived. The algorithm can be
viewed as an instantiation of the successive orthogonal projection (SOP)
algorithm. Various practical aspects of the algorithm are discussed and several
numerical simulations validate the potential of the approach.



We study the properties of the MDL (or maximum penalized complexity)
estimator for Regression and Classification, where the underlying model class
is countable. We show in particular a finite bound on the Hellinger losses
under the only assumption that there is a "true" model contained in the class.
This implies almost sure convergence of the predictive distribution to the true
one at a fast rate. It corresponds to Solomonoff's central theorem of universal
induction, however with a bound that is exponentially larger.



Revision of the paper previously entitled "Learning a Machine for the
Decision in a Partially Observable Markov Universe" In this paper, we are
interested in optimal decisions in a partially observable universe. Our
approach is to directly approximate an optimal strategic tree depending on the
observation. This approximation is made by means of a parameterized
probabilistic law. A particular family of hidden Markov models, with input
\emph{and} output, is considered as a model of policy. A method for optimizing
the parameters of these HMMs is proposed and applied. This optimization is
based on the cross-entropic principle for rare events simulation developed by
Rubinstein.



A class of analog computers built from large numbers of microscopic
probabilistic machines is discussed. It is postulated that such computers are
implemented in biological systems as ensembles of protein molecules. The
formalism is based on an abstract computational model referred to as Protein
Molecule Machine (PMM). A PMM is a continuous-time first-order Markov system
with real input and output vectors, a finite set of discrete states, and the
input-dependent conditional probability densities of state transitions. The
output of a PMM is a function of its input and state. The components of input
vector, called generalized potentials, can be interpreted as membrane
potential, and concentrations of neurotransmitters. The components of output
vector, called generalized currents, can represent ion currents, and the flows
of second messengers. An Ensemble of PMMs (EPMM) is a set of independent
identical PMMs with the same input vector, and the output vector equal to the
sum of output vectors of individual PMMs. The paper suggests that biological
neurons have much more sophisticated computational resources than the presently
popular models of artificial neurons.



We describe a novel approach to statistical learning from particles tracked
while moving in a random environment. The problem consists in inferring
properties of the environment from recorded snapshots. We consider here the
case of a fluid seeded with identical passive particles that diffuse and are
advected by a flow. Our approach rests on efficient algorithms to estimate the
weighted number of possible matchings among particles in two consecutive
snapshots, the partition function of the underlying graphical model. The
partition function is then maximized over the model parameters, namely
diffusivity and velocity gradient. A Belief Propagation (BP) scheme is the
backbone of our algorithm, providing accurate results for the flow parameters
we want to learn. The BP estimate is additionally improved by incorporating
Loop Series (LS) contributions. For the weighted matching problem, LS is
compactly expressed as a Cauchy integral, accurately estimated by a saddle
point approximation. Numerical experiments show that the quality of our
improved BP algorithm is comparable to the one of a fully polynomial randomized
approximation scheme, based on the Markov Chain Monte Carlo (MCMC) method,
while the BP-based scheme is substantially faster than the MCMC scheme.



Enormous successes have been made by quantum algorithms during the last
decade. In this paper, we combine the quantum game with the problem of data
clustering, and then develop a quantum-game-based clustering algorithm, in
which data points in a dataset are considered as players who can make decisions
and implement quantum strategies in quantum games. After each round of a
quantum game, each player's expected payoff is calculated. Later, he uses a
link-removing-and-rewiring (LRR) function to change his neighbors and adjust
the strength of links connecting to them in order to maximize his payoff.
Further, algorithms are discussed and analyzed in two cases of strategies, two
payoff matrixes and two LRR functions. Consequently, the simulation results
have demonstrated that data points in datasets are clustered reasonably and
efficiently, and the clustering algorithms have fast rates of convergence.
Moreover, the comparison with other algorithms also provides an indication of
the effectiveness of the proposed approach.



We present a theoretical analysis of Maximum a Posteriori (MAP) sequence
estimation for binary symmetric hidden Markov processes. We reduce the MAP
estimation to the energy minimization of an appropriately defined Ising spin
model, and focus on the performance of MAP as characterized by its accuracy and
the number of solutions corresponding to a typical observed sequence. It is
shown that for a finite range of sufficiently low noise levels, the solution is
uniquely related to the observed sequence, while the accuracy degrades linearly
with increasing the noise strength. For intermediate noise values, the accuracy
is nearly noise-independent, but now there are exponentially many solutions to
the estimation problem, which is reflected in non-zero ground-state entropy for
the Ising model. Finally, for even larger noise intensities, the number of
solutions reduces again, but the accuracy is poor. It is shown that these
regimes are different thermodynamic phases of the Ising model that are related
to each other via first-order phase transitions.



The special theme of DCM 2009, co-located with ICALP 2009, concerned
Computational Models From Nature, with a particular emphasis on computational
models derived from physics and biology. The intention was to bring together
different approaches - in a community with a strong foundational background as
proffered by the ICALP attendees - to create inspirational cross-boundary
exchanges, and to lead to innovative further research. Specifically DCM 2009
sought contributions in quantum computation and information, probabilistic
models, chemical, biological and bio-inspired ones, including spatial models,
growth models and models of self-assembly. Contributions putting to the test
logical or algorithmic aspects of computing (e.g., continuous computing with
dynamical systems, or solid state computing models) were also very much
welcomed.



We present a general probabilistic perspective on Gaussian filtering and
smoothing. This allows us to show that common approaches to Gaussian
filtering/smoothing can be distinguished solely by their methods of
computing/approximating the means and covariances of joint probabilities. This
implies that novel filters and smoothers can be derived straightforwardly by
providing methods for computing these moments. Based on this insight, we derive
the cubature Kalman smoother and propose a novel robust filtering and smoothing
algorithm based on Gibbs sampling.



We present a simple and clear foundation for finite inference that unites and
significantly extends the approaches of Kolmogorov and Cox. Our approach is
based on quantifying lattices of logical statements in a way that satisfies
general lattice symmetries. With other applications such as measure theory in
mind, our derivations assume minimal symmetries, relying on neither negation
nor continuity nor differentiability. Each relevant symmetry corresponds to an
axiom of quantification, and these axioms are used to derive a unique set of
quantifying rules that form the familiar probability calculus. We also derive a
unique quantification of divergence, entropy and information.



We consider a fuzzy linear system with crisp coefficient matrix and with an
arbitrary fuzzy number in parametric form on the right-hand side. It is known
that the well-known existence and uniqueness theorem of a strong fuzzy solution
is equivalent to the following: The coefficient matrix is the product of a
permutation matrix and a diagonal matrix. This means that this theorem can be
applicable only for a special form of linear systems, namely, only when the
system consists of equations, each of which has exactly one variable. We prove
an existence and uniqueness theorem, which can be use on more general systems.
The necessary and sufficient conditions of the theorem are dependent on both
the coefficient matrix and the right-hand side. This theorem is a
generalization of the well-known existence and uniqueness theorem for the
strong solution.



In the paper, combinatorial synthesis of structure for applied Web-based
systems is described. The problem is considered as a combination of selected
design alternatives for system parts/components into a resultant composite
decision (i.e., system configuration design). The solving framework is based on
Hierarchical Morphological Multicriteria Design (HMMD) approach: (i)
multicriteria selection of alternatives for system parts, (ii) composing the
selected alternatives into a resultant combination (while taking into account
ordinal quality of the alternatives above and their compatibility). A
lattice-based discrete space is used to evaluate (to integrate) quality of the
resultant combinations (i.e., composite system decisions or system
configurations). In addition, a simplified solving framework based on
multicriteria multiple choice problem is considered. A multistage design
process to obtain a system trajectory is described as well. The basic applied
example is targeted to an applied Web-based system for a communication service
provider. Two other applications are briefly described (corporate system and
information system for academic application).



In this thesis I develop a variety of techniques to train, evaluate, and
sample from intractable and high dimensional probabilistic models. Abstract
exceeds arXiv space limitations -- see PDF.



I am most honoured to have the privilege to present the Foreword to this
fascinating and wonderfully varied collection of contributions, concerning the
nature of computation and of its deep connection with the operation of those
basic laws, known or yet unknown, governing the universe in which we live.
Fundamentally deep questions are indeed being grappled with here, and the fact
that we find so many different viewpoints is something to be expected, since,
in truth, we know little about the foundational nature and origins of these
basic laws, despite the immense precision that we so often find revealed in
them. Accordingly, it is not surprising that within the viewpoints expressed
here is some unabashed speculation, occasionally bordering on just partially
justified guesswork, while elsewhere we find a good deal of precise reasoning,
some in the form of rigorous mathematical theorems. Both of these are as should
be, for without some inspired guesswork we cannot have new ideas as to where
look in order to make genuinely new progress, and without precise mathematical
reasoning, no less than in precise observation, we cannot know when we are
right -- or, more usually, when we are wrong.



Investigation of the underlying physics or biology from empirical data
requires a quantifiable notion of similarity - when do two observed data sets
indicate nearly identical generating processes, and when they do not. The
discriminating characteristics to look for in data is often determined by
heuristics designed by experts, $e.g.$, distinct shapes of "folded" lightcurves
may be used as "features" to classify variable stars, while determination of
pathological brain states might require a Fourier analysis of brainwave
activity. Finding good features is non-trivial. Here, we propose a universal
solution to this problem: we delineate a principle for quantifying similarity
between sources of arbitrary data streams, without a priori knowledge, features
or training. We uncover an algebraic structure on a space of symbolic models
for quantized data, and show that such stochastic generators may be added and
uniquely inverted; and that a model and its inverse always sum to the generator
of flat white noise. Therefore, every data stream has an anti-stream: data
generated by the inverse model. Similarity between two streams, then, is the
degree to which one, when summed to the other's anti-stream, mutually
annihilates all statistical structure to noise. We call this data smashing. We
present diverse applications, including disambiguation of brainwaves pertaining
to epileptic seizures, detection of anomalous cardiac rhythms, and
classification of astronomical objects from raw photometry. In our examples,
the data smashing principle, without access to any domain knowledge, meets or
exceeds the performance of specialized algorithms tuned by domain experts.



We show that the Survey Propagation-guided decimation algorithm fails to find
satisfying assignments on random instances of the "Not-All-Equal-$K$-SAT"
problem if the number of message passing iterations is bounded by a constant
independent of the size of the instance and the clause-to-variable ratio is
above $(1+o_K(1)){2^{K-1}\over K}\log^2 K$ for sufficiently large $K$. Our
analysis in fact applies to a broad class of algorithms described as
"sequential local algorithms". Such algorithms iteratively set variables based
on some local information and then recurse on the reduced instance. Survey
Propagation-guided as well as Belief Propagation-guided decimation algorithms -
two widely studied message passing based algorithms, fall under this category
of algorithms provided the number of message passing iterations is bounded by a
constant. Another well-known algorithm falling into this category is the Unit
Clause algorithm. Our work constitutes the first rigorous analysis of the
performance of the SP-guided decimation algorithm.
  The approach underlying our paper is based on an intricate geometry of the
solution space of random NAE-$K$-SAT problem. We show that above the
$(1+o_K(1)){2^{K-1}\over K}\log^2 K$ threshold, the overlap structure of
$m$-tuples of satisfying assignments exhibit a certain clustering behavior
expressed in the form of constraints on distances between the $m$ assignments,
for appropriately chosen $m$. We further show that if a sequential local
algorithm succeeds in finding a satisfying assignment with probability bounded
away from zero, then one can construct an $m$-tuple of solutions violating
these constraints, thus leading to a contradiction. Along with (citation), this
result is the first work which directly links the clustering property of random
constraint satisfaction problems to the computational hardness of finding
satisfying assignments.



"How to generate a sentence" is the most critical and difficult problem in
all the natural language processing technologies. In this paper, we present a
new approach to explain the generation process of a sentence from the
perspective of mathematics. Our method is based on the premise that in our
brain a sentence is a part of a word network which is formed by many word
nodes. Experiments show that the probability of the entire sentence can be
obtained by the probabilities of single words and the probabilities of the
co-occurrence of word pairs, which indicate that human use the synthesis method
to generate a sentence.



The problem is sequence prediction in the following setting. A sequence
$x_1,...,x_n,...$ of discrete-valued observations is generated according to
some unknown probabilistic law (measure) $\mu$. After observing each outcome,
it is required to give the conditional probabilities of the next observation.
The measure $\mu$ belongs to an arbitrary but known class $C$ of stochastic
process measures. We are interested in predictors $\rho$ whose conditional
probabilities converge (in some sense) to the "true" $\mu$-conditional
probabilities if any $\mu\in C$ is chosen to generate the sequence. The
contribution of this work is in characterizing the families $C$ for which such
predictors exist, and in providing a specific and simple form in which to look
for a solution. We show that if any predictor works, then there exists a
Bayesian predictor, whose prior is discrete, and which works too. We also find
several sufficient and necessary conditions for the existence of a predictor,
in terms of topological characterizations of the family $C$, as well as in
terms of local behaviour of the measures in $C$, which in some cases lead to
procedures for constructing such predictors. It should be emphasized that the
framework is completely general: the stochastic processes considered are not
required to be i.i.d., stationary, or to belong to any parametric or countable
family.



Nonparametric two sample testing deals with the question of consistently
deciding if two distributions are different, given samples from both, without
making any parametric assumptions about the form of the distributions. The
current literature is split into two kinds of tests - those which are
consistent without any assumptions about how the distributions may differ
(\textit{general} alternatives), and those which are designed to specifically
test easier alternatives, like a difference in means (\textit{mean-shift}
alternatives).
  The main contribution of this paper is to explicitly characterize the power
of a popular nonparametric two sample test, designed for general alternatives,
under a mean-shift alternative in the high-dimensional setting. Specifically,
we explicitly derive the power of the linear-time Maximum Mean Discrepancy
statistic using the Gaussian kernel, where the dimension and sample size can
both tend to infinity at any rate, and the two distributions differ in their
means. As a corollary, we find that if the signal-to-noise ratio is held
constant, then the test's power goes to one if the number of samples increases
faster than the dimension increases. This is the first explicit power
derivation for a general nonparametric test in the high-dimensional setting,
and also the first analysis of how tests designed for general alternatives
perform when faced with easier ones.



Dropout is a simple but effective technique for learning in neural networks
and other settings. A sound theoretical understanding of dropout is needed to
determine when dropout should be applied and how to use it most effectively. In
this paper we continue the exploration of dropout as a regularizer pioneered by
Wager, et.al. We focus on linear classification where a convex proxy to the
misclassification loss (i.e. the logistic loss used in logistic regression) is
minimized. We show: (a) when the dropout-regularized criterion has a unique
minimizer, (b) when the dropout-regularization penalty goes to infinity with
the weights, and when it remains bounded, (c) that the dropout regularization
can be non-monotonic as individual weights increase from 0, and (d) that the
dropout regularization penalty may not be convex. This last point is
particularly surprising because the combination of dropout regularization with
any convex loss proxy is always a convex function.
  In order to contrast dropout regularization with $L_2$ regularization, we
formalize the notion of when different sources are more compatible with
different regularizers. We then exhibit distributions that are provably more
compatible with dropout regularization than $L_2$ regularization, and vice
versa. These sources provide additional insight into how the inductive biases
of dropout and $L_2$ regularization differ. We provide some similar results for
$L_1$ regularization.



We recount recent history behind building compact models of nonlinear,
complex processes and identifying their relevant macroscopic patterns or
"macrostates". We give a synopsis of computational mechanics, predictive
rate-distortion theory, and the role of information measures in monitoring
model complexity and predictive performance. Computational mechanics provides a
method to extract the optimal minimal predictive model for a given process.
Rate-distortion theory provides methods for systematically approximating such
models. We end by commenting on future prospects for developing a general
framework that automatically discovers optimal compact models. As a response to
the manuscript cited in the title above, this brief commentary corrects
potentially misleading claims about its state space compression method and
places it in a broader historical setting.



Principal components analysis (PCA) is the optimal linear auto-encoder of
data, and it is often used to construct features. Enforcing sparsity on the
principal components can promote better generalization, while improving the
interpretability of the features. We study the problem of constructing optimal
sparse linear auto-encoders. Two natural questions in such a setting are: i)
Given a level of sparsity, what is the best approximation to PCA that can be
achieved? ii) Are there low-order polynomial-time algorithms which can
asymptotically achieve this optimal tradeoff between the sparsity and the
approximation quality?
  In this work, we answer both questions by giving efficient low-order
polynomial-time algorithms for constructing asymptotically \emph{optimal}
linear auto-encoders (in particular, sparse features with near-PCA
reconstruction error) and demonstrate the performance of our algorithms on real
data.



To analyze high-dimensional systems, many fields in science and engineering
rely on high-level descriptions, sometimes called "macrostates,"
"coarse-grainings," or "effective theories". Examples of such descriptions
include the thermodynamic properties of a large collection of point particles
undergoing reversible dynamics, the variables in a macroeconomic model
describing the individuals that participate in an economy, and the summary
state of a cell composed of a large set of biochemical networks.
  Often these high-level descriptions are constructed without considering the
ultimate reason for needing them in the first place. Here, we formalize and
quantify one such purpose: the need to predict observables of interest
concerning the high-dimensional system with as high accuracy as possible, while
minimizing the computational cost of doing so. The resulting State Space
Compression (SSC) framework provides a guide for how to solve for the {optimal}
high-level description of a given dynamical system, rather than constructing it
based on human intuition alone.
  In this preliminary report, we introduce SSC, and illustrate it with several
information-theoretic quantifications of "accuracy", all with different
implications for the optimal compression. We also discuss some other possible
applications of SSC beyond the goal of accurate prediction. These include SSC
as a measure of the complexity of a dynamical system, and as a way to quantify
information flow between the scales of a system.



Understanding efficiency in high dimensional linear models is a longstanding
problem of interest. Classical work with smaller dimensional problems dating
back to Huber and Bickel has illustrated the benefits of efficient loss
functions. When the number of parameters $p$ is of the same order as the sample
size $n$, $p \approx n$, an efficiency pattern different from the one of Huber
was recently established. In this work, we consider the effects of model
selection on the estimation efficiency of penalized methods. In particular, we
explore whether sparsity, results in new efficiency patterns when $p > n$. In
the interest of deriving the asymptotic mean squared error for regularized
M-estimators, we use the powerful framework of approximate message passing. We
propose a novel, robust and sparse approximate message passing algorithm
(RAMP), that is adaptive to the error distribution. Our algorithm includes many
non-quadratic and non-differentiable loss functions. We derive its asymptotic
mean squared error and show its convergence, while allowing $p, n, s \to
\infty$, with $n/p \in (0,1)$ and $n/s \in (1,\infty)$. We identify new
patterns of relative efficiency regarding a number of penalized $M$ estimators,
when $p$ is much larger than $n$. We show that the classical information bound
is no longer reachable, even for light--tailed error distributions. We show
that the penalized least absolute deviation estimator dominates the penalized
least square estimator, in cases of heavy--tailed distributions. We observe
this pattern for all choices of the number of non-zero parameters $s$, both $s
\leq n$ and $s \approx n$. In non-penalized problems where $s =p \approx n$,
the opposite regime holds. Therefore, we discover that the presence of model
selection significantly changes the efficiency patterns.



Nonparametric two sample testing is a decision theoretic problem that
involves identifying differences between two random variables without making
parametric assumptions about their underlying distributions. We refer to the
most common settings as mean difference alternatives (MDA), for testing
differences only in first moments, and general difference alternatives (GDA),
which is about testing for any difference in distributions. A large number of
test statistics have been proposed for both these settings. This paper connects
three classes of statistics - high dimensional variants of Hotelling's t-test,
statistics based on Reproducing Kernel Hilbert Spaces, and energy statistics
based on pairwise distances. We ask the question: how much statistical power do
popular kernel and distance based tests for GDA have when the unknown
distributions differ in their means, compared to specialized tests for MDA?
  We formally characterize the power of popular tests for GDA like the Maximum
Mean Discrepancy with the Gaussian kernel (gMMD) and bandwidth-dependent
variants of the Energy Distance with the Euclidean norm (eED) in the
high-dimensional MDA regime. Some practically important properties include (a)
eED and gMMD have asymptotically equal power; furthermore they enjoy a free
lunch because, while they are additionally consistent for GDA, they also have
the same power as specialized high-dimensional t-test variants for MDA. All
these tests are asymptotically optimal (including matching constants) under MDA
for spherical covariances, according to simple lower bounds, (b) The power of
gMMD is independent of the kernel bandwidth, as long as it is larger than the
choice made by the median heuristic, (c) There is a clear and smooth
computation-statistics tradeoff for linear-time, subquadratic-time and
quadratic-time versions of these tests, with more computation resulting in
higher power.



A factor-graph representation of quantum-mechanical probabilities (involving
any number of measurements) is proposed. Unlike standard statistical models,
the proposed representation uses auxiliary variables (state variables) that are
not random variables. All joint probability distributions are marginals of some
complex-valued function $q$, and it is demonstrated how the basic concepts of
quantum mechanics relate to factorizations and marginals of $q$.



This paper examines the role and efficiency of the non-convex loss functions
for binary classification problems. In particular, we investigate how to design
a simple and effective boosting algorithm that is robust to the outliers in the
data. The analysis of the role of a particular non-convex loss for prediction
accuracy varies depending on the diminishing tail properties of the gradient of
the loss -- the ability of the loss to efficiently adapt to the outlying data,
the local convex properties of the loss and the proportion of the contaminated
data. In order to use these properties efficiently, we propose a new family of
non-convex losses named $\gamma$-robust losses. Moreover, we present a new
boosting framework, {\it Arch Boost}, designed for augmenting the existing work
such that its corresponding classification algorithm is significantly more
adaptable to the unknown data contamination. Along with the Arch Boosting
framework, the non-convex losses lead to the new class of boosting algorithms,
named adaptive, robust, boosting (ARB). Furthermore, we present theoretical
examples that demonstrate the robustness properties of the proposed algorithms.
In particular, we develop a new breakdown point analysis and a new influence
function analysis that demonstrate gains in robustness. Moreover, we present
new theoretical results, based only on local curvatures, which may be used to
establish statistical and optimization properties of the proposed Arch boosting
algorithms with highly non-convex loss functions. Extensive numerical
calculations are used to illustrate these theoretical properties and reveal
advantages over the existing boosting methods when data exhibits a number of
outliers.



The human visual system can spot an abnormal image, and reason about what
makes it strange. This task has not received enough attention in computer
vision. In this paper we study various types of atypicalities in images in a
more comprehensive way than has been done before. We propose a new dataset of
abnormal images showing a wide range of atypicalities. We design human subject
experiments to discover a coarse taxonomy of the reasons for abnormality. Our
experiments reveal three major categories of abnormality: object-centric,
scene-centric, and contextual. Based on this taxonomy, we propose a
comprehensive computational model that can predict all different types of
abnormality in images and outperform prior arts in abnormality recognition.



We advance the state of the art in biomolecular interaction extraction with
three contributions: (i) We show that deep, Abstract Meaning Representations
(AMR) significantly improve the accuracy of a biomolecular interaction
extraction system when compared to a baseline that relies solely on surface-
and syntax-based features; (ii) In contrast with previous approaches that infer
relations on a sentence-by-sentence basis, we expand our framework to enable
consistent predictions over sets of sentences (documents); (iii) We further
modify and expand a graph kernel learning framework to enable concurrent
exploitation of automatically induced AMR (semantic) and dependency structure
(syntactic) representations. Our experiments show that our approach yields
interaction extraction systems that are more robust in environments where there
is a significant mismatch between training and test conditions.



Deep convolutional neural networks have led to breakthrough results in
numerous practical machine learning tasks such as classification of images in
the ImageNet data set, control-policy-learning to play Atari games or the board
game Go, and image captioning. Many of these applications first perform feature
extraction and then feed the results thereof into a trainable classifier. The
mathematical analysis of deep convolutional neural networks for feature
extraction was initiated by Mallat, 2012. Specifically, Mallat considered
so-called scattering networks based on a wavelet transform followed by the
modulus non-linearity in each network layer, and proved translation invariance
(asymptotically in the wavelet scale parameter) and deformation stability of
the corresponding feature extractor. This paper complements Mallat's results by
developing a theory that encompasses general convolutional transforms, or in
more technical parlance, general semi-discrete frames (including
Weyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned
filters), general Lipschitz-continuous non-linearities (e.g., rectified linear
units, shifted logistic sigmoids, hyperbolic tangents, and modulus functions),
and general Lipschitz-continuous pooling operators emulating, e.g.,
sub-sampling and averaging. In addition, all of these elements can be different
in different network layers. For the resulting feature extractor we prove a
translation invariance result of vertical nature in the sense of the features
becoming progressively more translation-invariant with increasing network
depth, and we establish deformation sensitivity bounds that apply to signal
classes such as, e.g., band-limited functions, cartoon functions, and Lipschitz
functions.



We study how to communicate findings of Bayesian inference to third parties,
while preserving the strong guarantee of differential privacy. Our main
contributions are four different algorithms for private Bayesian inference on
proba-bilistic graphical models. These include two mechanisms for adding noise
to the Bayesian updates, either directly to the posterior parameters, or to
their Fourier transform so as to preserve update consistency. We also utilise a
recently introduced posterior sampling mechanism, for which we prove bounds for
the specific but general case of discrete Bayesian networks; and we introduce a
maximum-a-posteriori private mechanism. Our analysis includes utility and
privacy bounds, with a novel focus on the influence of graph structure on
privacy. Worked examples and experiments with Bayesian na{\"i}ve Bayes and
Bayesian linear regression illustrate the application of our mechanisms.



We analyze dropout in deep networks with rectified linear units and the
quadratic loss. Our results expose surprising differences between the behavior
of dropout and more traditional regularizers like weight decay. For example, on
some simple data sets dropout training produces negative weights even though
the output is the sum of the inputs. This provides a counterpoint to the
suggestion that dropout discourages co-adaptation of weights. We also show that
the dropout penalty can grow exponentially in the depth of the network while
the weight-decay penalty remains essentially linear, and that dropout is
insensitive to various re-scalings of the input features, outputs, and network
weights. This last insensitivity implies that there are no isolated local
minima of the dropout training criterion. Our work uncovers new properties of
dropout, extends our understanding of why dropout succeeds, and lays the
foundation for further progress.



When encountering novel objects, humans are able to infer a wide range of
physical properties such as mass, friction and deformability by interacting
with them in a goal driven way. This process of active interaction is in the
same spirit as a scientist performing experiments to discover hidden facts.
Recent advances in artificial intelligence have yielded machines that can
achieve superhuman performance in Go, Atari, natural language processing, and
complex control problems; however, it is not clear that these systems can rival
the scientific intuition of even a young child. In this work we introduce a
basic set of tasks that require agents to estimate properties such as mass and
cohesion of objects in an interactive simulated environment where they can
manipulate the objects and observe the consequences. We found that state of art
deep reinforcement learning methods can learn to perform the experiments
necessary to discover such hidden properties. By systematically manipulating
the problem difficulty and the cost incurred by the agent for performing
experiments, we found that agents learn different strategies that balance the
cost of gathering information against the cost of making mistakes in different
situations.



A framework is presented for unsupervised learning of representations based
on infomax principle for large-scale neural populations. We use an asymptotic
approximation to the Shannon's mutual information for a large neural population
to demonstrate that a good initial approximation to the global
information-theoretic optimum can be obtained by a hierarchical infomax method.
Starting from the initial solution, an efficient algorithm based on gradient
descent of the final objective function is proposed to learn representations
from the input datasets, and the method works for complete, overcomplete, and
undercomplete bases. As confirmed by numerical experiments, our method is
robust and highly efficient for extracting salient features from input
datasets. Compared with the main existing methods, our algorithm has a distinct
advantage in both the training speed and the robustness of unsupervised
representation learning. Furthermore, the proposed method is easily extended to
the supervised or unsupervised model for training deep structure networks.



The vast majority of theoretical results in machine learning and statistics
assume that the available training data is a reasonably reliable reflection of
the phenomena to be learned or estimated. Similarly, the majority of machine
learning and statistical techniques used in practice are brittle to the
presence of large amounts of biased or malicious data. In this work we consider
two frameworks in which to study estimation, learning, and optimization in the
presence of significant fractions of arbitrary data.
  The first framework, list-decodable learning, asks whether it is possible to
return a list of answers, with the guarantee that at least one of them is
accurate. For example, given a dataset of $n$ points for which an unknown
subset of $\alpha n$ points are drawn from a distribution of interest, and no
assumptions are made about the remaining $(1-\alpha)n$ points, is it possible
to return a list of $\operatorname{poly}(1/\alpha)$ answers, one of which is
correct? The second framework, which we term the semi-verified learning model,
considers the extent to which a small dataset of trusted data (drawn from the
distribution in question) can be leveraged to enable the accurate extraction of
information from a much larger but untrusted dataset (of which only an
$\alpha$-fraction is drawn from the distribution).
  We show strong positive results in both settings, and provide an algorithm
for robust learning in a very general stochastic optimization setting. This
general result has immediate implications for robust estimation in a number of
settings, including for robustly estimating the mean of distributions with
bounded second moments, robustly learning mixtures of such distributions, and
robustly finding planted partitions in random graphs in which significant
portions of the graph have been perturbed by an adversary.



We present a novel method for frequentist statistical inference in
$M$-estimation problems, based on stochastic gradient descent (SGD) with a
fixed step size: we demonstrate that the average of such SGD sequences can be
used for statistical inference, after proper scaling. An intuitive analysis
using the Ornstein-Uhlenbeck process suggests that such averages are
asymptotically normal. From a practical perspective, our SGD-based inference
procedure is a first order method, and is well-suited for large scale problems.
To show its merits, we apply it to both synthetic and real datasets, and
demonstrate that its accuracy is comparable to classical statistical methods,
while requiring potentially far less computation.



Learning cooperative policies for multi-agent systems is often challenged by
partial observability and a lack of coordination. In some settings, the
structure of a problem allows a distributed solution with limited
communication. Here, we consider a scenario where no communication is
available, and instead we learn local policies for all agents that collectively
mimic the solution to a centralized multi-agent static optimization problem.
Our main contribution is an information theoretic framework based on rate
distortion theory which facilitates analysis of how well the resulting fully
decentralized policies are able to reconstruct the optimal solution. Moreover,
this framework provides a natural extension that addresses which nodes an agent
should communicate with to improve the performance of its individual policy.



We consider generalized linear models (GLMs) where an unknown $n$-dimensional
signal vector is observed through the application of a random matrix and a
non-linear (possibly probabilistic) componentwise output function. We consider
the models in the high-dimensional limit, where the observation consists of m
points, and $m/n\to\alpha$ where $\alpha$ stays finite in the limit
$m,n\to\infty$. This situation is ubiquitous in applications ranging from
supervised machine learning to signal processing. A substantial amount of
theoretical work analyzed the model-case when the observation matrix has i.i.d.
elements and the components of the ground-truth signal are taken independently
from some known distribution. While statistical physics provided number of
explicit conjectures for special cases of this model, results existing for
non-linear output functions were so far non-rigorous. At the same time GLMs
with non-linear output functions are used as a basic building block of powerful
multilayer feedforward neural networks. Therefore rigorously establishing the
formulas conjectured for the mutual information is a key open problem that we
solve in this paper. We also provide an explicit asymptotic formula for the
optimal generalization error, and confirm the prediction of phase transitions
in GLMs. Analyzing the resulting formulas for several non-linear output
functions, including the rectified linear unit or modulus functions, we obtain
quantitative descriptions of information-theoretic limitations of
high-dimensional inference. Our proof technique relies on a new version of the
interpolation method with an adaptive interpolation path and is of independent
interest. Furthermore we show that a polynomial-time algorithm referred to as
generalized approximate message-passing reaches the optimal generalization
error for a large set of parameters.



Preserving the utility of published datasets while simultaneously providing
provable privacy guarantees is a well-known challenge. On the one hand,
context-free privacy solutions, such as differential privacy, provide strong
privacy guarantees, but often lead to a significant reduction in utility. On
the other hand, context-aware privacy solutions, such as information theoretic
privacy, achieve an improved privacy-utility tradeoff, but assume that the data
holder has access to dataset statistics. We circumvent these limitations by
introducing a novel context-aware privacy framework called generative
adversarial privacy (GAP). GAP leverages recent advancements in generative
adversarial networks (GANs) to allow the data holder to learn privatization
schemes from the dataset itself. Under GAP, learning the privacy mechanism is
formulated as a constrained minimax game between two players: a privatizer that
sanitizes the dataset in a way that limits the risk of inference attacks on the
individuals' private variables, and an adversary that tries to infer the
private variables from the sanitized dataset. To evaluate GAP's performance, we
investigate two simple (yet canonical) statistical dataset models: (a) the
binary data model, and (b) the binary Gaussian mixture model. For both models,
we derive game-theoretically optimal minimax privacy mechanisms, and show that
the privacy mechanisms learned from data (in a generative adversarial fashion)
match the theoretically optimal ones. This demonstrates that our framework can
be easily applied in practice, even in the absence of dataset statistics.



We propose a novel approach for the generation of polyphonic music based on
LSTMs. We generate music in two steps. First, a chord LSTM predicts a chord
progression based on a chord embedding. A second LSTM then generates polyphonic
music from the predicted chord progression. The generated music sounds pleasing
and harmonic, with only few dissonant notes. It has clear long-term structure
that is similar to what a musician would play during a jam session. We show
that our approach is sensible from a music theory perspective by evaluating the
learned chord embeddings. Surprisingly, our simple model managed to extract the
circle of fifths, an important tool in music theory, from the dataset.



We introduce HoME: a Household Multimodal Environment for artificial agents
to learn from vision, audio, semantics, physics, and interaction with objects
and other agents, all within a realistic context. HoME integrates over 45,000
diverse 3D house layouts based on the SUNCG dataset, a scale which may
facilitate learning, generalization, and transfer. HoME is an open-source,
OpenAI Gym-compatible platform extensible to tasks in reinforcement learning,
language grounding, sound-based navigation, robotics, multi-agent learning, and
more. We hope HoME better enables artificial agents to learn as humans do: in
an interactive, multimodal, and richly contextualized setting.



The relationship between the Bayesian approach and the minimum description
length approach is established. We sharpen and clarify the general modeling
principles MDL and MML, abstracted as the ideal MDL principle and defined from
Bayes's rule by means of Kolmogorov complexity. The basic condition under which
the ideal principle should be applied is encapsulated as the Fundamental
Inequality, which in broad terms states that the principle is valid when the
data are random, relative to every contemplated hypothesis and also these
hypotheses are random relative to the (universal) prior. Basically, the ideal
principle states that the prior probability associated with the hypothesis
should be given by the algorithmic universal probability, and the sum of the
log universal probability of the model plus the log of the probability of the
data given the model should be minimized. If we restrict the model class to the
finite sets then application of the ideal principle turns into Kolmogorov's
minimal sufficient statistic. In general we show that data compression is
almost always the best strategy, both in hypothesis identification and
prediction.



We present results from the first geological field tests of the `Cyborg
Astrobiologist', which is a wearable computer and video camcorder system that
we are using to test and train a computer-vision system towards having some of
the autonomous decision-making capabilities of a field-geologist and
field-astrobiologist. The Cyborg Astrobiologist platform has thus far been used
for testing and development of these algorithms and systems: robotic
acquisition of quasi-mosaics of images, real-time image segmentation, and
real-time determination of interesting points in the image mosaics. The
hardware and software systems function reliably, and the computer-vision
algorithms are adequate for the first field tests. In addition to the
proof-of-concept aspect of these field tests, the main result of these field
tests is the enumeration of those issues that we can improve in the future,
including: first, detection and accounting for shadows caused by 3D jagged
edges in the outcrop; second, reincorporation of more sophisticated
texture-analysis algorithms into the system; third, creation of hardware and
software capabilities to control the camera's zoom lens in an intelligent
manner; and fourth, development of algorithms for interpretation of complex
geological scenery. Nonetheless, despite these technical inadequacies, this
Cyborg Astrobiologist system, consisting of a camera-equipped wearable-computer
and its computer-vision algorithms, has demonstrated its ability of finding
genuinely interesting points in real-time in the geological scenery, and then
gathering more information about these interest points in an automated manner.



The `Cyborg Astrobiologist' (CA) has undergone a second geological field
trial, at a red sandstone site in northern Guadalajara, Spain, near Riba de
Santiuste. The Cyborg Astrobiologist is a wearable computer and video camera
system that has demonstrated a capability to find uncommon interest points in
geological imagery in real-time in the field. The first (of three) geological
structures that we studied was an outcrop of nearly homogeneous sandstone,
which exhibits oxidized-iron impurities in red and and an absence of these iron
impurities in white. The white areas in these ``red beds'' have turned white
because the iron has been removed by chemical reduction, perhaps by a
biological agent. The computer vision system found in one instance several
(iron-free) white spots to be uncommon and therefore interesting, as well as
several small and dark nodules. The second geological structure contained
white, textured mineral deposits on the surface of the sandstone, which were
found by the CA to be interesting. The third geological structure was a 50 cm
thick paleosol layer, with fossilized root structures of some plants, which
were found by the CA to be interesting. A quasi-blind comparison of the Cyborg
Astrobiologist's interest points for these images with the interest points
determined afterwards by a human geologist shows that the Cyborg Astrobiologist
concurred with the human geologist 68% of the time (true positive rate), with a
32% false positive rate and a 32% false negative rate.
  (abstract has been abridged).



The probability distribution P from which the history of our universe is
sampled represents a theory of everything or TOE. We assume P is formally
describable. Since most (uncountably many) distributions are not, this imposes
a strong inductive bias. We show that P(x) is small for any universe x lacking
a short description, and study the spectrum of TOEs spanned by two Ps, one
reflecting the most compact constructive descriptions, the other the fastest
way of computing everything. The former derives from generalizations of
traditional computability, Solomonoff's algorithmic probability, Kolmogorov
complexity, and objects more random than Chaitin's Omega, the latter from
Levin's universal search and a natural resource-oriented postulate: the
cumulative prior probability of all x incomputable within time t by this
optimal algorithm should be 1/t. Between both Ps we find a universal
cumulatively enumerable measure that dominates traditional enumerable measures;
any such CEM must assign low probability to any universe lacking a short
enumerating program. We derive P-specific consequences for evolving observers,
inductive reasoning, quantum physics, philosophy, and the expected duration of
our universe.



We have used a simple camera phone to significantly improve an `exploration
system' for astrobiology and geology. This camera phone will make it much
easier to develop and test computer-vision algorithms for future planetary
exploration. We envision that the `Astrobiology Phone-cam' exploration system
can be fruitfully used in other problem domains as well.



Identifying and understanding modular organizations is centrally important in
the study of complex systems. Several approaches to this problem have been
advanced, many framed in information-theoretic terms. Our treatment starts from
the complementary point of view of statistical modeling and prediction of
dynamical systems. It is known that for finite amounts of training data,
simpler models can have greater predictive power than more complex ones. We use
the trade-off between model simplicity and predictive accuracy to generate
optimal multiscale decompositions of dynamical networks into weakly-coupled,
simple modules. State-dependent and causal versions of our method are also
proposed.

